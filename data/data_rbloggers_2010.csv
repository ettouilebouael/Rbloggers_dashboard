title	link	date	author	text	comment_rbloggers
Some Python Nooks and Crannies	https://www.r-bloggers.com/2010/01/some-python-nooks-and-crannies/	January 31, 2010	Ryan	"I spent this weekend reading Learning Python (Second Edition for Python 2.3!) by Mark Lutz. Python is my favorite programming language, but my experience with it has been mostly anecdotal; I come up with my own solutions and functions and I Google whatever I do not know. I decided to spend a couple of days with this incredibly out-of-date book to formalize my knowledge of the base Python language. It was fairly easy reading because I already had experience with about 80% of the constructs discussed. But it was fun to learn some things that I have not used, and some things that I did not even know existed. I want to share some of these gems here. Pardon me if all of this stuff is obvious to you  . Populating a String with a Dictionary


>> data = {}

>> data['first_name'] = ""Ryan""

>> data['age'] = 21 #some programming humor

>> print ""Hello, my name is %(first_name)s and I am %(age)d years old."" % data

Hello, my name is Ryan and I am 21 years old.
Notice that we can also put a function after the last % sign above, as long as the function returns a dictionary.
The s and the d after the dictionary keys are the usual format specifiers (s for string and d for a number). This gem is important to me because I use dictionaries containing tons of data and need to reformat it! This is what I used to have to do:


>> print ""Hello, my name is %s and I am %d years old."" % (data['first_name'], data['age']) Some More on Printing to Streams By default, separating variables by a comma will produce a space between the outputted variables


>> x = 3

>> y = 2

>> print x, y

3 2 Usually print automatically inserts a newline at the end of the output. We can suppress it by using a dangling comma. This is more useful when printing to another stream such as a file.


>>> x= 2

>>> y = 3

>>> out = open(""temp.txt"", ""w"")

>>> print >> out, x,

>>> print >> out, y,

>>> out.close()
The file then contains the line


2 3

Do it, Or Else…or Not A for or while loop can have an else, to perform actions when control leaves the loop without encountering a break. Personally, I think done, when-complete or something similar would have been better than else. Consider the example of searching a list for a value.


>>> names = [""Sarah"", ""Nick"", ""Sam"", ""Chloe""]

>>> for name in names:

...     if name == ""Ryan"":

...             break

... else:

...     print ""Not found!""

...

Not found!
Global only Matters for Assignment

If we define a variable outside any class or a function it is global. We can access the variable in any enclosing functions, but we cannot modify it. This is new to me. What I used to do was this:


>>> x = 3

>>> def f():

...     global x

...     print x

...

>>> f()

3
when all I really need is: 

>>> x = 3

>>> def f():

...     print x

...

>>> f()

3
However, we do need the statement global x if we modify the variable. Class Properties Class properties simplify the creation of getters and setters…sort of. Of course, if we define a class attribute outside of a class method, we can access it without a getter or setter:


>>> class MyClass:

...     myvar = 2

...

>>> a = MyClass()

>>> a.myvar

2

>>> a.myvar = 3

>>> a.myvar

3 But if we want to be more careful, we do not define the variable in such a way. Instead, we can define it in a class method then write our getter and setter methods. By creating a property, we essentially overload the = operator and allow access to the variable as if it were defined as above. Using the property constructor, we tell what methods to use as the getter and setter. In other words, we do not need to use the getter and setter methods. After typing all of that, this seems trivial… 

>>> class MyClass:

...     def __init__(self):

...             self.myvar = 2

...     def getmyvar(self):

...             return self.myvar

...     def setmyvar(self, val):

...             self.myvar = val

...     myvar = property(getmyvar, setmyvar, None, None)

...

>>> h = MyClass()

>>> h.myvar

2

>>> h.myvar = 3

>>> h.myvar

3

>>> Finally, Exceptions I use exceptions a lot, but there are some constructs I just now read about, particularly, finally. Ha ha. try/finally Suppose, we want to run some code in a try block. We know that we want a particular code block to run whether or not an exception occurs. It is assumed that the exception is caught by the caller (or higher caller). The try block will run regardless and so will the finally block. This does not leverage the power of exceptions, unless the caller catches the exception. Catch Multiple Exceptions Our code can catch multiple exceptions by enclosing the exception types in a tuple. We can also get data associated with the exception. I have used this data several times, but always forget the syntax. >>> try:

...     html = urllib2.urlopen(url).read()

... except (urllib2.HTTPError, httplib.BadStatusLine), e:

...     print ""An error %s occurred. Let's sleep it off."" % str(e.code)

...     time.sleep(1) try/except/else Our code in the try block is executed and one of two things happen: an exception occurs, or not. If an exception occurs, hopefully we catch it using except. If we do not, we hope the caller catches it. If no exception occurs, the code within else is run.


try:

html = urllib2.urlopen(url).read()

except (urllib2.HTTPError, httplib.BadStatusLine):

print ""Website is down or something.""

time.sleep(1)

except:

print ""Something else tragic happened.""

else:

parse_html(html)
try/else Unlike the try/finally construct, the else block only runs if the code in the try block runs successfully. This allows us to avoid using boolean flags to test for success! Although I learned a lot from the second edition, I think it is time to buy the fourth edition… "	 0 Comments
Rcpp 0.7.4	https://www.r-bloggers.com/2010/01/rcpp-0-7-4/	January 31, 2010	Thinking inside the box	"
The release once again combines a number of necessary fixes with numerous new features:
 

Lastly, we had a remaining Windows build issue. Also, Brian Ripley and Uwe
Ligges kindly sent us a small patch supporting the new Windows 64-bit builds using
the new MinGW 64-bit compiler for Windows — so release 0.7.5 may follow in
due course.


 
The NEWS file entry for release 0.7.4 is as follows:
 
As always, even fuller details are in the ChangeLog on the
Rcpp page which also
leads to the downloads, the 
browseable
doxygen docs and zip files of doxygen output for the standard formats.
Questions, comments etc should go to the
rcpp-devel mailing list
off the R-Forge page

 "	 0 Comments
With With	https://www.r-bloggers.com/2010/01/with-with/	January 31, 2010	R Tips	"No that is not a typo in the title.  In my programming a came across a solution that I thought was pretty cool.  I have a function that basically takes two objects and passes the elements of the objects to another function as arguments.  This is a pretty simple thing to do but can be painful to type everything out


f<-function(obj1,obj2){

 g(obj1$a,obj1$b,obj1$c,obj2$x,obj2$y,obj2%z)

}

 read more "	 0 Comments
Congruential generators all are RANDUs!	https://www.r-bloggers.com/2010/01/congruential-generators-all-are-randus/	January 30, 2010	xi'an	In case you did not read all the slides of Regis Lebrun’s talk on pseudo-random generators I posted yesterday, one result from Marsaglia’s (in a 1968 PNAS paper) exhibited my ignorance during Regis’ Big’ MC seminar on Thursday. Marsaglia indeed showed that all multiplicative congruential generators  lie on a series of hyperplanes whose number gets ridiculously small as the dimension d increases! If you turn the ’s into uniforms  and look at the d dimensional vectors  they are on a small number of hyperplanes, at most , which gives 41 hyperplanes when … So in this sense all generators share the same poor property as the infamous RANDU which is such that that  is always over one of 16 hyperplanes, an exercise we use in both Introducing Monte Carlo Methods with R and Monte Carlo Statistical Methods (but not in our general audience out solution manual). I almost objected to the general result being irrelevant as the ’s share ’s, but of course the subsequence  also share enjoys this property! 	 0 Comments
Practical Implementation of Neural Network based time series (stock) prediction – PART 2	https://www.r-bloggers.com/2010/01/practical-implementation-of-neural-network-based-time-series-stock-prediction-part-2/	January 30, 2010	Intelligent Trading		 0 Comments
Mining Tuition Data for US Colleges and Universities, and a Tangent	https://www.r-bloggers.com/2010/01/mining-tuition-data-for-us-colleges-and-universities-and-a-tangent/	January 30, 2010	Ryan Rosario	"I wrote this script for the UCLA Statistical Consulting Center. I don’t know all of the specifics, but one of our faculty members has this idea that we can help our paper, The Daily Bruin, with their graphics or something to that effect. I don’t quite understand because our paper has never really been big on graphics for data, but apparently some undergraduates are going to work on this. Anyway, we need datasets that are of interest to UCLA students so that our undergraduates can create cool graphics that will stun the readers. Some of the data we were considering: The tuition data was presented in a bunch of tables presented on several pages. Unfortunately, the type of school is not reported. Due to this limitation, I had to execute separate queries to access each year of data, and each type of school. tuition.py is the result of my labor. It is always a lot of fun, and it is an awesome feeling to be able to extract bulky data! This was also one of my first experiences with pylint. As much as I love Python, it is easy to write ugly code. pylint checks the style of code for violations of Python style such as tabs vs. spaces, spaces between binary operators, function naming conventions, line length and commenting conventions. It also checks for most (if not all) syntax errors, and some logic errors. 
I provide this code for educational purposes only. Some may be tempted to ask for the dataset, but for me to grant the request would be in violation of the copyright.

Although extracting the data was the fun part, I feel it would be “sudden” for me to end the post here. So, I should take a quick look at it to show how important mining messy data is. There has recently been an uproar in California regarding increasing fees at the University of California and the California State University systems. The University of California system is the “research” University system in California, whereas California State University does not emphasize research as much (sorry, that’s the best way I can explain it) and does not offer a PhD degree. UCs are generally harder to be admitted to. Some of the better CSUs (such as Cal Poly etc.) can be more highly regarded than some of the lower tier UCs however. Anyway, I wanted to take a look at how fees at UCLA, the UC system and the California education system have changed over time. I also want to dispel some myths that students have propagated on campus about the current state of fees in the UC system. Note that next year these fees are expected to increase by 30%… Myth #1: UC and CSU are in this together, and equally. One would hope that the burden of higher in-state fees would be shared equally between UC and CSU. The figures below indicates that this is sadly not the case. Since 2002, the difference between UCs and CSUs began growing. This may suggest that the services offered only at UC have grown more expensive over time (more research centers, more specialized staff?). Starting in 2002 and more so in 2006, UC fees began to skyrocket compared to those of CSU. There seems to be some non-statistical evidence that the State of California has disproportionately raised fees for UC students mainly due to difference in philosophy and demographics at both University systems. MYTH. 
  Myth #2: “We went from being one of the cheapest public school systems to one of the most expensive!” While we are hurting here in California, the only reliable way to compare how our fees fare over time is to compare them to the national average over time. One argument is that UC fees are now some of the most expensive in the nation. My guess is that in-state fees are approximately normally distributed, but I chose to use the median to point out how important it is to understand what the median is! The plot below compares UC in-state fees to other public 4-year schools across the country. Our fees have been above the national median since data re[orting began in 1999, but since 2003 or so, our fees have grown from being about $2000 above the median to about $3000 above the median. This means that our fees are in the top 50% of school systems, but it does not display how high into the top 50% the UC in-state fees fall. While the median is easy to interpret, top 50% and bottom 50% are not black and white. Instead, let’s look at the percentile rank of UC’s fees over time which is displayed in the second plot below. This gives us a way to quantitatively compare UC fees to national median fees without a “shock” factor.  From 1999-00 school year to 2009-10 school year, UC fees have consistently been above the national median for 4-year public schools. It appears that UC fees increased from the 70th percentile to about 85th percentile in the past 10 years. I do not have data from before 1999, so this rumor may be true, but based on this data, it is false. First part of the myth: Not enough data to conclude, second part: TRUE. Myth 3: “UC(LA) fees have gotten so ridiculous, it is practically becoming a private school!” or “I might as well just go to a private school!” Um, think again. This one should be simple to dispel. Let’s take the private school across town that Bruins love to hate: USC. We see that USC’s in-state fees have risen linearly since 1999 and the gap between UCLA is growing, not shrinking! Even when compared to the national median for private 4-year not for profit colleges, UCLA is not anywhere close. It may be true that UC is more expensive than some cheaper private schools though. So put away the USC gear and the notion that you won’t pay much more at USC than you would at UCLA. MYTH.  Conclusion So, yes, the California education system is a mess, but it is not the apocalypse that many students at UC schools have made it out to be…yet. California is in a deeper mess than the nation as a whole (relatively speaking, of course) so it is to be expected that our fees increase higher than other public school systems especially due to the cost of living in the state. Based on this data, I predict that even with an improving national economy, the national median in-state fees will catch up closer to UC’s fees. Of course, all of my analysis considers data before the 30% increase that takes effect in Fall 2010. So stay tuned. "	 0 Comments
Practical Implementation of Neural Network based time series (stock) prediction  – PART 1	https://www.r-bloggers.com/2010/01/practical-implementation-of-neural-network-based-time-series-stock-prediction-part-1/	January 29, 2010	Intelligent Trading		 0 Comments
Big’MC seminar	https://www.r-bloggers.com/2010/01/big%e2%80%99mc-seminar/	January 29, 2010	xi'an	Two very interesting talks at the Big’ MC seminar on Thursday: – Phylogenetic models and MCMC methods for the reconstruction of language history by Robin Ryder  – Uniform and non-uniform random generators by Régis Lebrun  which are both on topics close to my interest, evolution of languages (I’ll be a philologist in another life!) and uniform random generators. 	 0 Comments
R creators win prestigious Statistical Computing and Graphics Award	https://www.r-bloggers.com/2010/01/r-creators-win-prestigious-statistical-computing-and-graphics-award/	January 29, 2010	David Smith	The American Statistical Association recently created a new, bi-annual award to to recognize an individual or team for innovation in computing, software, or graphics that has had a great impact on statistical practice or research. The committee has just announced the winner (or in this, joint winners) of the first award: Robert Gentleman and Ross Ihaka, for their work in initiating the R Project for Statistical Computing. It really can’t be overstated how well-deserved this award is. No other project has made world-class software available to so many people while also by encouraging so many to participate in advancing the art and science of statistical computing through the medium of an open-source project. While many, many people have made significant contributions to the R project over the years, it’s undeniable that Robert and Ross were the ones that got it all started. Congratulations, R&R! (For more information about the history of the R project, this New York Times article from 2009 is a great resource. You can also read a profile of Ross Ihaka in the New Zealand Herald. Disclosure: Robert Gentleman recently joined the board of REvolution Computing.) ASA Sections on Statistical Computing and Statistical Graphics: Statistical Computing and Graphics Award 	 0 Comments
Crayola crayon colors, 1949-present	https://www.r-bloggers.com/2010/01/crayola-crayon-colors-1949-present/	January 29, 2010	David Smith	Here’s an example I featured in my list of 7 Awesome Things about R (awesome thing #3: graphics and data visualization). The Learning R blog features a reproduction of a graphic that recently appeared on Flowing Data. It shows the colors in a box of Crayola crayons: before 1949 there were only 8, but over the years additional colors have been added to the mix. Today, there are 120 colors, and the chart below shows the progression of colors added over time.  The amazing thing about this graph isn’t just that it’s reproduced in R from a hand-crafted original. The amazing thing is that this chart is completely automated: it reads the list of colors from history directly from the list of Crayola colors on Wikipedia, creates barcharts for each time period, and sorts the colors in a pleasing manner (by converting them to the HSV color space) before exporting the custom chart to a PNG file. All of this is done in less than 30 lines of R code. If Crayola introduces a new set of colors this year, then as long a someone updates the Wikipedia page, a new, up-to-date version of this graph can be created in seconds. Now that’s awesome. Learning R: ggplot2: Crayola Crayon Colours (via @gaygoygourmet) 	 0 Comments
Looking for a Bayésien PhD	https://www.r-bloggers.com/2010/01/looking-for-a-bayesien-phd/	January 28, 2010	xi'an	"I just got this email (yes, in French) looking for a Bayesian ready to work on algorithms: Dans le cadre de la société Vekia, nous recherchons un Docteur en statistiques bayésiennes pour un poste sur Lille à pourvoir dès que possible. Vekia est  un éditeur de logiciel pour le commerce fondée en 2007 par deux chercheurs (Pierre-Arnaud COQUELIN et Manuel DAVY) et compte actuellement 17 personnes. Nous éditions des logiciels à forte valeur ajoutée scientifique pour le commerce, réalisons des études pour nos clients hors du secteur du commerce. Dans le cadre de la structuration de ses activités de R&D, Vekia recherche un docteur en statistiques bayésiennes et/ou machine learning, pour les missions suivantes :
– conception de modèles bayésiens et maquettage d’algorithmes en vue de leur validation
– Conception d’algorithmes de contrôle optimal en contexte stochastique et maquettage
– réalisation d’étude de données ponctuelles Pour toute candidature, merci de contacter Pierre-Arnaud Coquelin au 06 50 44 22 58 ou [email protected] "	 0 Comments
Introduction to R webinar today, slides available	https://www.r-bloggers.com/2010/01/introduction-to-r-webinar-today-slides-available/	January 28, 2010	David Smith	Just a quick reminder that I’ll be hosting an introductory webinar about R today, The R Project: Data Analysis and Statistical Graphics for the Enterprise. It’s at 9AM Pacific, so you might still have time to register for the live session at the link below. Otherwise, if you did catch the live session, you can pick up the slides at the link below: the PDF has live links to all the resources and sites I mentioned in the talk. And if you have any comments or questions that we didn’t have time to address live, please use the comments section below and I’ll answer as many as I can. And if you missed the live session, the replay will be posted in the next couple of days — stay tuned for an announcement here on the blog. REvolution Computing: The R Project: Data Analysis and Statistical Graphics for the Enterprise 	 0 Comments
Advanced Graphics in R	https://www.r-bloggers.com/2010/01/advanced-graphics-in-r-2/	January 27, 2010	Ryan Rosario	 Each quarter the UCLA Statistical Consulting Center hosts minicourses twice per week in R and LaTeX. Tonight was my turn to present. I presented Advanced Graphics in R. This was the same presentation I gave at the LA R Users’ Group in August will a fellow consultant. She and I had trouble coming together to make one presentation, so we shared our outlines, and we deemed her outline was deemed “Intermediate Graphics in R” with some ggplot, and mine was deemed “Advanced.” It seems to work. My slides are here, and the handout version is here. The corresponding code is here. Topics include: Many think that “advanced” graphics would be lattice or ggplot. We chose to address those packages in their own minicourses. My advisor gave me some good advice on writing R code that fits well in Beamer slides and lstlisting: 	 0 Comments
From the “blogosphere”? Hardly.	https://www.r-bloggers.com/2010/01/from-the-%e2%80%9cblogosphere%e2%80%9d-hardly/	January 27, 2010	nsaunders	"I generally skip over “From the Blogosphere”, a (mostly) weekly-summary of one or two blog posts in Nature’s “Authors” section (here is the latest).  Why?  Well, I’ve always suspected that the title is rather misleading. Now, I have the hard numbers to prove it.

My feed reader contains an archive of 128 articles, dating back to May 10 2007.  I used them to create this CSV file with 3 fields:  Date, Blog and Source.  The “Blog” field contains the name of the primary blog mentioned in “From the Blogosphere”.  On rare occasions where 2 blogs are mentioned, 2 entries were added to the CSV file.  I skimmed each article rather quickly, so I may have missed blogs that were mentioned in passing. Next, a short R script: Result:

As I suspected, “From the Blogosphere” should really be called something else.  “From the NPG stable”, perhaps?  Only one blog post outside of Nature or Nature Network received a notable mention:  Egon’s review of Nature Chemistry. Given NPG’s excellent blog tracking effort, Nature Blogs, I suggest that “From the Blogosphere” be retired.  Or at least, renamed to something less misleading. "	 0 Comments
Re-mapping Massachusetts Special election results	https://www.r-bloggers.com/2010/01/re-mapping-massachusetts-special-election-results/	January 27, 2010	jjh	"I had previously posted maps showing the difference in major party vote share between the 2008 Presidential election and the 2010 special Senate election in Massachusetts. Colleagues and readers of the Revolutions blog had some very insightful criticisms of these maps, in particular that the color scale was over-stating the swing in voter sentiment. I’ve decided to perform the process again, taking some of their advice into account, and hopefully producing more useful output in the process.  One concern was my using arbitrary-sized bins for the range comparison. Revo blog commenter Mike Lawrence suggested 3 equal sized bins with a neutral color for near zero values. Given the variance in election returns between townships (sd=28 points in 2010,sd=20 in 2008) I’m going to choose 10 bins instead of 3, but otherwise this is an excellent suggestion. Initially we’ll be measuring the difference between the Republican and Democratic vote percentage in each township for the 2008 and 2010 elections. This tells us not only which party won, but by how much.  Here is the code to recreate the MA 2010 senate returns map with breaks of 15% and a different color palette showing neutral middle colors. Which gives us the following output:
 I believe this map more accurately represents the results. As a companion to the 2010 major party results I’d like to recreate the 2008 presidential results, using the same scale and bins.  Now the two graphs can more or less be compared apples to apples, and we can draw some very interesting conclusions from that comparison. The nine townships that were heavily democratic in 2008 actually stayed heavily Democratic in 2010, white most all other townships showed an increase in Republican support. These townships didn’t necessarily flip from being won by a Democrat to being won by a Republican, but every township showed an increase.  The least successful map I had previously produced was attempting to illustrate the degree of increased Republican support. What I really wanted to show was the increase in Republican support between the elections 2008 and 2010 elections.  From this map we can see that from 2008 to 2010 the Republican vote percentage actually increased in every single township in MA. This is kind of shocking, especially when you consider that Democrats enjoy a 15 point registration advantage statewide(source: warning PDF).  If I were a campaign manager for Richard Neal (2nd Cd) or Jim McGovern (3rd CD) I’d take a long look at what the Coakley campaign did to so many democratic voters to either stay home or cross party lines in the 2010 special election.  jjh "	 0 Comments
How to combine Google maps and data in R	https://www.r-bloggers.com/2010/01/how-to-combine-google-maps-and-data-in-r/	January 27, 2010	David Smith	Every good artist needs a canvas, and when it comes to displaying geographic data placing those data in context — on a map — makes all the difference. A new package for R from Markus Loecher, RgoogleMaps, allows you to download a street or satellite map from Google simply by specifying the bounding latitude/longitude coordinates. (You need to sign up for a free Google API key first, though.) You can then overlay data from objects in R, using tools provided to convert to the map-based coordinate system. Here’s an example from the package vignette overlaying the locations of faults (provided as data in the geomapdata package) on a satellite map:    You can find the commands to create this map in R, along with other examples, in the package vignette linked below. RgoogleMaps package: Vignette: Plotting on Google Static Maps in R (PDF) (via @DataJunkie) 	 0 Comments
Bayesian courses in København	https://www.r-bloggers.com/2010/01/bayesian-courses-in-k%c3%b8benhavn/	January 26, 2010	xi'an	"I received this announcement about two incoming courses given in København by Andrew Lawson: 1) “*An Introduction to Bayesian Disease Mapping*” A Two-Day Course, April 12.- 13. 2010, University of Southern Denmark
This course is designed to provide an introduction to the area of Bayesian disease mapping in applications to Public Health and Epidemiology: 2) “*Advanced Bayesian Disease Mapping*” A Two-Day Course, April 15. – 16. 2010, University of Southern Denmark, Copenhagen, Denmark
This course is designed to provide advanced coverage of Bayesian disease mapping topics in applications to Public Health and Epidemiology: It is intended as an extension to the course: *An Introduction to Bayesian Disease Mapping*. Emphasis on the course is placed on spatial and spatio-temporal Bayesian modeling issues, and some knowledge of Bayesian computation and WinBUGS is assumed. "	 0 Comments
What programmers should know about Statistics	https://www.r-bloggers.com/2010/01/what-programmers-should-know-about-statistics/	January 26, 2010	David Smith	Reader KW pointed me to this rant essay from Ruby on Rails enfant terrible Zed Shaw on what computer programmers don’t know about statistical analysis, but should. (Spoiler alert: a lot, apparently.) Perhaps surprisingly, building complex software systems often involves a lot of simulation, experimentation, and measurement for which statistical methods would be an asset. But according to Shaw, many programmers often have no idea how many iterations to run a test for, or why an average is often meaningless if you don’t also consider the variation, or how confounding factors can mess up an experiment. There’s actually some good statistical advice here, illustrated with examples from R.  Zed Shaw: Programmers Need To Learn Statistics Or I Will Kill Them All 	 0 Comments
What Countries are ‘Pulling their Weight’ for Haiti?	https://www.r-bloggers.com/2010/01/what-countries-are-%e2%80%98pulling-their-weight%e2%80%99-for-haiti/	January 26, 2010	Drew Conway	Using the data provided ReliefWeb on the Appeals and Funding to Haiti (h/t DataBlog) and the most recent GNP estimates, I decided to do a little “back of the envelope” analysis.  With GNP as a proxy for a country’s wealth, the hypothesis is that pledges should roughly be a linear function of wealth, i.e., the more you have the more you can give.  Again, this is by no means a complete, or even partially complete analysis, simply a quick exploration of the data; that said, throwing the data into the ggplot2 sausage grinder reveals some interesting features. A few things to note from the above figure.  First, the axises are log-log plots of GNP and pledge amounts in U.S. dollars.  Also, country names are sized and colored by their residual from , and absolute values are taken on size to avoid negative sizes.  A red coloring indicates a positive outlier, and blue a negative.  The black line is the best fit line corresponding to the above linear model. Immediately, Norway and Sweden standout as countries that (by this very rough approximation) appear to be contributing above their wealth level, and conversely China and Japan (text overlapping) seem to be under-pledging.  Also, though difficult to see, Finland, Switzerland and Spain all fall almost perfectly on the best fit line, while the United States is an extreme outlier. I will resist editorializing this data, as there are many factors that this is not picking up; most notably, the difference between pledges and committed funds.  Also, the data does not include pledges from private or international organizations, which would most certainly alter the plot.  A better analysis may be to fit a gravity-like model to this data, where pledges are a function of the distance of Port-au-Prince to another country and the level of trade between Haiti and that country.  Gravity models are very popular, and seem relevant to this data. 	 0 Comments
Free GIS Resources	https://www.r-bloggers.com/2010/01/free-gis-resources/	January 26, 2010	James	 Over the last couple of days I have utilised some excellent free GIS resources. I have listed these and some others below. Geospatial Analysis: This is the free online version of de Smith, Longley and Goodchild’s excellent book by the same title. It provides full coverage of current GIS methodologies. It also provides extensive information regarding the various GIS software available. Analysing Spatial Point Patterns in R: 200 pages of workshop notes written by Adrian Baddeley. These provide extremely detailed and comprehensive overview of the spatstat in R. GeoDa Center Tutorials: A range of tutorial material provided by creators of the GeoDa Software. I would focus on the R tutorials as the GeoDa tutorials are awaiting an update in line with the software’s latest release. Spatial Stats. in ArcGIS: A preview chapter from the Springer’s Handbook of Applied Spatial Analysis. CATMOGs: A hugely successful series of publications that cover the basics of spatial analysis, they have been written by many of the pioneers in the field. Topics include The Modifiable Areal Unit Problem (Openshaw), Voronoi (Thiessen) Polygons (Boots), Spatial Autocorrelation (Goodchild). CASA Working Papers: A shameless plug for my fellow researchers. The nice thing about these is you don’t need to be part of an academic institution to access academic research. I am sure there are many others and I welcome your suggestions… 	 0 Comments
ggplot2: Quick Heatmap Plotting	https://www.r-bloggers.com/2010/01/ggplot2-quick-heatmap-plotting/	January 25, 2010	learnr	"A post on FlowingData blog demonstrated how to quickly make a heatmap below using R base graphics. This post shows how to achieve a very similar result using ggplot2.  
 FlowingData used last season’s NBA basketball statistics provided by databasebasketball.com, and the csv-file with the data can be downloaded directly from its website. The players are ordered by points scored, and the Name variable converted to a factor that ensures proper sorting of the plot. Whilst FlowingData uses heatmap function in the stats-package that requires the plotted values to be in matrix format, ggplot2 operates with dataframes. For ease of processing, the dataframe is converted from wide format to a long format. The game statistics have very different ranges, so to make them comparable all the individual statistics are rescaled. There is no specific heatmap plotting function in ggplot2, but combining geom_tile with a smooth gradient fill does the job very well. A few finishing touches to the formatting, and the heatmap plot is ready for presentation. In preparing the data for the above plot all the variables were rescaled so that they were between 0 and 1. Jim rightly pointed out in the comments (and I did not initally get it) that the heatmap-function uses a different scaling method and therefore the plots are not identical. Below is an updated version of the heatmap which looks much more similar to the original. "	 0 Comments
Mapping the Massachusetts election upset with R	https://www.r-bloggers.com/2010/01/mapping-the-massachusetts-election-upset-with-r/	January 25, 2010	David Smith	The blog Offensive Politics has done some in-depth analysis of the recent Senate special-election upset in Massachusetts, comparing the results of victorious Republican candidate Scott Brown to those of the unsuccessful Republican Presidential candidate John McCain in 2008. It’s pretty clear that Brown out-performed expectations with Democratic voters, but this chart of the change in Democratic voters from 2008 to 2010 makes the contrast stark:   You can find more charts, and the data and R code to reproduce them, at the link below. Update 10:56: Political scientist Boris Shor has also analyzed this election, concluding that Scott Brown is more liberal than current Senate Republican Olympia Snowe and congressional Republican challenger Dede Scozzafava (who lost her primary amongst claims she was too liberal). Incidentally, Boris relies on the big-data capabilities of REvolution R Enterprise for Windows to make these analyses possible. Offensive Politics: Mapping MA election results 	 0 Comments
Robert Gentleman joins REvolution’s board of directors	https://www.r-bloggers.com/2010/01/robert-gentleman-joins-revolutions-board-of-directors/	January 25, 2010	David Smith	We’re so excited here at REvolution Computing to announce that Robert Gentleman has joined our board of directors. Robert is one of the two originators of the R Project: a research project between Robert and Ross Ihaka in 1996 was the genesis of the R language. (Both Robert and Ross were profiled in an article in the New York Times about R last year.) Today, the R Project has grown tremendously, with estimates of more than 2 million users worldwide, thousands of volunteer contributors to R packages, and more than 20 world-leading statisticians and computer scientists leading the core development. Robert was also the leader of the BioConductor project for many years, developing cutting-edge tools in R for the analysis of genetic data. Today, Robert continues his research in genomics as a senior director in bioinformatics at Genentech. He had this to say about joining our board: “REvolution has made important contributions to the R community and to the commercial use of R on an enterprise level,” Gentleman said. “I am eager to help Norman and the team expand the role R has in the commercial world and to help bring high-quality analytic and graphical software to many new areas of application.” Robert’s expertise in R, the R project, and open-source projects in general will be invaluable to us as we develop our plans for REvolution Computing.   We’re also pleased to announce a second new appointment to our board of directors: financial expert Donald Nickelson. You can read more about both new board members in our press release linked below.  REvolution Computing: REvolution Computing Names Robert Gentleman and Donald Nickelson to Board 	 0 Comments
Mapping MA election results	https://www.r-bloggers.com/2010/01/mapping-ma-election-results/	January 25, 2010	jjh	The Swing State Project recently had some very interesting maps comparing last week’s election results from Massachusetts to 2008 presidential primary results. Their maps posted show some very interesting trends, but the maps themselves are lacking in information and the color schemes are pretty ugly. Using my own source data I recreated their election night maps, along with a few more. The geography was taken directly from the US Census so some of the waterlines are pretty strange compared to other result maps like the Boston Globe and Swing State Project. First look at the results from the MA 2010 Senate special election.  MA Senate 2010 Results Now compare that to the results of the 2008 presidential primary in MA, blue gradients for Obama and red gradients for Clinton.  Democrat 2008 Presidential Primary results Using the same color scale and data cut-points we can see the same results that DavidNYC came up with, namely that winning townships for Clinton in 2008 tracked decently to winning townships for Scott Brown in the 2010 Senate race.  Obama lost the MA presidential primary in 2008, but he won in the general. Here is a map showing the vote margins from the 2008 presidential general:  MA 2008 Presidential results  Compared to the MA 2010 Senate race last week these 2008 numbers are astounding. The democratic vote margin in 2008 was 26 points (62% to 36%), while the 2010 senate race was -5 points (47-52), with 30% less turnout in 2010 than in 2008. I decided to create another map showing the decline in democratic vote support  Democratic vote change 2010 to 2008 Does this mean voters have turned against democrats in MA? Maybe, but it is interesting that Scott Brown got more votes in the special election (1,168,107) as McCain did in the 2008 general (1,108,854), even though 900,000 less people voted in the special. So Brown was able to do two things: 1) Activate republican McCain voters, 2) Cause democrats to cross party lines. I don’t think this spells certain doom for congressional democrats in the midterms, but it does show that Democrats will stay home on election day, even in a historically democratic state, for the wrong candidate.  You can create the maps yourself or play with the data by downloading the source + data files here. With a recent installation of R you can recreate the maps above by running the the following command: Thanks. 	 0 Comments
rgdal and other GIS-related packages for Mac OS X	https://www.r-bloggers.com/2010/01/rgdal-and-other-gis-related-packages-for-mac-os-x/	January 23, 2010	Jeffrey Breen	"CRAN contains ready-made binary packages for nearly all of its packages, but rgdal is one which I keep finding myself trying to install from source whenever I upgrade R. Compiled versions of rgdal, along with prerequisites and complements like the GDAL framework, GRASS, and even the old FFTW3 can be found at KyngChaos’s Wiki: Update 10/10/10: 
I don’t remember needing to do this before, but  $ ln -s /Library/Frameworks/GDAL.framework/Programs/gdal-config /usr/local/bin

 makes the rgdal source package installation work on R 2.11.1. "	 0 Comments
R Tutorial Series: Regression With Interaction Variables	https://www.r-bloggers.com/2010/01/r-tutorial-series-regression-with-interaction-variables/	January 23, 2010	John M. Quick	 	 0 Comments
Fluctuation plot using ggplot2 in R	https://www.r-bloggers.com/2010/01/fluctuation-plot-using-ggplot2-in-r/	January 22, 2010	Stephen Turner		 0 Comments
What to Expect?	https://www.r-bloggers.com/2010/01/what-to-expect/	January 22, 2010	Ryan	In 2007, I was introduced to Twitter via the written qualifying exam towards my Ph.D.. At first, I did not know what to do with it. After a good year or so (maybe even sooner) passed, I began to follow some very interesting people that share the same interests as me. It has transformed my academic experience. It is great to run across tweets promoting conferences and newly released papers in my field. One of my favorite parts about Twitter, aside from interacting with tweeps, is the ability for me to quickly post a status update on what I am doing and I can even refer to it later. I consider it a platform for collaboration because I see what others are doing via tweets as well as linked blogs, whether it is a Twitter user, or some offline user. I quickly realized that 140 characters were not enough to solidify my thoughts and participate in the community. Thus, I decided to start this blog so I can share cool things I have found in my research/work with others anywhere on the web and communicate in more than 140 characters. Here are some things that I am very interested in and will post about. Of course this is not an exhaustive list and a lot of this stuff overlaps! Now that I’ve written my introductory posts, the real fun begins…   	 0 Comments
User Group news: DC and Philly	https://www.r-bloggers.com/2010/01/user-group-news-dc-and-philly/	January 22, 2010	David Smith	There’s a new local R User Group starting up in Philadelphia. Alex McClung and Jeremy Leipzig have set up a website for the Philly UseR Group to get things started, so if you’re in the area join up and tell them what you’d like from the group. Also, the DC UseR Group is seeing renewed activity after a hiatus since its first meeting last year: a couple of new members have stepped forward to organize the group. They’re looking for input on the group’s direction, so why not drop in and vote? 	 0 Comments
Web Development with R (video)	https://www.r-bloggers.com/2010/01/web-development-with-r-video/	January 22, 2010	David Smith	Jeroen Ooms’ BARUG talk on Web Development with R is now available (with thanks again to Drew Conway):    If you want to try the web based apps Jeroen demos in the video yourself, you can find links at Jeroen’s homepage (along with the slides for his talk). Click on the links for stockplot, lme4, and gpplot2.  vcasmo: Web Development with R 	 0 Comments
Because it’s Friday: The Internet is Made of Cats	https://www.r-bloggers.com/2010/01/because-its-friday-the-internet-is-made-of-cats/	January 22, 2010	David Smith	Here’s a fascinating video that looks at the exponential growth of broadband technology and network-based models … uh … who am I kidding? This video has nothing to do with R, statistics, or open-source. It’s about cats. But it’s funny. And it’s Friday.  (Some slightly NSFW language, but it’s in a strong British accent so you might not even notice it.) 	 0 Comments
Haplotype names in R	https://www.r-bloggers.com/2010/01/haplotype-names-in-r/	January 21, 2010	Samuel Brown		 0 Comments
Rcpp 0.7.3	https://www.r-bloggers.com/2010/01/rcpp-0-7-3/	January 21, 2010	Thinking inside the box	"
This release combines a number of under-the-hood fixes and enhancements with
one bug fix:
 
As always, full details are in the ChangeLog on the
Rcpp page which also
leads to the downloads, the 
browseable
doxygen docs and zip files of doxygen output for the standard formats.
Questions, comments etc should go to the
rcpp-devel mailing list
off the R-Forge page

 "	 0 Comments
M. Edward (Ed) Borasky	https://www.r-bloggers.com/2010/01/m-edward-ed-borasky/	January 21, 2010	Ed Borasky		 0 Comments
Short Monte Carlo course in San Antonio	https://www.r-bloggers.com/2010/01/short-monte-carlo-course-in-san-antonio/	January 21, 2010	xi'an	Just a reminder that I will  teach a very short course on March 17 in San Antonio,  Texas, based on our  “Introducing Monte Carlo Methods with R” book, in the first day of the meeting  Frontiers of Statistical Decision Making and Bayesian Analysis, in honour of Jim Berger, There still are a few places left on the four courses provided that day, register now!   	 0 Comments
How to make a heat map in R	https://www.r-bloggers.com/2010/01/how-to-make-a-heat-map-in-r/	January 21, 2010	David Smith	FlowingData has just posted a nice step-by-step tutorial on how to make a heat-map in R, like this one on attributes of NBA scorers:  If you want to take the concept a step further with time-series data, you can also create calendar heat-maps in R. For example, you can track the level of activity in a forum, or the pace of iPhone app sales. FlowingData: How to Make a Heatmap – a Quick and Easy Solution 	 0 Comments
Web Development with R	https://www.r-bloggers.com/2010/01/web-development-with-r/	January 21, 2010	VCASMO - drewconway		 0 Comments
R AnalyticFlow	https://www.r-bloggers.com/2010/01/r-analyticflow-2/	January 21, 2010	Gregor Gorjanc		 0 Comments
writeNIfTI() is fully compatible with FSLView	https://www.r-bloggers.com/2010/01/writenifti-is-fully-compatible-with-fslview/	January 21, 2010	Brandon Whitcher		 0 Comments
UseR 2010 and commercial applications of R	https://www.r-bloggers.com/2010/01/user-2010-and-commercial-applications-of-r/	January 21, 2010	David Smith	Preparations are well underway for the annual R user conference useR! 2010, to be held at NIST (about 25 miles outside of Washington, DC) July 20-23. This meeting is a true user conference: almost all of the presentations at the conference are contributed by ordinary R users. So, to continue the success of previous years the conference committee needs you — yes, you — to submit a talk about what you do with R. Maybe you’ve made some interesting discovery in a dataset you’ve analyzed with R — why not share the process of how you got there with other R users? Created a novel (or just plain beautiful) graphic with R? Show it off! If you’ve written a package for R, why not explain how to use it (and find more users in the process). The possibilities are endless. The March 1 deadline for submissions is approaching fast, so why not submit your abstract today? One use of R that I’m particularly interested in is how R is used in commercial settings. There’s a whole section of this blog devoted to applications of R, and many of the posts document uses of R in commercial organizations like Google, Facebook and the New York Times. But I know there are many more interesting uses of R at companies that go unsung. To take just one example, at last year’s UseR conference, there was an outstanding presentation by Jan Wijffels about how vacations company Thomas Cook uses R to automatically price 250,000 holiday packages every day in less than 90 minutes. It would be great to see more examples of commercial uses of R at this year’s conference, and to that end committee-member Lou Bajuk-Yorgan from TIBCO-Spotfire is organizing a track on “Commercial applications of R”. If you’re using R at work, why not submit an abstract about R’s impact at your company?  By the way, if you do work for a company that makes widespread use of R, have you thought about asking your company to sponsor the useR! conference? This is a great way to support the R community and the people who created R and the packages you use for your work. My employer REvolution Computing is a proud sponsor, and perhaps your employer could be, too.  	 0 Comments
ggplot2: Crayola Crayon Colours	https://www.r-bloggers.com/2010/01/ggplot2-crayola-crayon-colours/	January 21, 2010	learnr	Statistical Algorithms blog attempted to recreate a graph depicting the growing colour selection of Crayola crayons in ggplot2 (original graph below via FlowingData). He also asked the following questions: Is there an easier way to do this? How can I make the axes more like the original? What about the white lines between boxes and the gradual change between years? The sort order is also different. I will present my version in this post, trying to address some of these questions.  Data Import The list of Crayola crayon colours is available on Wikipedia, and also contains one duplicate colour (#FF1DCE) that was excluded to make further processing easier. Plotting Instead of geom_rect() I will show two options of plotting the same data using geom_bar() and geom_area() to plot the data, and need to ensure that there’s one entry per colour per year it was(is) in the production. The plot colours are manually mapped to the original colours using scale_fill_identity(). And now the geom_area() version: Final Formatting Next, the x-axis labels suggested by ggplot2 will be manualy overridden. Also I use a little trick to make sure that the labels are properly aligned. The order of colours could be changed by sorting the colours by some common feature, unfortunately I did not find an automated way of doing this. Sorting by Colour Thanks to Baptiste who showed a way to sort the colours, the final version of the area plot resembles the original even more closely. 	 0 Comments
Scheduled Parallel Computing with R: R + Rmpi + OpenMPI + Sun Grid Engine (SGE)	https://www.r-bloggers.com/2010/01/scheduled-parallel-computing-with-r-r-rmpi-openmpi-sun-grid-engine-sge/	January 20, 2010	Vinh Nguyen	"Recently I’ve learned how to do parallel computing in R on a cluster of machines thanks to the R packages snowfall, snow, and Rmpi.  I’ve been using the SOCKET method with snowfall since together they make things simple.  With these tools, I can reduce day/week long jobs to hours or a day across many (100) cores/cpus. However, system admins would prefer me to do things using the sun grid engine (sge) or one of their job scheduler since clusters are usually a shared resource and having “rogue” jobs like mine that hog all the resources aren’t really a good thing.  Aside from scheduling jobs, another great thing about SGE is that it determines which nodes to use (idle?) with R so I don’t have to determine the list of nodes. Luckily, people have attacked this problem already.  First, Revolutions Computing has an internal document that gives instructions on how to install R, Rmpi, OpenMPI, and SGE to get them to work together.  If you email them and ask for it, they are more than willing to share it.  The document is “sge-snow.pdf.”  After things are installed, here is how to get things to work. First, copy the content of Rprofile that is packaged in Rmpi into ~/.Rprofile.
Place the following in a shell script to be submitted by qsub (an example script is at the end): NOTE: 51 is the number of cores/cpus to use, 1 master + 50 slaves.  Inside the R script, do not use anything that belongs to snow or
snowfall.  Just use Rmpi’s functions.  Also, by using mpirun, we do
NOT need to spawn slaves as they are spawned in the mpirun command.
We also do not need to call library(Rmpi).  Put the following in the R
script (SGEtest.R) to see that things are running: First, place the location of the executable RMPISNOW from the snow
package in the PATH variable (or use the direct location wherever you
see RMPISNOW in the command line).  DO NOT put the Rprofile from Rmpi
into ~/.Rprofile.  Place the following in the shell script to be
submitted by qsub: In the R script, use only snow functions (not Rmpi or snowfall).  No
need to call library(snow).  Put the following in the R script (SGEtest2.R) to
test: Similar to 2, but run instead of the qsub command to get an interactive session. For the first two cases, a sample openMPI_R.sh script is: Finally, I would like to point out that currently snowfall does not yet work with SGE because it requires a call to sfInit(), and this conflicts with the cluster called from mpirun.  This made me learn some functions from snow, which aren’t all that much different from snowfall. Also, there is an rsge package for R that seems to work too. You can name the PE anything and set the number slots.  Make sure the user list has you in it.  Also, make sure you add the Q’s u want to work with into this PE.
3.  specifying an outfile in the makeCluster() command in RMPISNOW doesn’t do anything since the cluster is called at the RMPISNOW’s invocation.  If we look at the RMPISNOWprofile, we see that the output is sink to /dev/null.  I tried a few ways to get the workers’ output out, via sink() on each worker via clusterEvalQ or setting the OUT or R_SNOW_OUTFILE variables (see RMPInode.R and RMPInode.sh).  How I got it to work was with: StrictHostKeyChecking no in ~/.ssh/config according to this page.  Check the stderr of your SGE log. "	 0 Comments
Remote download and viewing of brain atlas	https://www.r-bloggers.com/2010/01/remote-download-and-viewing-of-brain-atlas/	January 20, 2010	Brandon Whitcher		 0 Comments
How to integrate R into web-based applications (video)	https://www.r-bloggers.com/2010/01/how-to-integrate-r-into-web-based-applications-video/	January 20, 2010	David Smith	The slides and videos from Jeff Horner’s talk at last week’s Bay Area User Group session on R-Powered Web Apps are now available. I’ve embedded it here below, but you might want to click through to larger size for better readability on the slides.       Thanks go to Drew Conway for uploading the videos at VCASMO so that everyone can share in these local user-group talks. Drew is paying for the hosting himself, so if you enjoy these videos please consider contributing to his hosting fund. (We’ve contributed here at REvolution Computing as part of our program to support local R user groups.) Unfortunately the video of Jeoren’s talk from the meeting last week isn’t yet available, but if you’re in the LA area, you can see a live version of Jeroen’s talk tonight. (Update Jan 22: video now available!) VCASMO: R-Powered Web Apps with Rapache  	 0 Comments
GWAS Manhattan plots and QQ plots using ggplot2 in R	https://www.r-bloggers.com/2010/01/gwas-manhattan-plots-and-qq-plots-using-ggplot2-in-r/	January 20, 2010	Stephen Turner		 0 Comments
Example 7.23: the Monty Hall problem	https://www.r-bloggers.com/2010/01/example-7-23-the-monty-hall-problem/	January 20, 2010	Ken Kleinman		 0 Comments
New UseR group in Seattle, WA	https://www.r-bloggers.com/2010/01/new-user-group-in-seattle-wa/	January 19, 2010	David Smith	There’s a new local R user group to add to the list. The latest addition is in Seattle, WA and the first meeting of the Seattle useR Group is tomorrow. The group is organized by Zack Stednick (a data analyst and R user at the Fred Hutchinson Cancer Research Center in Seattle), and this meeting is hosted by my colleague and R technical support maestro Stephen Weller in the REvolution Computing offices on West Lake Union in Seattle. If you use R, and you’re in the Seattle area, come along, enjoy pizza and spontaneous talks, and help make their first meeting a success. Seattle useR Group: January Meetup 	 0 Comments
Solution manual for Introducing Monte Carlo Methods with R	https://www.r-bloggers.com/2010/01/solution-manual-for-introducing-monte-carlo-methods-with-r/	January 19, 2010	xi'an	After the complete solution manual for  Bayesian Core, the solution manual for the odd numbered exercises of “Introducing Monte Carlo Methods with R” is now arXived. The fuller 133 page version for instructors is available from Springer Verlag by demand only, in order to keep the appeal of the book as a textbook (even though this is open to debate). Since the LaTeX code is available from the arXiv deposit, it can also be used and modified freely. (It may be argued that publishing a solution manual on arXiv is somehow borderline, because, while it is hopefully useful to readers and original, it does not truly qualify as research. And won’t be published anywhere else. I agree with this perspective but the final decision was up to the administrators of the site who did not object. So I do not complain!) The warnings associated with publishing the complete solution manual for  Bayesian Core, are worth repeating with this solution manual for “Introducing Monte Carlo Methods with R”, namely that “some self-study readers will undoubtedly come to the realisation that the solutions provided here are too sketchy for them because the way we wrote those solutions assumes some minimal familiarity with the maths, with the probability theory and with the statistics behind the arguments. There is unfortunately a limit to the time and to the efforts we can dedicate to this solution manual“, which is about a week for both manuals. As of earlier, comments and suggestions are welcome. 	 0 Comments
New features in ggplot2 version 0.85	https://www.r-bloggers.com/2010/01/new-features-in-ggplot2-version-0-85/	January 18, 2010	David Smith	Hadley Wickham recently updated the ggplot2 package for R to version 0.8.5. In addition to bugfixes and performance improvements, this version introduces some handy new features. Among them: the ability to display mathematical equations as text, automatic legends for color scales,  and user-configurable axis labels and legend titles. The Learning R blog runs through the new features with worked examples. Learning R: New Features in ggplot2 version 0.8.5 	 0 Comments
Coming to R from SQL, Python, SAS, Matlab, or Lisp	https://www.r-bloggers.com/2010/01/coming-to-r-from-sql-python-sas-matlab-or-lisp-2/	January 18, 2010	Stephen Turner		 0 Comments
Updates to SoilWeb	https://www.r-bloggers.com/2010/01/updates-to-soilweb/	January 17, 2010	dylan	Soil Profiles in Color Added color support to the mini-profiles used in graphical map unit summaries, the Google Earth interface, and iPhone application. SSURGO doesn’t contain soil color data, so colors (in Munsell notation) were extracted from the OSD database, and converted into RGB triplets. Using horizon information from the OSD database also results in much more realistic horizonation, as compared to what is stored in older SSURGO databases. Example of the Yolo series soil, from the Yolo County (1972) soil survey:   read more 	 0 Comments
Donate to the R Video Hosting Fund	https://www.r-bloggers.com/2010/01/donate-to-the-r-video-hosting-fund/	January 17, 2010	Drew Conway	Today I finished uploading a couple new talks to the growing repository of online presentations from the NYC R Statistical Meetup.  Going forward, my plan is—whenever possible—to make all talks and presentation materials available online, in an effort to reach as wide an audience as possible.  Our volunteer speakers bring immense expertise to these talks with respect to both R specifically, as well as applied analytics more generally, and we are fortunate to have such a diverse group of people willing to invest their time and effort into these seminars. There is; however, a non-negligible cost associated with hosting these videos at VCASMO.  While alternative free video hosting services exist, I decided to use VCASMO because I felt having the videos synced with presentation materials provided the online audience the closest approximation to seeing the talks in person.  Currently, I am shouldering the cost of hosting by myself, but given the popularity of these videos (already viewed over 3,500 times) and my desire to grow and sustain this repository, I am now asking you to consider donating to the video hosting fund. I will not try to guilt you with statistics on the median income of a Ph.D. student in the social sciences (adjusted for cost of living in NYC), but if you watch the videos, find them useful and informative, and would like to see more in the future; please consider making a donation by clicking the PayPal Donation button below. Donate to the R Video hosting fund by clicking here, or the “Donate” button in the right column on ZIA Feel free to donate as much or as little as you like—everything helps.  Also, the button will remain in the right column permanently, so if you cannot donate now feel free to do so whenever you can.  Thank you for your consideration, and I look forward to posting new videos in the coming months! 	 0 Comments
MATLAB/R Dictionary (Rosetta Stone Talk – 1/3)	https://www.r-bloggers.com/2010/01/matlabr-dictionary-rosetta-stone-talk-13/	January 17, 2010	VCASMO - drewconway		 0 Comments
Data munging with SQL and R (Rosetta Stone Talk – 3/3)	https://www.r-bloggers.com/2010/01/data-munging-with-sql-and-r-rosetta-stone-talk-33/	January 17, 2010	VCASMO - drewconway		 0 Comments
Typos in Chapter 8	https://www.r-bloggers.com/2010/01/typos-in-chapter-8/	January 16, 2010	xi'an	Phew!, we are now done with the solution manual in the sense that we have compiled solutions for all odd-numberedd exercises (but one!) and solved a fair number of even-numbered exercises. As it stands, the manual is 120 pages long and I am exhausted by the run to produce it over the past week. I hope this will be helpful for readers of “Introducing Monte Carlo Methods with R”, but, if nothing else, looking at the book from a student’s perspective has helped in uncovering typos. Here is the last batch: – In Exercise 8.1, we need a bit more stability in the Markov chain to ensure that it has a finite variance. The assumption that both first moments are finite is not enough. Thomas Clerc from Fribourg also pointed out to me that the lazy programming  (I stole from my R course students!) of the form beta=c(beta,betaprop) should not be encouraged! – in Example 8.9,  should be  and the ’s are normal, not the ’s… – in Exercise 8.7, it is the distribution on  that is closed-form, not the one on . – in Exercise 3.17, George Casella found another typo, namely that in question b it should be , not . 	 0 Comments
Getting Started with the R Programming Language	https://www.r-bloggers.com/2010/01/getting-started-with-the-r-programming-language/	January 16, 2010	Ed Borasky		 0 Comments
Connecting to a DB2 database from R	https://www.r-bloggers.com/2010/01/connecting-to-a-db2-database-from-r/	January 16, 2010	The Average Investor	Unlike RMySQL and RSQLite there is no RDB2. However, I found it pretty  straightforward connecting to a DB2 database using the JDBC driver and the RJDBC  package. For all this to work, DB2 should be setup to use TCPIP, which is not used by  default. You need to set the DB2COMM DB2 environment variable to  TCPIP: ﻿You also need to make sure that the dbm configuration parameter SVCENAME is set,  either to a port number like 50000 or to a string like db2c_DB2. If SVCENAME (the line starting with “TCP/IP Service name” is not set you can set it to use port 50000, which is the default used by DB2. You also need to install the RJDBC package. Then you have to determine the location of the DB2 JDBC driver. If DB2 is installed locally, the driver is already installed and it is located under sqllib\java. For example, I installed DB2 in C:\installed\sqllib, thus the driver I am going to use is C:\installed\sqllib\java\db2jcc4.jar. Now it’s time to startup R, once the GUI comes up, we can load the package: Then we need to load the DB2 JDBC driver: At this point we can establish a database connection: Let’s run a query (to select all records from the employee table) and store the results into a data frame: Until now we were assuming that DB2 is installed. The alternative is to connect to a server running DB2. In this case you only need to install the JDBC driver from the IBM web site – the following video outlines the steps. 	 0 Comments
Interesting use of levelplot() for time series data	https://www.r-bloggers.com/2010/01/interesting-use-of-levelplot-for-time-series-data/	January 16, 2010	dylan	levelplot example: soil temperature (left) and moisture (right) Several recent articles appeared on the R-bloggers feed aggregator that demonstrated an interesting visualization of time series data using color. This style of visualization was readily adapted for the time series data I regularly collect (soil moisture and temperature), and quickly implemented with the levelplot() function from the lattice package. I hadn’t previously considered using a mixture of factor (categorical) and continuous variables within a call to levelplot(), however the resulting figure was more useful than expected (see above). A single day’s observation is represented by a colored strip (redder hues are higher temperature values, and lower soil moisture values), placed along the x-axis according to the date of that observation, and in a row defined by the location where that observation was collected from. Paneling of the data can be used to represent a more complex hierarchy, such as sensor depth or landscape position. At the expense of quantitative data retrieval (which is better supported be scatter plots), qualitative patterns are quickly identified within the new graphic. read more 	 0 Comments
Gay Marriage: Another Data Point	https://www.r-bloggers.com/2010/01/gay-marriage-another-data-point/	January 16, 2010	John Myles White	Relevant to my earlier post about the relationship between direct democracy and laws prohibiting gay marriage, Pew Research just published poll data showing that a majority of Americans disapprove of same-sex marriage. 	 0 Comments
A tale of two visualizations (because it’s Friday)	https://www.r-bloggers.com/2010/01/a-tale-of-two-visualizations-because-its-friday/	January 15, 2010	David Smith	GEEK FIGHT!!! says JD Long on Twitter as the New York Times publishes a widely-reposted interactive graphic about Netflix rental data, and the Wall Street Journal also gets into the interactive-viz game with a graphic on bank bonuses.  If it’s a fight, it’s a knockout in the first round, if you ask me. There’s no surprise why the Netflix chart has been tweeted and blogged to death since it was released: it’s a joy to play with and explore the patterns between DVD titles and their relative popularity between different cities and districts within. It’s no surprise to learn that Milk was wildly popular in the San Francisco area (though comforting to see it confirmed); more interesting is the pattern in a red-state city like Atlanta. Interest in the film in the cosmopolitan city core dwindles rapidly in the suburbs:  You can spend hours exploring the pattens of other movies and asking yourself why the pattern looks as it does (and sometimes the more interesting question is, why doesn’t it?) A good visualization prompts followup questions and gives you the means (or at least the motivation) to answer them. (By the way, the NYT did use R to look at the data using Principal Components Analysis. But the results of PCA are difficult to express in a way that easily be consumed by readers of a graphic like this, so none of the R-based analysis made it into the chart online. Maybe next time.) Compare that to the WSJ’s effort. The data source isn’t nearly as rich: simply the proportion of revenue allocated to compensation and benefits in various Wall Street firms. With all the furore about record bonuses it’s certainly a topical chart, but for me the delivery leaves me … well … shortchanged. I can’t get over the data presentation — pie charts when presented as squares are still pie charts, and I find it impossible to generate any meaningful comparisons from them. The fact that none of the detail charts are labeled by company name (except by rollover) inhibits meaningful comparisons, too.   In any case, I look forward to Round 2 of Geek Fight. Bring the popcorn!  	 0 Comments
Equivalents to SAS, SPSS modules in R	https://www.r-bloggers.com/2010/01/equivalents-to-sas-spss-modules-in-r/	January 15, 2010	David Smith	Bob Muenchen, the author of R for SAS and SPSS Users, has created a handy table listing the R packages that implement the statistical methods included in add-on modules for SAS and SPSS. For example, if you’re looking for the equivalent structural equation modeling functions you might be using in PROC CALIS from SAS/STAT or the SPSS “Amos” software, the table points you to the OpenMX and sem packages for R where you can find similar functionality. By the way, Bob has greatly expanded the materials he provides to support his book at a new website, r4stats.com. You’ll also find there some teaser information about his forthcoming book, R for Stata Users.  r4stats: R-SAS-SPSS Add-on Module Comparison 	 0 Comments
R Tutorial Series: Hierarchical Linear Regression	https://www.r-bloggers.com/2010/01/r-tutorial-series-hierarchical-linear-regression/	January 15, 2010	John M. Quick	  	 0 Comments
Learning R via Python…or the other way around (Rosetta Stone Talk – 2/3)	https://www.r-bloggers.com/2010/01/learning-r-via-python-or-the-other-way-around-rosetta-stone-talk-23/	January 15, 2010	VCASMO - drewconway		 0 Comments
RQuantLib 0.3.2 released	https://www.r-bloggers.com/2010/01/rquantlib-0-3-2-released/	January 14, 2010	Thinking inside the box	"

This versions brings a few cleanups due to minor
Rcpp changes (in
essence: we now define the macro R_NO_REMAP before including R’s
headers and this separate non-namespaced functions like error()
or length() out into prefixed-versions Rf_error()
and Rf_length() which is a good thing).
 

It also adds a number of
calendaring and holiday utilities that Khanh just added: tests for weekend,
holiday, endOfMonth as well dayCount, date advancement and year fraction
functions commonly used in fixed income.

 
Full changelog details, examples and more details about this package are at
my RQuantLib page.

 "	 0 Comments
Typos in Chapters 6-7	https://www.r-bloggers.com/2010/01/typos-in-chapters-6-7/	January 14, 2010	xi'an	Over the weekend and during the R exams, I managed to complete the solution set for Chapters 6 and 7 of “Introducing Monte Carlo Methods with R”. Chapter 6 only exhibited a few typos, despite me covering most exercises in Chapter 6, hence the merging of both chapters. – in Exercise 6.13, both  and  must use a double exponential proposal in the Metropolis-Hastings algorithm of question b, in Exercise 6.15, the  distribution should be a  normal distribution, – in Example 7.3, part of the code is wrong: it should be instead of (I frankly don’t understand where those curly brackets came from!) – in Example 7.6, I forgot to include the truncation probability  in the likelihood (!) and the notations are not completely coherent with Example 5.13 and 5.14 in that the x’s became y’s… – in Exercise 7.21, rtnorm is missing sigma as one of its arguments. – Exercise 7.23 has nothing wrong per se but it is rather a formal (mathematical) exercise – in Exercise 7.25 the  in question a should be  to avoid any confusion. 	 0 Comments
Quick thoughts on “R-Powered Web Apps”	https://www.r-bloggers.com/2010/01/quick-thoughts-on-r-powered-web-apps/	January 13, 2010	David Smith		 0 Comments
Example 7.22: the Knapsack problem	https://www.r-bloggers.com/2010/01/example-7-22-the-knapsack-problem/	January 13, 2010	Nick Horton		 0 Comments
Making Maps with R	https://www.r-bloggers.com/2010/01/making-maps-with-r/	January 13, 2010	James	***This tutorial has been updated, please click here*** I frequently use R (a free software environment for statistical computing and graphics) for data analysis.  As almost all my data are spatial it is often good to produce a map of the results without having to export the data into another GIS package. I am often asked how to do this so I have included here the code I used to create the map you see below. It should be quite straightforward to substitute my data with your own shapefile and alter some of the parameters such as the colour and the break points to produce your own map. For those interested in more advanced spatial analysis with R I recommend this book.  	 0 Comments
Rcpp 0.7.2	https://www.r-bloggers.com/2010/01/rcpp-0-7-2-2/	January 13, 2010	romain francois	"Rcpp 0.7.2 is out, checkout Dirk’s blog  for details selected highlights from this new version:  if one wants to mimic this R code in C Rcpp lets you express the same like this : or like this if you have GCC 4.4 :  or :  and it will get better with the next release, where you will be able to just call call.eval() and stats[""rnorm""]. Using the regular R API, you'd write these liks this :  or : "	 0 Comments
Academics’ Slang: Orthogonal	https://www.r-bloggers.com/2010/01/academics%e2%80%99-slang-orthogonal/	January 12, 2010	John Myles White	H. G. Wells famously said that, “statistical thinking will one day be as necessary for efficient citizenship as the ability to read and write.” I think we’re getting closer to that day: even the Supreme Court of the United States plans to start using the word ‘orthogonal’ colloquially. 	 0 Comments
A new twist on the identifier mapping problem	https://www.r-bloggers.com/2010/01/a-new-twist-on-the-identifier-mapping-problem/	January 11, 2010	nsaunders	Yesterday, Deepak wrote about BridgeDB, a software package to deal with the “identifier mapping problem”.  Put simply, biologists can name a biological entity in any way that they like, leading to multiple names for the same object.  Easily solved, you might think, by choosing one identifier and sticking to it, but that’s apparently way too much of a challenge. However, there are times when this situation is forced upon us.  Consider this code snippet, which uses the Bioconductor package GEOquery via the RSRuby library to retrieve a sample from the GEO database: All good so far.  What if I try to save the data table, which contains entries such as { “DETECTION.P.VALUE” => “0.000146581″ }, to my new favourite database, MongoDB? So what am I to do, other than modify the key using something like: Voilà, my own personal contribution to the identifier mapping problem. What’s the solution?  Here are some options – rank them in order of silliness if you like: 	 0 Comments
Typos in Chapter 5	https://www.r-bloggers.com/2010/01/typos-in-chapter-5/	January 11, 2010	xi'an	After writing the solutions to the odd-numbered exercises in Chapter 5 of “Introducing Monte Carlo Methods with R”., I alas found the following typos, two of which are rather major (Exercise 5.3 and Example 5.16). I apologise to the readers these typos may confuse. – Exercise 5.3 has no simple encompassing set and the constraint should be replaced by  – In Example 5.14, the sentence between parentheses should end up with “equal to 0″ (merci, Robin!) – Example 5.16 has a typo in that the EM sequence should be  – In Exercise 5.15, the Z’s in the formula should be in capital letters, namely  – In Exercise 5.17, the function  should be written  to be coherent with the notation of Example 5.14 – Exercise 5.21 should have been removed as it duplicates Exercise 5.11…but this redundancy  is not going to confuse anyone! As written earlier, this is a confirmation that I should have followed my own rule of “Always write the correction before the first printing”! 	 0 Comments
R exams	https://www.r-bloggers.com/2010/01/r-exams/	January 11, 2010	xi'an	As posted yesterday, today was the day of my Exploratory Statistics exam, turned into 3 R exams because of the lack of terminals for the students to work on. (We tried to encourage students to use their own laptop but less than twenty registered…) If you happen to be interested in those exams, they are available here, there, and there. The students were supposed to download their .Rhistory file or something more elaborate onto a web depository and the usual disasters stroke, from students unable to start an R session to those erasing their history just before dowloading and everything in between. Eventually, they all got their files secured and most left whinning that it was tooo haaard… Serves them well, ha! 	 0 Comments
Bay Area User Group: R-Powered Web Apps	https://www.r-bloggers.com/2010/01/bay-area-user-group-r-powered-web-apps/	January 11, 2010	David Smith	For those in the San Francisco area, tomorrow night’s Bay Area R User Group meeting — to be held at Twitter’s HQ — is a must-see. The theme is “R-Powered Web Apps” and features guest speakers Jeroen Ooms and Jeff Horner. (Disclosure: REvolution Computing is sponsoring Jeroen’s appearance at this event.) We’ve featured Jeroen’s awesome web-based applications using R here on the blog before, and tomorrow night he’ll show how to create similar web-based applications using R. Jeff Horner will be talking about RApache and how to run R code within a web application. For more details and to register, follow the link below. Bay Area R User Group: R-Powered Web Apps, Tuesday Jan 12 	 0 Comments
ggplot2 Tutorial: Scatterplots in a Series of Small Multiples	https://www.r-bloggers.com/2010/01/ggplot2-tutorial-scatterplots-in-a-series-of-small-multiples/	January 11, 2010	Stephen Turner		 0 Comments
Lattice: Multivariate Data Visualization with R	https://www.r-bloggers.com/2010/01/lattice-multivariate-data-visualization-with-r/	January 11, 2010	James	I have just reviewed Sarkar‘s Lattice: Multivariate Data Visualization with R for the Journal of the Royal Statistical Society Series A.  I would highly recommend the book to all R users who wish to produce publication quality graphics using the software. You can read the full review here.   	 0 Comments
Progress bars in R (part II) – a wrapper for apply functions	https://www.r-bloggers.com/2010/01/progress-bars-in-r-part-ii-a-wrapper-for-apply-functions/	January 10, 2010	markheckmann	In a previous post I gave some examples of how to make a progress bar in R. In the examples the bars were created within loops. Very often though I have situations where I would like have a progress bar when using apply(). The plyr package provides several apply-like functions also including progress bars, so one could have a look here and use a plyr function instead of apply if possible. Anyway, here comes a wrapper for apply, lapply and sapply that has a progressbar. It seems to work although one known issue is the use of vectors (like c(1,2)with the MARGIN argument in apply_pb(). Also you can see in the performance comparison below that the wrapper causes overhead to a considerable extent, which is the main drawback of this approach.  Nice up to now, but now let’s see what the difference in performance due to the wrapper overhead looks like. So, what we see is that performance radically goes down. This is extremely problematic in our context as one will tend to use progress bars in situations where processing times are already quite long. So if someone has an improvement for that I would be glad to hear about it. Latest version with more comments on github. 	 0 Comments
LSPM with snow	https://www.r-bloggers.com/2010/01/lspm-with-snow/	January 10, 2010	Joshua Ulrich	"
 "	 0 Comments
Outlawing Gay Marriage	https://www.r-bloggers.com/2010/01/outlawing-gay-marriage/	January 10, 2010	John Myles White	"Given the recent votes on same-sex marriage in New Jersey and Portugal, I wanted to test a seemingly innocuous claim that touches upon very broad issues in political theory: does the degree of directness of a “democratic” vote predict whether the vote will promote or prohibit same-sex marriage? Naively, it seemed clear to me that this was true: every single direct vote in my memory has outlawed same-sex marriage, while the only decisions that have allowed same-sex marriage have originated among elected representatives or unelected judges. I decided to compile some rough data to test this idea using Wikipedia’s articles on the topic. I took what seemed to be the non-redundant decisions since the 1990′s from the following three articles: And I arrived at this table:
  You can find a CSV file with this data at the GitHub repository where I’ve stored all of the analyses I’ve completed so far. I’d love for people to add more data or contribute some visualizations of the patterns I’m pointing out. And, as always, I’d love to hear criticisms or suggestions about my approach. The summary statistics provide pretty clear support for my intuition, though they’re rarely statistically significant, given the small sample sizes: For me the conclusion to be drawn from all of this is not that our respect for the will of the majority should compel us to prohibit same-sex marriage, but rather that we should be more hostile to direct democracy, because it is a powerful force for promoting intolerance in American society. I imagine this will sound un-American to many readers, but I think it’s quite clear that this sentiment was foundational for the American experiment in government. You see this distrust of direct democracy repeatedly reiterated in the Federalist Papers, especially in Federalist Number 10. Indeed, one can argue that the major purpose of our Constitution is to limit the ability of direct democracy to harm minorities within our society. "	 0 Comments
Pivot tables in R	https://www.r-bloggers.com/2010/01/pivot-tables-in-r/	January 9, 2010	Chris	A common data-munging operation is to compute cross tabulations of measurements by categories. SQL Server and Excel have a nice feature called pivot tables for this purpose. Here we’ll figure out how to do pivot operations in R. Let’s imagine an experiment where we’re measuring the gene activity of an organism under different conditions — exposure to different nutrients and toxins. Our conditions are silly: copper, beer, pizza, and cheetos. First we make a list of genes. Then expand.grid generates all combinations of genes and conditions. Finally, we tack on a column of randomly generated measurements. We want to pivot the conditions into columns so that we end up with one column for each condition and one row for each gene. The easy way is to use the reshape package by Hadley Wickham, which is made for restructuring data and does this job nicely. If you don’t already have it, you’ll have to run install.packages, then load the library. Using cast to move conditions into columns is a snap. Done! That was too easy Just as an exercise, what would we have to do without reshape? And, just to keep ourselves honest, let’s make sure we can deal with missing data (as reshape can). Make some data go missing: Now, split the data frame up by condition. This produces a list where each element is a data frame containing a subset of the data for each condition. Notice that the cheetos data frame has values for 8 of the 10 genes. We’re going to recombine the data into a data frame with one row for each gene, so let’s get that started: Now comes some executable line noise. We’re going to loop through the list and add a column to the result data frame during each iteration of the loop. We pull the column out of the data frame in the list, but we have to make sure the column has an element for each gene. Merging with the all parameter set is like an outer join. We get a row for each gene, inserting NA’s where there data is missing. Extra finesse points if you can figure out how to do that last step with Reduce instead of a loop. 	 0 Comments
Typos in Chapter 4	https://www.r-bloggers.com/2010/01/typos-in-chapter-4/	January 9, 2010	xi'an	Chapter 4 of “Introducing Monte Carlo Methods with R” has four typos (so far) in the exercises: – In Exercise 4.5, the  should not be in bold fonts (!) – In Exercise 4.9, I commented too many lines when revising and thus the variance terms vanished. It should read   – In Exercise 4.13, following the removal of one exercise, Exercise 4.2 should read Exercise 4.1 – In Exercise 4.15,  should be  (as in Problem 4.5 of Monte Carlo Statistical Methods) 	 0 Comments
sequential ideal point estimates	https://www.r-bloggers.com/2010/01/sequential-ideal-point-estimates/	January 9, 2010	jackman	"Out of curiosity, I produced a “sequential” set of ideal point estimate for the (current) 111th U.S. Senate, plotting the results in the graph attached below (click on the thumbnail); as is conventional, red is Republican and blue is Democratic.  The analysis uses all 373 non-unanimous roll calls in the 111th Senate thus far.
 Each senator starts with a prior centered at zero.  Each roll call induces a partitioning of the senators, and a branching in the trajectories of the estimated ideal points, as the space of voting profiles gets richer.  We also see the roll calls tending to discriminate among the Republicans more rapidly than the Democrats (the latter being the majority party in the 111th Senate).  It is rather striking how quickly we recover a reasonably stable rank-ordering of the senators, at least  in 1d in a Congress like that 111th with strong/stable separation along party lines.  Also of interest is the Specter party switch (for a short while the voting history of Specter as a “D” wasn’t especially “D”).  Note also that we don’t know much about extremist legislators; until we get votes that have cut-points close to their ideal points, the roll calls just aren’t revealing much about their preferences.  Hence we see the trajectories of ideal point estimates of the most liberal senators wavering around a little bit. I used the doMC package in R to do this on a 4 core machine.  Each set of ideal point estimates are based on 4 parallel chains (one on each core), generated using ideal in pscl.  The “ideal point estimates” here are Monte Carlo estimates of the mean of the marginal posterior density of each respective ideal points.  I used 4 chains (in parallel) 150K iterations each, for each of the 373 non-unanimous roll calls. Even with this many iterations there is a little bit of Monte Carlo error; i.e., with the same priors, and the same voting histories, ideal point estimates should coincide, modulo MC error. The other thing is that there doesn’t seem to be any obvious “vote 1″ update for ideal points.  That is, there is no simple mapping from the ideal point estimate based on m roll call to ideal point estimates based on m+1 roll calls. You have to start the fitting algorithm from scratch each time (and hence the appeal of exploiting multiple cores etc), although the results from the previous run giving pretty good start values. "	 0 Comments
Killing Yourself: An Addendum	https://www.r-bloggers.com/2010/01/killing-yourself-an-addendum/	January 9, 2010	John Myles White	In further support of the claim that a lot of deaths are partly self-induced, here’s a fascinating piece by Wired on the extraordinary rise in the percent of deaths among the young caused by their own poor decisions. It’s remarkable that, for the young, modern science has already made the world so safe that humanity, rather than nature, is now responsible for a majority of its own suffering. 	 0 Comments
Because it’s Friday: Gravity Wells	https://www.r-bloggers.com/2010/01/because-its-friday-gravity-wells/	January 8, 2010	David Smith	It’s a little strange to see a web comic come up with such interesting visualizations, but xkcd has followed up on their movie timelines charts with this illustration of the gravitational attraction of the various bodies in the solar system. The gravitational force at the surface of the planet or moon determines how high you’d need to jump in order to be launched into orbit (or, more realistically, how fast you’d have to launch a projectile for it to escape the planet into space). Physicists refer to this energy required to escape as the “gravity well” of a body and xkcd represents this by … the depth of an actual well, scaled to Earth standards. Ingenious! Be sure to click the image for the fullsize version.   xkcd: Gravity Wells (via Flowing Data) 	 0 Comments
External pointers with Rcpp	https://www.r-bloggers.com/2010/01/external-pointers-with-rcpp/	January 8, 2010	romain francois	One of the new features of Rcpp is the XPtr class template, which lets you treat an R external pointer as a regular pointer. For more information on external pointers, see Writing R extensions.  To use them, first we need a pointer to some C++ data structure, we’ll use a pointer to a vector<int> :  Then, using the XPtr template class we wrap the pointer in an R external pointer The first parameter of the constructor is the actual (sometimes called dumb) pointer, and the second parameter is a flag indicating that we need to register a delete finalizer with the external pointer. When the external pointer goes out of scope, it becomes subject to garbage collection, and when it is garbage collected, the finalizer is called, which then calls delete on the dumb pointer.  Wrapping it all together thanks to the inline package, here’s a function that creates an external pointer to a vector and return it to R At that point, xp is an external pointer object Then, we can pass it back to the C(++) layer, an continue to work with the wrapped stl vector of ints. For this we use the other constructor for the XPtr class template, that takes an R object (SEXP) of sexp type EXTPTRSXP.  Again, we can wrap this up for quick prototyping using the inline package : The example is extracted from one unit tests that we use in Rcpp, see the full example :  See also the announcement for the release of Rcpp 0.7.1 here to get a list of new features, or wait a few days to see version 0.7.2.  Using the XPtr class template is the bread and butter of the CPP package I blogged about here 	 0 Comments
Example 7.21: Write a function to simulate categorical data	https://www.r-bloggers.com/2010/01/example-7-21-write-a-function-to-simulate-categorical-data/	January 8, 2010	Ken Kleinman		 0 Comments
Learning R via Python…or the other way around (with video!)	https://www.r-bloggers.com/2010/01/learning-r-via-python%e2%80%a6or-the-other-way-around-with-video/	January 8, 2010	Drew Conway	Last night I was one of five speakers at the NYC R Statistical Programming Meetup.  The topic last night was dubbed the R Rosetta Stone, and the intent was to show how R as a language translated into several other analytical platforms and programming languages.  Other speakers covered MATLAB, SAS, SQL/Postges and Clojure/Incanter—I discussed Python. Soon, videos of the talks will be posted online, but until then you can download my talk, “Learning R via Python (or the other way around)” here.  Also, the slides from all other talks are available at the meetup website.  Enjoy! UPDATE: The video of my talk is now available, and can be viewed after the jump.  Due to upload throttling at VCASMO, my talk is the only one currently online, but I will be adding the others as bandwidth becomes available. UPDATE II: Additional videos from Josh Reich’s talk “Data munging with R and SQL“, and Harlan Harris’s on “MATLAB/R Dictionary” are now available for your viewing pleasure.  	 0 Comments
R Tutorial Series: ANOVA Tables	https://www.r-bloggers.com/2010/01/r-tutorial-series-anova-tables/	January 8, 2010	John M. Quick	The commonly applied analysis of variance procedure, or ANOVA, is a breeze to conduct in R. This tutorial will explore how R can be used to perform ANOVA to analyze a single regression model and to compare multiple models. Before we begin, you may want to download the sample data (.csv) used in this tutorial. Be sure to right-click and save the file to your R working directory. This dataset contains information used to estimate undergraduate enrollment at the University of New Mexico (Office of Institutional Research, 1990). Note that all code samples in this tutorial assume that these data have already been read into an R variable and have been attached. Prior to running ANOVA, we need to have one or more regression models. In the segments on simple linear regression and multiple linear regression, we created a series of models using one, two, and three predictors to estimate the fall undergraduate enrollment at the University of New Mexico. The complete code used to derive these models is provided in their respective tutorials. This article assumes that you are familiar with these models and how they were created. Therefore, a shorthand method for generating the models is displayed below. In R, the anova(MODEL) function can be used to run ANOVA, where MODEL is the variable containing the model to be analyzed. The output of the anova(MODEL) function is a standard ANOVA table. An example of how to use the anova(MODEL) function is demonstrated below. The output of the preceding function is pictured below. A similar procedure could be followed to produce ANOVA tables for the one and three predictor models.  ANOVA can also be used to compare successive models. The following code demonstrates how to do this using the anova(MODEL1, MODEL2, … MODELi) function, where MODEL1, MODEL2, etc. are all model variables. The output of the preceding function is pictured below. These results give us one context in which to compare the models.  To see a complete example of how ANOVA tables can be generated in R, please download the ANOVA tables example (.txt) file. Office of Institutional Research (1990). Enrollment Forecast [Data File]. Retrieved November 22, 2009 from http://lib.stat.cmu.edu/DASL/Datafiles/enrolldat.html 	 0 Comments
Codecogs – Open-Source library of numerical components	https://www.r-bloggers.com/2010/01/codecogs-%e2%80%93-open-source-library-of-numerical-components/	January 8, 2010	Ralph	The Codecogs website provides an Open-source library of functions for numerical analysis. One interesting component available on the website is the LaTeX equation editor which can be used to create graphics files of equations to include on webpages. The webpage describe this component as a A web-based LaTeX equation editor that generates graphical images and HTML code to embed any equation into any website, forum or blog. This editor could be used in conjunction with powerpoint to include equations that look more professional than using the built in equation editor. 	 0 Comments
Review of ‘Computational Statistics: An Introduction to R’ in JSS	https://www.r-bloggers.com/2010/01/review-of-computational-statistics-an-introduction-to-r-in-jss/	January 7, 2010	Thinking inside the box		 0 Comments
Data Visualization and R Programming Books (Updated)	https://www.r-bloggers.com/2010/01/data-visualization-and-r-programming-books-updated/	January 7, 2010	Ed Borasky		 0 Comments
Samples per series/dataset in the NCBI GEO database	https://www.r-bloggers.com/2010/01/samples-per-seriesdataset-in-the-ncbi-geo-database/	January 7, 2010	nsaunders	"Andrew asks: 
I want to get an NCBI GEO report showing the number of samples per series or data set. Short of downloading all of GEO, anyone know how to do this? Is there a table of just metadata hidden somewhere?
 At work, we joke that GEO is the only database where data goes in, but it won’t come out.  However, there is an alternative:  the GEOmetadb package, available from Bioconductor. The R code first, then some explanation: We install GEOmetadb (lines 2-4), then download and unpack the SQLite database (line 7).  This generates the file ~/GEOmetadb.sqlite, which is currently a little over 1 GB. Next, we connect to the database via RSQLite (lines 7-8).  The gds table contains GDS dataset accession and sample count, so extracting that information is very easy (line 11). GSE series are a little different.  The gsm table contains GSM sample accession and GSE series accession (in the series_id field).  We can count up the samples per series using table(), on line 22.  However, this generates some odd-looking results, such as: Fear not.  In this case, GSE10026 is a super-series comprised from the series GSE10011 (45 samples), GSE9973 (9 samples), GSE9975 (36 samples) and GSE9977 (24 samples), total = 114 samples. "	 0 Comments
Typos in Chapter 3	https://www.r-bloggers.com/2010/01/typos-in-chapter-3/	January 7, 2010	xi'an	"Here are two more typos in the exercises of Chapter 3 of “Introducing Monte Carlo Methods with R”. – due to the (later) inclusion of an extra-exercise in the book, the “above exercise” in Exercise 3.5 actually means Exercise 3.3. – in Exercise 3.11, question c, a line got commented by mistake in the LaTeX file and it should read “Explore the gain in efficiency from this method. Take a = 4.5 in part (a) and run an experiment to determine how many normal  random variables would be needed to calculate P(Z > 4.5) to the same accuracy obtained from using 100 random variables in this importance sampler.” – in Exercise 3.17, George Casella found that in question b it should be , not . I have also received the following email from Australia Dear Professor Robert
I have just received (courtesy of Amazon) a copy of your book, as above.
I am wondering if / when any “solutions” to the book will be available.
I am neither a student nor an instructor, but a full time clinician who has an interest in Bayesian statistics and would value appropriate instruction from the solutions to the exercises.
Much obliged for any advice on this
Best wishes which keenly shows why the solution manual must be produced as quickly as possible! "	 0 Comments
R’s exponential package growth, ctd.	https://www.r-bloggers.com/2010/01/rs-exponential-package-growth-ctd/	January 7, 2010	David Smith	Reader SK has collected the most recent data on R’s package growth, through the latest 2.10 release. The three most recent releases fall slightly below the exponential growth line, which isn’t altogether surprising (that’s a lot of growth to sustain!). Another interesting thing to look at would be the combined rate of new packages submitted to CRAN and packages updated on CRAN: as R packages begin to cover the domain space of possible applications I’d expect updates to be where much of the package activity lies.  For anyone interested, the raw data (as calculated by SK) is available after the jump. 	 0 Comments
Survey question biases and crowdsourcing	https://www.r-bloggers.com/2010/01/survey-question-biases-and-crowdsourcing/	January 7, 2010	David Smith	It’s common knowledge that the way you ask a question in a survey can bias the results you get. (It’s been a staple of political pollsters since the dawn of time.) But Aaron Shaw from Dolores Labs has used an interesting technique to demonstrate that bias: crowdsourcing. He asked the same question of crowdsourced respondents assigned randomly to one of two groups, and offered a different way for each group to respond:  Q: About how many hours do you spend online per day? Group 1 selected from these responses: (a) 0 – 1 hour (b) 1 – 2 hours (c) 2 – 3 hours (d) More than 3 hours Group 2 selected from these responses: (a) 0 – 3 hours (b) 3 – 6 hours (c) 6 – 9 hours (d) More than 9 hours Each set of answers covers the entire range of possible hours in a day, just grouping them into different buckets. In theory, you can estimate the true underlying distribution from either set of responses. Since the groups were selected randomly, the underlying distributions for each group should be the same. With some analysis in R, though, Aaron discovers that’s not the case. See the link below for the details.  Dolores Labs: Ask a Stupid Question 	 0 Comments
subsetting a matrix/array in R	https://www.r-bloggers.com/2010/01/subsetting-a-matrixarray-in-r/	January 7, 2010	jackman	"From the R-devel svn log.  Nice addition… r50896 | falcon | 2010-01-05 12:05:31 -0800 (Tue, 05 Jan 2010) | 7 lines
Changed paths:
   M /trunk/NEWS
   M /trunk/src/include/Defn.h
   M /trunk/src/library/base/man/Extract.Rd
   M /trunk/src/main/subassign.c
   M /trunk/src/main/subscript.c
   M /trunk/src/main/subset.c
   M /trunk/tests/Makefile.common
   A /trunk/tests/array-subset.R Allow n-dim arrays to be subsetted by an n-column character matrix The character matrix is converted to the corresponding integer matrix
by matching against the dimnames of the array.  NA values in any row
of the character matrix are propagated to the result.  Unmatched
values result in a subscript out of bounds error.  Empty string “” is
not allowed to match and therefore always results in an error. I wonder about an option controlling the behavior of non-matches? "	 0 Comments
Scatter plot with 4 axes labels and grid	https://www.r-bloggers.com/2010/01/scatter-plot-with-4-axes-labels-and-grid/	January 7, 2010	Paolo Sonego		 0 Comments
R Journal, Volume 1/2, December 2009	https://www.r-bloggers.com/2010/01/r-journal-volume-12-december-2009/	January 7, 2010	romain francois	The issue 1/2 of the R Journal has been published. It features an article that I co-authored with Spencer Graves and Sundar Dorai-Raj about the sospackage.  	 0 Comments
3d scatter plot using R	https://www.r-bloggers.com/2010/01/3d-scatter-plot-using-r/	January 6, 2010	er	" The other day I saw a three dimensional scatterplot in Montgomery’s Regression book. I wanted to redraw the graph using the provided data. A simple google search revealed that there is a package called scatterplot3d. The scatterplot3d() can be used to draw a 3-dimensional scatter plot. Here is what the steps are: Download and install the package from your nearest CRAN.
Load the package using the command library(scatterplot3d) Use the attached file to run the following code in R  RESULT
 DATA
Data: Copy and paste in a file  and save it as 65-555-reg.txt
 "	 0 Comments
Programming a custom Backtest Profile in R	https://www.r-bloggers.com/2010/01/programming-a-custom-backtest-profile-in-r/	January 6, 2010	Milk Trader		 0 Comments
RInside release 0.2.1	https://www.r-bloggers.com/2010/01/rinside-release-0-2-1/	January 6, 2010	Thinking inside the box	"
This is a maintenance release building on the recent
0.2.0 release
which added Windows support (provided you use the Rtools toolchain for
Windows).  In this release, we changed the startup initialization so that
interactive() comes out FALSE (just as we had done for
littler just yesterday)
and with that no longer call Rf_KillAllDevices() from the destructor as we may not have
had devices in the first place.  A few minor things were tweaked around the
code organisation and build process, see the ChangeLog for details.

 
The new release should hit CRAN mirrors tomorrow, and is (as always) available 
from my machine too.

 "	 0 Comments
New Features in ggplot2 0.8.5	https://www.r-bloggers.com/2010/01/new-features-in-ggplot2-0-8-5/	January 6, 2010	Stephen Turner		 0 Comments
Earthquake maps	https://www.r-bloggers.com/2010/01/earthquake-maps/	January 6, 2010	Samuel Brown		 0 Comments
New Features in ggplot2 version 0.8.5	https://www.r-bloggers.com/2010/01/new-features-in-ggplot2-version-0-8-5/	January 6, 2010	learnr	"Just before Christmas ggplot2 version 0.8.5 was released, closely following the release of version 0.8.4 a week or so earlier. Whilst both versions included included numerous bugfixes (25 in 0.8.4 and 17 in 0.8.5), the latest version also incorporated some new features. As ggplot2 is all about graphical display, so I went through the list of new features and below is a visual example of each new feature, plotted most often utilising the code examples included in the respective bugtracker issues.  
 1)geom_text gains parse argument which makes it possible to display expressions 2) all scales now have legend parameter, which defaults to TRUE. Setting to false will prevent that scale from contributing to the legend. In previous version the legend of the above plot looked like this and there was now way to change this. 3) default axis labels and legend titles are now stored in the options, instead of in each scale. This allows to specify them using the opts call. The other options to set labels exist as previously: 4) coord_equal: when ratio = NULL (the default), it will adjust the aspect ratio of the plot, rather than trying to extend the shortest axis. This is what the default looked like in the previous version, and it was impossible to generate the plot above. 5)  x and y positions can be set to Inf or -Inf to refer to the top/right and bottom/left extents of the panel. This is useful for annotations, and for geom_rect, geom_vline and geom_hline. 6)expand_limits(): a new function to make it easy to force the inclusion of any set of values in the limits of any aesthetic. is the equivalent of It would be good if similar functionality could be extended to xlim, so that the following would work. This way the setting of limits would be encapsulated in one function (xlim/ylim). "	 0 Comments
The number of R packages is growing exponentially	https://www.r-bloggers.com/2010/01/the-number-of-r-packages-is-growing-exponentially/	January 6, 2010	David Smith	The second issue of the R Journal is out now, and in addition to a bevy of contributed articles and some news from the R Core Group on the new help system introduced in R 2.10, there’s an invited section called, intriguingly, “The Future of R”. In that section John Fox provides an exhaustively researched and insightful review of the history of the R project, its social structure as an open-source project, and an analysis of why the project succeeded and its prospects for the future. A recommended read. One tiny nugget that caught my eye though, is quantitative evidence for a claim that I’ve made anecdotally for a long time: R is growing exponentially. The proof is in the packages: the chart below shows the number of packages on CRAN for every release since 2001. The Y axis is the number of packages (on a log scale); the X axis is time (annotated by the version number).     Exponential growth. It’s tailed off very slightly for the last two releases shown (and 2.10 isn’t included, having just been released), but it’s a clear indicator of the growth of the R community that creates and submits the wealth of packages on CRAN. (Update Jan 7: An updated chart is available.) The R Journal: Aspects of the Social Organization and Trajectory of the R Project 	 0 Comments
Latin squares design in R	https://www.r-bloggers.com/2010/01/latin-squares-design-in-r/	January 6, 2010	Todos Logos		 0 Comments
littler 0.1.3	https://www.r-bloggers.com/2010/01/littler-0-1-3/	January 5, 2010	Thinking inside the box	"
littler provides r
(pronounced littler), a shebang / scripting / quick eval / pipelining
front-end to the the R language and system.

 
This version adds a few minor behind-the-scenes improvements:
 

As usual, our code is available via our
svn archive or from tarballs off my littler page and the
local directory here. A fresh package
is in Debian’s incoming queue and will hit mirrors shortly. 



 "	 0 Comments
Typos in Chapter 2	https://www.r-bloggers.com/2010/01/typos-in-chapter-2/	January 5, 2010	xi'an	When grading homeworks for my Monte Carlo graduate class, I found that my students had pointed out two typos in the exercises of Chapter 2 of “Introducing Monte Carlo Methods with R”. – In Exercise 2.17, question d. should be “d. Show that the maximum of  is attained at .“ – In Exercise 2.21, in item (ii),  should be replaced by  and question b. should be removed. I am currently working on the solution manual. It should have been ready when the book came out (if only because it is an efficient way to chase typos down), but I lazily waited for those homeworks to be handed back to reduce my workload… So keep posted for further typos in Chapters 3, 6 and 7, since my students only covered those chapters. 	 0 Comments
Transitions and transversions in R	https://www.r-bloggers.com/2010/01/transitions-and-transversions-in-r/	January 5, 2010	Samuel Brown		 0 Comments
Setting the HTML title tag in SAS ODS (the right way)	https://www.r-bloggers.com/2010/01/setting-the-html-title-tag-in-sas-ods-the-right-way/	January 5, 2010	heuristicandrew	In our department and various places on the Intertubes, SAS programmers set the HTML title tag (which sets the title in web browsers and on search engines) in ODS using the headtext option: This may work in some situations, but it’s ugly and wrong.   To see why, view the raw HTML: Each HTML page should only have one TITLE tag, but this method creates an additional tag instead of replacing the first.  Some web browsers and spiders may use the second tag instead of the first, and the document won’t validate as HTML (not that you can fix SAS’s HTML completely).  If your document is only on an intranet think of personal and corporate document indexing systems (Google Search Appliance, Google Desktop Search, Windows Search, etc.), and even if it works now, you want to future-proof your HTML reports when (not if) new technology comes in to the mix. I discovered the correct way thanks to Chevell Parker: This was tested on SAS 9.1.3 SP4 on Windows XP SP3. 	 0 Comments
Announcing r-ORM: A Pure R Object-Relational Mapper	https://www.r-bloggers.com/2010/01/announcing-r-orm-a-pure-r-object-relational-mapper/	January 5, 2010	John Myles White	My apologies for the long break between posts. Before the end of this week I’ll return to my series of posts on image processing in R. In the intervening time, I’ve finished a piece of code that I’d like to officially release to the public. The code in question is a very minimal object-relational mapper written entirely in R. As of today, you can find the code on GitHub  here. If you’re not familiar with using an ORM, I’d suggest reading a bit about how to use ActiveRecord, the ORM I’ve tried to emulate. As it stands, the code I have is able to connect to a MySQL database and extract information about a specified table using MySQL’s SHOW COLUMNS. The code then automatically builds up R code that will map the rows of the table onto R objects. This code can subsequently be eval’d or cat’d to a file. Let me note right off the bat that the code I’ve written is fairly heinous stylistically: because my understanding of R metaprogramming is rather limited, I’ve produced what amounts to the ugliest sort of code generation tool. If you check out the source in orm.R, you’ll quickly see what I mean: the output R code is generated as a string using a long series of paste() operations, many of which embed paste() operations inside of themselves. In the future, I would like to rewrite the code in a clearer fashion, possibly constructing expressions and parse trees directly, rather than using an intermediate layer of code as string. That said, I want to release the code so that I can get feedback on my approach. To help you give me feedback, here’s a quick walkthrough of how’d you use the current code: (1) Download the code from its GitHub home. Place the files you get into a directory of your choice. Let’s assume you use ~/r-ORM. (2) Install the R YAML and MySQL packages if you don’t already have them on your system: (3) Create a test database in MySQL called `sample_database`: (4) Give permissions on this test database to `sample_user`: (5) Create a test table called `users` in `sample_database`: (6) Edit the database.yml file if you didn’t use the database name, user name or password I just suggested. You’ll also need to edit it if you’re not working on localhost. (7) Open up an R interpreter. Set your working directory and source orm.R: (8) Build code for our user objects using orm.build.model('user'): At this point, you can review the code that’s been generated to see whether it should work on your system. If it will, you can eval it now: With that done, you should have a working set of functions that handle creating, finding, manipulating and deleting R objects that are serialized to the database. To test out the resulting model, let’s start by creating a user object. In general, an object of class foo will be created using an auto-generated function called create.foo: Calling this functions builds a user object in memory. Nothing is in the database so far. To see the object, type user at the command line: Now you can edit this user to make it a real piece of data. The columns of the database table are mapped onto object attributes with appropriate getter and setter methods, like so: Once the user object is worth keeping, we store it in the database using store: This stores the user object in the database. To get the ID’s edited correctly, you have to perform the assignment as indicated above. In the future I may change this. After storing something, you might want to retrieve it later. Since we know that we just created the first user object, we can get it again by using a find.user call: In general, you can find objects of class foo by calling find.foo(). If you provide an integer as an input, you’ll get the object with that ID. If you provide the string 'all', you’ll get a list containing all of the objects in your database. Other inputs produce an error. You can see that we got the correct object using the getter methods: We can edit it again to see that updating rows of the database works as expected: Finally, now that we’re done with it, we can delete it from the database: That, in a nutshell, is the use of this ORM solution. If you have any questions, please let me know: I’ll be happy to answer them. I should note that there are many weaknesses of functionality with the current code. Notably: That said, I find the system usable for my current needs. I hope it will help you, and I really hope that you’ll consider making suggestions for hoping to improve the code. Patches would be especially welcome. I would also love feedback about the interface that’s shown to the end user of this code. Here are some outstanding questions I’d like to hear from people about: 	 0 Comments
Analyzing a FriendFeed group with Ruby and R	https://www.r-bloggers.com/2010/01/analyzing-a-friendfeed-group-with-ruby-and-r/	January 5, 2010	David Smith	FriendFeed is a social media service, where groups of people can post interesting information from the Web, and “like” or comment posts from others. Statistical Bioinformatician Neil Saunders is a member of the “Life Scientists” group, and has posted an analysis of the group’s activity in 2009 to his blog. He used Ruby and the FriendFeed API to extract the data (group members, posts, comments, and “like”s), and then used R to analyze and visualize the data. For example, here’s a look at the daily traffic in posts, comments, and likes represented as a calendar heat map.   You can see all of the Ruby and R code used to implement this and other analyses (What makes a popular post? Is there a relationship between “like” and comment activity?) at the link below. What You’re Doing Is Rather Desperate: The Life Scientists at FriendFeed: 2009 summary 	 0 Comments
R Journal 1/2	https://www.r-bloggers.com/2010/01/r-journal-12/	January 5, 2010	Paolo Sonego		 0 Comments
R: Memory usage statistics by variable	https://www.r-bloggers.com/2010/01/r-memory-usage-statistics-by-variable/	January 4, 2010	heuristicandrew		 0 Comments
Example 7.20: Simulate categorical data	https://www.r-bloggers.com/2010/01/example-7-20-simulate-categorical-data/	January 4, 2010	Ken Kleinman		 0 Comments
O’Reilly’s R is a Harpy Eagle	https://www.r-bloggers.com/2010/01/oreillys-r-is-a-harpy-eagle/	January 4, 2010	David Smith	Today marks the hardcopy availability of the first book dedicated to R from O’Reilly, R in a Nutshell. In the familiar O’Reilly style, the cover is adorned with an illustration of an animal, in this case a harpy eagle:   The book is written by Joe Adler, a data analyst and the author of Baseball Hacks. In contrast to the many books that exist today that regard R through the prism of Statistics, Joe takes a much more general approach, treating R primarily as a language. (Disclaimer: I reviewed the book for O’Reilly, and a quote from me appears on the cover.) It’s a great introduction for newcomers to the language, but also goes into enough depth that even longtime R users will find useful nuggets of information. O’Reilly Media: R in a Nutshell 	 0 Comments
Welcome!	https://www.r-bloggers.com/2010/01/welcome/	January 4, 2010	Ryan	Welcome to my new blog, Byte Mining! Data is all around us, all the time. It flows in from places you would least expect it, and more times that not, it remains in its original form untouched by human and machine. When data simply flows in and out of our lives, we miss out on the story that it tells us, and the clues that it provides to help solve our mysteries. We humans are becoming more and more astute to the data that we exude and how we release it. There are two side effects to this – concerns for privacy is one. The second is that people are aware that their data is out there for consumption. In other words, people are no longer astonished when they realize how much of their data is readily available either publicly or to particular entities. This is a weight off of my back because I no longer get weird looks when I mention some tidbit of information I learned about someone to them on Facebook or MySpace. No, I am not a stalker, I was just more appreciative, or aware, of data ubiquity much quicker than most people my age, and was able to put together a picture of a person by the details they so readily provided. Of course, once people realized how much data they were releasing, privacy controls became much more refined. There are many hats in the data-to-information conversion business. There are the analysts that take data and, well, analyze it using statistical methods. They may be called data analysts, or statisticians depending on the extent of their analysis and their qualifications. Until recently, analysts lived in a rectangular world. All of their data consisted of columns of data called fields, with observations structured as rows containing the same fields (usually), delimited by some character like the comma (,), tab or space, or my favorite, the pipe (|). Rectangular datasets do not grow on trees. Someone, or something, somewhere, put that data into a rectangle. Usually this is a restriction imposed by a database, and the rest is history. Other times, a person has converted some crazy chaotic data into the rectangle. I will get to this next in a bit. The point to take home: Analysts take data and tell a story with it. There is a the pseudo-hat of a modeler. These people are very important, but I really don’t know where to put them. I guess you can say they are a cross between an analyst and a miner. They are sort of like philosophers, or psychics. Modelers take a bunch of data and answer the question, “what does this say about you?” Another hat is that of the visualizer. These folks are kind of similar to analysts, and they may be analysts, but there is one major difference. Rather than focus on describing what the data is saying in words, they create lots of pretty pictures whose goal is to excite the consumer and engage him/her with the data. Visualizers take data and make it sing. Finally, there is the miner hat. In my opinion, data miners make the field go round – no longer is the world a square (see what I did there?). Data miners stick their hands in the air and reach for the data as it flies by. They grab it, give it some soul and present it to the consumer. The consumer may be an analyst, a visualizer, another miner (probably a programmer) or in some cases, Joe Schmoe. Miners extract data by using APIs or scraping and then parse the output to turn it into data, using text mining and regular expression magic. The miner may then re-broadcast this data in some other form for consumption. Miners stick their hands in the air and reach for the data as it flies by. They grab it, give it some soul and present it to the consumer. So, which one are you? It should be no surprise that I consider myself a miner. Next, by trade, I am an analyst and modeler. I do not consider myself a visualizer by any means, but I definitely appreciate and keep up with those individuals. I am sure I will write about visualization every once in a while. So where do statistics and computer science come in? Loosely speaking, I believe that a computer scientist takes bytes and turns it into data, and a statistician takes data and turns it into information. There is a lot of blurring between these two fields however. There are many computer scientists that now turn a lot of data into information as well. One would find many computer scientists as miners, and maybe some as visualizers. It has been my experience that very few are pure analysts, but a good number are modelers. On the other hand, one would find statisticians in all of these fields, but more so as analysts, visualizers and modelers and less so as miners. 	 0 Comments
Soical Network Analysis in R	https://www.r-bloggers.com/2010/01/soical-network-analysis-in-r/	January 3, 2010	VCASMO - drewconway		 0 Comments
directlabels: Adding direct labels to ggplot2 and lattice plots	https://www.r-bloggers.com/2010/01/directlabels-adding-direct-labels-to-ggplot2-and-lattice-plots/	January 3, 2010	learnr	Sometimes it is preferable to label data series instead of using a legend. This post demonstrates one way of using labels instead of legend in a ggplot2 plot. The addition of labels requires manual calculation of the label positions which are then passed on to geom_text(). If one wanted to move the labels around, the code would need manual adjustment – label positions need to be recalculated.. This problem is easily solved with the help of directlabels package by Toby Dylan Hocking that “is an attempt to make direct labeling a reality in everyday statistical practice by making available a body of useful functions that make direct labeling of common plots easy to do with high-level plotting systems such as lattice and ggplot2″. The above plot can be reproduced with one line of code. In addition to several predefined positioning functions, one can also write their own positioning function. For example, placing the rotated labels at the starting values of each series. I agree with the author’s conclusion that the directlabels package simplifies and makes more convenient the labeling of data series in both lattice and ggplot2. Thanks to Baptiste for bringing this package to my attention. 	 0 Comments
Rcpp 0.7.1	https://www.r-bloggers.com/2010/01/rcpp-0-7-1/	January 2, 2010	Thinking inside the box	"
A lot has changed under the hood since 0.7.0, and this is the first release
that really reflects many of Romain‘s additions.
Some of the changes are
 "	 0 Comments
LSPM Examples	https://www.r-bloggers.com/2010/01/lspm-examples/	January 2, 2010	Joshua Ulrich	"
 "	 0 Comments
How to use mcsm	https://www.r-bloggers.com/2010/02/how-to-use-mcsm/	February 27, 2010	xi'an	"Within the past two days, I received this email Dear Prof.Robert
I have just bought your recent book on Introducing Monte Carlo Methods with R.  Although I have checked your web page for the R programs (bits of the code in the book, codes for generating the figures and tec – not the package available on cran)  used in the book, I have not found them.
I wonder whether you could make them available.
Thank you very much for your time and patience.
Yours Sincerely and that one Dear Prof. Robert,
I bought “Introducing Monte Carlo Methods with R” from Amazon booksore. I am a teacher at […] University, and I choose this book as a textbook in my class.
I can not find the R package “mcsm” according to your book (page 5). Where can I download the R package “mcsm”?
I highly appreciate your help.
Best regards, so I fear that readers may miss the piece of information provided in the book. As indicated on pages 36-37 of Introducing Monte Carlo Methods with R, mcsm is a registred R package, readers can therefore download it manually from CRAN,  but they should first try using install.packages in R as this is both easier and safer. (They should check on the main R project webpage for more help in installing packages.) Another useful information for readers is that the code used on the examples of Introducing Monte Carlo Methods with R is available from mcsm through the demo command/code. Typing demo(Chapter.3) starts the production of the examples of Chapter 3: > demo(Chapter.3) demo(Chapter.3)
————————
Type     to start :
> # Section 3.1, Introduction
>
> ch=function(la){ integrate(function(x){x^(la-1)*exp(-x)},0,Inf)$val}
> plot(lgamma(seq(.01,10,le=100)),log(apply(as.matrix(
+  seq(.01,10,le=100)),1,ch)),xlab=”log(integrate(f))”,
+  ylab=expression(log(Gamma(lambda))),pch=19,cex=.6)
> S=readline(prompt=”Type     to continue : “)
Type     to continue : and obviously the same for all other chapters. This also means the code is available in the corresponding file, something like /usr/lib/R/site-library/mcsm/demo/Chapter.3.R depending on your system. "	 0 Comments
Calculating LT50 (median lethal temperature, aka LD50) quickly in R	https://www.r-bloggers.com/2010/02/calculating-lt50-median-lethal-temperature-aka-ld50-quickly-in-r/	February 27, 2010	Luke Miller		 0 Comments
Be Careful Searching Python Dictionaries!	https://www.r-bloggers.com/2010/02/be-careful-searching-python-dictionaries/	February 27, 2010	Ryan	"For my talk on High Performance Computing in R (which I had to reschedule due to a nasty stomach bug), I used Wikipedia linking data, an adjacency list of articles and the articles to which they link. This data was linked from DataWrangling and was originally created by Henry Haselgrove. The dataset is small on disk, but I needed a dataset that was huge, very huge. So, without a simple option off the top of my head, I took this data and expanded a subset of it into an incidence matrix, occupying 2GB in RAM. Perfect! The subsetting was a bit of a problem because I had to maintain the links within the subgraph induced by the same. This required me to search dictionary objects for keys. This is where things went awry. Usually, I try to be efficient as possible. Since I was just producing a little example, and would never ever run this code otherwise, I wasn’t as careful. The data were presented as a follows  1. First, I looked at from, and if from was in the chosen subset, keep it and proceed to 2, otherwise, throw it out.
2. Then, take the to nodes and only keep the nodes that are also in the subset. How we search for an element in a dict makes a huge, huge difference in running time. It is pretty easy to tell when you have fallen into this pitfall because your program will go ridiculously slow. In my case I had 50,000 unique keys, all of type int (originally). The Wrong Way Anybody with a background in mathematics may easily see this as the problem: is k in the set of keys for the dictionary? I mean, it is the keys we are searching, not the values. or similarly to = filter(lambda x: x in index.keys(), to) This is bad. Using this method is the difference between your coding taking a few seconds, and running for a few hours. The Better Ways This correct way doesn’t make as much sense logically unless we understand one thing: the in operator is overloaded for dictionaries, to search for a key.
If we do not know this, it seems that k in d is ambiguous. My first thought was that it would search the values. Actually, it searches the keys. So we can use the following, or similarly, result = filter(lambda x: x in index, to) To just check for the existence of a key, such as in my case, we can use has_key(). Another way is to use exceptions. This may be natural for the beginning programmer. If the user queries k and it is not a key in the dictionary, an exception is thrown. The user can catch this exception just to shut up the interpreter. Timing the Methods I stripped down my code to run one iteration of the processing using the various methods, all Python scripts. Since the subsets are chosen randomly each time, I ran each method 100 times and averaged the running time.  Clarification: I did not want to limit my code sample too much. The curious reader will notice that 5000ms is pretty high for just a search operation. There is some other stuff going on before the search takes place: generating a random list, opening a file, reading a line from it, and parsing the line. This overhead increases the running time. The table below is solely to show the differences in time among the methods. To time a function, we can use a function decorator as follows. Define the function print_timing, and then add a decorator to the function being time: Adapted from a post on DaniWeb. The table below shows the results. The point to take home is never, ever search dict.keys(). Using has_key() and exceptions gives a pretty good improvement. Simply iterating over the dictionary itself takes the least amount of time.   The difference in the average time between the slowest method and the fastest method is 74ms. The difference between these methods is 7 seconds after processing 100 nodes. The difference becomes unacceptable when processing even just 1,000 to nodes. So why is the d.keys() so slow? When we call d.keys(), we get a list. Searching the list is an  operation. The other methods that I studied rely on the dict and only the dict object. A Python dict is like a hash table, and checking for existence of a key is an  operation. While writing this post, I found this web page discussing common Python idioms and hints to improve efficiency. I recommend it for anyone that programs in Python. "	 0 Comments
An interesting paper	https://www.r-bloggers.com/2010/02/an-interesting-paper/	February 27, 2010	Shige		 0 Comments
oro.dicom 0.2.4	https://www.r-bloggers.com/2010/02/oro-dicom-0-2-4/	February 26, 2010	Brandon Whitcher		 0 Comments
Steve Miller on R at Predictive Analytics World	https://www.r-bloggers.com/2010/02/steve-miller-on-r-at-predictive-analytics-world/	February 26, 2010	David Smith	At the Information Management blog, Steve Miller has provided two great reviews (here and here) of last week’s Predictive Analytics World conference, including a recap of the Bay Area User’s Group meeting featuring John Chambers. (My personal highlight from John’s talk? A photograph of the very first sketch of what was to become the S system, which ultimately  begat the R language.) But I also wanted to point you to Steve’s review of Mike Driscoll’s jaw-dropping talk The Social Effect: Predicting Telecom Customer Churn with Call Data: Mike Driscoll’s: The Social Effect: Predicting Telecom Customer Churn with Call Data, was a good illustration of predictive analytics in a larger data warehousing and BI context. Driscoll and his team analyzed billions of calls, millions of records and thousands of defectors from a Greenplum DW looking for predictors of churn. Driscoll’s a big proponent of the open source R Project for Statistical Computing to support his work flow of data munge, data model, and data visualize. And with a Ph.D. in Bioinformatics, he often thinks like an epidemiologist, in this case looking for indications of contagious churn behavior. Using several social network analysis packages available in R, Driscoll’s team appears to have found that churn in an individual’s social network of calling accounts in a given month is likely to lead to more churn in subsequent months, a clear indication of a network effect. That contagion is overwhelmingly the strongest signal the team found from the data. A next step is to work with marketing to intervene on early network churn with email campaigns to minimize losses from the affected networks. I’d love to see the results from those experiments. Mike’s talk was jaw-dropping for me for two reasons. Firstly, in innovation: using a caller’s social network (defined by the people they call the most, information drawn from the call-log data) was an elegent and powerful way to predict probability of switching to another network. It seems intuitive — if your friends switch to another provider, you’re likely to, as well — and it’s always good to see intuition borne out by data. And secondly, in scale: we’re talking about 10Gb of data here, analyzed in just a few hours using R. Not too long ago that type of computation would be reserved for high-tech computing grids and expensive software; these days, it’s amazing what you can do with a hefty 64-bit workstation and open-source tools. Unfortunately Mike’s slides aren’t available just now (some great social network charts in there), but if they do become available I’ll let you know. Information Management: Predictive Analytics World – Take 2 	 0 Comments
Because it’s Friday: Visualizing an email chain	https://www.r-bloggers.com/2010/02/because-its-friday-visualizing-an-email-chain/	February 26, 2010	David Smith	We’ve all been there: someone sends an email to a mailing list with a Reply-To directing responses back to the mailing list. Before long, someone replies (unwittingly, to everyone) to ask to be taken of the list. And before long, the entire affair devolves into an endless cycle of requests to unsubscribe and pleas to stop mailing the entire list. This process is visualized beautifully in this timeline of emails to the Caltrans Performance Measurement System mailing list (click to see the enlarged original on FlowingData):    Some of the classic r-help threads could also benefit from such a treatment. FlowingData: Data Underload #8 – Unsolicited  Data Underload #8 – Unsolicited  	 0 Comments
R tip: Finding the location of minimum and maximums	https://www.r-bloggers.com/2010/02/r-tip-finding-the-location-of-minimum-and-maximums/	February 26, 2010	Stewart MacArthur		 0 Comments
R and Sudoku solvers: Plus ca change…	https://www.r-bloggers.com/2010/02/r-and-sudoku-solvers-plus-ca-change/	February 25, 2010	Thinking inside the box	"
But what everybody seems to be forgetting is that
R has had a Sudoku solver for years,
thanks to the sudoku
package by David Brahm and Greg Snow which was first posted four years
ago.  What comes around, goes around.

 
With that, and about one minute of Emacs editing to get the Le Monde
puzzle into the required ascii-art form, all we need to do is this:
 
 
Just in case we needed another illustration that it is hard to navigate the
riches and wonders that is CRAN…


 "	 0 Comments
Welcome, Robin!	https://www.r-bloggers.com/2010/02/welcome-robin/	February 25, 2010	xi'an	Robin Ryder started his new blog with his different solutions to Le Monde puzzle of last Saturday (about the algebraic sum of products…), solutions that are much more elegant than my pedestrian rendering. I particularly like the one based on the Jacobian of a matrix! (Robin is doing a postdoc in Dauphine and CREST—under my supervision—on ABC and other computational issues, after completing a PhD in Oxford on philogenic trees for language history with Geoff Nicholls. His talk at the Big’MC seminar last month is reproduced there.) And, in a totally unrelated way, here is the Sudoku (in Le Monde) that started my post on simulated annealing, nicely represented on Revolutions. (Although I cannot see why the central columns are set in grey…) I must mention that I am quite surprised at the number of visits my post received, given that using simulated annealing for solving Sudokus has been around for a while. Even my R code, while original, does not compete with simulated annealing solutions that take a few seconds… I thus completely share Dirk Eddelbuettel‘s surprise in this respect (but point to him that Robin’s blog entry has nothing to do with Sudokus, but with another Le Monde puzzle!) 	 0 Comments
Responding to the Flowingdata GDP Graph Challenge	https://www.r-bloggers.com/2010/02/responding-to-the-flowingdata-gdp-graph-challenge/	February 25, 2010	Hrishi Mittal	"Nathan Yau of Flowingdata put up a challenge earlier today to improve upon a graph showing government spending as a percentage of GDP, published in the Economist. The underlying data wasn’t available. So I put on my graph-to-numbers glasses on and pulled out some data. Here it is in case you want to have a go. I took on the first part of the challenge i.e. Can you think of a way to make this graph easier to read? The Original Graph from the Economist:  I hacked up the following version in R. It was a bit of a challenge to get it right given the constraints of fitting all the data and legend within a 290 x 300 image.  Do you think this is an improvement? Leave a comment below or in Nathan’s original post at http://flowingdata.com/2010/02/25/challenge-make-this-graph-easier-to-read/. And here’s the R code: #Read the file
gdp
 #Reset the column name from United.States to United States; R replaces spaces in variable names with dots; you’ll see why below.
colnames(gdp)[6]
 #Define our colour palette so that we can edit it in one place and refer to colours by index as shown below
pal=c(“black”,”darkorange”,”blue”,”forestgreen”,”tomato”) #Start PNG device with the given constraints of 300×290 (boy that’s a small image!)
png(“gdp.png”,height=300,width=290,units=”px”) #Plot settings
par(mar=c(2,2,3,1) #Small images call for small margins
,las=1) #For some reason, the default orientation (las=0) of the axis labels is parallel to the axis. This works OK for the X axis but makes it hard to read Y axis labels, so set to horizontal. #Finally, the main plot command
plot(Canada~Year,data=gdp
,type=”l”
,xaxt=”n” #Don’t draw default X axis; we’ll draw a custom one below.
,xaxs=”i” #X axis style (internal) just finds an axis with pretty labels that fits within the original data range. If you don’t use this then an extra space is added at the edges even if you set xlim
,yaxs=”i”#Style – Same reason as X Axis
,main=”Total government spending n(% of GDP by year)” #Got rid of ‘The shape of the beast’ for space constraints
,col=pal[1]
,ylim=c(30,70) #Setting the top Y axis limits to allow space for the legend
,lwd=4 #Quite unusually high line width but good for improving visibility in a small graph.
) #Custom X axis
axis(side=1 #That’s the bottom X axis side
,at=Year[2:16] #labels starting from 1996; using at=Year places the labels at odd years.
,labels=substr(Year[2:16],3,4)) #Instead of using the full year, use just 2 digits. grid(lwd=0.4,lty=1,col=”#000000″) #Very faint grid to guide the eyes. #Add the rest of the lines
#France
lines(France~Year,data=gdp,col=pal[2],lwd=4) #Germany
lines(Germany~Year,data=gdp,col=pal[3],lwd=4) #Britain
lines(Britain~Year,data=gdp,col=pal[4],lwd=4) #United States; Note we can’t use United States~Year because of the space between United and States. This calls for use of the data[[“variable name”]] notation.
lines(gdp[[“United States”]]~Year,data=gdp,col=pal[5],lwd=4) #Lastly the legend
legend(“top” #Align it at the top in the center
,ncol=2 #Number of columns to spread the legend labels overs; 2 works best for our graph.
,legend=colnames(gdp)[2:6]
,lty=1
,lwd=4
,col=pal #Make sure to use the same colour palette as the graph lines!
,bg=”#FFFFFF” #White background to make it merge with the plot background.
,inset=0.01) #Inset the legend so that it doesn’t quite touch the border of the plot. dev.off() #Close the graphics device "	 0 Comments
Nutritional supplements efficacy score – Graphing plots of current studies results (using R)	https://www.r-bloggers.com/2010/02/nutritional-supplements-efficacy-score-%e2%80%93-graphing-plots-of-current-studies-results-using-r/	February 25, 2010	Tal Galili	"In this post I showcase a nice bar-plot and a balloon-plot listing recommended Nutritional supplements , according to how much evidence exists for thier benefits, scroll down to see it(and click here for the data behind it)
*  *  *  *
The gorgeous blog “Information Is Beautiful” recently publish an eye candy post showing a “balloon race” image (see a static version of the image here) illustrating how much evidence exists for the benefits of various Nutritional supplements (such as: green tea, vitamins, herbs, pills and so on) . The higher the bubble in the Y axis score (e.g: the bubble size) for the supplement the greater the evidence there is for its effectiveness (But only for the conditions listed along side the supplement). There are two reasons this should be of interest to us: The advantage of having the data on a google doc means that we can see when the data will be updated. But more then that, it means we can easily extract the data into R and have our way with it  (Thanks to David Smith’s post on the subject) For example, I was wondering what are ALL of the top recommended Nutritional supplements, an answer that is not trivial to get from the plot that was in the original post. In this post I will supply two plots that present the data: A barplot (that in retrospect didn’t prove to be good enough) and a balloon-plot for a table (that seems to me to be much better). Barplot
(You can click the image to enlarge it)
 The R code to produce the barplot of Nutritional supplements efficacy score (by evidence for its effectiveness on the listed condition). Also, the nice things is that if the guys at Information Is Beautiful will update there data, we could easily run the code and see the updated list of recommended supplements. Balloon plot
So after some web surfing I came around an implementation of a balloon plot in R (Thanks to R graph gallery)
There where two problems with using the command out of the box. The first one was that the colors where non informative (easily fixed), the second one was that the X labels where overlapping one another. Since there is no “las” parameter in the function, I just opened the function up, found where this was plotted and changed it manually (a bit messy, but that’s what you have to do sometimes…) Here are the result (you can click the image for a larger image):  And here is The R code to produce the Balloon plot of Nutritional supplements efficacy score (by evidence for its effectiveness on the listed condition).
 (it’s just the copy of the function with a tiny bit of editing in line 146, and then using it)  Got any good ideas of how else to plot the data? let me know in the comments   "	 0 Comments
Solving Sudoku with Simulated Annealing	https://www.r-bloggers.com/2010/02/solving-sudoku-with-simulated-annealing/	February 25, 2010	David Smith	How long would it take you to solve this devlishly hard Sudoku puzzle (from Le Monde)?   You could do it the old-fashioned way — with a pencil — but Xi’an decided to solve it by programming a simulated annealing solver in R. The algorithm works by first guessing a solution at random — filling in the empty cells above with random digits between 1 and 9. Then it “scores” this solution by counting the number of digits duplicated in all the rows, columns and blocks. Next it evaluates a number of candidate new solutions by tweaking one of the free digits, and scores those. The algorithm then selects one of the candidate solutions at random for the next step, weighted by the change in the score. The code to do all this , which you can use to solve your own Soduku puzzles if, say, you’ve lost your pencil, is at Xi’an’s blog. Xi’an’s og: Sudoku via simulated annealing 	 0 Comments
inkblot: an alternative to stacked bar graphs	https://www.r-bloggers.com/2010/02/inkblot-an-alternative-to-stacked-bar-graphs/	February 25, 2010	Karsten W.	Sometimes it is not easy to get useful information from a stacked bar chart, see for instance this blogpost at Support Analytics. So-called inkblot charts, as discussed at Kaiser Fung’s Junk Charts, allow the reader to focus on the evolution of a time series. Now how to make this kind of charts with R? I asked on StackOverflow. The given answers led to an implementationof an inkblot function. It is delivered with the wzd package on r-forge. Here is an example which visualizes the income per capita fromvarious countries, as reported by gapminder:  	 0 Comments
Interaction plot from cell means	https://www.r-bloggers.com/2010/02/interaction-plot-from-cell-means/	February 24, 2010	Thom Baguley		 0 Comments
FFT (Fast Fourier Transform) of time series  — promises and pitfalls towards trading	https://www.r-bloggers.com/2010/02/fft-fast-fourier-transform-of-time-series-promises-and-pitfalls-towards-trading/	February 24, 2010	Intelligent Trading		 0 Comments
ggplot2: Plotting Dates, Hours and Minutes	https://www.r-bloggers.com/2010/02/ggplot2-plotting-dates-hours-and-minutes/	February 24, 2010	learnr	Plotting timeseries with dates on x-axis and times on y-axis can be a bit tricky in ggplot2. However, with a little trick this problem can be easily overcome.  Let’s assume that I wanted to plot when the sun rises in London in 2010. sunriset function in maptools package calculates the sunrise times using algorithms provided by the National Oceanic & Atmospheric Administration (NOAA). I need to input the location details in a matrix format and specify for which dates I require the sunrise times. The time variable now includes information about both the date and time of sunrise in class POSIXct. I would like to plot date on x-axis and time on y-axis, thus the time element needs to be extracted first. However, as the times must be in POSIXct (only times of class POSIXct are supported in ggplot2), a two-step conversion is needed. First the time is converted to a character vector, effectively stripping all the date information. The time is then converted back to POSIXct with today’s date – the date is of no interest to us, only the hours-minutes-seconds are. Now everything is set for plotting. The default x-axis labels could be made somewhat clearer: 	 0 Comments
PoRtable…	https://www.r-bloggers.com/2010/02/portable%e2%80%a6/	February 24, 2010	M. Parzakonis	"Jobless as I might be, I do have some clients for data analysis. I try not to visit them in their office coz then things get really slow and time-consuming. When I can’t escape this, the worst thing is tuning data and software with client. So, I have a USB with portable versions of my toolbox. Yesterday, I installed R and today I tested it. It worked fine! If you want to try it, download the software platform from here. Install it (when prompt to extract/unzip) into a file in your USB, ie /OS, and then download the R portable from here. From the portable OS menu (down-right on your screen) select install software and browse to the R portable file. After a couple of minutes you’re done! 
Special thanks to Andrew Redd for  providing us with the portable version. "	 0 Comments
Object types in R: The fundamentals	https://www.r-bloggers.com/2010/02/object-types-in-r-the-fundamentals/	February 24, 2010	David Smith	If you’re a self-taught R programmer, you’ve probably grappled with the different kinds of objects you can use in the language. When should you use a list instead of a vector? What’s the difference between a factor and character vector? These questions are easier to answer when you have some of the basics of R’s object types down pat, and Chris Bare lays out the fundamentals quite nicely in his blog post The R Type System. An excerpt: Because the purpose of R is programming with data, it has some fairly sophisticated tools to represent and manipulate data. First off, the basic unit of data in R is the vector. Even a single integer is represented as a vector of length 1. All elements in an atomic vector are of the same type. The sizes of integers and doubles are implementation dependent. Generic vectors, or lists, hold elements of varying types and can be nested to create compound data structures, as in Lisp-like languages. He goes on from with useful descriptions and examples of matrices, arrays, data frames, factors and more. Well worth checking out if you want to understand how R’s object types tick. Digithead’s Lab Notebook: The R type system  	 0 Comments
SoilWeb iPhone App: Beta-Testers?	https://www.r-bloggers.com/2010/02/soilweb-iphone-app-beta-testers/	February 23, 2010	dylan	"iPhone App Screenshot rev 0.2 – icon iphone App Screenshot rev 0.2 – in Fresno  
More Updates:
The application is now available on the Apple iTunes Store. Preliminary documentaion here.  read more "	 0 Comments
Reminder: useR! 2010 abstracts due Monday	https://www.r-bloggers.com/2010/02/reminder-user-2010-abstracts-due-monday/	February 23, 2010	David Smith	Don’t forget, if you’re planning to attend the R user conference useR! 2010 and are going to present a talk (and if not, why not?), abstracts are due for submission this coming Monday, March 1. That’s also the deadline for early-bird registrations, so if you haven’t registered yet, now is the time. useR! 2010: The R User Conference 	 0 Comments
Numerical Integration/Differentiation in R: FTIR Spectra	https://www.r-bloggers.com/2010/02/numerical-integrationdifferentiation-in-r-ftir-spectra/	February 23, 2010	dylan	" 
Stumbled upon an excellent example of how to perform numerical integration in R. Below is an example of piece-wise linear and spline fits to FTIR data, and the resulting computed area under the curve. With a high density of points, it seems like the linear approximation is most efficient and sufficiently accurate. With very large sequences, it may be necessary to adjust the value passed to the subdivisions argument of integrate(). Strangely, larger values seem to solve problems encountered with large datasets… FTIR Spectra Integration read more "	 0 Comments
Slides from “R Productivity Environment” webinar	https://www.r-bloggers.com/2010/02/slides-from-r-productivity-environment-webinar/	February 23, 2010	David Smith	Thanks to everyone who attended for the great turnout at this morning’s live webinar, 7 Ways to Increase your R Productivity. I really appreciate all the feedback and questions, seems like a lot of people are interested in a code editing and debugging environment for R.  If you missed the webinar and want to learn about REvolution R Enterprise and the R Productivity Environment, the replay will be live at the link below tomorrow. The slides are there for download now. REvolution Computing: Webinar: 7 Ways to Increase Your R Productivity 	 0 Comments
Happy Birthday GGD! The 10 Most Popular Posts Since GGD’s Launch	https://www.r-bloggers.com/2010/02/happy-birthday-ggd-the-10-most-popular-posts-since-ggds-launch/	February 23, 2010	Stephen Turner		 0 Comments
Getting Started with Sweave: R, LaTeX, Eclipse, StatET, & TeXlipse	https://www.r-bloggers.com/2010/02/getting-started-with-sweave-r-latex-eclipse-statet-texlipse/	February 23, 2010	Jeromy Anglim		 0 Comments
Mexico’s Economy	https://www.r-bloggers.com/2010/02/mexicos-economy/	February 22, 2010	Diego Valle-Jones		 0 Comments
Time Series Calendar Heat Maps Using R	https://www.r-bloggers.com/2010/02/time-series-calendar-heat-maps-using-r/	February 22, 2010	Intelligent Trading		 0 Comments
A quicky..	https://www.r-bloggers.com/2010/02/a-quicky/	February 22, 2010	M. Parzakonis	If you’re (and you should) interested in principal components then take a good look at this. The linked post will take you by hand to do everything from scratch. If you’re not in the mood then the dollowing R functions will help you. An example. 	 0 Comments
Sudoku via simulated annealing	https://www.r-bloggers.com/2010/02/sudoku-via-simulated-annealing/	February 22, 2010	xi'an	"The Sudoku puzzle in this Sunday edition of Le Monde was horrendously difficult, so after spending one hour with only 4 entries filled, I decided to feed it to the simulated annealing R program I wrote while visiting SAMSI last year. The R program reached the exact (and only) solution in about 6000 iterations, as shown (?) on the graph above. The Sudoku grid is defined in the R program by a 9×9 matrix s and the simulated annealing target function counts the number of duplicates target=function(s){
tar=sum(apply(s,1,duplicated)+apply(s,2,duplicated))
for (r in 1:9){ 
bloa=(1:3)+3*(r-1)%%3
blob=(1:3)+3*trunc((r-1)/3)
tar=tar+sum(duplicated(as.vector(s[bloa,blob])))
}
return(tar)
} After pruning out the deterministic entries (3 in my case!), the R program uses the temperature sequence lmax=10^5#/target(matrix(sample(1:9,81,rep=T),ncol=9))
temps=exp(sqrt(seq(1,log(lmax)^2,le=Niter+1))) to weight the target function. and it runs over the 10,000 iterations random moves on some of the unallocated sites. On the graph above, the green dots correspond to accepted moves. The yellow dots correspond to accepted proposals to move a single site. These choices lead to a correct solution most of the time, the other cases most often producing a penalty of two. (Please note there is nothing optimised about my code. It takes ten to twenty minutes to produce the graph above. a far cry from the fastest Sudoku solvers!) 
Filed under: R, Statistics Tagged: simulated annealing, sudoku      

 "	 0 Comments
Siegel-Tukey: a Non-parametric test for equality in variability (R code)	https://www.r-bloggers.com/2010/02/siegel-tukey-a-non-parametric-test-for-equality-in-variability-r-code/	February 22, 2010	Tal Galili	"1. Group medians
2. Wilcoxon-test for between-group differences in median (after the median
adjustment if specified)
3. Unique values of x and their tie-adjusted Siegel-Tukey ranks
4. Xs of group 0 and their tie-adjusted Siegel-Tukey ranks
5. Xs of group 1 and their tie-adjusted Siegel-Tukey ranks
6. Siegel-Tukey test (Wilcoxon test on tie-adjusted Siegel-Tukey ranks) "	 0 Comments
Speeding up R code: A case study	https://www.r-bloggers.com/2010/02/speeding-up-r-code-a-case-study/	February 22, 2010	David Smith	On his Psychology and Statistics blog, Jeromy Anglim tells how he was analyzing some data from a skill acquisition experiment. Needing to run a custom R function across 1.3 million data points, Jeromy estimated it would take several hours for the computation to complete. So, Jeromy set out to optimise the code. First, he used the Rprof function, which inspects your R functions as they run, and counts the amount of time spent in each sub-function. This is a useful tool to identify the parts of your functions that are ripe for optimisation, and in this case (with some help from the system.time function to time a specific section of the code) he learned that most of the time wasn’t taken performing actual calculations: most time was actually spent selecting the subset of the data to analyze! And thus a solution was born: rather than repeatedly selecting from the large data frame in an iterative loop, he instead split the data frame into its constituent parts once, and then looped over the parts. This reduced the analysis time from hours down to just a couple of minutes. As the end of his case study, Jeromy shares some valuable lessons learned about optimising R functions:     Jeromy Anglim’s Blog: Psychology & Statistics: A Case Study in Optimising Code in R 	 0 Comments
ggplot2 (qplot) text size	https://www.r-bloggers.com/2010/02/ggplot2-qplot-text-size-2/	February 22, 2010	Jim		 0 Comments
Time-Space Cloud with R	https://www.r-bloggers.com/2010/02/timespace-cloud-with-r/	February 22, 2010	Benedikt Orlowski	Here comes another option to analyze a TimeSpace-Track with R. A lattice cloud plots every recorded trackpoint into a 3d-time-space-cube. As the data (planar point pattern) is marked with the daytime, cluster of everyday routines become visible. Here the direct comparison between a function of density and the time-space-cloud. Time space Clowd spatstat density plot Code example: cloud(time_hours ~ PPP_selection$x * PPP_selection$y, data = daten, zlim = c(23,0), xlim = c(653000,643000), screen = list(z = 160, x = 120), panel.aspect = 0.75, xlab = “Longitude”, ylab = “Latitude”, zlab = “Time”, scales = list(z = list(arrows = FALSE, distance = 2), x = list(arrows =FALSE, distance = 2), y = list(arrows = FALSE, distande = 2)),) This examle is inspired by: http://lmdvr.r-forge.r-project.org/ (Figure 6.2) 	 0 Comments
Post hoc analysis for Friedman’s Test  (R code)	https://www.r-bloggers.com/2010/02/post-hoc-analysis-for-friedman%e2%80%99s-test-r-code/	February 22, 2010	Tal Galili	"My goal in this post is to give an overview of Friedman’s Test and then offer R code to perform post hoc analysis on Friedman’s Test results. (The R function can be downloaded from here) Friedman test is a non-parametric randomized block analysis of variance.  Which is to say it is a non-parametric version of a one way ANOVA with repeated measures. That means that while a simple ANOVA test requires the assumptions of a normal distribution and equal variances (of the residuals), the Friedman test is free from those restriction. The price of this parametric freedom is the loss of power (of Friedman’s test compared to the parametric ANOVa versions). The hypotheses for the comparison across repeated measures are: The test statistic for the Friedman’s test is a Chi-square with [(number of repeated measures)-1] degrees of freedom. A detailed explanation of the method for computing the Friedman test is available on Wikipedia. Performing Friedman’s Test in R is very simple, and is by using the “friedman.test” command. Assuming you performed Friedman’s Test and found a significant P value, that means that some of the groups in your data have different distribution from one another, but you don’t (yet) know which. Therefor, our next step will be to try and find out which pairs of our groups are significantly different then each other. But when we have N groups, checking all of their pairs will be to perform [n over 2] comparisons, thus the need to correct for multiple comparisons arise.
The tasks:
Our first task will be to perform a post hoc analysis of our results (using R) – in the hope of finding out which of our groups are responsible that we found that the null hypothesis was rejected. While in the simple case of ANOVA, an R command is readily available (“TukeyHSD”), in the case of friedman’s test (until now) the code to perform the post hoc test was not as easily accessible.
Our second task will be to visualize our results. While in the case of simple ANOVA, a boxplot of each group is sufficient, in the case of a repeated measures – a boxplot approach will be misleading to the viewer. Instead, we will offer two plots: one of parallel coordinates, and the other will be boxplots of the differences between all pairs of groups (in this respect, the post hoc analysis can be thought of as performing paired wilcox.test with correction for multiplicity). The analysis will be performed using the function (I wrote) called “friedman.test.with.post.hoc”, based on the packages “coin” and “multcomp”. Just a few words about it’s arguments: (The code for the example is given at the end of the post) Let’s make up a little story: let’s say we have three types of wine (A, B and C), and we would like to know which one is the best one (in a scale of 1 to 7). We asked 22 friends to taste each of the three wines (in a blind fold fashion), and then to give a grade of 1 till 7 (for example sake, let’s say we asked them to rate the wines 5 times each, and then averaged their results to give a number for a persons preference for each wine. This number which is now an average of several numbers, will not necessarily be an integer). After getting the results, we started by performing a simple boxplot of the ratings each wine got. Here it is:  The plot shows us two things: 1) that the assumption of equal variances here might not hold. 2) That if we are to ignore the “within subjects” data that we have, we have no chance of finding any difference between the wines. So we move to using the function “friedman.test.with.post.hoc” on our data, and we get the following output: The conclusion is that once we take into account the within subject variable, we discover that there is a significant difference between our three wines (significant P value of about  0.0034). And the posthoc analysis shows us that the difference is due to the difference in tastes between Wine C and Wine A (P value 0.003). and maybe also with the difference between Wine C and Wine B (the P value is 0.053, which is just borderline significant). Plotting our analysis will also show us the direction of the results, and the connected answers of each of our friends answers:  Here is the code for the example: If you find this code useful, please let me know (in the comments) so I will know there is a point in publishing more such code snippets… "	 0 Comments
The R type system	https://www.r-bloggers.com/2010/02/the-r-type-system/	February 21, 2010	Chris	R is a weird beast. Through it’s ancestor the S language, it claims a proud heritage reaching back to Bell Labs in the 1970’s when S was created as an interactive wrapper around a set of statistical and numerical subroutines. As a programming language, R takes ideas from Unix shell scripting, functional languages (Lisp and ML), and also a little from C. Programmers will usually have at least some background in these languages, but one aspect of R that might remain puzzling is it’s type system. Because the purpose of R is programming with data, it has some fairly sophisticated tools to represent and manipulate data. First off, the basic unit of data in R is the vector. Even a single integer is represented as a vector of length 1. All elements in an atomic vector are of the same type. The sizes of integers and doubles are implementation dependent. Generic vectors, or lists, hold elements of varying types and can be nested to create compound data structures, as in Lisp-like languages. R objects can have attributes – arbitrary key/value pairs – attached to them. One use for this is that elements in vectors or lists can be named. R’s object system is based on the class attribute. (OK, I really mean the simpler of R’s two object systems, but let’s avoid that topic.) Attributes are also used to turn one-dimensional vectors into multi-dimensional structures by specifying their dimensions, as we’ll see next. Matrices and arrays are special types of vectors, distinguished by having a dim (dimensions) attribute. A matrix has two dimensions, so the value of its dim attribute is a vector of length 2 specifying numbers of rows and columns in the matrix. Arrays are n dimensional vectors, sometimes used like an OLAP data cube, with dimension vectors of length n. Statisticians divide data into four types: nominal, ordinal, interval and ratio. Factors are for the first two, depending on whether they are ordered or not. This makes a difference for some of the stats algorithms in R, but from a programmers point of view, a factor is just an enum. R turns character vectors into factors at the slightest provocation. It’s sometimes necessary to coerce factors back to character strings, using as.character(). A data frame is a special list in which all elements are vectors of equal length. It is analagous to a table in a database, except that it’s column-oriented rather than row-oriented. Because the vectors are constrained to be of the same length, you can index any cell in a data frame by its row and column. There’s more, of course, but this gives you enough to be dangerous. Note that, because R natively works with vectors, many operations in R are vectorized, meaning they operate on whole vectors at once, rather than on a single scalar value. The key to performance in R is making good use of vectorized operations. Also, being functional, R inherits a full compliment of higher-order functions – Map, Reduce, Filter and many forms of apply (lapply, sapply, and tapply). Mixing higher-order functions and vectorized operations can get confusing (and is the source of the proliferation of apply functions). Both these techniques, as well as the organization of the type system, encourage you to work with blocks of data as a unit. This is what John Chambers called high-level prototyping for computations with data. 	 0 Comments
The truncated Poisson	https://www.r-bloggers.com/2010/02/the-truncated-poisson/	February 21, 2010	M. Parzakonis	"A common model for counts data is the Poisson. There are cases however that we only record positive counts, ie there is a truncation of 0. This is the truncated Poisson model. To study this model we only need the total counts and the sample size. This comes from the sufficient statistic principle as the likelihood is , where . Let’s set  and  . The gradient is .  The second derivative (the hessian) is . Now, we simply call the s/t function of maxLik package to fit the model. There is an nice & handy function I found somewhere in the net (sorry I don’t remember the author;() to plot the likelihood of the truncated Poisson.
    "	 0 Comments
Visual Interpretation of Principal Coordinates (of) Neighbor Matrices (PCNM)	https://www.r-bloggers.com/2010/02/visual-interpretation-of-principal-coordinates-of-neighbor-matrices-pcnm/	February 21, 2010	dylan	"Principal Coordinates (of) Neighbor Matrices (PCNM) is an interesting algorithm, developed by P. Borcard and P. Legendre at the University of Montreal, for the multi-scale analysis of spatial structure. This algorithm is typically applied to a distance matrix, computed from the coordinates where some environmental data were collected. The resulting “PCNM vectors” are commonly used to describe variable degrees of possible spatial structure and its contribution to variability in other measured parameters (soil properties, species distribution, etc.)– essentially a spectral decomposition spatial connectivity. This algorithm has been recently updated by  and released as part of the PCNM package for R. Several other implementations of the algorithm exist, however this seems to be the most up-to-date.   
Related Presentations and Papers on PCNM read more "	 0 Comments
Uh!	https://www.r-bloggers.com/2010/02/uh/	February 20, 2010	M. Parzakonis	Didn’t know this… It’s becoming clear that I have learned R in  the most unstructured way…I always do it in two stages :ashamed: It’s really useful to wrap it all in a single function. Share/Bookmark 	 0 Comments
Design of Experiments – Block Designs	https://www.r-bloggers.com/2010/02/design-of-experiments-%e2%80%93-block-designs/	February 20, 2010	Ralph	In many experiments where the investigator is comparing a set of treatments there is the possibility of one or more sources of variability in the experimental measurements that can be accounted for during the design stage of the experimentation. For example we might be investigating four different pieces of machinery using say two different operators, who would be expected to display different degrees of competence with the equipment. Or we might not be able to run all of the experimental combinations in one session so we would want to take into account systematic differences that are due to experiments in the various sessions. The least complicated scenario is where we would have a single (nuisance) factor that we want to control for in the experiment. The statistical model used to describe the data collected in such an experiment could be written in the form:   where there are v treatments in b blocks and the number of units in each block does not have to be the same and is denoted using the k subscript. In a complete block design all treatments occur the same number of times in every block, usually one replicate of all treatments per block. There will be situations where the number of treatments is too large for all of them to be included in every block of the design. In these situations an incomplete block design would be used for running an experiment. A special type of design is the balanced incomplete block design (BIBD), where the v  treatments are investigated by allocating them to b blocks of equal size k. We have that k is less than t and b and k are chosen so that b * k is a multiple of v. All of the treatments occur exactly r times in the design and every pair of treatments occur together in lambda of the b blocks. Two-way analysis of variance (ANOVA) is used to analyse data collected from an experiment using a block design, as discussed elsewhere in this post. 	 0 Comments
Does a Proclamation of Increased Workout Load Matter?	https://www.r-bloggers.com/2010/02/does-a-proclamation-of-increased-workout-load-matter/	February 20, 2010	Millsy		 0 Comments
Genetic Algorithm Systematic Trading Development — Part 3  (Python/VBA)	https://www.r-bloggers.com/2010/02/genetic-algorithm-systematic-trading-development-part-3-pythonvba/	February 20, 2010	Intelligent Trading		 0 Comments
lme4 stands 4 Linear mixed-effects…	https://www.r-bloggers.com/2010/02/lme4-stands-4-linear-mixed-effects%e2%80%a6/	February 19, 2010	Manos Parzakonis	"There is a certain hype about mixed (and random) effects among statistician and analysts. You can show some love to Douglas Bates and Martin Maechler for maintaing the lme4 package for our cupid, R   I copy the entity of the information of the projects page. Doxygen documentation of the underlying C functions  is here. The project summary page you can find here. References to articles and other research using nlme or lme4 can be found here.  The LaTeX bibliography file can be accessed from here. If  you would like to add your work to this database, please email  vasishth.shravan at gmail dot com Slides from short courses on lme4 are here. Chapter drafts of the book lme4: Mixed-effects Modeling with R are available here. To complete this quick post, I append the following vignettes. Implementation  Details
PLS  vs GLS for LMMs
Computational  Methods [source] "	 0 Comments
R exam postprocessing	https://www.r-bloggers.com/2010/02/r-exam-postprocessing/	February 19, 2010	xi'an	Following my three-fold R exam of last month, I had a depressing afternoon meeting (with other faculty members) some students who had submitted R codes that were suspiciously close to other submitted R codes… In other words, it looked very  likely they had cheated. (A long-term issue with my R course, alas!) During this meeting, they actually admitted either to directly copying on their neighbour’s screen, due to the limited number of terminals that forces students to be too close to one another, or to looking at (and copying) another student’s  R code file from an earlier exam.  I used different exams but with enough of the same spirit that some of the R code could be recycled.) Besides the pain of having to turn to disciplinary action at a level where students should see the point of getting real skills towards an incoming hiring, the depressing consequence of this state of affairs is that next we will have to move to a higher level of “security” when running the R exam, which most likely means we will turn back to a pencil-and-paper exam…. A paradoxical situation when teaching a computer programming course! But unless some unlikely sponsor delivers a computer room able to handle 180 students all at once, I do not see any other solution. Suggestions?! 	 0 Comments
Where did all the bankers go?	https://www.r-bloggers.com/2010/02/where-did-all-the-bankers-go/	February 19, 2010	David Smith	When Lehman Brothers, Bear Stearns and Merrill Lynch went kablooie in the financial crisis, what happened to all their employees? Thanks to the magic of LinkedIn data, their Chief Scientist DJ Patil can answer that question: they went to the surviving banks:   It’s a great, if tantalizingly incomplete visualization — I’d love to see this with “Other (non-bank) employers” (even if it is just a “handful” of the set, as Patil mentions) and “Unemployed” as additional leaf nodes. LinkedIn blog: Where did all the people go from the collapsed financial institutions?  	 0 Comments
How to call C++ from R with ease	https://www.r-bloggers.com/2010/02/how-to-call-c-from-r-with-ease/	February 19, 2010	David Smith	At last night’s meeting of the ACM Student Chapter at the University of Chicago, DIrk Eddelbuettel gave an invited guest lecture, “Programming with Data: Using and Extending R”. I wasn’t there myself, but Dirk has already posted his slides, and they’re a treat. After a backgrounder on R itself (BTW, I’m flattered he referenced my Introduction to R talk here), Dirk dives into some case studies on various ways to call C, C++ and Fortran code from R. Most of the time is spent on the Rcpp interface (by Dirk and Romain François), which greatly simplifies the process of writing C++ code to be called from R. You can even “inline” C++ code directly in your R program, freeing you from the need to write complete C++ source files, and compile and link the code yourself. Dirk presents some impressive performance gains from using this technique.  [Update 9:42am: Attribution of Rcpp] Dirk Eddelbuettel / Thinking Inside the Box: U of C ACM talk 	 0 Comments
Newspaper flubs probability calculation	https://www.r-bloggers.com/2010/02/newspaper-flubs-probability-calculation/	February 19, 2010	David Smith	Bad Science: Guns don’t kill people, puppies do 	 0 Comments
U of C ACM talk	https://www.r-bloggers.com/2010/02/u-of-c-acm-talk/	February 18, 2010	Thinking inside the box		 0 Comments
Corruption indicators in Mexico	https://www.r-bloggers.com/2010/02/corruption-indicators-in-mexico/	February 18, 2010	Diego		 0 Comments
Joining R-bloggers	https://www.r-bloggers.com/2010/02/joining-r-bloggers/	February 18, 2010	xi'an	Upon request by the blog administrator, Tal Galili, I have joined R-bloggers, which aggregate blog entries about R into a central place. I feel I have much more to learn than to teach about R (as can be seen from earlier comments on my R programs in Introducing Monte Carlo Methods with R). As I was tagging some of my older posts with the newly created R category, I realised most tags were either about typos or books! Anyway, I figure joining a conglomerate of blogs cannot hurt! 	 0 Comments
Press Enter in LyX Sweave as You Wish	https://www.r-bloggers.com/2010/02/press-enter-in-lyx-sweave-as-you-wish/	February 18, 2010	Yihui Xie	For a long time I’ve been wondering why we are not able to use Enter in the LyX Scrap environment which was set up by Gregor Gorjanc for Sweave. Two weeks ago, I (finally!) could not help asking Gregor about this issue, as I’m using “LyX + Sweave” more and more in my daily work. He explained it here: LyX-Sweave: mandatory use of control+enter in code chunks After digging into the LyX customization manual for a while, I found a solution which allows us to press the Enter key just as we normally do when typing in a LyX document. The key is to use Environment instead of paragraph as LatexType for the style definition of Scrap. Besides, I used the LatexName as wrapsweave, as a LatexName is required by LyX. The definition for wrapsweave is simple: just two empty lines by par. (If you define it as newenvironment{wrapsweave}{}{}, you will run into troubles sometimes; especially when you use indent for paragraphs.) As we know, LaTeX environment cannot be centered in LyX (only paragraphs can), so I defined a special environment ScrapCenter when I want to insert graphics via Sweave and make them center-aligned. To put all efforts together, this is the new literate-scrap.inc (compare it with http://cran.r-project.org/contrib/extra/lyx/literate-scrap.inc): Enjoy pressing a single Enter key in LyX now!   P.S. the side effect of this modification is, your R code will be separated by empty lines. But Sweave will remove blank lines by default, so that is not really a big trouble. P.P.S. Next time I will write my solution of using pgfSweave in LyX — if you have never tried the pgfSweave package in R, I’d strongly recommend you do it right now!! My comment on this package is: amazingly beautiful! See a preview lyx-pgfsweave-demo. 	 0 Comments
R IDE and debugger now available for 64-bit Windows; Webinar Tuesday	https://www.r-bloggers.com/2010/02/r-ide-and-debugger-now-available-for-64-bit-windows-webinar-tuesday/	February 18, 2010	David Smith	We’ve just upgraded REvolution R Enterprise to version 3.1 and expanded the available platforms to include 64-bit Windows. (REvolution R Enterprise is our subscription-based distribution of R.) This means that you can now create R programs on Windows that use all of your available memory, instead of being constrained by the 3Gb limit imposed by 32-bit versions of Windows. And you can create and run such programs using REvolution’s R Productivity Environment, with all of the features of a modern development environment including breakpoints and a step-debugger. If you’d like to learn more, I’ll be hosting a live webinar this coming Tuesday, February 23 to highlight the programming and debugging capabilities of REvolution R Enterprise. If you attended our previous webinar 7 Ways to Increase Your R Productivity this one will be similar, but with additional focus on the capabilities to analyze large data sets with the 64-bit version. Click the link below for more details and to register. REvolution Computing: Webinar: 7 Ways to Increase Your R Productivity 	 0 Comments
SPSS Co-Founder Tex Hull Joins REvolution Computing	https://www.r-bloggers.com/2010/02/spss-co-founder-tex-hull-joins-revolution-computing/	February 18, 2010	David Smith	We’re proud to announce that Tex Hull, who together with REvolution CEO Norman Nie created the first version of SPSS, has joined the REvolution team. Tex will be working with Norman and our CTO David Champagne to take REvolution R Enterprise to the next level, specifically to improve its scalability to handle very large data sets. You can read more about Tex’s appointment in the press release issued this morning. REvolution Computing: SPSS Co-Founder “Tex” Hull Joins REvolution Computing 	 0 Comments
Gas price seasonality	https://www.r-bloggers.com/2010/02/gas-price-seasonality/	February 18, 2010	kafka	"Last spring I read “Quantitative Trading” by Ernest P. Chan. In his book, he suggested to buy gas futures contract at the end of February and sell it later, in March. Today, I decided to test this strategy by using R-language.
The most important thing for such investigation is data. For this purpose, I used this public data: www.eia.doe.gov
I made 2 test – for the first one, I used futures contract, which will be settled after 4 months and for the second one, I used gas spot price.
In the first plot, we can see monthly price returns scaled by months (1-12). It is clear, that mean of March’s returns are above 0. What is encouraging in this plot is that the ranges of returns are above 0 as well (meaning, that majority of March’s returns was above 0).
Anova test gives p-value below 0.01 – some months in the group has different mean (that supports seasonality idea). March’s t-test gives these values: t = 2.8064, df = 15, p-value = 0.01329.  I used gas spot prices to generate second plot. It is similar to the first one, except that March’s returns has longer tail. It is worth to note, that September stands as a positive month as well. The statistics for this test are not so strong, as it was in the first example.
P-value of Anova test is only 0.11 and March’s t-test is:
t = 1.3791, df = 15, p-value = 0.1881  R-language code to run these tests: "	 0 Comments
Analysis of Winter Olympic Medal Data Using R	https://www.r-bloggers.com/2010/02/analysis-of-winter-olympic-medal-data-using-r/	February 18, 2010	Jeromy Anglim		 0 Comments
raster images and RImageJ	https://www.r-bloggers.com/2010/02/raster-images-and-rimagej/	February 18, 2010	romain francois	The next version of R includes support for raster images in standard and grid graphics.  The RImageJ package uses ImageJ through rJava to read and manipulate images from various formats Paul Murrell closed the gap and contributed code that allows using images from the RImageJ package as raster objects.  makes the graph : This feature depends on R >= 2.11.0, so will only get available when this version becomes current, in the meantime, you can get the package from its rforge project page 	 0 Comments
Genetic Algorithm Systematic Trading Development– Part 2	https://www.r-bloggers.com/2010/02/genetic-algorithm-systematic-trading-development-part-2/	February 17, 2010	Intelligent Trading		 0 Comments
R project named in Intelligent Enterprise 2010 Editor’s Choice Awards	https://www.r-bloggers.com/2010/02/r-project-named-in-intelligent-enterprise-2010-editors-choice-awards/	February 17, 2010	David Smith	Intelligent Enterprise has announced its 2010 “Editors Choice” Awards, and the R project is included as one of twelve “Companies to Watch” in the Business Intelligence category. R Project is an open-source statistical programming environment that is winning broad praise and accelerating uptake as a language for in-database analytics. The likes of SAS, SPSS and Information Builders are even using it to extend their proprietary suites. It might seem surprising that an open-source project is named as a “Company” to watch, but this reflects the notice that commercial companies are taking of R in recent years. As a company dedicated to the support and evolution of R, we at REvolution Computing are thrilled to see such recognition afforded to R. Intelligent Enterprise: Intelligent Enterprise Editors’ Choice Awards 2010 	 0 Comments
Real-World, Real-Time Analytics	https://www.r-bloggers.com/2010/02/real-world-real-time-analytics/	February 17, 2010	JD Long	Stop wasting time reading my drivel. You need to head over the the DataWrangling.com blog and read Peter Skomoroch’s interview with Bradford Cross of FlightCaster. Peter wrote up this interview back in August 2009, so I’m a little late to this party. There’s some really great quotes in this interview. Here’s a few of my fav quotes from Cross: At Google, the research scientists prototype in python and R, and then port to C++ for the real scalable map reduce runs. Building layer upon layer of abstraction is a big key…    The technical term for this is “wrap the crap.” Here’s a problem I think anyone who works with data and models can relate to: I made a lot of mistakes early in my career in building trading models where I let me theories get too far ahead of what I could really test in practice. That is not a good place to be. Unfortunately, this is an easy mistake to make. 	 0 Comments
hash-1.99.x	https://www.r-bloggers.com/2010/02/hash-1-99-x/	February 17, 2010	Christopher Brown	"hash-2.0.0 has been released please read about it here:  Earlier today, hash-1.99.x was released to CRAN.  This is a stable release and adds some more functions to an already full-featured hash implementation.  This version fixes some bugs, adds some features, improves performance and stability.  You can read about the hash package in my previous blog post, The hash package: hashes come to R.  All changes were responsible from users who wrote in and contributed, thoughts, ideas and use cases.  Keep the good ideas coming.  Two of the major changes are summarized below.  Matthias Buch-Kromann of the Copenhagen Business School recommended the ability to access multiple keys from a single call and even access the same key multiple times.  This was previously allowed using the [[ method, but was deprecated.  By convention, the [[ method returns only one value.  ( You can read about the conventions of this and other R accessors in my previous blog post, R Accessors Explained. ) This behavior has returned to hash-1.99.x the use of the values method and the and optional keys argument: 

h <- hash( c('a','b','c'), 1:3 )

values(h)

values(h, keys=c('a','b','c','a','b','c' ) )

 Matthias suggested calling the method mget, but there was some disparity with the mget function in base.  The generic function that I needed just wouldn't play nice with base::mget. Another change in the behavior was prompted by Mohammad Fahim of the Department of Computer Engineering and Computer Science at the University of Louisville.  He wrote me to ask if there is a way to suppress warnings when trying to access non-existent keys.  When accessing  hashes hundreds of thousands of times, it becomes a drag to continually see: key: xxxx not found in the hash : hash_table_name I have refactored the behavior to be more R-like by following na.action-type conventions.  Now the default behavior is to return NA when trying to access non-existing keys. 

> library(hash)

>h <- hash( c('a','b','c'), 1:3 )

> h  h[ letters[1:5] ]

containing 6 key-value pair(s).

a : 1

b : 2

c : 3

d : NA

e : NA

 The behavior is also controllable by na.action.hash option.  The functions are provided for most use cases: Behaviors can be set by setting the na.hash.action option.  For example, to get the default behavior: 

> options( na.hash.action = na.fail.hash )

> h$d

Error: key, d, not found in hash.

> h[[ 'd' ]]

Error: key, d, not found in hash.

 And , for the [ and [[ methods, this behavior can be declared at access time: 

> h[[ 'd', na.action=na.warn.hash ]]

Warning: key, d, not found in hash.

d

NA

> h[[ 'd', na.action=na.fail.hash ]]

Error: key, d, not found in hash.

> h[[ 'd', na.action=na.default.hash ]]

d

NA

 If you don’t like these hash-key-miss behaviors, you are free to write your own.  Functions should minimally accept arguments of the hash and the key. Thanks to both Matthias and Mohammed for your feedback. New features are on their way.  Notably, the ability to use any object as keys and to preserve the order of the hash.  These are sometimes called Indexed Hashes.  Look for that in the hash-2.00.x release.  If you would like to see features added contact me at cbrown -at- opendatagroup.com References: "	 0 Comments
Springer solution manuals on line	https://www.r-bloggers.com/2010/02/springer-solution-manuals-on-line/	February 17, 2010	xi'an	Springer Verlag has just posted on its webpage both the student and the instructor solution manuals to “Introducing Monte Carlo Methods with R”. Yes, both! Before you rush there, the Catch-22 in this announcement is that the access to the instructor version is restricted to registered instructors. So, if you are registered as an instructor with Springer Verlag, you are welcome to it. If not and you think you should be registered, feel free to contact Springer Verlag. Else, enjoy the student odd-numbered exercise version. 	 0 Comments
Visualize dynamic data from R in 3d	https://www.r-bloggers.com/2010/02/visualize-dynamic-data-from-r-in-3d/	February 17, 2010	» R	"In this video i demonstrate a nice feature of Bio7 to visualize 3d data created in “R” dynamically. The data for the points is generated in “R” and then transferred to the  OpenGL view of Bio7. In the first example a random plot is generated and updated.
In the second example 10000 random (lighted) spheres are normally distributed.
Finally the third example shows 100000 random (primitive) points normally distributed. "	 0 Comments
Generalized linear mixed effect model problem	https://www.r-bloggers.com/2010/02/generalized-linear-mixed-effect-model-problem/	February 16, 2010	Shige		 0 Comments
How to make a mosaic plot in R	https://www.r-bloggers.com/2010/02/how-to-make-a-mosaic-plot-in-r/	February 16, 2010	David Smith	Mosaic plots (aka treemaps) are a great way to visualize hierarchical data. A collection of rectangles represents all the elements to be visualized (customers, news items, blog posts), with the size and color of the rectangles coding attribute. But what makes this chart unique is the arrangement of the elements: where there is hierarchy (customer segments, news topics, post categories) those elements are collected and labelled together, perhaps even with subcategories. It’s easier to show than explain: you might have seen this mosaic plot of stories in Google News before, where the stories are arranged by topic area (news, sports, etc.) and sized by the number of mentions. You can create your own mosaic plot yourself in R, too: FlowingData explains how, using their own blog posts (arranged by category) as data.   Click the link below for the simple five-step process, using the map.market function from the portfolio package (with some final touch-up in Adobe Illustrator). FlowingData: An Easy Way to Make a Treemap 	 0 Comments
You can Hadoop it! It’s elastic! Boogie woogie woog-ie!	https://www.r-bloggers.com/2010/02/you-can-hadoop-it-it%e2%80%99s-elastic-boogie-woogie-woog-ie/	February 16, 2010	JD Long	This blog's name in Chinese!  I just came back from the future and let me be the first to tell you this: Learn some Chinese. And more than just cào nǐ niáng  (肏你娘) which your friend in grad school told you means “Live happy with many blessings”. Trust me, I’ve been hanging with Madam Wu and she told me it doesn’t mean that. So how did I travel to the future to visit with Madam Wu, you ask? Well the short answer is Hadoop. Yeah, the cute little elephant. As I have told you before, multicore makes your R code run fast by using worm holes to shoot your results back from the future. Well Hadoop actually takes you to the future on the back of an elephant and you can bring your own results back! I couldn’t make this up if I tried, so you know it’s true! And what’s fantastic about all of this is Hadoop works with R! And Amazon will let you rent a time traveling elephant through their Elastic MapReduce service! I think Amazon coined the term “Time Travel as a Service” or TTaaS  generally pronounced as “ta-tas” in the industry. If you are a CTO be sure and use this in your next “vision statement” pitch so everyone will know you’re hip to all this cloud stuff. So you use R and you want to travel into the future on the back of an elephant to visit Madam Wu and get your model results back, don’t you? Well it’s a damn good thing you read this blog because I’m going to give you the keys to the Wu dynasty and a little 福寿 while we’re at it. I’ve never had an original thought in my life so I started with this discussion over at the AMZN E M/R discussion forum. Peter Skomoroch from Data Wrangling gives a very good example with all the data and code provided so you can run it yourself.  Pete’s example really shakes the  yáng guǐzi, as we say in the future. In addition I read the documentation for David Rosenberg’s HadoopStreaming package which was good for insight, but I didn’t use the package as it’s really focused on the ‘big data’ problem. That elephant is so freaking cute!  Prior to my foray into time travel, I knew that Hadoop could be used to process big text files and do something like rip out all the links and count them. But I thought that Hadoop was all about processing big data. I never paid attention to the big Hadoop elephant in the room because I don’t have big data. I have big CPU hogging models (mostly slow because I don’t code worth a shit). What got me reconsidering my world view was John Myles White’s comment on my multicore post earlier. John encouraged me to look into running my simulations on AMZN’s E M/R service using Hadoop streaming. So instead of giving Hadoop  a big fat text file to parse, I just gave it a text file with 10,000 rows each containing an integer from 1:10,000. Then I refactored my R code to read a line from stdin, trim it down to just the integer, and then go run the simulation with that number. When done I had it serialize the resulting model output and return that to stdout. Hadoop takes care of chopping up the input and pulling together the output. I learned a few “gotchas” or, as we say in the future: 臭婊子(I think that should be plural). I’ll do a whole blog post on gotchas soon, but here’s the bullet points:  -cacheFile s3n://rdata/plyr_0.1.9.tar.gz#plyr_0.1.9.tar.gz More to come later. I’ve gotta get back to the future. You hold the elephant and I'll plug this in.  	 0 Comments
For fun: Correlation of US State with the number of clicks on online banners	https://www.r-bloggers.com/2010/02/for-fun-correlation-of-us-state-with-the-number-of-clicks-on-online-banners/	February 16, 2010	Tal Galili	"“Chitika research” published today a fun small dataset (you can download it from here) in a post titled “The Educated are Harder to Advertise To”. In this post I had three goals in mind: The data set offers us 51 two dimensional data points, one fore each US states with the variables: CTR and % of the population who graduated college.
The CTR means “Click Through Rate” and is from chitika data base and collected from over two random days in January (a total of 31,667,158 total impressions), and is from the full range of Internet users (they don’t have traditional demographic data – every impression is completely anonymous). Coupled with the percent of the population who graduated from college, this data presents a stunning -0.63 correlation between the two measurements. Hinting that “The Educated are Harder to Advertise To” (as the original post suggested). The data can be easily visualized using just a few lines of code in R: Resulting with the following image:
 I was asked in the comments (by Eyal) to add my own conclusions to the analysis. Does higher intelligence imply lower chances of clicking ads, my answer (under the present data) is simple “I don’t know”. The only real conclusion I can make of the data is that there might be a point in checking this effect in a more rigorous way (which I am sure is already being done). What should we have done in order to know? When doing scientific research, we often ask ourselves how sure are we of our results. The rule of thumb for this type of question is called “the pyramid of evidence“. It is a way to organize various ways of getting “information” about the world, in an hierarchy of reliability. Here is a picture of this pyramid:  We can see that the most reliable source is a systematic review of randomized controlled trials. In our case, that would mean having controlled experiments where you take groups of people with different levels of “intelligence” (how would you measure that?), and check their CTR (click through rates) on banner ads. This should be done in various ways, correcting for various confounders , and later the results and conclusions (from several such experiments) should be systematically reviewed by experts on the subject. All of this should be done in order to make a real assessment of the underlying question – how does smarts effects banner clicking.
And the reason we need all of this work is because of what is said in the title of the next section: As is written in the article on wikipedia: “Correlation does not imply causation” is a phrase used in science and statistics to emphasize that correlation between two variables does not automatically imply that one causes the other (though it does not remove the fact that correlation can still be a hint, whether powerful or otherwise). The opposite belief, correlation proves causation, is a logical fallacy by which two events that occur together are claimed to have a cause-and-effect relationship. But a much clearer explenation of it was given by the following XKCD comic strip:
 The motivation for my post is based on this digg post trying to hint how Religiousness is connected to “negative” things such as crimes, poverty and so on. That post was based on the following links: If someone is motivated, he/she can extract that data and combine it with the current provided data. In conclusion: this simplistic dataset, combined with other data resources, provides opportunity for various fun demonstrations of pairs correlation plots and of nice spatial plots (of states colored by their matching variable). It is a good opportunity to emphasize (to students, friends and the like) that “Correlation does not imply causation!”.
And finally – If you are an R lover/blogger and feel like playing with this – please let me know   . "	 0 Comments
A Case Study in Optimising Code in R	https://www.r-bloggers.com/2010/02/a-case-study-in-optimising-code-in-r/	February 16, 2010	Jeromy Anglim		 0 Comments
Sugar price seasonality	https://www.r-bloggers.com/2010/02/sugar-price-seasonality/	February 16, 2010	kafka	"Recently, Orion securities have issued a “BUY” recomendation for Cugar ETF. Because, neither I follow the recommendations nor I’m big fan of TA (I have to admit, that I was…), I decided to check sugar price seasonality. Voila, the mean of monthly returns are presented in the graph. February, April and May tend to be negative and June and July show positive returns.
BUT! Don’t forget to ask – are these results significant? P-value for July is 14%, 34% for April. The rest is above 50%. So, keep in mind, that these results are very weak…  More descriptive plot:  "	 0 Comments
R Web Application – “Hello World” using RApache (~7min video tutorial)	https://www.r-bloggers.com/2010/02/r-web-application-%e2%80%93-%e2%80%9chello-world%e2%80%9d-using-rapache-7min-video-tutorial/	February 16, 2010	Tal Galili	I just noticed a google buzz from Jeroen ooms, with a Youtube video titled “RApache Hello World + POST arguments + catching errors.” In this ~7 min video tutorial, Jeroen shares with us: Thank you Jeroen for a very simple, step by step, tutorial:  p.s: For more videos by Jeroen, have a look at 	 0 Comments
Using Google Reader	https://www.r-bloggers.com/2010/02/using-google-reader/	February 15, 2010	Rob J Hyndman	Google Reader is a fantastic way to keep track of new papers that are appearing in many different journals, and also to follow some of the interesting research blogs (and blogs on other topics) that are out there. Google Reader checks websites for you and lets you know of any new material that appears. Instead of you having to look at dozens of different websites to discover new information, all you need to do is open up Google Reader and all the information comes to you. In some ways it is like an email account, but where the messages contain new additions to websites that you are interested in. Google Reader is called an “RSS reader” because it reads RSS feeds. RSS stands for “Really Simple Syndication”. A website with an RSS feed makes it possible to track additions to the site without actually visiting it yourself.  There are other RSS readers, but Google Reader is the most widely used. Recently Google Reader added a facility so that it now also tracks sites that don’t have RSS feeds. If you haven’t used it before, here’s how to get started.   Each morning I read through anything new on Google Reader including new research papers in journals that I track, new articles on some statistics blogs that I follow, etc. In fact, I have over 500 subscriptions! I don’t read every article or it would take all day, but I do scan the headlines and read what looks interesting. It can take a while to collect all the subscriptions for journals you might want to read. To make it easy, you can just piggy-back on my journal collection (which covers all statistics journals, both forecasting journals, plus a few econometrics and demography journals, as well as all statistical preprints on arxiv). Click here if you want to subscribe to all the same journals as me. If you are interested in R, R-bloggers is very useful as it combines the posts from a large number of blogs about R.  Just go to the site and click on the RSS feed icon and you will be able to add a subscription to your Google Reader account. For those who like to keep up with LaTeX, the TeX community aggregator does something similar for bloggers writing about LaTeX and related topics. Again, just click on the RSS feed icon. Here is a list of statistics research blogs. Check them out and subscribe to anything that takes your fancy. This website has an RSS feed, as do my other websites. Just click the orange button at the top-right of the page and select “Google Reader” and then you will receive any new posts I make in your Google Reader account.   	 0 Comments
Genetic Algorithm Systematic Trading Development — Part 1	https://www.r-bloggers.com/2010/02/genetic-algorithm-systematic-trading-development-part-1/	February 15, 2010	Intelligent Trading		 0 Comments
Two-way Analysis of Variance (ANOVA)	https://www.r-bloggers.com/2010/02/two-way-analysis-of-variance-anova/	February 15, 2010	Ralph	The analysis of variance (ANOVA) model can be extended from making a comparison between multiple groups to take into account additional factors in an experiment. The simplest extension is from one-way to two-way ANOVA where a second factor is included in the model as well as a potential interaction between the two factors. As an example consider a company that regularly has to ship parcels between its various (five for this example) sub-offices and has the option of using three competing parcel delivery services, all of which charge roughly similar amounts for each delivery. To determine which service to use, the company decides to run an experiment shipping three packages from its head office to each of the five sub-offices. The delivery time for each package is recorded and the data loaded into R: The data is then displayed using a dot plot for an initial visual investigation of any trends in delivery time between the three services and across the five sub-offices. The colour aesthetic is used to distinguish between the three services in the plot. This code produces the following graph: Graph of the delivery time for different services and destintions The graph shows a general pattern of service carrier 1 having shorter delivery times than the other two services. There is also an indication that the differences between the services varies for the five sub-offices and we might expect the interaction term to be significant in the two-way ANOVA model. To fit the two-way ANOVA model we use this code: The * symbol instructs R to create a formula that includes main effects for both Destination and Service as well as the two-way interaction between these two factors. We save the fitted model to an object which we can summarise as follows to test for importance of the various model terms: We have strong evidence here that there are differences between the three delivery services, between the five sub-office destinations and that there is an interaction between destination and service in line with what we saw in the original plot of the data. Now that we have fitted the model and identified the important factors we need to investigate the model diagnostics to ensure that the various assumptions are broadly valid. We can plot the model residuals against fitted values to look for obvious trends that are not consistent with the model assumptions about independence and common variance. The first step is to create a data frame with the fitted values and residuals from the above model: Then a scatter plot is used to display the fitted values and residuals where the colour asthetic highlights which points correspond to the three competing delivery services: The xlab() and ylab() are used to change the text on the axis labels. The residual diagnostic plot is: Diagnostic Residual Plot for Delivery Time Model There are no obvious patterns in this plot that suggest problems with the two-way ANOVA model that we fitted to the data. As an alternative display we could separate the residuals into destination sub-offices, where the facet_wrap() function instructs ggplot to create a separate display (panel) for each of the destinations. To produce the following alternative residual plot: Diagnostic Residual Plot for Delivery Time Model by Destination No obvious problems in this diagnostic plot. We could also consider dividing the data by delivery service to get a different view of the residuals: This creates the following graph: Diagnostic Residual Plot for Delivery Time Model by Service Again there is nothing substantial here to lead us to consider an alternative analysis. Lastly we consider the normal probability plot of the model residuals, using the stat_qq() option: The quantile plot is: Normal Probability Plot for Delivery Time Model This plot is very close to the straight line we would expect to observe if the data was a close approximation to a normal distribution. To round off the analysis we look at the Tukey HSD multiple comparisons to confirm that the differences are between delivery service 1 and the other two competing services: Even with the multiple comparison post-hoc adjustment there is very strong evidence for the differences that we have consistenly observed throughout the analysis. We can use ggplot to visualise the difference in mean delivery time for the services and the 95% confidence intervals on these differences. We create a data frame from the TukeyHSD output by extracting the component relating to the delivery service comparison and add the text labels by extracting the row names from the data frame. We then use the geom_pointrange() to specify lower, middle and upper values based on the three pairwise comparisons of interest. The coord_flip() is used to make the confidence intervals horizontal rather than vertical on the graph. This can be confusing for creating the axis labels as we specify the label where it would appear prior to the filp of coordinates. In the example above we add text to the y axis but this now appears on the x axis in the final graph: Plot of Confidence Intervals for Mean Differences using Tukey HSD 	 0 Comments
R vs. Matlab – a small example	https://www.r-bloggers.com/2010/02/r-vs-matlab-a-small-example/	February 15, 2010	nattomi	"At the institute I’m working quite a lot of people prefer using Matlab and only a few of them know about R. Today one of my colleagues — who is also an eager user of Matlab — ran into the following problem: He struggled for long minutes of how he should design a loop for doing this task. Of course writing such a loop is not a highly difficult task, but why would we waste our time, if we can get the same result in a single line of R code? For the sake of illustration, I’ve generated an input vector for the case of  (the value of  was 99 in my colleague’s problem as well): v <- rep(99:1,times=99:1) and used the one-liner M <- t(matrix(unlist(tapply(v,rep(1:99,times=99:1),function(x) c(x,rep(0,99-length(x))))),nrow=99)) This is the kind of compactness I like pretty much in R. At the end I would like to emphasize that this post is not against Matlab, it just points out how the different logic of the R language can simplify problem solving in many situations. As a bonus let me share the visualization of the resulted matrix using the color2D.matplot function of the plotrix package:


library(plotrix)

color2D.matplot(M,c(0,1),c(1,0),c(0,0))  "	 0 Comments
Bay Area meet-up Tuesday: John Chambers speaks, cocktail reception	https://www.r-bloggers.com/2010/02/bay-area-meet-up-tuesday-john-chambers-speaks-cocktail-reception/	February 15, 2010	David Smith	At tomorrow’s meeting of the Bay Area R User’s Group, ACM Software Systems award winner and R Core Group member John Chambers will give a keynote talk on the R language. The talk, titled Interfaces and Paradigms, will explore choices for computing with data with visits along the way to the history of R, reasons for the growth of R, and the importance of different concepts for the computational tasks. Before the meeting REvolution Computing is hosting a cocktail reception where you can mingle over drinks with fellow R users. The reception is part of the Predictive Analytics World conference, but it’s free to attend if you’re coming for the user group meeting. The reception will be held starting at 6:30PM in the Gold Ballroom of the Palace Hotel (right next to the venue for the R user group meeting). RSVPs are requested at the link below. Bay Area UseR Group: Cocktail Hour & John Chambers on Interfaces and Paradigms at PAW Conference 	 0 Comments
Rcpp 0.7.7	https://www.r-bloggers.com/2010/02/rcpp-0-7-7-2/	February 14, 2010	romain francois	"A good 2 days after 0.7.6 was released, here comes Rcpp 0.7.7. The reason for this release is that a subtle bug installed itself and we did not catch it in time The new version also includes two new class templates : unary_call and binary_call that help integration of calls (e.g. Rcpp::Language objects) with STL algorithms. For example here is how we might use
unary_call This emulates the code As usual, more examples in the unit tests "	 0 Comments
Cure model using R	https://www.r-bloggers.com/2010/02/cure-model-using-r/	February 13, 2010	Shige		 0 Comments
Rcpp 0.7.6	https://www.r-bloggers.com/2010/02/rcpp-0-7-6-2/	February 13, 2010	Thinking inside the box	"
The changes are summarised below in the NEWS file snippet, more details are
in the ChangeLog as well.

 
As always, even fuller details are in the ChangeLog on the
Rcpp page which also
leads to the downloads, the 
browseable
doxygen docs and zip files of doxygen output for the standard formats.
Questions, comments etc should go to the
rcpp-devel mailing list
off the R-Forge page

 "	 0 Comments
highlight 0.1-5	https://www.r-bloggers.com/2010/02/highlight-0-1-5/	February 13, 2010	romain francois	"I’ve pushed the version 0.1-5 of highlight to CRAN, it should be available in a couple of days. This version fixes highlighting of code when one wants to display the prompt and the continue prompt. For example, this code : 

 gets highlighted like this:  using this code: Under the hood, highlight now depends on Rcpp and uses some of the C++ classes of the new Rcpp API. See the get_highlighted_text function in the code.  "	 0 Comments
Exact Complexity of Mergesort, and an R Regression Oddity	https://www.r-bloggers.com/2010/02/exact-complexity-of-mergesort-and-an-r-regression-oddity/	February 13, 2010	Ryan Rosario	"It’s nice to be back after a pretty crazy two weeks or so.  Let me start off by stating that this blog post is simply me pondering and may not be correct. Feel free to comment on inaccuracies or improvements! In preparation for an exam and my natural tendencies to be masochistic, I am forcing myself to find the exact complexities of some sorting algorithms and I decided to start with a favorite – mergesort. Mergesort divides an array or linked list first into two halves (or close to it) and then recursively divides the successive lists into halves until it ends up with two lists containing 1 element each – the base case. The elements are then compared and switched so that they are in order, and form their own list.  At successive levels we compare the last element of the first sublist to the first element of the second sublist and merge them together to form another list. This process continues up the recursion tree until the entire original list is sorted. For a more comprehensive and precise description, see this article on mergesort. Easy: The Worst Case The worst case is easy as any CS student will tell you. Looking at the recursion trees below, we see that in the worst case, we must perform  “layers” of “parallel” merges corresponding to the height of the recursion tree, each merge performing  comparisons. Then, the worst case complexity of merge sort is . Three Cases Trivially, if the number of elements is a power of 2 (figure a), all of the sublists at each level of the recursion tree will have the same size. This forms a recursion tree that is full and balanced. In this case, we have the worst case complexity because each element is involved in exactly  merge operations each of which take  time. In situation b the number of elements is 1 smaller than a power of 2 (i.e. 3, 7, 15). In situation c, the number of elements is 1 greater than a power of 2 (i.e. 3, 9, 17).   In situation a, the number of merge operations required for each of the  elements is  which yields  operations. In situations b and c, some elements require more merge operations than others; however, the number of merges differs by at most 1. The number of merges for each element is approximately  and is exactly  or , then, the total number of work performed is equal to
 Then what? We need to find expressions for the constants  and . I will call them  and . I drew out several recursion trees and got the following table:  From the table, I extracted the following relationships. I will leave the proof by induction to the reader.  
 This is not quite exactly  as I have heard some people say, but it is pretty darn close (). Using the code below, I simulated several values for  and  and the corresponding plot for .  Note that there is not a perfect overlap here. The Precise Regression Issue Naturally, if the exact running time of merge sort is  then I should get a regression model that has a perfect fit to the data… Hmm. The  value is exactly 1. This confused me for a while, until I realized that this was a precision issue in R, and not a regression issue. Note that if this were a perfect fit, the Residuals section should be completely 0. Also, the base of the logarithm ( or 2) does not matter; the results are exactly the same! So why is ?  So why is this an issue? Most people do simply round up 0.99999809 to 1, but 1 is profound to statisticians: it means that the linear model has a perfect fit. Ok, so maybe it is a precision issue or option within R. But maybe not… look at the p-value! The question to take home is: why is it that the p-value has such precision, while the  value does not? Personally, if we had to round, I would round the p-value to 0, and keep the precision of . Thoughts? Conclusion to the Original Purpose of this Post So, to make a really long story just long, the total running time is not exactly , rather it is . "	 0 Comments
Introduce your friends to R	https://www.r-bloggers.com/2010/02/introduce-your-friends-to-r-2/	February 12, 2010	Jim		 0 Comments
R – Sorting a data frame by the contents of a column	https://www.r-bloggers.com/2010/02/r-sorting-a-data-frame-by-the-contents-of-a-column/	February 12, 2010	i82much	Let’s examine how to sort the contents of a data frame by the value of a column This last statement might look surprising if you’re used to Java or a traditional programming language.  Rather than becoming a single boolean/truth value, minor actually becomes a vector of truth values, one per row in the age column.  It’s equivalent to the much more verbose code in Java: Just as expected, the value of minor is a vector: Next we create a data frame, which groups together our various vectors into the columns of a data structure: The arguments (sex=sex, age=age, income=income, minor=minor) assign the same names to the columns as I originally named the vectors; I could just as easily call them anything.  For instance, But I prefer the more descriptive labels I gave previously. Now let's say we want to order by the age of the people.  To do that is a one liner: This is not magic; you can select arbitrary rows from any data frame  with the same syntax: The order function merely returns the indices of the rows in sorted order. Note the $ syntax; you select columns of a data frame by using a dollar sign and the name of the column.  You can retrieve the names of the columns of a data frame with the names function. As you can see, they are exactly the same. So what we're really doing with the command is Note the trailing comma; what this means is to take all the columns.  If we only wanted certain columns, we could specify after this comma. 	 0 Comments
Making publication-ready tables with xtable	https://www.r-bloggers.com/2010/02/making-publication-ready-tables-with-xtable/	February 12, 2010	David Smith	When you use R at the command-line, the textual output is limited by the medium: one monospaced font, with no typesetting of any kind. That’s great when you’re doing exploratory analysis, but what about when you want to include R output in a report or publication? In other words, what if you want to convert this Analysis of Variance table: to this:   or this:    With the xtable package, you can. It’s a handy tool for converting the output of many of R’s statistical functions into a presentation-ready table in LaTeX or HTML format. For example, to create the table above, I simply did the following: and then pasted the HTML it generated straight into this blog post. I also tweaked the border=”1″ table directive to border=”0″ — if you have a decent Web editor you could pretty the table up further to your heart’s desire. You can find more examples of xtable in action in the package vignette. xtable package: vignette  	 0 Comments
Seminar: Statistical Methods for DNA Resequencing Analysis in Disease-Gene Studies	https://www.r-bloggers.com/2010/02/seminar-statistical-methods-for-dna-resequencing-analysis-in-disease-gene-studies/	February 12, 2010	Stephen Turner		 0 Comments
One-way ANOVA (cont.)	https://www.r-bloggers.com/2010/02/one-way-anova-cont/	February 12, 2010	Ralph	In a previous post we considered using R to fit one-way ANOVA models to data. In this post we consider a few additional ways that we can look at the analysis. In the analysis we made use of the linear model function lm and the analysis could be conducted using the aov function. The code used to fit the model is very similar: The output from using the summary function of the fitted model object shows the analysis of variance table with the p-value showing evidence of differences between the three groups. In R we can investigated the particular groups where there are differences using Tukey’s multiple comparisons: The multiple comparison tests highlight that the difference is due to comparing treatments 1 and 2. These 95% confidence intervals for the differences shown above can be plotted: which gives  The post-hoc adjustments are recommended as we are testing after looking at the data rather than undertaking a pre-planned analysis. 	 0 Comments
Highlight the R syntax on your (WordPress) blog using the wp-syntax plugin	https://www.r-bloggers.com/2010/02/highlight-the-r-syntax-on-your-wordpress-blog-using-the-wp-syntax-plugin/	February 12, 2010	Tal Galili	"In case you have a self hosted WordPress blog, and you wish to show your R code in it, how would you do it? The simplest solution would be to just paste the code as plain text, which will look like this: x 
plot(x, xlab = “index”, main = “Example code”) But if you would like to help our readers orient themselves inside your code by giving different colors to different commands in the code (a.k.a: syntax highlighting). So it would like something like this: How then would you do it? The easiest way to do this inside a self hosted WordPress blog is by installing a plugin called WP-Syntax: WP-Syntax provides clean syntax highlighting using GeSHi — supporting a wide range of popular languages (including R). It supports highlighting with or without line numbers and maintains formatting while copying snippets of code from the browser. But there is a problem. The current WP-Syntax version is using an old version of GeSHi, and only the newer version (currently GeSHi version 1.0.8.6) includes support for R syntax. In order to solve this I patched the plugin and I encourage you to download (the fixed version of) WP-Syntax from here, which will allow you to highlight your R code. After installing (and activating) the plugin, in order to add R code to your post you will need to:
1) Only work in HTML mode (not the Visual mode). Or else, the code you will paste will be messed up.
2) Put your code between the  If you wish to have R syntax higlight inside an HTML file, I encourage you can have a look at the highlight package, by Romain Francois. If you want to higlight your R syntax inside wordpress.com, here is a blog post by Erik Iverson showing how to do that using Emacs. p.s: If you have a blog in which you write about R, please let me know about it in the comments (Or just join R-bloggers.com) – I’d love to follow you   "	 0 Comments
Self Aware Classes	https://www.r-bloggers.com/2010/02/self-aware-classes/	February 11, 2010	R Tips	I thought this up the other night but I’m not sure where I’m going to use it.  But I thought that I would through it out there anyway, maybe it will solve all of someone else’s problems. I was thinking about classes in R and R is SO NOT object oriented, as much as people claim that it is.  There are so many things that it lacks, like data hiding, encapsulation, methods, class level variables, etc.  Then I realized that the environments in R can be used in much the same way.  I’ll show a couple of examples. read more 	 0 Comments
Handling hierarchical data structure in R	https://www.r-bloggers.com/2010/02/handling-hierarchical-data-structure-in-r/	February 11, 2010	Shige		 0 Comments
Future of Open Source Survey	https://www.r-bloggers.com/2010/02/future-of-open-source-survey/	February 11, 2010	David Smith	Our good friends at North Bridge Venture Partners have just opened the 2010 Future of Open Source Survey, an annual look at the state of open source technology and business models, the driving factors in rising adoption of open source products and how the market for open-source software is evolving. Anyone who uses open-source software in the commercial world, whether as a user or vendor of open-source products, should participate in the survey and have their voice heard. Creator Michael Skok describes the background of the survey in this blog post:  Well over a decade ago, in my operating career, Open Source presented itself as a solution to a critical development deadline for one of my engineering teams. I was inspired. Little did I know at the time how significant a moment that was. Shortly thereafter, upon entering the Venture world, I was determined to support Open Source Software development by seeking out and investing in the best and brightest potential people behind it.  And yet as I began that work, I quickly found a deep gulf between those who were driving the OSS  movement and those attempting to learn from and work with it. So I arranged “The Great Open Source Debate” to bring this into focus. See pictures below of some of the OSS leaders and leading CIOs of the day. It was a fun start and further encouraged me to create an enduring program, and to literally open it up to the community at large. Now 5 years later, North Bridge is once again proud to be sponsoring the 4th annual Future of Open Source survey in conjunction with InfoWorld, and with more than 25 leading OSS industry collaborators and thousands of participants.  As usual, the results of the survey will be presented at the Open Source Business Conference, held this year March 17-18 in San Francisco. The results are made available to the public: here are the results from the 2008 and 2009 surveys. 2010 Open Source Survey: Take the Survey 	 0 Comments
Artificial Immune Systems and Financial Applications?	https://www.r-bloggers.com/2010/02/artificial-immune-systems-and-financial-applications/	February 11, 2010	Intelligent Trading		 0 Comments
Running totals in R	https://www.r-bloggers.com/2010/02/running-totals-in-r/	February 11, 2010	i82much	Let’s say we wanted to simulate flipping a coin 50 times using the statistical language R, where a 1 is a heads and 0 is tails. Now we can plot the values to see which were heads and which were tails: What if we want to see a running total of the number of heads over time?  I was faced with just this problem for a completely different domain; I’ve written the function myself multiple times in Java and other languages but I was hoping it would be built-in to a stats language like R.  Fortunately I was right; the command you want is cumsum (cumulative sum).  There are a total of four functions like this: Cumulative Sums, Products, and Extremes They work just as you’d expect. Running total of number of heads This is a trivial example, but it certainly simplifies my life. 	 0 Comments
Using J48 Decision Tree Classifier to Dynamically Allocate Next Day Position in Stocks or Bonds	https://www.r-bloggers.com/2010/02/using-j48-decision-tree-classifier-to-dynamically-allocate-next-day-position-in-stocks-or-bonds/	February 11, 2010	Intelligent Trading		 0 Comments
Two New Soils-Related KMZ Demos	https://www.r-bloggers.com/2010/02/two-new-soils-related-kmz-demos/	February 10, 2010	dylan	"LCC KMZ
Soil Texture KMZ  
Forgot to post these KMZ files: 1-km scale, aggregate LCC and soil texture data, derived from SSURGO. These are part of a series of KMZ / raster datasets that will be published soon. See attached files at the bottom of the page. Enjoy! read more "	 0 Comments
Video: What is R?	https://www.r-bloggers.com/2010/02/video-what-is-r/	February 10, 2010	David Smith	By popular demand, we’ve made the video of our 30-minute webcast “The R Project” available on YouTube so that everyone can easily watch it. If you (or a friend!) have ever wondered what this R thing is all about, this is the video for you. Here’s the first part:   Because of YouTube restrictions it’s split up into four parts, but you can find the other three parts at the link below. There you’ll also find also a download link for the slides in PDF format, with lots of useful links to R resources like tutorials, blogs, books and more. REvolution Computing: The R Project: Data Analysis and Statistical Graphics for the Enterprise 	 0 Comments
Loglinear models using R	https://www.r-bloggers.com/2010/02/loglinear-models-using-r/	February 10, 2010	Shige		 0 Comments
Speeding up simulations with Amazon EC2	https://www.r-bloggers.com/2010/02/speeding-up-simulations-with-amazon-ec2/	February 10, 2010	David Smith	Over at Cerebral Mastication, JD Long tells a characteristically entertaining and informative story about how he uses R to run stochastic simulations of insurance portfolios and reinsurance treaties. A typical job involves 10,000 simulations, and when each estimate takes over 20 seconds you’re talking some serious time to get the job done.  Fortunately, this is the kind of problem that lends itself to parallelization: with a multiprocessor machine running R, you can run as many simulations as you have processors at the same time, and cut the total computation time by the same factor. JD describes how he boots up an 8-core instance running R on Amazon EC2, and then uses the multicore package on Linux to reduce the computation time by a factor of 8. Sadly for Windows users, multicore doesn’t run on Windows. But as Boris Shor points out in a comment to JD’s post, REvolution R Enterprise allows you to do parallel programming like this (using the foreach operator) on a multiprocessor Windows machine (or even a cluster of many machines). Cerebral Mastication: Using the R multicore package in Linux with wild an passionate abandon 	 0 Comments
LocusZoom: Plot regional association results from GWAS	https://www.r-bloggers.com/2010/02/locuszoom-plot-regional-association-results-from-gwas/	February 10, 2010	Stephen Turner		 0 Comments
Easy way of determining number of lines/records in a given large file using R	https://www.r-bloggers.com/2010/02/easy-way-of-determining-number-of-linesrecords-in-a-given-large-file-using-r/	February 10, 2010	Pradeep Mavuluri		 0 Comments
Typos in Chapters 1, 4 & 8	https://www.r-bloggers.com/2010/02/typos-in-chapters-1-4-8/	February 10, 2010	xi'an	Thomas Clerc from Fribourg pointed out an embarassing typo in Chapter 8 of “Introducing Monte Carlo Methods with R”, namely that I defined on page 247 the complex number  as the squared root of 1 and not of -1! Not that this impacts much on the remainder of the book but still an embarassment!!! An inconsistent notation was uncovered by Bastien Boussau from Berkeley this time for the book The Bayesian Choice. In Example 1.1.3, on page 3, I consider an hypergeometric  distribution, while in Appendix A, I denote hypergeometric distributions as , inverting the role of the population size and of the sample size. Sorry about that, inconsistencies in notations are alas occuring in my books… In case I have not mentioned it so far, Example 4.3.3 further involves a typo (detected by Cristiano Passerini from Pontecchio Marconi) again with the hypergeometric distribution  ! The ratio should be  	 0 Comments
Frank Harrell to teach Regression Modeling Strategies short course	https://www.r-bloggers.com/2010/02/frank-harrell-to-teach-regression-modeling-strategies-short-course/	February 9, 2010	David Smith	If you’re using regression models but want really hone your regression-fu this short course on Regression Modeling Strategies by Frank Harrell looks really interesting. Frank is the author of the book Regression Modeling Strategies which is my go-to reference whenever I’m doing regression of any kind in R, so it’s definitely worth a trip to Nashville to if you want to learn how to make regression really work. Registration closes March 1, and the 3-day course begins March 31. By the way, if you haven’t tried them already check out Frank’s rms package (formerly the Design package, a package of regression tools to accompany the book) and also his Hmisc package. Hmisc has a bunch of useful utilities (PDF) to extend the R language in various ways — the tools for creating attractive tables in LaTeX are especially useful.  Department of Biostatistics, Vanderbilt University: 2010 Upcoming short courses in Regression Modeling Strategies 	 0 Comments
Using the R multicore package in Linux with wild and passionate abandon	https://www.r-bloggers.com/2010/02/using-the-r-multicore-package-in-linux-with-wild-and-passionate-abandon/	February 9, 2010	JD Long	One of my primary uses for R is to build stochastic simulations of insurance portfolios and reinsurance treaties. It’s not uncommon for each of my simulations to take 20 seconds or more to complete (if you’re doing the math, that’s 55 hours for 10K sims or, approximately 453 games of solitaire) . Initially I ran my sims in R running on an Oracle VirtualBox (Oracle now owns Virtualbox! *gasp* ) running Ubuntu. Lately I’ve moved to running my sims on EC2 machines. I’m not yet doing RMPI clustering, although that is on my roadmap. Currently I just fire up a couple of 8 core instances and run 5K sims on each one then FTP the results back to my desktop. It’s not very sexy, but it gets the job done… I guess the same could be said of myself, except substitute “makes slurping sounds eating udon” in the place of “gets the job done.” When running processor intensive crap (that’s a stochastic modeling term) the single threaded nature of R is painful. In Linux or Mac (i.e. NOT Windows) the multicore package is a real godsend. I did a quick code review and, from what I can tell, multicore exploits worm holes to travel back in time and reports your results in a fraction of the time you would expect it to take. Seriously. I expect that as the code matures my computer will fill up with simulation results from simulations which I have not even coded yet. It’s almost like magic, except without the rabbit and hat. The crux of the package is a parallel-ized version of lapply() called mclapply(). I believe the mc stands for ‘magic carpet’ and is an allusion to the worm hole technology. So how does one harness this package for nefarious self interest doing parallel operations in R? The ultra short answer is: write your R code so that the most processor intensive bit is done with an lapply() function. Then replace the lapply() with mclapply().  Of course you have to load the multicore package before you run it. But that’s basically it. How I implement mcapply() is thusly: I build a table with all my random draws for my simulations. So if I have 20 variables and want to run 10,000 simulations then I’ll build a data frame with all 200,000 values (generally 10K rows and 21 columns for 20 variables + and index). The index keeps track of the draw number. Then I have code that performs the ‘valuation’ based on a single observation of the 20 variables. I wrap the valuation step in a function and then call the valuation process 10,000 times with mclapply(). So it might look something like this: myOutput  The drawList object is simply a list of the possible indexes (i.e. 1:10000). When the code has iterated over each value from drawList the results will be in the myOutput object. Tada! I recommend the htop program for tracking what’s going on with processor utilization in Linux (I presume Mac too if you ask Steve Jobs nicely). If everything is cranking well, and you have 8 cores, you might see an image that looks something like this:  I don’t understand time travel, but I’ve found that I have better luck if I set mc.preschedule=FALSE. Apparently prescheduled magic carpets are finicky. If I leave mc.preschedule to the default of TRUE then I find that often some of my cores go underutilized. Let me know if you have other multicore tips and tricks. If you want to give me shit for running my simulations as root, feel free. I’m impervious to your “best practices” mumbo jumbo. La la la la la la!! Not listening! Special thanks to John Cavazos over at the University of Delaware from whom I stole the MC for Dummies image. John, your a gentleman and a humble scholar. Damn few of us left. 	 0 Comments
Package Update Roundup: Dec 2009 – Jan 2010	https://www.r-bloggers.com/2010/02/package-update-roundup-dec-2009-jan-2010/	February 9, 2010	David Smith	A special double edition of the Package Update Roundup this month! This is a list of new or updated packages that were released for R in December and January, as announced on the r-packages mailing list. To include other updates on this list, please email David Smith. For a complete list of all updates on CRAN, see the CRANberries archive for December 2009 and January 2010. Follow package name links for ratings and other information on crantastic.org. dcemriS4, a package for medical image analysis, has been released to update package dcemri.  Deducer, a cross-platform data analysis GUI, has been updated.  doMPI, a backend for the foreach package for parallel computing, has been updated.  exact2x2, a package for exact conditional tests and confidence Intervals for 2×2 tables, was updated to allow optionally give an exact McNemar’s test together with odds ratio estimates and confidence intervals. exactci, a new package that calculates exact tests and confidence intervals for binomial and Poisson tests, has been released. ff and bit, two packages that help manage large datasets in R by mapping them to disk instead of storing them in memory, have been updated. fortunes, a package delivering snippets of amusing R wisdom, has been updated (and now includes more than 250 fortunes!) GGally, a package to draw plot matrices (like pairs does, but using ggplot2), was released. ggplot2, the comprehensive high-level graphics system for R, has been updated adding several new features. itertools, a new package of tools for manipulating iterators, has been released.  mecdf, a package for multivariate ECDF based models, has been released. micEcon, micEconAids, and miscTools, three packages for microeconomic analysis and microeconomic modelling have been released (these packages were previously combined into a single micEcon package) mspath, a new package to fit multi-state path-dependent models in discrete time, has been released. parcor, a new package of tools to estimate the matrix of partial correlations based on different regularized regression methods, has been released. pgfSweave, a package that improves the process of including graphs in Sweave documents, has been released. randomSurvivalForest, a package for ensemble survival analysis based on a random forest of trees, has been updated with new model controls. randtoolbox, a package for random number generation and testing, has been updated with new features. RobASt, a family of packages for robust asymptotic statistics, has been updated. Rcpp, the package interfacing R with C++, has been updated to make integration between R and C++ even easier. sp, a package that  provides class definitions for spatial data, and utilities for spatial data handling and manipulation, has been updated to improve performance. sqldf, a package for accessing data in data frames using SQL, has been updated.     	 0 Comments
Python in Sweave document	https://www.r-bloggers.com/2010/02/python-in-sweave-document/	February 9, 2010	Matti Pastell	"The driver now catches the input in verbatim environment  (which can be easily changed to listings) and the output to python environment. The tex document can then be processed with the python latex package to evaluate the python expressions. Tho use the driver you need to put the option “engine=”python” in your code chunks.
 My example is python code that calculates and plots the frequency response of a moving average filter. Here is the code in the  Sweave document ma.Rnw. Process the file in R using the custom driver above: It should produce ma.tex. Then run latex (make sure you have the python package installed): Which should then in turn output this ma.pdf file.
 "	 0 Comments
Rcpp 0.7.5	https://www.r-bloggers.com/2010/02/rcpp-0-7-5-2/	February 9, 2010	romain francois	Dirk released Rcpp 0.7.5 yesterday The main thing is the smarter wrap function that now uses techniques of type traits and template meta-programming to have a compile time guess at whether an object is wrappable, and how to do it. Currently wrappable types are :  Here comes an example (from our unit tests) :  Apart from that, other things have changed, here is the relevant section of the NEWS for this release 	 0 Comments
Linux Server Profiling: Using Open Source Tools For Bottleneck Analysis	https://www.r-bloggers.com/2010/02/linux-server-profiling-using-open-source-tools-for-bottleneck-analysis/	February 9, 2010	Ed Borasky		 0 Comments
Spatial Statistics in R: An Introduction	https://www.r-bloggers.com/2010/02/spatial-statistics-in-r-an-introduction/	February 8, 2010	VCASMO - drewconway		 0 Comments
Spatial Analytics in R Video	https://www.r-bloggers.com/2010/02/spatial-analytics-in-r-video/	February 8, 2010	Drew Conway	The video from John Myles White’s outstanding introductory talk on spatial analysis with R to the NYC R Statistical Meetup is now available in the R video repository, and is also embedded after the jump. I would like to thank John for this fantastic talk, and for those interested the slides from this talk have also been posted at the meetup website. Also, please help keep the R video archive open by donating today to the fund by clicking the “Donate” button at the right.  	 0 Comments
Registration open for R/Finance 2010	https://www.r-bloggers.com/2010/02/registration-open-for-rfinance-2010/	February 8, 2010	David Smith	Registrations are now open for the R/Finance 2010 conference, to be help April 16-17 in Chicago. Last year’s meeting was a great success, and this year’s looks to be just as good, with some great keynotes lined up:   There’s also a great series of pre-conference tutorials lined up:   The tutorials are a steal at $50 each and sell out fast. Register for the conference and tutorials now at the link below. (REvolution Computing is a proud sponsor of R/Finance 2010.) R/Finance 2010: Conference and Tutorial Registration 	 0 Comments
Classification for stock directional prediction	https://www.r-bloggers.com/2010/02/classification-for-stock-directional-prediction/	February 8, 2010	Intelligent Trading		 0 Comments
Regression Modeling Strategies Course by Frank Harrell	https://www.r-bloggers.com/2010/02/regression-modeling-strategies-course-by-frank-harrell/	February 8, 2010	Stephen Turner		 0 Comments
R Tutorial Series: Basic Polynomial Regression	https://www.r-bloggers.com/2010/02/r-tutorial-series-basic-polynomial-regression/	February 8, 2010	John Quick		 0 Comments
Updated R code for Bayesian Core	https://www.r-bloggers.com/2010/02/updated-r-code-for-bayesian-core/	February 8, 2010	xi'an	Simply to mention a minor change made in the prog4.R code for Bayesian Core. Nothing life-threatening, mind you!, just a  term replacing a  term… 	 0 Comments
Practical Implementation of Neural Network based time series (stock) prediction  -PART 5	https://www.r-bloggers.com/2010/02/practical-implementation-of-neural-network-based-time-series-stock-prediction-part-5/	February 7, 2010	Intelligent Trading		 0 Comments
Barnard’s exact test – a powerful alternative for Fisher’s exact test (implemented in R)	https://www.r-bloggers.com/2010/02/barnard%e2%80%99s-exact-test-%e2%80%93-a-powerful-alternative-for-fisher%e2%80%99s-exact-test-implemented-in-r/	February 7, 2010	Tal Galili	"(The R code for Barnard’s exact test is at the end of the article, and you could also just download it from here)  About half a year ago, I was studying various statistical methods to employ on contingency tables. I came across a promising method for 2×2 contingency tables called “Barnard’s exact test“. Barnard’s test is a non-parametric alternative to Fisher’s exact test which can be more powerful (for 2×2 tables) but is also more time-consuming to compute (References can be found in the Wikipedia article on the subject). The test was first published by George Alfred Barnard (1945). Mehta and Senchaudhuri (2003) explain why Barnard’s test can be more powerful than Fisher’s under certain conditions: When comparing Fisher’s and Barnard’s exact tests, the loss of power due to the greater discreteness of the Fisher statistic is somewhat offset by the requirement that Barnard’s exact test must maximize over all possible p-values, by choice of the nuisance parameter, π. For 2 × 2 tables the loss of power due to the discreteness dominates over the loss of power due to the maximization, resulting in greater power for Barnard’s exact test. But as the number of rows and columns of the observed table increase, the maximizing factor will tend to dominate, and Fisher’s exact test will achieve greater power than Barnard’s. After finding about Barnard’s test I was sad to discover that (at the time) there had been no R implementation of it. But last week, I received a surprising e-mail with good news. The sender, Peter Calhoun, currently a graduate student at the University of Florida, had implemented the algorithm in R. Peter had  found my posting on the R mailing list (from almost half a year ago) and was so kind as to share with me (and the rest of the R community) his R code for computing Barnard’s exact test. Here is some of what Peter wrote to me about his code: On a side note, I believe there are more efficient codes than this one.  For example, I’ve seen codes in Matlab that run faster and display nicer-looking graphs.  However, this code will still provide accurate results and a plot that gives the p-value based on the nuisance parameter.  I did not come up with the idea of this code, I simply translated Matlab code into R, occasionally using different methods to get the same result.  The code was translated from: Trujillo-Ortiz, A., R. Hernandez-Walls, A. Castro-Perez, L. Rodriguez-Cardozo. Probability Test.  A MATLAB file. URL http://www.mathworks.com/matlabcentral/fileexchange/loadFile.do?objectId=6198 My goal was to make this test accessible to everyone.  Although there are many ways to run this test through Matlab, I hadn’t seen any code to implement this test in R.  I hope it is useful for you, and if you have any questions or ways to improve this code, please contact me at [email protected] p.s: I added some minor cosmetics to the code, like allowing the input to be a table/matrix and the output to be a list.  Fisher’s Exact Test for Count Data data:  Convictions
p-value = 0.0004652
alternative hypothesis: true odds ratio is less than 1
95 percent confidence interval:
0.0000000 0.2849601
sample estimates:
odds ratio
0.04693661 $contingency.table
A  B
A  2 15
B 10  3 $Wald.Statistic
[1] 3.609941 $Nuisance.parameter
[1] 0.4401 $p.value.one.tailed
[1] 0.0001528846 Final note: I would like to thank Peter Calhoun again for sharing his code with the rest of us – Thanks Peter! "	 0 Comments
Updated Tactical Asset Allocation Results	https://www.r-bloggers.com/2010/02/updated-tactical-asset-allocation-results/	February 6, 2010	Joshua Ulrich	"
 "	 0 Comments
Consumption and Shopping as an Event	https://www.r-bloggers.com/2010/02/consumption-and-shopping-as-an-event/	February 6, 2010	Benedikt Orlowski	"This is an recent project about consumption and the categorization of  consumers. The whole survey you can explore on: http://www.geographie.uni-erlangen.de/projects/konsum/  (german!).
For the evaluation, R was used.  “Der Verhaltensraum des Konsumenten – The Space of Consumer Behaviour” The Space shows the consumer in the center of a two dimensional abstract space. He is able to “move” to every point in the coordinate system. Each buy he takes a new position. The first dimension is the dimension of hedonism (buying – shopping), the second is the dimension of autonomy. They are both independent. 31 test persons took part in the survey. They filled out a diary about their buys.  This is a sample person with 16 buys. The arrows indicate the direction of time, the circle grows for each buy at the same position.  This graph compares the buys of different people in the same category of shops.
It shows a clustering towards hedonism in fashion shops and a clustering towards buying in drugstores.
More on the project homepage. "	 0 Comments
Time-Space Analysis with R	https://www.r-bloggers.com/2010/02/timespace-analysis-with-r/	February 6, 2010	Benedikt Orlowski	"Beside the visualisation of TimeSpace Tracks, I’m trying to find a way to analyze GPX-Tracks with statistical software. This are the first results with R (The R Project for Statistical Computing): GPS track analized with R package “trip”   density plot 3D ^This graph is a result of the analysis with the package trip (Spatial analysis of animal track data). Unfortunatelly i’m do not understand witch scale is used by the package. ^Trackpoints as a function of density. Since there is a trackpoint recorded every 10 sec., it is possible to interpretate the density of the trackpoints as time-spend. This is a two day track. The highest peak in the right corner is my home (Nuremberg). The peaks in the backstage are both university in Erlangen. The path on the rigth side I did with my bicycle, the left one with the train. But how to examine specific areas? trackdata density plot 3D   ^1500 m arround my house in the city center. With clickppp() from the spatstat package it’s possible to choose e.g. a point with the mouse: ####### Example Code:
plot(tripdata_utm) # plots the recorded trackpoints (converted to UTM)
P_center 
center 
D 
P_selection 
 density plot 2D   ^Another function of density (2D). qqcout plot   ^Trackpoints as a function of time. Here the trackpoints are divided by a grid and counted. Since the device records the position every 10 sec. The qqcount can be clearly interpreted as time-spend. The next step is to add this data to a gis layer. "	 0 Comments
R / Finance 2010 Open for Registration	https://www.r-bloggers.com/2010/02/r-finance-2010-open-for-registration/	February 5, 2010	Thinking inside the box	"
 
R / Finance 2010: Applied Finance with R
April 16 and 17, 2010
Chicago, IL, USA
 
The second annual R / Finance conference for applied finance using R, 
the premier free software system for statistical computation and graphics,
will be held this spring in Chicago, IL, USA on Friday April 16 and
Saturday April 17.
 
Building on the success of the inaugural R / Finance 2009 event, this
two-day conference will cover topics as diverse as portfolio theory,
time-series analysis, as well as advanced risk tools, high-performance
computing, and econometrics. All will be discussed within the context of
using R as a primary tool for financial risk management and trading.
 
Invited keynote presentations by Bernhard Pfaff, Ralph Vince, Mark Wildi
and Achim Zeileis are complemented by over twenty talks (both full-length
and ‘lightning’) selected from the submissions.  Four optional tutorials
are also offered on Friday April 16.
 
R / Finance 2010 is organized by a local group of R package authors and
community contributors, and hosted by the International Center for Futures
and Derivatives (ICFD) at the University of Illinois at Chicago.
 
Conference registration is now open. Special advanced registration pricing is
available, as well as discounted pricing for academic and student
registrations. 
 
More details and registration information can be found at the website at  

http://www.RinFinance.com

 
For the program committee:
 
See you in Chicago in April! 

 "	 0 Comments
[Event] R / Finance 2010: Applied Finance with R	https://www.r-bloggers.com/2010/02/event-r-finance-2010-applied-finance-with-r/	February 5, 2010	Manos Parzakonis	"One of the greatest event on R is under way… R / Finance 2010: Applied Finance with R
April 16 & 17, Chicago, IL, US The second annual R / Finance conference for applied finance using R,
the premier free software system for statistical computation and graphics,
will be held this spring in Chicago, IL, USA on Friday April 16 and
Saturday April 17. Building on the success of the inaugural R / Finance 2009 event, this
two-day conference will cover topics as diverse as portfolio theory,
time-series analysis, as well as advanced risk tools, high-performance
computing, and econometrics. All will be discussed within the context of
using R as a primary tool for financial risk management and trading. Invited keynote presentations by Bernhard Pfaff, Ralph Vince, Mark Wildi
and Achim Zeileis are complemented by over twenty talks (both full-length
and ‘lightning’) selected from the submissions.  Four optional tutorials
are also offered on Friday April 16. R / Finance 2010 is organized by a local group of R package authors and
community contributors, and hosted by the International Center for Futures
and Derivatives [ICFD] at the University of Illinois at Chicago. Conference registration is now open. Special advanced registration pricing is
available, as well as discounted pricing for academic and student
registrations. More details and registration information can be found at the website at http://www.RinFinance.com For the program committee: Gib Bassett, Peter Carl, Dirk Eddelbuettel, John Miller,
Brian Peterson, Dale Rosenthal, Jeffrey Ryan –
Registration is open for the 2nd International conference R / Finance 2010
See http://www.RinFinance.com for details, and see you in Chicago in April! One of the invited speakers is Ralph Vince, an author that I’m really fond of as he is quite practical in the core of things he writes about. "	 0 Comments
R/Finance 2010: Registration Open	https://www.r-bloggers.com/2010/02/rfinance-2010-registration-open/	February 5, 2010	Joshua Ulrich	"
 "	 0 Comments
Because it’s Friday: Evolution in song	https://www.r-bloggers.com/2010/02/because-its-friday-evolution-in-song/	February 5, 2010	David Smith	With thanks to my favourite science blog Bad Astronomy, the late great Carl Sagan sings again, this time on the topic of evolution. He’s joined by autotuned Richard David Attenborough and Jane Goodall. Great stuff.    Bad Astronomy: The Unbroken Thread  	 0 Comments
Practical Implementation of Neural Network based time series (stock) prediction  -PART 4	https://www.r-bloggers.com/2010/02/practical-implementation-of-neural-network-based-time-series-stock-prediction-part-4/	February 4, 2010	Intelligent Trading		 0 Comments
Eight R Video Tutorials on VCASMO	https://www.r-bloggers.com/2010/02/eight-r-video-tutorials-on-vcasmo/	February 4, 2010	Ed Borasky		 0 Comments
RProtoBuf: protocol buffers for R	https://www.r-bloggers.com/2010/02/rprotobuf-protocol-buffers-for-r/	February 4, 2010	romain francois	"We (Dirk and I) released the initial version of our package RProtoBuf to CRAN this week. This packages brings google’s protocol buffers to R I invite you to check out the main page for protobuf to find the language definition for protocol buffers as well as tutorial for officially (i.e. by google) supported languages (python, c++ and java) as well as the third party support page that lists language bindings offered by others (including our RProtoBuf package. Protocol buffers are a language agnostic data interchange format, based on a using a simple and well defined language. Here comes the classic example that google uses for C++, java and python tutorials.  First, the proto file defines the format of the message. Then you need to teach this particular message to R, which is simply done by the readProtoFiles function.  Now we can start creating messages :  And then access, modify fields of the message using a syntax extremely close to R lists In R, protobuf messages are stored as simple S4 objects of class ""Message"" that contain an external pointer to the underlying C++ object. The Message class also defines methods that can be accessed using the dollar operator The package already has tons of features, detailed in the vignette .. and there is more to come "	 0 Comments
Mapping the Massachusetts election upset with R, ctd	https://www.r-bloggers.com/2010/02/mapping-the-massachusetts-election-upset-with-r-ctd/	February 4, 2010	David Smith	Last week we looked at an analysis done in R by the good folks at Offensive Politics, looking at the political climate surrounding the recent Senate election in Massachusetts. There were some very insightful comments (thanks, Revolutions readers!) about the design of the charts, especially in the choice of color schemes used (the originals didn’t use a neutral white midpoint to represent no change). Well, Offensive Politics have taken another look at the charts, and produced some new ones which are a great improvement. With the revisions, the chart comparing the increase in Republican vote percentage from 2008 to 2010 looks like this:   Offensive Politics notes: From this map we can see that from 2008 to 2010 the Republican vote percentage actually increased in every single township in MA. This is kind of shocking, especially when you consider that Democrats enjoy a 15 point registration advantage statewide. Check the link below for all the updated charts, and of course the R code to create them. Offensive Politics: Re-mapping Massachusetts Special election results 	 0 Comments
RProtoBuf 0.1-0	https://www.r-bloggers.com/2010/02/rprotobuf-0-1-0/	February 3, 2010	Thinking inside the box	"
RProtoBuf
had a funny start. I had blogged about 
the 12 hour passage from proof of concept to R-Forge project
following the
ORD session hackfest in October.
What happened next was as good.  Romain emailed within hours of
the blog post and reminded me of a similar project that is part of Saptarshi Guha’s
RHIPE R/Hadoop
implementation.  So the three of us–Romain, Saptarshi and I—started emailing
and before long it becomes clear that Romain is both rather intrigued by this (whereas
Saptarshi has slightly different needs for the inner workings of his Hadoop
bindings) and was able to devote some time to it. So the code kept growing and
growing at a fairly rapid clip.  Til that stopped as we switched to working feverishly
on Rcpp to both support the
needs of this project, and to implement ideas we had while working on this.
That now lead to the point where 
Rcpp
is maturing in terms of features, so we will probably have time come back to
more work on 
RProtoBuf
to take advantage of the nice templated autoconversions we now have in
Rcpp.  Oddly
enough, the 
initial blog post
seemed to anticipate changes in 
Rcpp.  

 
Anyway — 
RProtoBuf
is finally here and it already does a fair amount of magic based of code reflection
using the proto files.  The Google documentation has a simple
example of a ‘person’ entry in an ‘addressbook’ which, when translated to R,
goes like this:

 
There is more information at the
RProtoBuf
page, and we already have a
draft package vignette,
a ‘quick’ overview vignette
and a
unit test summary vignette.

 
More changes should be forthcoming as
Romain and I find time to
code them up.  Feedback is as always welcome.

 "	 0 Comments
Three Must-Have Books on Data Visualization	https://www.r-bloggers.com/2010/02/three-must-have-books-on-data-visualization/	February 3, 2010	Ed Borasky		 0 Comments
One-way Analysis of Variance (ANOVA)	https://www.r-bloggers.com/2010/02/one-way-analysis-of-variance-anova/	February 3, 2010	Ralph	"Analysis of Variance (ANOVA) is a commonly used statistical technique for investigating data by comparing the means of subsets of the data. The base case is the one-way ANOVA which is an extension of two-sample t test for independent groups covering situations where there are more than two groups being compared. In one-way ANOVA the data is sub-divided into groups based on a single classification factor and the standard terminology used to describe the set of factor levels is treatment even though this might not always have meaning for the particular application. There is variation in the measurements taken on the individual components of the data set and ANOVA investigates whether this variation can be explained by the grouping introduced by the classification factor. As an example we consider one of the data sets available with R relating to an experiment into plant growth. The purpose of the experiment was to compare the yields on the plants for a control group and two treatments of interest. The response variable was a measurement taken on the dried weight of the plants. The first step in the investigation is to take a copy of the data frame so that we can make some adjustments as necessary while leaving the original data alone. We use the factor function to re-define the labels of the group variables that will appear in the output and graphs: The labels argument is a list of names corresponding to the levels of the group factor variable. A boxplot of the distributions of the dried weights for the three competing groups is created using the ggplot package: The geom_boxplot() option is used to specify background and outline colours for the boxes. The axis labels are created with the xlab() and ylab() options. The plot that is produce looks like this:  Initial inspection of the data suggests that there are differences in the dried weight for the two treatments but it is not so clear cut to determine whether the treatments are different to the control group. To investigate these differences we fit the one-way ANOVA model using the lm function and look at the parameter estimates and standard errors for the treatment effects. The function call is: We save the model fitted to the data in an object so that we can undertake various actions to study the goodness of the fit to the data and other model assumptions. The standard summary of a lm object is used to produce the following output: The model output indicates some evidence of a difference in the average growth for the 2nd treatment compared to the control group. An analysis of variance table for this model can be produced via the anova command: This table confirms that there are differences between the groups which were highlighted in the model summary. The function confint is used to calculate confidence intervals on the treatment parameters, by default 95% confidence intervals: The model residuals can be plotted against the fitted values to investigate the model assumptions. First we create a data frame with the fitted values, residuals and treatment identifiers: and then produce the plot: which produces this graph:

We can see that there is no major problem with the diagnostic plot but some evidence of different variabilities in the spread of the residuals for the three treatment groups. "	 0 Comments
Advanced graphics in R – links to slides and code	https://www.r-bloggers.com/2010/02/advanced-graphics-in-r/	February 3, 2010	David Smith	A lot of attention recently has gone to the more modern (and, dare I say, sexier) graphics systems in R, ggplot2 and lattice. But there’s a lot of power in the base graphics system built into core R, especially when you want control over every aspect of how the graph is laid out. Ryan Rosario has put together some slides detailing some more advanced tricks capable with the base graphics system. For example:  All the details are in Ryan’s slides, and yes, the R code to create the example charts is also available. Byte Mining: Advanced Graphics in R  	 0 Comments
Web Development with R – an HD video tutorial of Jeroen Ooms talk	https://www.r-bloggers.com/2010/02/web-development-with-r-%e2%80%93-an-hd-video-tutorial-of-jeroen-ooms-talk/	February 3, 2010	Tal Galili	Here is a HD version of a video tutorial on web development with R, a lecture that was given by Jeroen Ooms (the guy who made A web application for R’s ggplot2). This talk was given at the Bay Area UseR Group meeting on R-Powered Web Apps.  You can also view the slides for his talk and view (great) examples for: stockplot, lme4, and gpplot2. Thanks again to Jeroen for sharing his knowledge and experience! 	 0 Comments
Predicting the Locations of ‘Emergency’ Ushahidi Reports in Port-au-Prince, and Implications for Crowdsourcing	https://www.r-bloggers.com/2010/02/predicting-the-locations-of-%e2%80%98emergency%e2%80%99-ushahidi-reports-in-port-au-prince-and-implications-for-crowdsourcing/	February 2, 2010	Drew Conway	Recently, Patrick Meier, PhD candidate at Tufts University and member of the Ushahidi Advisory Board, provided me with a dataset containing the first 72 hours of reports registered with Ushahidi in Port-au-Prince after the January 12th earthquake.  First, a huge thank you to Patrick for providing me with this data and the opportunity to analyze it.  If you are unfamiliar with Ushahidi check out their dedicated site to the Haiti deployment, but I believe this technology has great potential for social science research. The data are quite interesting, with each report including precise longitude and latitude information, date, time, type of report, and a description of the incident.  While all of this information is fascinating, the report types acts as an excellent categorical variable and thus are a natural starting point for analysis.  Ushahidi defines six general report categories: (1) emergency, (2) threats, (3) vital lines, (4) response, (5) other, and (6) persons news.  For every longitude and latitude pair there are often multiple reports of each type observed; therefore, it would be useful to know the count of each type of report in each locale.  Once this information is aggregated it may be possible to generate predictions for the probability of observing various report types in given location in Port-au-Prince. First, using the magical data munging power of R, I created a new dataset containing these counts in the form below:   Next, to generate predictions we will need to specify a spatial model that assumes a distribution appropriate for the event count data above.  Unlike the autoregressive spatial models previously discussed, for count data the generalized linear spatial model proposed by Christensen and Ribeiro, implemented in R with the geoRglm package, provides the necessary Poisson link.   The process for estimating the probabilities begins by laying an imaginary grid over Port-au-Prince.  Then, using Markov-chain Monte Carlo simulation we will use the observed occurrences of Category 1 Ushahidi reports to predict the probability of observing these reports for every cell in the grid.  For brevity, I will not discuss the mathematical assumptions of this model (for an introduction read the geoRglm vignette), but below I include the final few lines of R code used to generate these predictions. UPDATE: I have added an R file with the entire data cleaning and analysis, and Ushahidi dataset to the ZIA Code Repository for anyone interested in replicating the analysis.  Notice, this process requires some tuning as well as several provided values, such as the covariance pairs and the initial  value for the Bayesian prior.  After these values have been inputted; however, the resulting analysis is quite interesting. The above figure depicts the predicted probabilities for each grid cell generated by the GLM-MCMC process as a choropleth, where darker regions indicate a higher probability of an emergency report.  Included in this figure are also all of the places (longitude and latitude pairs) where Ushahidi reports were observed, depicted as dark points over the choropleth.  This analysis may indicate some interesting aspects crisis areas dynamics, as well as the effect of Ushahidi itself. The most striking observation is that locations with the highest concentration of Ushahidi reports also have the lowest probability of being emergencies.  Instead, places where emergency reports are most likely are sparsely dispersed around throughout the grid.  This is counterintuitive, as we might expect that a high concentration of reports in one place would be predicated by several emergency reports, i.e., an emergency occurs, which leads to follow on reports, etc. There are at least two possible explanations for this; first, that emergency responders (within the first 72 hours) were poorly allocated, since most emergency reports occur in isolated areas.  This; however, seems unlikely given the sheer number of responders present in Port-au-Prince during this time period.  Alternatively, it may indicate a weakness in the crowd-sourced reporting for this instance, as from this data we would conclude that in worst areas—where there are the most reports—no emergencies are reported.  The question; then, is: how accurately are the Ushahidi reports reflecting the reality of the crisis? This is a fundamental issue with using this data for rigorous analysis, as there are clearly several dynamics contributing to the data generating process.  I look forward to exploring this data further in the future, and incorporating more of the covariates provided in the dataset. 	 0 Comments
Survey: Share your thoughts about predictive models with Aberdeen Group	https://www.r-bloggers.com/2010/02/survey-share-your-thoughts-about-predictive-models-with-aberdeen-group/	February 2, 2010	David Smith	Analyst firm Aberdeen Group is conducting research into the use of predictive models in business with a 10-minute survey. It’s focused mainly on businesses that are using (or plan to use) predictive models to forecast aspects of their business and the systems they have in place (or plan to put in place) to do so. If you’re using predictive models at work, or plan to, consider adding your voice to the survey. Here’s the description of the survey from Aberdeen: “Top performing organizations are increasingly using predictive models to improve their business. This research study will determine what strategies and actions companies are taking to:  Whether you use predictive techniques or not, take this 10 minute survey and you’ll be able to compare your organizations performance against its peers, learn where predictive analytics and data mining might bring benefits, and how to get started.  Everyone that completes the survey will receive a complimentary copy of the final research report.  Individual responses will be kept strictly confidential and data will only be used in aggregate. We greatly appreciate your participation in this research effort and look forward to sharing our findings with you. “ 	 0 Comments
The Power to … What did you say?	https://www.r-bloggers.com/2010/02/the-power-to-what-did-you-say/	February 2, 2010	martin	It is just about a year ago (exactly January 6th, 2009) that a New York Times article on R did fuel the dispute on what statistical analysis tool is “the best”. One of the highlight of the article was a quote from SAS’ Anne H. Milley: “I think it addresses a niche market for high-end data analysts that want free, readily available code,” said Anne H. Milley, director of technology product marketing at SAS. She adds, “We have customers who build engines for aircraft. I am happy they are not using freeware when I get on a jet.” I recently found a SAS press release (dating March 23, 2009) entitled: “SAS to offer R integration to support analytical innovation”, which reads: “It is no secret that SAS has been working on interfacing with R,” said Anne Milley, SAS’ Senior Director of Technology Product Marketing. “SAS and R are here to stay, and as organizations work to harness the full potential of their data, an expanded set of analytics options can only help.” First let’s be cheerful about this move (whatever the actual solution will look like anyway), but on the other side, if Anne Milley’s quotes stand for SAS’ reliability, I doubt they deserve their reputation. 	 0 Comments
Ensemble Prediction	https://www.r-bloggers.com/2010/02/ensemble-prediction/	February 2, 2010	joe	Weather is unpredictable. Small differences in initial conditions can develop into big differences in the pattern of circulation, in the timing and location of cyclones, rainfall etc. This is true no matter how good the initial observing system is. The approach taken by organisations such as ECMWF or NCEP is to re-run numerical forecast models with a range of carefully chosen initial conditions. The collection of runs is called the ensemble. Ensemble prediction systems (EPS) give probabilistic forecasts for variables such as rainfall, temperature etc. Current operational EPS have 20 (GFS)  or 51 (ECMWF) ensemble members from which the probability distributions are derived. ECMWF give an overview of their system here. The probability distributions capture part of the intrinsic uncertainty in weather or climate. The graph below shows histograms of 20 ensemble member temperatures near some major cities. The data were extracted from NCEP GENS 16-day 2m temperature forecast produced at 00UTC 2 Feb 2010 (i.e GFS forecasts for 18 Feb).  The maps below show some corresponding ensemble statistics for the entire globe (1° resolution, equal area cylindrical projection).  The upper map indicates that forecast uncertainty (standard error) is high between 40° and 60° in both hemispheres (related to the chaotic behaviour of  jet streams.) Currently, 16 day temperatures north of Lake Baikal in Siberia are very uncertain, for example. The contours indicate ensemble median temperatures. Skewness in ensemble temperatures is shown in the lower map. For example, large negative skewness is found in north central US, eastern mediterranean, and Paraguay/Mato Grosso. This suggests tail risk of low temperatures relative to ensemble mean in these areas.       EPS is the future of weather and climate forecasting. These systems produce huge amounts of data. Building useful applications of EPS is both a challenge and an opportunity. For anyone interested, the R code used to produce these graphs is given here. 	 0 Comments
Practical Implementation of Neural Network based Time Series (Stock) Prediction – PART 3	https://www.r-bloggers.com/2010/02/practical-implementation-of-neural-network-based-time-series-stock-prediction-part-3/	February 1, 2010	Intelligent Trading		 0 Comments
InfoWorld: SAS and SPSS rise to R opportunity	https://www.r-bloggers.com/2010/02/infoworld-sas-and-spss-rise-to-r-opportunity/	February 1, 2010	David Smith	At InfoWorld’s “Open Source” blog Salvio Rodrigues found R co-inventor Robert Gentleman’s appointment to the REvolution Computing board “a great impetus for me to look at R again”. He notes that both SAS and SPSS have recognized the opportunity presented by R: I suspect that SPSS and SAS made their individual decisions based on three factors. First, they likely both realized that based on the penetration of SAS and SPSS in the statistical community, neither was going away anytime soon. Second, adding R support enabled both vendors to take advantage of the community of users building extensions and new statistical methods for R. Finally, both vendors likely realized that customers have different skills and analysis needs, and as such, R would be used in conjunction with SAS and SPSS’s programming languages for statistical analysis. By the way, Robert Gentleman’s appointment was also noted by Greg Laden at scienceblogs.com. InfoWorld Open Sources: SAS and IBM/SPSS rise to the open source R opportunity 	 0 Comments
R Tutorial Series: Regression With Categorical Variables	https://www.r-bloggers.com/2010/02/r-tutorial-series-regression-with-categorical-variables/	February 1, 2010	John Quick		 0 Comments
abbreviating personality measures in R: a tutorial	https://www.r-bloggers.com/2010/03/abbreviating-personality-measures-in-r-a-tutorial/	March 31, 2010	Tal Yarkoni	A while back I blogged about a paper I wrote that uses genetic algorithms to abbreviate personality measures with minimal human intervention. In the paper, I promised to put the R code I used online, so that other people could download and use it. I put off doing that for a long time, because the code was pretty much spaghetti by the time the paper got accepted, and there are any number of things I’d rather do than spend a weekend rewriting my own code. But one of the unfortunate things about publicly saying that you’re going to do something is that you eventually have to do that something. So, since the paper was published in JRP last week, and several people have emailed me to ask for the code, I spent much of the weekend making the code presentable. It’s not a fully-formed R package yet, but it’s mostly legible, and seems to work more or less ok. You can download the file (gaabbreviate.R) here. The rest of this (very long) post is basically a tutorial on how to use the code, so you probably want to stop reading this now unless you have a burning interest in personality measurement. Although you won’t need to know much R to follow this tutorial, you will need to have R installed on your system. Fortunately, R is freely available for all major operating systems. You’ll also need the genalg and psych packages for R, because gaabbreviate won’t run without them. Once you have R installed, you can download and install those packages like so: install.packages(c(‘genalg’, ‘psych’)) Once that’s all done, you’re ready to load gaabbreviate.R: source(“/path/to/the/file/gaabbreviate.R”) …where you make sure to specify the right path to the location where you saved the file. And that’s it! Now you’re ready to abbreviate measures. The file contains several interrelated functions, but the workhorse is  gaa.abbreviate(), which takes a set of item scores and scale scores for a  given personality measure as input and produces an abbreviated version  of the measure, along with a bunch of other useful information. In  theory, you can go from old data to new measure in a single line of R code, with almost no knowledge of R required (though I think  it’s a much better idea to do it step-by-step and inspect the results at  every stage to make sure you know what’s going on). The abbreviation function is pretty particular about the format of the input it expects. It takes two separate matrices, one with item scores, the other with scale scores (a scale here just refers to any set of one or more items used to generate a composite score). Subjects are in rows, item or scale scores are in columns. So for example, let’s say you have data from 3 subjects, who filled out a personality measure that has two separate scales, each composed of two items. Your item score matrix might look like this: 3 5 1 1 2 2 4 1 2 4 5 5 …which you could assign in R like so: items = matrix(c(3,2,2,5,2,2,1,4,5,1,1,5), ncol=3) I.e., the first subject had scores of 3, 5, 1, and 1 on the four items, respectively; the second subject had scores of 2, 2, 4, and 1… and so on. Based on the above, if you assume items 1 and 2 constitute one scale, and items 3 and 4 constitute the other, the scale score matrix would be: 8 2 4 5 6 10 Of course, real data will probably have hundreds of subjects, dozens of items, and a bunch of different scales, but that’s the basic format. Assuming you can get your data into an R matrix or data frame, you can feed it directly to gaa.abbreviate() and it will hopefully crunch your data without complaining. But if you don’t want to import your data into R before passing it to the code, you can also pass filenames as arguments instead of matrices. For example: gaa = gaa.abbreviate(items=”someFileWithItemScores.txt”, scales=”someFileWithScaleScores.txt”, iters=100) If you pass files instead of data, the referenced text files must be tab-delimited, with subjects in rows, item/scale scores in columns, and a header row that gives the names of the columns (i.e., item names and scale names; these can just be numbers if you like, but they have to be there). Subject identifiers should not be in the files. Assuming you can get gaabbreviate to read in your data, you can then set about getting it to abbreviate your measure by selecting a subset of items that retain as much of the variance in the original scales as possible. There are a few parameters you’ll need to set; some are mandatory, others aren’t, but should really be specified anyway since the defaults aren’t likely to work well for different applications. The most important (and mandatory) argument is iters, which is the number of iterations you want the GA to run for. If you pick too high a number, the GA may take a very long time to run if you have a very long measure; if you pick too low a number, you’re going to get a crappy solution. I think iters=100 is a reasonable place to start, though in practice, obtaining a stable solution tends to require several hundred iterations. The good news (which I cover in more detail below) is that you can take the output you get from the abbreviation function and feed it right back in as many times as you want, so it’s not like you need to choose the number of iterations carefully or anything. The other two key parameters are itemCost and maxItems. The itemCost is what determines the degree to which your measure is compressed. If you want a detailed explanation of how this works, see the definition of the cost function in the paper. Very briefly, the GA tries to optimize the trade-off between number of items and amount of variance explained. Generally speaking, the point of abbreviating a measure is to maximize the amount of explained variance (in the original scale scores) while minimizing the number of items retained. Unfortunately, you can’t do both very well at the same time, because any time you drop an item, you’re also losing its variance. So the trick is to pick a reasonable compromise: a measure that’s relatively short and still does a decent job recapturing the original. The itemCost parameter is what determines the length of that measure. When you set it high, the GA will place a premium on brevity, resulting in a shorter (but less accurate) measure; when you set it low, it’ll allow a longer measure that maximizes fidelity. The optimal itemCost will vary depending on your data, but I find 0.05 is a good place to start, and then you can tweak it to get measures with more or fewer items as you see fit. The maxItems parameter sets the upper bound on the number of items that will be used to score each scale. The default is 5, but you may find this number too small if you’re trying to abbreviate scales comprised of a large number of items. Again, it’s worth playing around with this to see what happens. Generally speaks, the same trade-off between brevity and fidelity discussed above holds here too. Given reasonable values for the above arguments, you should be able to feed in raw data and get out an abbreviated measure with minimal work. Assuming you’re reading your data from a file, the entire stream can be as simple as: gaa = gaa.abbreviate(items=”someFileWithItemScores.txt”,  scales=”someFileWithScaleScores.txt”, iters=100, itemCost=0.05, maxItems=5, writeFile=’outputfile.txt’) That’s it! Assuming your data are in the correct format (and if they’re not, the script will probably crash with a nasty error message), gaabbreviate will do its thing and produce your new, shorter measure within a few minutes or hours, depending on the size of the initial measure. The writeFile argument is optional, and gives the name of an output file you want the measure saved to. If you don’t specify it, the output will be assigned to the gaa object in the above call (note the “gaa = ” part of the call), but won’t be written to file. But that’s not a problem, because you can always achieve the same effect later by calling the gaa.writeMeasure function (e.g., in the above example, gaa.writeMeasure(gaa, file=”outputfile.txt”) would achieve exactly the same thing). Although you don’t really need to do anything else to produce abbreviated measures, I strongly recommend reading the rest of this document and exploring some of the other options if you’re planning to use the code, because some features are non-obvious. Also, the code isn’t foolproof, and it can do weird things with your data if you’re not paying attention. For one thing, by default, gaabbreviate will choke on missing values (i.e., NAs). You can do two things to get around this: either enable pairwise processing (pairwise=T), or turn on mean imputation (impute=T). I say you can do these things, but I strongly recommend against using either option. If you have missing values in your data, it’s really a much better idea to figure out how to deal with those missing values before you run the abbreviation function, because the abbreviation function is dumb, and it isn’t going to tell you whether pairwise analysis or imputation is a sensible thing to do. For example, if you have 100 subjects with varying degrees of missing data, and only have, say, 20 subjects’ scores for some scales, the resulting abbreviated measure is going to be based on only 20 subjects’ worth of data for some scales if you turn pairwise processing on. Similarly, imputing the mean for missing values is a pretty crude way to handle missing data, and I only put it in so that people who just wanted to experiment with the code wouldn’t have to go to the trouble of doing it themselves. But in general, you’re much better off reading your item and scale scores into R (or SPSS, or any other package), processing any missing values in some reasonable way, and then feeding gaabbreviate the processed data. Another important point to note is that, by default, gaabbreviate will cross-validate its results. What that means is that only half of your data will be used to generate an abbreviated measure; the other half will be used to provide unbiased estimates of how well the abbreviation process worked. There’s an obvious trade-off here. If you use the split-half cross-validation approach, you’re going to get more accurate estimates of how well the abbreviation process is really working, but the fit itself might be slightly poorer because you have less data. Conversely, if you turn cross-validation off (crossVal=F), you’re going to be using all of your data in the abbreviation process, but the resulting estimates of the quality of the solution will inevitably be biased because you’re going to be capitalizing on chance to some extent. In practice, I recommend always leaving cross-validation enabled, unless you either (a) really don’t care about quality control (which makes you a bad person), or (b) have a very small sample size, and can’t afford to leave out half of the data in the abbreviation process (in which case you should consider collecting more data). My experience has been that with 200+ subjects, you generally tend to see stable solutions even when leaving cross-validation on, though that’s really just a crude rule of thumb that I’m pulling out of my ass, and larger samples are always better. There are a bunch other less important options that I won’t cover in any detail here, but that are reasonably well-covered in the comments in the source file if you’re so inclined. Some of these are used to control the genetic algorithm used in the abbreviation process. The gaa.abbreviate function doesn’t actually do the heavy lifting itself; instead, it relies on the genalg library to run the actual genetic algorithm. Although the default genalg parameters will work fine 95% of the time, if you really want to manually set the size of the population or the ratio of initial zeros to ones, you can pass those arguments directly. But there’s relatively little reason to play with these parameters, because you can always achieve more or less the same ends simply by adding iterations. Two other potentially useful options I won’t touch on, though they’re there if you want them, give you the ability to (a) set a minimum bound on the correlation required in order for an item to be included in the scoring equation for a scale (the minR argument), and (b) apply non-unit weightings to the scales (the sWeights argument), in cases where you want to emphasize some scales at the cost of others (i.e., because you want to measure some scales more accurately). The following two examples assume you’re feeding in item and scale matrices named myItems and myScales, respectively:  This will run a genetic algorithm for 500 generations on mean-imputed data with cross-validation turned off, and assign the result to a variable named my.new.shorter.measure. It will probably produce an only slightly shorter measure, because the itemCost is low and up to 10 items are allowed to load on each scale.  This will run 100 iterations with cross-validation enabled (the default, so we don’t need to specify it explicitly) and write the result to a file named shortMeasure.txt. It’ll probably produce a highly abbreviated measure, because the itemCost is relatively high. It also assigns more weight (twice as much, in fact) to the fourth and fifth scales in the measure relative to the first three, as reflected in the sWeights argument (a vector where the ith element indicates the weight of the ith scale in the measure, so presumably there are five scales in this case). Assuming you’ve read this far, you’re probably wondering what you get for your trouble once you’ve run the abbreviation function. The answer is that you get… a gaa (which stands for GA Abbreviate) object. The gaa object contains almost all the information that was used at any point in the processing, which you can peruse at your leisure. If you’re familiar with R, you’ll know that you can see what’s in the object with the attributes function. For example, if you assigned the result of the abbreviation function to a variable named ‘myMeasure’, here’s what you’d see:  The gaa object has several internal lists (data, settings, results, etc.), each of which in turn contains several other variables. I’ve tried to give these sensible names. In brief: To see the contents of each of these lists in turn, you can easily inspect them:  So the ‘measure’ attribute in the gaa object contains a bunch of other variables with information about the resulting measure. And here’s a brief summary: Supposing you’re not really interested in plumbing the depths of the gaa object or working within R more than is necessary, you might just be wondering what the quickest way to get an abbreviated measure you can work with is. In that case, all you really need to do is pass a filename in the writeFile argument when you call gaa.abbreviate (see the examples given above), and you’ll get out a plain text file that contains all the essential details of the new measure. Specifically you’ll get (a) a mapping from old items to new, so that you can figure out which items are included in the new measure (e.g., a line like “4 45″ means that the 4th item on the new measure is no. 45 in the original set of items), and (b) a human-readable scoring key for each scale (the only thing to note here is that an “R” next to an item indicates the item is reverse-keyed), along with key statistics (coefficient alpha and convergent correlations for the training and validation halves). So if all goes well, you really won’t need to do anything else in R beyond call that one line that makes the measure. But again, I’d strongly encourage you to carefully inspect the gaa object in R to make sure everything looks right. The fact that the abbreviation process is fully automated isn’t a reason to completely suspend all rational criteria you’d normally use when developing a scale; it just means you probably have to do substantially less work to get a measure you’re happy with. Depending on how big your dataset is (actually, mainly the number of items in the original measure), how many iterations you’ve requested, and how fast your computer is, you could be waiting a long time for the abbreviation function to finish its work. Because you probably want to know what the hell is going on internally during that time, I’ve provided a rudimentary monitoring display that will show you the current state of the genetic algorithm after every iteration. It looks like this (click for a larger version of the image):  This is admittedly a pretty confusing display, and Edward Tufte would probably murder several kittens if he saw it, but it’s not supposed to be a work of art, just to provide some basic information while you’re sitting there twiddling your thumbs (ok, ok, I promise I’ll label the panels better when I have the time to work on it). But basically, it shows you three things. The leftmost three panels show you the basic information about the best measure produced by the GA as it evolves across generations. Respectively, the top, middle,and bottom panels show you the total cost, measure length, and mean variance explained (R^2) as a function of iteration. The total cost can only ever go down, but the length and R^2 can go up or down (though there will tend to be a consistent trajectory for measure length that depends largely on what itemCost you specified). The middle panel shows you detailed information about how well the GA-produced measure captures variance in each of the scales in the original measure. In this case, I’m abbreviating the 30 facets of the NEO-PI-R. The red dot displays the amount of variance explained in each trait, as of the current iteration. Finally, the rightmost panel shows you a graphical representation of which items are included in the best measure identified by the GA at each iteration.Each row represents one iteration (i.e., you’re seeing the display as it appears after 200 iterations of a 250-iteration run); black bars represent items that weren’t included, white bars represent items that were included. The point of this display isn’t to actually tell you which items are being kept (you can’t possibly glean that level of information at this resolution), but rather, to give you a sense of how stable the solution is. If you look at the the first few (i.e., topmost) iterations, you’ll see that the solution is very unstable: the GA is choosing very different items as the “best” measure on each iteration. But after a while, as the GA “settles” into a neighborhood, the solution stabilizes and you see only relatively small (though still meaningful) changes from generation to generation. Basically, once the line in the top left panel (total cost) has asymptoted, and the solution in the rightmost panel is no longer changing much if at all, you know that you’ve probably arrived at as good a solution as you’re going to get. Incidentally, if you use the generic plot() method on a completed gaa object (e.g., plot(myMeasure)), you’ll get exactly the same figure you see here, with the exception that the middle figure will also have black points plotted alongside the red ones.  The black points show you the amount of variance explained in each trait for the cross-validated results. If you’re lucky, the red and black points will be almost on top of each other; if you’re not, the black ones will be considerably to the left of the red ones . The last thing I’ll mention, which I already alluded to earlier, is that you can recycle gaa objects. That’s to say, suppose you ran the abbreviation for 100 iterations, only to get back a solution that’s still clearly suboptimal (i.e., the cost function is still dropping rapidly). Rather than having to start all over again, you can simply feed the gaa object back into the abbreviation function in order to run further iterations. And you don’t need to specify any additional parameters (assuming you want to run the same number of iterations you did last time; otherwise you’ll need to specify iters); all of the settings are contained within the gaa object itself. So, assuming you ran the abbreviation function and stored the result in ‘myMeasure’, you can simply do: myMeasure = gaa.abbreviate(myMeasure, iters=200) and you’ll get an updated version of the measure that’s had the benefit of an extra 200 iterations. And of course, you can save and load R objects to/from files, so that you don’t need to worry about all of your work disappearing next time you start R. So save(myMeasure, ‘filename.txt’) will save your gaa object for future use, and the next time you need it, you can call myMeasure = load(‘filename.txt’) to get it back (alternatively, you can just save the entire workspace). Anyway, I think that covers all of the important stuff. There are a few other things I haven’t documented here, but if you’ve read this far, and have gotten the code to work in R, you should be able to start abbreviating your own measures relatively painlessly. If you do use the code to generate shorter measures, and end up with measures you’re happy with, I’d love to hear about it. And if you can’t get the code to work, or can get it to work but are finding issues with the code or the results, I guess I’ll grudgingly accept those emails too. In general, I’m happy to provide support for the code via email provided I have the time. The caveat is that, if you’re new to R, and are having problems with basic things like installing packages or loading files from source, you should really read a tutorial or reference that introduces you to R (Quick-R is my favorite place to start) before emailing me with problems. But if you’re having problems that are specific to the gaabbreviate code (e.g., you’re getting a weird error message, or aren’t sure what something means), feel free to drop me a line and I’ll try to respond as soon as I can. 	 0 Comments
Social Media Analytics Research Toolkit ([email protected]) Is Moving Into Private Beta	https://www.r-bloggers.com/2010/03/social-media-analytics-research-toolkit-smartznmeb-is-moving-into-private-beta/	March 31, 2010	Ed Borasky		 0 Comments
How ideological is Google?	https://www.r-bloggers.com/2010/03/how-ideological-is-google/	March 31, 2010	David Smith	Adam Bonica, a grad student in political science at NYU, recently published a ranking of the political slant of various professions, based on the amount and recipient (Republican or Democratic) of political donations by lawyers, lobbyists, physicians and many other occupations. This paper (PDF) gives the complete analysis, but the chart below (created using the ggplot2 graphics package in R) sums up the results nicely (click to enlarge):   (I liked this quote from Paul Kedrosky about this chart: “How come gas station attendants are so damn partisan?”) Now, Adam has taken the analysis to the next level, by looking at employees of individual companies, instead of professions as a whole.   (Unfortunately, it’s not entirely clear if the data point lies to the left or at the middle of the company label; this table helps sort out the exact rankings.) It may not be a surprise, for example, that Google’s employees tend to give to Democratic-leaning candidates, but does that influence Google’s policies as a whole? Adam’s article at the link below delves into this question in more detail. Ideological Cartography: The University of Google: Was the decision to exit China ideological or business as usual? 	 0 Comments
Why isn’t my 2X Ultra ETF keeping pace with the market and what is path asymmetry (R ex)?	https://www.r-bloggers.com/2010/03/why-isnt-my-2x-ultra-etf-keeping-pace-with-the-market-and-what-is-path-asymmetry-r-ex/	March 31, 2010	Intelligent Trading		 0 Comments
Predicting April month return	https://www.r-bloggers.com/2010/03/predicting-april-month-return/	March 31, 2010	kafka	"Bespoke blogged about average monthly returns of the DJI and emphasized April. Before jumping on that information, let’s check some weak points.
In that post, only average returns are presented. We need at least extreme points (min;max) and confidence ranges. Second problem – the normal market have upward trend and we need to get rid of that. To do so, either we have subtract the  rolling mean (a tough way) or use logarithmic prices (that’s the easy way!). Instead of DJI, I took S&P500 from 1950 until now.  Sure, average return in April is above 0, but based on historical data, negative return is possible. I conducted t-test, where null hypothesis was, that average return is equal to zero. I got p-value of 0.0042, so null hypothesis can be rejected (return is above 0). The graph below shows cumulative return of investment, investing only in April. Keep in mind, that this is log scale and real return would be higher.
 Conlusion: based on this data, expect positive return in April. R code "	 0 Comments
Lotka-Volterra model ~ intro	https://www.r-bloggers.com/2010/03/lotka-volterra-model%c2%a0%c2%a0intro/	March 30, 2010	apeescape	So many know about the Lotka-Volterra model (i.e. the predator-prey model) in ecology. This model portrays two species, the predator (y) and the prey (x), interacting each other in limited space.  The prey grows at a linear rate () and gets eaten by the predator at the rate of (). The predator gains a certain amount vitality by eating the prey at a rate (), while dying off at another rate (). Given this base, we can ask questions like, what parameterizations can we expect to find a coexistence between the fox and the hare (for example)? Let’s choose some values for the model: . These values assume a weaker growth of the rabbits relative to the strength of the death of foxes. Below, I simulated these values in R.  And we get coexistence, they live happily forever after. With this simple model, we can play around by generalizing (logistic growth of prey, etc.). I will put up some posts doing so. The way to do this in R is as follows (just use the deSolve package, which will supersede the odesolve package): 	 0 Comments
Some Code for Dumping Data from Twitter Gardenhose	https://www.r-bloggers.com/2010/03/some-code-for-dumping-data-from-twitter-gardenhose/	March 30, 2010	Ryan	Gardenhose is a Streaming API feed that continuously sends a sample (roughly 15% according to Ryan Sarver at the 140tc in September 2009) of all tweets to feed recipients. This is some code for dumping the tweets to files named by date and hour. It is in PHP which is not my favorite language, but works nonetheless. I received a few requests to post it, so here it is. 	 0 Comments
TTR_0.20-2 on CRAN	https://www.r-bloggers.com/2010/03/ttr_0-20-2-on-cran/	March 30, 2010	Joshua Ulrich	"
 "	 0 Comments
Scientists misusing Statistics	https://www.r-bloggers.com/2010/03/scientists-misusing-statistics/	March 30, 2010	David Smith	In ScienceNews this month, there’s controversial article exposing the fact that results claimed to be “statistically significant” in scientific articles aren’t always what they’re cracked up to be. The article — titled “Odds Are, It’s Wrong” is interesting, but I take a bit of an issue with the sub-headline, “Science fails to face the shortcomings of Statistics”. As it happens, the examples in the article are mostly cases of scientists behaving badly and abusing statistical techniques and results:   Statisticians, in general, are aware of these problems and have offered solutions: there’s a vast field of literature on multiple comparisons tests, reporting bias, and alternatives (such as Bayesian methods) to P-value tests. But more often than not, these “arcane” issues (which are actually part of any statistical training) go ignored in scientific journals. You don’t need to be a cynic to understand the motives of the authors for doing so — hey, a publication is a publication, right? — but the cooperation of the peer reviewers and editorial boards is disturbing. ScienceNews: Odds Are, It’s Wrong     	 0 Comments
Example 7.30: Simulate censored survival data	https://www.r-bloggers.com/2010/03/example-7-30-simulate-censored-survival-data/	March 30, 2010	Ken Kleinman		 0 Comments
Smoothing time series with R	https://www.r-bloggers.com/2010/03/smoothing-time-series-with-r/	March 29, 2010	David Smith	Smoothing is a statistical technique that helps you to spot trends in noisy data, and especially to compare trends between two or more fluctuating time series. It’s a useful visualization tool that I’m pleased to see cropping up more and more in statistical graphics on the Web — it’s now a staple in econometric charts and is heavily used in polling analysis. For example, here’s smoothing used to combine data from various polls over time on Obama’s job approval (from pollster.com). The S language was, to the best of my knowledge, the first software that made statistical smoothing a core part of the graphics system: first with the lowess function and later with other more powerful alternatives. These days in R (S’s successor), loess (local polynomrial regression fitting) is the usual go-to alternative for smoothing. With just a couple of lines of code, you can take a noisy time series in R and overlay a smooth trend line to guide the eye. Nathan Yau at FlowingData shows us how to take data like this:  and, with just a few lines of R code and some touching-up in Illustrator, create a chart like this:  FlowingData: How to: make a scatterplot with a smooth fitted line 	 0 Comments
Looking for Software Paths in Windows Registry	https://www.r-bloggers.com/2010/03/looking-for-software-paths-in-windows-registry/	March 28, 2010	Yihui Xie	However, we may be able to find the paths through the registry if the installation will save the path info in the registry hive. The R function is readRegistry(): There is no guarantee for this approach to work on any Windows platforms, but I think this is better than explaining what is the PATH variable to some Windows users… Here I show two examples for ImageMagick and OpenBUGS respectively. Make sure you are using Windows and have already installed ImageMagick and OpenBUGS. The first example shows how to find ImageMagick and call it to convert a sequence of images (generated by R) to a GIF animation: You should be able to see the output as this:  ImageMagick Example OpenBUGS example: You might now be able to run the above examples in your Windows version, and I’ll appreciate feedbacks from Vista and Win7 users. If the above approaches do not work, you may run the command regedit (Start –> Run) and search for ImageMagick/OpenBUGS, then change my code accordingly, because the locations of the registry hive for ImageMagick/OpenBUGS may not be the same for different versions of Windows. My session info is as below: 	 0 Comments
Example 7.29: Bubble plots colored by a fourth variable	https://www.r-bloggers.com/2010/03/example-7-29-bubble-plots-colored-by-a-fourth-variable/	March 27, 2010	Ken Kleinman		 0 Comments
Finance::YahooQuote 0.24	https://www.r-bloggers.com/2010/03/financeyahooquote-0-24/	March 26, 2010	Thinking inside the box	"
Anyway,  a new version 0.24 of 
Finance::YahooQuote
which addresses the issue that required 
upload 0.23 yesterday 
is now in the Debian queue and on
CPAN and my local
yahooquote page. This time it may even work.
A big thanks to the CPAN Testers for getting me reports on this one too.

 "	 0 Comments
Rcpp 0.7.11	https://www.r-bloggers.com/2010/03/rcpp-0-7-11/	March 26, 2010	Thinking inside the box	"
This version fixes a somewhat serious bug uncovered by Doug Bates when
working with vectors of strings. We also added a few new accessor functions as
well as a new convenience function create that is particularly
useful for creating (possibly named) list objects that are returned to R.
  
 

Here is the full NEWS entry for this release:
 
As always, even fuller details are in 
Rcpp Changelog page and the 
Rcpp page which also
leads to the downloads, the 
browseable
doxygen docs and zip files of doxygen output for the standard formats.
A local directory  has
source and documentation too. 
Questions, comments etc should go to the
rcpp-devel mailing list
off the R-Forge page

 "	 0 Comments
"‘R’ = dna.translate(""AGG"") . A custom C function for R, My notebook."	https://www.r-bloggers.com/2010/03/r-dna-translateagg-a-custom-c-function-for-r-my-notebook/	March 26, 2010	Pierre Lindenbaum		 0 Comments
Code Highlights in WordPress	https://www.r-bloggers.com/2010/03/code-highlights-in-wordpress/	March 26, 2010	Lee	I’ve come across a very useful plugin for WordPress which highlights code in posts using GeSHi called WP-Syntax.  This plugin is easy to use and adds highlights simply by putting the appropriate tags around code blocks.  For instance, we can make the following R code much more readable by using WP-Syntax. A patch is avaliable which includes R code highlights, something which isn’t available in the default WP-Syntax version. 	 0 Comments
Predicting Pizza	https://www.r-bloggers.com/2010/03/predicting-pizza/	March 26, 2010	David Smith	What’s the secret to the best pizza in New York? That’s what statistical consultant and R user Jared Lander sought to find out, by analyzing the rankings of NY pizza joints at MenuPages.com, and building a regression model for ratings based on variables like localion, price, number of reviews, and pizza-oven type (gas, coal or wood)? Here’s a scatterplot matrix of the data set:  Jared published his conclusions in a paper (PDF), “New York Pizza: How to Find The Best”. He used a logit analysis in R to model the five-star rank from the various variables. His conclusions? First of all, there’s a big discrepancy between critics’ “Top 10” pizza rankings and those of the general public (at least as measured at MenuPages.com), with only one of MenuPage’s Top 10 listed in the typical critic’s list. Secondly, while an Uptown location and a coal oven both popular draws (as measured by number of the reviews) none of the variables have a significant influence in rating: Slice: The ‘Moneyball’ of Pizza? Using Statistics to Find NYC’s Best Pies and Slices  	 0 Comments
Summarising data using dot plots	https://www.r-bloggers.com/2010/03/summarising-data-using-dot-plots/	March 26, 2010	Ralph	A dot plot is a type of display that compares counts, frequencies, totals or other summary measures for a series of categories. The dot plot can be arranged with the categories either on the vertical or horizontal axis of the display to allow comparising between the different categories as well as comparison within categories where there are multiple symbols used to denote say different years. In this post we will considered creating a dot plot using the base graphics, lattice graphics and ggplot2 approaches. To illustrate creating a dot plot we used data from the FAO website on the total irrigation area for Africa, Latin America, North America and Europe. We create a data frame using the following code: Base Graphics In the base graphics system we build up the dotplot with a series of commands. The first function call creates the graph region based on the data set but we do not plot any data by setting the type = “n” argument. The axis labels for the horizontal and vertical scales are set along with the title in the initial function call: To add the points with separate colours for each of the four years we use the points function and subset to the particular year by testing a condition on the year. The col argument is used with a text string to specify the colour for the symbols for the given year: The code is rather long winded compared to the using the other two graphics packages. We can add a legend to the graph so that the years can be identified: The placement of the legend uses the x and y coordinates within the graph to position the box. All the code above produces the following graph: Base Graphics Dot Plot The graph is basic but we can consider the changes over time for the four regions. One downside is that the regions have been labelled with numbers rather than text strings. Lattice Graphics The lattice graphics package has a function dotplot that is used to create dot plots. The first argument to the function is a formula describing the variables to use for the horizontal and vertical axes. We also specify the data frame to use for the graph and which column to determine different symbols and/or colours to highlight groupings within the plot: The lattice variant of the graph is shown here: Lattice Graphics Dot Plot The graph is simple and very similar to the one produced using the base graphics with the advantage that the R code is not as complicated. ggplot2 The ggplot function is used to create the dot plot where we first specify the name of the data frame with the information to be displayed and then use the aes argument to list the variables to plot on the horizontal and vertical axes. The colour argument determines the variable to use for assigning colours to (usually) a categorical variable. The ggplot2 version of the dot plot is shown below: ggplot2 Dot Plot This graph is very similar to the ones produced using the other graphics packages but has the distinctive background and legend style that is used as the default option in ggplot2. This blog post is summarised in a pdf leaflet on the Supplementary Material page. 	 0 Comments
BioMart (and biomaRt)	https://www.r-bloggers.com/2010/03/biomart-and-biomart/	March 26, 2010	nsaunders	"I’ve been vaguely aware of BioMart for a few years.  Inexplicably, I’ve only recently started to use it.  It’s one of the most useful applications I’ve ever used.

The concept is simple.  You have a set of identifiers that describe a biological object, such as a gene.  These are called filters.  They have values – for example, HGNC symbols.  You want to retrieve other identifiers – attributes – for your objects. You can use BioMart as a web application called MartView.  However, R users should check out the biomaRt package, part of the Bioconductor suite.  Here’s a couple of examples. Example 1: fetch Ensembl gene identifiers given HGNC symbols
Let’s start with a simple example.  You have a CSV file in which one of the fields is a HGNC symbol (with the column header “hgnc”) and you want to obtain Ensembl gene IDs. You do need to know in advance that “ensembl_gene_id” and “hgnc_symbol” are valid attributes.  You can get a list of all attributes for the current biomart object using “listAttributes(mart)”. Example 2:  fetch genes for microarray probesets
In this example, I assume that you have normalised some microarray samples using, for example, RMA in the affy package and used a method such as exprs() to generate a matrix of RMA values, where rows = probeset IDs and columns = sample names.  We’d like to get the gene names for those probesets. Summary
That’s your basic usage of biomaRt.  In the next post:  how to combine biomaRt with GenomeGraphs, to generate attractive plots of features and quantitative data in genomic context. "	 0 Comments
Finance::YahooQuote 0.23	https://www.r-bloggers.com/2010/03/financeyahooquote-0-23/	March 25, 2010	Thinking inside the box	"
Which lead the automated Perl test scripts to remind me for a few days now
that the full company name for symbol IBM no longer corresponded to what I had
encoded. Not really a bug, but a failure in tests anyway.
 
So without
further ado: a new version 0.23 of 
Finance::YahooQuote
which addressed this issue
is now in the Debian queue and on
CPAN and my local
yahooquote page. 

 "	 0 Comments
How Misinformed are Tea Party Protesters About Tax Policy?	https://www.r-bloggers.com/2010/03/how-misinformed-are-tea-party-protesters-about-tax-policy/	March 25, 2010	Drew Conway	For those of you used to reading about international relations, I apologize for the following  brief foray into American politics.  It appears that the American Enterprise Institute and David Frum have decided to (abruptly) part ways.  Before David left, however, he and his team of interns provided some interesting statistical insight into the Tea Party movement, as he writes: Over the course of our survey, FrumForum interviewed approximately 60 people of the estimated 300-500 protesters assembled on Capitol Hill to protest the healthcare bill currently before the House*. We asked them questions about their perception of current taxation rates and the economy. *To be sure, this survey lacked a control group and a statistically significant sample, but based on our estimates, we surveyed between 11% and 19% of the protesters on Capitol Hill. It is perhaps more valid to treat the results as that of a focus group, and a general contribution to the understanding of the Tea Party movement. Frum asserts that the survey, “shows that Tea Partiers tend to be more financially pessimistic than average Americans, and perceive the United States’ tax burden to be significantly higher than it actually is.”  Given the small number of respondents (as noted in the highlighted footnote above), it is difficult to get a sense of the actual distribution of beliefs within the population of Tea Partiers.  We, can, however bootstrap (simulate) these distribution by making parametric assumption, and then more accurately test the assertion of Frum. A brief caveat: as Frum has pointed out, these data were by no means scientifically collected, and therefore any results generated will be biased in whatever direction the collection pointed them.  In the follow experiments, I will be treating these data as though they were legitimate, despite the dangers of doing so.  That said, we will model the results of these questions using the Gamma distribution and approximate the shape (k) and scale () parameters using descriptive statistice from the data gathered in the survey. To generate the approximate values for k and  I will use the mean and standard deviation values provided for each of the survey questions.  From the definition of the Gamma distribution, the mean is equal to  and the variance is equal to .  By substituting the standard deviation of the survey results for the variance we can easily approximate values of these parameters.  To do so, I use sympy, an open source symbolic mathematics Python package, which among other things simply makes my life easier. Once these values have been calculated it is very easy to generate simulated distributions.  I use the following R code to do so:  Next, we will visualize these distributions with ggplot2, noting where the actual values for these questions fall on the distributions using a vertical red line.      With the simulated distributions it is a bit easier draw conclusions as to the level of “misinformation” within the population of the Tea Party movement.  My initial reaction is that while the actual values for both questions are clearly in the tails of these distributions, they are not so far in these tails as to make them extreme.  In fact, for the federal income tax question the actual value is well within one standard deviation of the mean.  Perhaps the Tea Partiers are not as misinformed as many would be presumed from their rhetoric.  At the same time, however, these are not necessarily difficult questions, so any deviation from the actual value could be viewed negatively.  What are your thoughts? 	 0 Comments
R plotting fun	https://www.r-bloggers.com/2010/03/r-plotting-fun/	March 25, 2010	Tal Galili	 Not easy to produce cool looking graphs in R, but it can be done. The results of some messing around are above. Here is the code I used:  	 0 Comments
Future of Open Source Survey – Results	https://www.r-bloggers.com/2010/03/future-of-open-source-survey-results/	March 25, 2010	David Smith	The results of the 2010 Future of Open Source survey were presented at last week’s Open Source Business Conference in San Francisco, and here are they are in slide format:  While I was at the presentation I captured a few additional tidbits from the presentation that weren’t in the slides. The continued growth of open-source generally was a prevalent theme. For example, did you know that more than 19,000 open-source projects were initiated in 2009? The growth of open-source software in the commercial sector was also noted, and the promotion of open-source projects by commercial open-source companies was cited as a factor. (REvolution Computing promoting the R Project was one example given.) The other theme I spotted was innovation: while lowering costs is still the #1 ranked feature of open-source, access to new methods and the rapid pace of innovation in FOSS compared to proprietary software is now being listed as a critical reason to switch. R blogger Tal Galili has some other insights on the survey results, from an R user’s perspective. I recommend checking them out.   	 0 Comments
A von Mises variate…	https://www.r-bloggers.com/2010/03/a-von-mises-variate%e2%80%a6/	March 25, 2010	M. Parzakonis	Inspired from a mail that came along the previous random generation post the following question rised : How to draw random variates from the Von Mises distribution? First of all let’s check the pdf of the probability rule, it is , for . Ok, I admit that Bessels functions can be a bit frightening, but there is a work around we can do. The solution is a Metropolis algorithm simulation. It is not necessary to know the normalizing constant, because it will cancel in the computation of the ratio. The following code is adapted from James Gentle’s notes on Mathematical Statistics .  	 0 Comments
Create odf, pdf and html report from a single Sweave document	https://www.r-bloggers.com/2010/03/create-odf-pdf-and-html-report-from-a-single-sweave-document/	March 25, 2010	Matti Pastell	"reStructuredText is a simple plain text mark up that can be converted to several formats using the Python Docutils. It is simpler to write than Latex and it also the syntax that is used in Sphinx documents. The ascii package has a Sweave driver for weaving R code from documents written with reST and some other mark up languages as well.  I have created a simple example document ascii-example.Rnw, which demonstrates the capabilities of the combination. The source file is a Sweave document with reST mark up instead of Latex. I had to make small modifications to the in ReST driver in ascii package to get this example to work1 (download it here: newRest.R). I then processed the example with the new driver in R: Weaving produced this ascii-example.rst reST document which I then converted to odf, html, pdf with pdflatex and pdf with rst2pdf using:  And here are the results: odt, html, pdf from latex and pdf from rst2pdf.  You’ll need to have Docutils and odtwriter for the conversion. I have used the default options, but there are a lot options that can be passed to the docutils writers to customize the output, such as custom stylesheets. The image format in the pdfs is not optimal, but in can be changed with the “res” option in code chunk or you can also choose to use pdf format instead of jpg, but then it won’t show up in html documents. I think using reST with ascii package is a good option for producing reports and tutorials in multiple output formats. That is also because I’m already familiar with reST directives and also use Sphinx for other purposes too. The reST syntax is also very fast to learn and I think it is definitely worth exploring. I also recommend using Sphinx if you only want to get html and pdf output, because it has more directives than plain reST e.g. math support.
 
1 I changed the driver to output code chunks in reST literal environment ‘::’ instead of ‘.. codeblock:: r’, because the codeblock directive is not supported in odt conversion and very poorly supported in latex conversion. "	 0 Comments
NetLogo & R Extension	https://www.r-bloggers.com/2010/03/netlogo-r-extension-2/	March 25, 2010	E.Crema		 0 Comments
Matlab and R (getting started)	https://www.r-bloggers.com/2010/03/matlab-and-r-getting-started/	March 24, 2010	glebanon	Matlab and R are two popular languages for data analysis and visualization. The similarity between the two languages is high. Both are interpreted languages that run in a shell-like environment (while also allowing to run scripts or functions written off-line). Both tend to be slow if your code contains many loops but are fast when running vectorized code (vectorized code means that a repeated operation is cast as an operation on matrices or tensors). One difference between them is that Matlab is commercial while R is open-source. Another difference is that Matlab is traditionally more popular in engineering and scientific computing while R is traditionally used by statisticians. As a result, Matlab is probably more polished and can probably handle large computations faster. R, on the other hand, has a larger library of data analysis and visualization routines-often contributed by a vibrant network of users. I would choose R for most statistical data analysis and visualization but revert to Matlab for the heavy lifting (although for really heavy lifting there is no escaping C/C++). In this note and some subsequent ones I will describe a few commands and features of the two languages. I think it is useful to do it side by side for the two languages as many people know one of the two languages. Considering how the same task is done in both language is instructive in teaching Matlab (R) programmers how to program in R (Matlab). Getting Help and Reference Material Matlab’s online getting started is available here. Note in particular the link to the pdf manuals. R’s manuals are available here. In Matlab to get help on a certain function type for a plain text explanation within the Matlab window, or for help in html format (including figures sometimes) viewed through a browser. In R, to get help type The help will typically appear inside the R environment as plain text. It is possible to get html help in a browser by typing help.start() before the help command. R also have the following specific command to provide examples Variables In both languages simply type the name of the variable followed by the return key to see its value. In Matlab type to see what variables are currently defined and to remove a variable from the workspace. Typing saves the workspace variables to a file which may be loaded using the command load. In R, type to see a list of defined variables and to remove some variables from memory. The workspace is saved to the file .RData when the program quits (user is prompted) which is then automatically loaded when a new R session starts (in the same directory). It is also possible to save all variables at any point using the command  Indexing  Both R and Matlab make heavy use of vectors, matrices, and tensors. To access a specific element or elements in Matlab use to extract one element, a pair of elements, or an entire slice of a third dimensional array. To accomplish the same effect in R use Arr[3,2,5] Arr[3,2:4,5] Arr[3,,5]  Interfacing with the OS and other Languages In Matlab you can issue commands to the shell by prefixing it with an exclamation mark, for example The same thing is accomplished in R using Sooner or later you will want to call C programs from Matlab or R. This can be useful since Matlab and R can be very very slow. Coding the computational bottleneck routine in C may speed up things considerably with only a modest programming effort. The simplest way to do that is to save the data to a file in Matlab/R, execute a compiled program as shown above which reads the data from the file and writes the result to a new file. The process is completed by read the file containing the results from Matlab/R. A more smooth solution is to write C code in a slightly modified way which will enable calling them from Matlab/R using a standard parameter passing syntax without the use of files to transfer input/output data. More info on this may be found here for Matlab and here for R. Matlab’s version is more powerful but is more tedious and less programmer-friendly. Cell Array and Lists Standard arrays in both languages contains multiple elements of the same atomic type (double, character, binary, etc.). In many cases it is useful to have an array of non-atoms or an array where each element has a different types. This concept is called a cell array in Matlab and a list in R. In Matlab, cell arrays are created using the command cell, for example Assigning values to the elements and accessing them is done with curly braces (regular parenthesis in the example will create a cell array of reduced size) In R the same thing is accomplished by creating a list (the [[i]] notation means return the content of the i list element-rather than the i-element itself which is a list of length 1)  Redirecting Input and Output Programming in Matlab/R by interacting withe the shell environment is ultimately limited. Redirecting the input means that you run Matlab/R commands that were written off-line and saved to a file (sometime called a script). Redirecting the output means that the output produced by Matlab/R is saved as a separate file for off-line investigation later on. In Matlab redirecting input and running the commands in a script file is done by simply typing the name of the script file (without the conventional .m extension). To redirect output use In R, we have the following commands A different kind of output redirection is for graphic figures which are normally printed to the screen. To print an existing figure as an eps file type (replace epsc with other options to create png, jpeg, etc.) To create a pdf file it’s best to first create an eps and then convert it rather than to use the pdf flag in the print command (see this post). In R, redirecting figures to be printed to a file instead of the screen is done in the following way Note that the pdf file is not created in its final form until after the dev.off command. To redirect to a ps or eps file use the command postscript. The figures may be printed to multiple devices simultaneously as follows.  Installing Contributed Libraries Both languages have a set of user contributed libraries which are very useful in extending the functionality of the language. Generally speaking R’s libraries are more extensive and high quality, probably due to the fact that it’s open source. To install a user contributed library on Matlab simply download the .m files and put them in a directory (and add it to the Matlab path using addpath if needed). In R, new libraries may be installed using which automatically downloads all the necessary files and selects an appropriate place to download them and updates the path. To use the library (in other words bring it into scope) use 	 0 Comments
Modified Donchian Band Trend Follower using R, Quantmod, TTR  -Part 2: Parameter Sweep Sensitivity over long run	https://www.r-bloggers.com/2010/03/modified-donchian-band-trend-follower-using-r-quantmod-ttr-part-2-parameter-sweep-sensitivity-over-long-run/	March 24, 2010	Intelligent Trading		 0 Comments
Using vectors to customize placement of tick marks and labels on a plot	https://www.r-bloggers.com/2010/03/using-vectors-to-customize-placement-of-tick-marks-and-labels-on-a-plot/	March 24, 2010	Jim		 0 Comments
A Demo for the Ratio Estimation in Sampling Survey (Animation)	https://www.r-bloggers.com/2010/03/a-demo-for-the-ratio-estimation-in-sampling-survey-animation/	March 24, 2010	Yihui Xie	As we know, the benefit of ratio estimation is that sampling skewness may be  adjusted for, because the estimation of  will make use of the information in the relationship of X and Y: . Here is a demo (we can see the ratio estimate, denoted by the red line, generally performs better than ):  An animation demo for the ratio estimation  And here is the code: 	 0 Comments
ECG Signal Processing	https://www.r-bloggers.com/2010/03/ecg-signal-processing/	March 24, 2010	Matt Shotwell	"After reading (most of) “The Scientists and Engineers Guide to Digital Signal Processing” by Steven W. Smith, PhD, I decided to take a second crack at the ECG data. I wrote a set of R functions that implement a windowed (Blackman) sinc low-pass filter. The convolution of filter kernel with the input signal is conducted in the frequency domain using the fast Fourier transform, which is much of the focus of Smith’s book.You can check out the complete R script. Also, you can reproduce the analysis and the image below in R by running the following command The low-pass filter was first applied to eliminate the high frequency noise, anything greater than 30Hz. I next applied the filter at a cutoff frequency of 1Hz in order to isolate the slow wave that corresponds to respirations. The image below gives the sequence of filtering.
 "	 0 Comments
Statistical learning with MARS	https://www.r-bloggers.com/2010/03/statistical-learning-with-mars/	March 24, 2010	David Smith	Steve Miller at the InformationManagement blog has been looking at predictive analytics tools for business intelligence applications, and naturally turns to the statistical modeling and prediction capabilities of R. Says Steve: The technique which is the subject of his most recent post is MARS: Multivariate Adaptive Regression Splines, available for R in the “earth” package: “Mars is an adaptive procedure for regression, and is well suited for high-dimensional (i.e., a large number  of inputs). It can be viewed as a generalization of stepwise linear regression or a modification of the CART procedure to improve the latter’s performance in the regression setting.”  Read on in Steve’s post for a more detailed review of MARS and its applications. Information Management blogs: Predictive Models, Mars to Earth – Part 1  	 0 Comments
RXQuery	https://www.r-bloggers.com/2010/03/rxquery/	March 24, 2010	omegahat	I have put a new version of the RXQuery package which interfaces to the Zorba XQuery engine.  This makes the package compatible with the 1.0.0 release of Zorba for external functions.   The package allows one to use XQuery from within R and to use R functions within XQuery scripts. 	 0 Comments
Lessons Learned from EC2	https://www.r-bloggers.com/2010/03/lessons-learned-from-ec2/	March 24, 2010	Ryan	A week or so ago I had my first experience using someone else’s cluster on Amazon EC2. EC2 is the Amazon Elastic Compute Cloud. Users set up a virtual computing platform that runs on Amazon’s servers “in the cloud.” Amazon EC2 is not just another cluster. EC2 allows the user to create a disk image containing an operating system and all of the software they need to perform their computations. In my case, the disk image would contain Hadoop, R, Python and all of the R and Python packages I need for my work. This prevents the user (and the provider) from having to worry about providing or upgrading software and having compatibility issues. No subscription is required. Users pay for the amount of resources used for the computing session. Hourly prices are very cheap, but accrue quickly. Additionally, Amazon charges for pretty much everything single thing you can do with an OS: transferring data to/from the cloud per GB, data storage per GB, CPU time per hour per core etc.  This is somewhat of a tangent, but EC2 was a brilliant business move in my opinion.  Anyway, life gets a bit more difficult when the EC2 instance you’re working with is not your own. My experience using someone else’s instance was new, and short, and I think both parties were not entirely aware of how foreign the system may be to a user outside of the loop. I provide these tips to others, so they know what to expect when working with someone else’s instance. Some of these points I add in hindsight and did not reflect my experience, but are important anyway. The people I worked with were great and assured me that I wasn’t being an absolute pain in the butt. Most of these points apply to Hadoop, but may be applicable to other systems as well. What are your bits of advice when using someone else’s Amazon EC2 cluster? Map-Reduce on the Fly If all you need is Hadoop, your best bet is to use Amazon Elastic Map Reduce.   Elastic Map Reduce boots Hadoop on EC2 instances without the user having to do it themselves. Your data is read from S3, and the output is written back to S3. This keeps your work organized without having to worry about where to put it in the filesystem. The user simply writes a data processing application (mapper and reducer) in Hive, Pig, Cascading, Java, Ruby Perl, Python, PHP, R, C++ etc. The user uploads the data and the application code into S3.  Elastic Map-Reduce also keeps all of the output logs in one nice place for you! When processing is complete, Amazon tears down the instance so you don’t pay for what you don’t use. 	 0 Comments
Font Families for the R PDF Device	https://www.r-bloggers.com/2010/03/font-families-for-the-r-pdf-device/	March 24, 2010	Yihui Xie	"Here is a merged PDF containing the above single PDF files: R-PDF-font-families.pdf  (29K)
 It seems that ""Bookman"", ""NewCenturySchoolbook"", ""Palatino"" and ""Times"" can be better choices when using Sweave because they are serif fonts, which are usually more consistent with LaTeX PDF. "	 0 Comments
oro.nifti 0.1.4	https://www.r-bloggers.com/2010/03/oro-nifti-0-1-4/	March 24, 2010	Brandon Whitcher		 0 Comments
oro.dicom 0.2.5	https://www.r-bloggers.com/2010/03/oro-dicom-0-2-5/	March 24, 2010	Brandon Whitcher		 0 Comments
R 2.11.0 due date	https://www.r-bloggers.com/2010/03/r-2-11-0-due-date/	March 23, 2010	M. Parzakonis	This is the announcement as posted in the mailing list : 	 0 Comments
The “Future of Open Source” Survey – an R user’s thoughts and conclusions	https://www.r-bloggers.com/2010/03/the-%e2%80%9cfuture-of-open-source%e2%80%9d-survey-%e2%80%93-an-r-user%e2%80%99s-thoughts-and-conclusions/	March 23, 2010	Tal Galili	"Over a month ago, David Smith published a call for people to participate in the “Future of Open Source” Survey.  550 people (and me) took the survey, and today I got an e-mail with the news that the 2010 survey results are analysed and where published in the “Future.Of.Open.Source blog” In the following (38 slides) presentation:  I would like to thank Bryan House and anyone else who took part in making this survey, analyzing and publishing it’s results. The presentation has left me with some thoughts and conclusions, I would like to share with you here.  Pre conclusions 1 – thoughts about the graphical/statistical presentation:
(p.s: all in good faith, please – no taking offense from anything I write.  And if you have anything to comment on – please enlighten me in the comments) Pre conclusions 2 – A plea for providing the source data for the Survey: My big hope is to see the release of the source data collected in the survey published so that other people (me   ) will be able to analyse it.  ”Setting the data free” as can be derived from O’Reilly’s keynote at OSBC conference, is a bit virtue.  Here’s a link to his talk slides, and to David’s wonderful notes about that talk (A great read.) And now for some (humble) conclusions from the survey. Conclusion 1 –  Let’s invest in making the following of R extension even more scalable Slide 12 – people believe (now more then in previous years) that one of OSS attractive features are it’s rapid pace of innovation. That’s good news for R, since R is known for that it gives more “up to date” statistical tools then any other statistical package in existence.  That is due to amazing community of statisticians and statistical programmers, coupled with a solid structure for creating R extensions. But at the same time, there are several challenges in having open source innovation. One such drawback is given by John Chambers on the subject in “Facets of R”  (A Special invited paper on “The Future of R”  – see page 3 section “Modular design and collaborative support”), and I quote: On the downside, a large collaborative enterprise with a general practice of making collective decisions has a natural tendency towards conservatism. Radical changes do threaten to break currently working features. The future benefits they might bring will often not be sufficiently persuasive. The very success of R, combined with its collaborative facet, poses a challenge to cultivate the “next big step” in software for data analysis. Another good discussion of this was made by John Fox in Aspects of the Social Organization and Trajectory of the R Project. The R Journal, 1(2):5-13, December 2009 Both authors reflect on how CRAN is having so many packages (extensions to R core).  While the diversity is wonderful, the scalability in the user’s ability to handle the variety is limited.  From a user’s perspective it is very hard to find/follow/manage all the innovative R extensions out there.  One hope for improvement in this front is the project “Crantastic“, which I hope will get (much) more attention and expansion.  An optimistic news regarding the future of the project was published recently by Dirk Eddelbuettel who shared with all of us about the open (R) projects in 2010 google summer of code, two important projects (in this respect) are Crantastic2 and cran_stats, which I hope will come through. Conclusion 2 –  If you want R to spread – support open source in general slide 13 – shows that people believe that the there are 3 main drivers for the adaptation of OSS (such as R): Conclusion 3 –  get to know what “the cloud” can do for you! slide 24 – This year, 40% of the people answering the survey (twice as much in the past two years), said that Cloud computing is gonne have an impact on OSS vendors.  If you don’t know what you can do with R and the cloud, it might be time for you to learn the subject and see if you are not missing out on something. Some of this year’s tutorials on useR2010 conference, will talk about cloud computing and R: My current (humble) contribution to the subject is the post I recently published about How to use google forms with R to Easily collect and access data for analysis. * * * I welcome any comments (or reply posts) on the subject. Please let me know what you think (of the survey results and on the points I brought up) "	 0 Comments
Video: ggplot2 Creator Hadley Wickham’s Short Course on Data Visualization Using R	https://www.r-bloggers.com/2010/03/video-ggplot2-creator-hadley-wickhams-short-course-on-data-visualization-using-r/	March 23, 2010	Stephen Turner		 0 Comments
Video: Hadley Wickham gives a short course on graphics with R	https://www.r-bloggers.com/2010/03/video-hadley-wickham-gives-a-short-course-on-graphics-with-r/	March 23, 2010	David Smith	Hadley Wickham (the creator of the popular ggplot2 graphics package for R) has posted video of a 2-hour short course on Visualisation in R at his blip.tv channel. The video is split into four thirty-minute segments:  The course is peppered with self-guided exercises, for which data, code and the supporting slides are all available. So if you want to be able to create beautiful, informative visualizations of data sets large and small, like this:  then it’s well worth two hours of your time to watch these videos. (And don’t forget Hadley’s book, ggplot2: Elegant Graphics for Data Analysis, for a complete reference.) Hadley Wickham: data vis mini course 	 0 Comments
Updated Site Map for Jeromy Anglim’s Blog Psychology and Statistics	https://www.r-bloggers.com/2010/03/updated-site-map-for-jeromy-anglims-blog-psychology-and-statistics/	March 23, 2010	Jeromy Anglim		 0 Comments
Playing with the ‘playwith’ package	https://www.r-bloggers.com/2010/03/playing-with-the-playwith-package/	March 23, 2010	nattomi	"Abilities of R for creating graphics is great, but one thing I always missed is the possibility of creating interactive plots and being able to look at graphs while changing one ore more parameters. I know that there is rggobi, but so far I always ran into problems with flexibility each time I wanted to use it. So I kept on searching until I found playwith which is “an R package, providing a GTK+ graphical user interface for editing and interacting with R plots” as its homepage says. The homepage includes a lot of screenshots with code snippets so this post doesn’t intend to give an extensive review about the possibilities of the playwith package to the reader. All I want to do now is present a small application of it. I had some geospatial data I wanted to visualize. The data was a result of a computer simulation and consisted of a set of geographical coordinates and corresponding frequency values expressed in Hz. The values associated to the coordinates   tells us the first eigenfrequency of the Earth-ionosphere cavity that would be measured at Nagycenk Observatory in the case of an assumed lightning source at  with certain properties. At first, as always, we need to load some packages.


library(R.basic) # for creating perspective plot

library(playwith) # for creating interactive plot

library(fields) # for plotting a map of the world Then, we read in our data (which is available online, so the example must be reproducible): 

regs <- list(Africa=""Africa"",Americas=""Americas"",Asia=""Asia"")

cols <- c(Africa=""red"",Americas=""green"",Asia=""blue"")

url <- ""http://storage.ggki.hu/~nattomi/ryouready/20100303""

x <- lapply(regs, function(x) {read.table(file.path(url,x),header=TRUE)}) Before plotting, I determine the range of the plottted values.


data(world.dat) # data used for plotting a world map

zAxs <- unlist(lapply(x,function(x) x$fnERT1))

r <- range(zAxs)

 And finally, the interactive plot itself:


playwith({plot3d(world.dat$x,world.dat$y,lowZ,pch=""."",

zlim=c(lowZ,upZ),xlab=""longitude"",
 ylab=""latitude"",zlab=""F1 (Hz)"",

theta=theta,phi=phi,ticktype=""detailed"")

for (i in regs) {

d <- x[[i]]

points3d(d$Dc,d$Hc,d$fnERT1,col=cols[[i]],pch=""."")

}},

parameters=list(

theta=seq(0,360,by=5),

phi=seq(0,90,by=5),

lowZ=seq(r[1],r[2],0.5),

upZ=seq(r[2],r[1],-0.5))) You should see a window popping up with 4 sliders allowing you to set different paramters of the plot, for example vertical and horizontal rotation. You can already see from the example that the syntax of the playwith command is very simple, you specify a set of commands necessary for creating the plot (with possible parameters included such as theta in this example) between curly braces then a list specifying values to be looped through for the parameters. What more could I say? If you are a visual type (or your boss is one) then play with playwith! Remark 1: Installing the package R.basic goes in a little bit unusual way, see http://www.braju.com/R/ for details. Remark 2: My world map is just a plot of a cloud of points on the plane. It would be nice if the points would be connected accordingly. This can be achieved by using the world() command in the fields package although I wasn’t able to integrate this into the 3d display. Any suggestions are very welcome. The world.dat dataset has an another drawback: it doesn’t include Corsica and Sardinia so this world map is not of too much use for the locals.  "	 0 Comments
ggplot2: Changing the Default Order of Legend Labels and Stacking of Data	https://www.r-bloggers.com/2010/03/ggplot2-changing-the-default-order-of-legend-labels-and-stacking-of-data/	March 23, 2010	learnr	"“How to change the order of legend labels” is a question that gets asked relatively often on ggplot2 mailing list. A variation of this question is how to change the order of series in stacked bar/lineplots. While these two questions seem to be related, in fact they are separate as the legend is controlled by scales, whereas stacking is controlled by the order of values in the data. Recently I spent some time getting my head around this, and below is a quick recap.  
 The standard stacked barplot looks like this: You notice that in the legend “Fair” is at the top and “Ideal” at the bottom. But what if I would like to order the labels in the reverse order, so that “Ideal” would be at the top? The order of legend labels can be manipulated by reordering the factor levels of the cut variable mapped to fill aesthetic. The legend entries are now in reverse order (and so is the stacking). The order aesthetic changes the order in which the areas are stacked on top of each other. The following aligns the order of both the labels and the stacking. Or, alternatively, reordering the factor levels again: "	 0 Comments
R Tips in Stat 511	https://www.r-bloggers.com/2010/03/r-tips-in-stat-511/	March 22, 2010	Yihui Xie	Here are some (trivial) R tips in the course Stat 511. I’ll update this post till the semester is over. Reading code is pain, but the well-formatted code might alleviate the pain a little bit. The function tidy.source() in the animation package can help us format our R code automatically. By default it will read your code in the clipboard, parse it and return the well-formatted code. You have options to keep or remove the comments/blank lines and set the width of the code, etc. Spaces and indent will be added automatically. This can save us time typing spaces and paying attention to indent.  We often deal with matrices like  in 511 and may wonder what on earth they are. If we directly compute solve(t(X)%*%X)%*%t(X) (or generalized inverse ginv() in MASS) we often end up with seeing a lot of decimals, which makes it difficult to see what these numbers really mean. The function fractions() in the MASS package can approximate rationals by fractions. For example: Strip chart is a common tool for batch comparisons. When points get overlapped in the plot, we may “jitter” the points by adding a little noise to the data. The R function jitter() is an option to manipulate the data, but stripchart() already supports jittered points. Jittered Strip Chart by stripchart() Jittered Strip Chart by ggplot2 R base does not provide a general test for the coefficients of a linear model, but we can use the function glh.test() in the gmodels package to do it. If you take a look at its source code, you will find unsurprisingly it is nothing but the code in page 7 of slide set 9 of Dr Nettleton’s lecture notes. I created a dynamic demo to illustrate the power of the F test here: Demonstrating the Power of F Test with gWidgets. Play with it and have fun! Many people do not realize the possibility of converting the data types of columns in read.table() and always use such specific post hoc conversion: But in fact, we can specify the types of columns while reading data: There are other tips in read.table() but I find this one the most useful. Check the 22 arguments in ?read.table if you want to know more magic (e.g. how to specify the first column in the data file as the row names). There is a function newton.method() in the package animation which shows the detailed iterations in Newton’s method. Here is a demo: Newton-Raphson Method for Root-finding I hope this is useful for understanding iterative algorithms. Some little tips: 	 0 Comments
RInside release 0.2.2	https://www.r-bloggers.com/2010/03/rinside-release-0-2-2/	March 22, 2010	Thinking inside the box	"

RInside is a set of convenience classes to facilitate embedding of R
inside of C++ applications. It works particularly well with 
Rcpp and now
depends on it. 
 
This is the first release since version 0.2.1 in early January.
Romain and I made
numerous changes to
Rcpp
in the meantime.  With this release, RInside is
starting to catch up by taking advantage of many new automatic (templated)
type converters.  We have updated the existing examples, and added several
new ones.  These are all visibile directly via the 

Doxygen-generated documentation under the Files heading. Two examples are
also shown directly on the
RInside
page.

 
Also added are new examples showing how to use 
RInside to embed
R inside C++ applications using MPI for parallel computing. This was
contributed via two examples files by Jianping Hua, and we reworked the
examples slightly (and added two variants that use MPI’s C++ API).
 
As it is so short, here is the basic ‘Hello, World’ example now showing the
simpler Rcpp-based variable assignment:
 
One minor setback is that the examples currently segfault on Windows. That
may be an issue with linking and class instantiation or something
related. Romain and I focus much more on Linux and OS X, so this has not
gotten a lot of attention.  Debugging help would be appreciated.


 "	 0 Comments
R 2.11.0 scheduled for April 22	https://www.r-bloggers.com/2010/03/r-2-11-0-scheduled-for-april-22/	March 22, 2010	David Smith		 0 Comments
Charting SVN commits with R	https://www.r-bloggers.com/2010/03/charting-svn-commits-with-r/	March 22, 2010	David Smith	Want to get a quick sense of who are the most active committers to your SVN project? Using just a few lines of R code and the SVN log file, reader and new R user Rhys Kidd created this chart to review commits to the Freespace 2 Source Code Project:   Rhys posts the 6 lines of R code to create the plot in this forum post, and I recreate it here (with some slight reformatting):  system(“svn log > fs2_open.svnlog”)x rx who ctab dotplot(ctab[order(ctab)],  scales=list(x=list()),   xlab=””,   main=”Number of fs2_open SVN commits Jan 2007 to Mar 2010″)   Note the use of the system command to dynamically create the SVN log — each time the R code is run, an up-to-date analysis of the commit rates is made. Regular expressions extract the commits and user names from the log file, which are then tabulated for display with the dotplot command. That’s some pretty tight code for (in RK’s words) a “new-ish R user” — although he says he did get inspirations from Dirk Eddelbuettel’s recent talk at the University of Chicago. (Update: Dirk points out that Ben Bolker originated the idea at UseR 2007. It’s a great example of how reading from connections — here, a log file — makes for compact, powerful code.) Incidentally, I was amused by the first response on the forum: “Independent variable…on vertical axis…*twitch*”. Actually, a dot chart is a great choice for data like this: the sorting gives a convenient read of the most and least prolific committers; it’s easy for the eye to make numerical comparison between rows, and the use of dots instead of the “natural” bars makes for a clean, easy-to-read chart. The FreeSpace Source Code Project forum: Charting the development of the fs2_open codebase 	 0 Comments
New FECHell 0.1.9	https://www.r-bloggers.com/2010/03/new-fechell-0-1-9/	March 22, 2010	jjh	Our FEC report file library FECHell has been updated to 0.1.9. The release includes a half dozen bug fixes and the following new features:   Other major forms (F3X,F1,F2,F6) are in progress, as are the outstanding schedules (SC2). Visit the FECHell page for more information, installation instructions, and more examples.  	 0 Comments
Example 7.28: Bubble plots	https://www.r-bloggers.com/2010/03/example-7-28-bubble-plots/	March 22, 2010	Ken Kleinman		 0 Comments
A Visual History Of Twitter’s Growth (Updated 2010-08-23)	https://www.r-bloggers.com/2010/03/a-visual-history-of-twitter%e2%80%99s-growth-updated-2010-08-23/	March 22, 2010	Ed Borasky		 0 Comments
March insanity	https://www.r-bloggers.com/2010/03/march-insanity/	March 22, 2010	Ted Hart		 0 Comments
Health Care Reform vote	https://www.r-bloggers.com/2010/03/health-care-reform-vote/	March 21, 2010	jackman	"A little bit of churn relative to the House’s 1st shot at this but otherwise a remarkably similar vote, with an estimated cutpoint almost at the same place; see some raw R output, below the fold, after the thumbnail… 
y is the vote to take up the Senate amendments; yEarly is the previous go at this by the House: Probits of the votes against ideal points: Cutpoints: "	 0 Comments
2010 March Madness Half Marathon in Cary	https://www.r-bloggers.com/2010/03/2010-march-madness-half-marathon-in-cary/	March 21, 2010	Thinking inside the box	"
As for the race conditions, we had fantastic weather all week with
temperatures up to the sixties and then all of a sudden a forecast of rain,
snow and even sleet for the weekend. Luckily, and while yesterday was sucky, today
was allright or better.  A little chilly and damp, but neither rain nor
snow — or even wind. So the conditions were good, with the course challenging as usual.

 
The race itself went fine. I ran more or less steadily, never had to stop but
was not particularly fast at 1:39:38 or a pace of 7:36.3.  I had aimed for
beating 1:40, had missed that target by miles 4 to 6 and was about 10 or 15
seconds behind but managed to get a negative split on the second half of the
course to reach that goal. Which is nice, but the time is still the slowest
I’ve ever run that race, and my slowest half-marathon since 2004.
 

Training had been sluggish all winter. Oddly enough, already in
last year’s post
I stated pretty much the same and feared that Boston may become tough —
which
it did. But this year may well be a lot worse as I had no spring in my step all
winter long. No fire in the belly for training will make for a long
race. We’ll see how it goes. Four weeks to go.

 "	 0 Comments
Converting Siemens MOSAIC	https://www.r-bloggers.com/2010/03/converting-siemens-mosaic/	March 21, 2010	Brandon Whitcher		 0 Comments
R: Add vertical line to a plot	https://www.r-bloggers.com/2010/03/r-add-vertical-line-to-a-plot/	March 21, 2010	Tal Galili	If you have a plot open and want to add a vertical line to it:  	 0 Comments
The distribution of rho…	https://www.r-bloggers.com/2010/03/the-distribution-of-rho%e2%80%a6/	March 21, 2010	M. Parzakonis	"There was a post here about obtaining non-standard p-values for testing the correlation coefficient. The R-library deals with this problem efficiently. This is how it looks like,


Now, let’s construct a table of critical values for some arbitrary or not significance levels. We can calculate p-values as usual too… "	 0 Comments
My Experience at ACM Data Mining Camp #DMcamp	https://www.r-bloggers.com/2010/03/my-experience-at-acm-data-mining-camp-dmcamp/	March 21, 2010	Ryan Rosario	"My parents and I made plans to visit San Jose and Saratoga on my grandmother’s birthday, March 19, since that is where she grew up. I randomly saw someone tweet about the ACM Data Mining Camp unconference that happened to be the next day, March 20, only a couple of miles from our hotel in Santa Clara. This was an opportunity I could not pass up. Upon arriving at eBay/PayPal’s “Town Hall” building, I was greeted by some very hyper people! Surrounding me were a lot of people my age and my interest. I finally felt like I was in my element. The organizers of the event also had a predetermined Twitter hashtag for the event #DMCAMP, and also set up a blog where people could add material and write comments about the sessions. I felt like a kid in a candy shop when I saw the proposed sessions for the breakout sessions. Some of the proposed topics I found really interesting: About half of these actually made it onto the schedule. Unfortunately, I was only able to attend 4 sessions due to the schedule, but that’s OK because at the end of the day I was still exhausted.   Session 1: Status of Mahout
 This one was particularly exciting. Dr. Ted Dunning, a committer for the Mahout project stated that the purpose of Mahout (muh-hoot) is to make machine learning and data mining algorithms scalable. The purpose is not to make the most efficient or highest performing algorithm. In particular, Mahout is built on top of Hadoop so algorithms can take advantage of map-reduce. Mahout is not currently a top-level project, but a subproject of Lucene. It may become a top-level project, like Hadoop, some time in the future. Learning about Mahout was exciting because I learned what it can do for me and other researchers. Recently, I had a huge incidence matrix that I wanted to find the singular values and vectors for. Astonishingly, NumPy cannot do this yet despite its awesome sparse matrix support. Mahout, on the other hand, is also working on it and seems to be pretty close. Mahout can already perform very fast SVD using a Hebbian method. Now they are working on distributed SVD for sparse matrices using stochastic decomposition. Unfortunately, there does not currently seem to be any plans to incorporate methods for non-negative matrix factorization. Session 2: Data Mining in the Cloud The second session was roundtable style and showed the amount of diversity in the crowd. Experience ranged from cloud expert to not understanding what the cloud is. There was also some discussion on the blurring of data mining and data processing. We also struggled a bit with the sound “sass” as it came up in different contexts: SAS the statistical package, SaaS “software as a service” and SAS hard disks, “serial attached SCSI” sometimes used for big data. Introduction to Amazon EC2 and the services it provides dominated the discussion. A common question is how much Amazon EC2 costs in comparison to a private hardware cluster. One case study mentioned was RazorFish‘s experience with EC2. They spent about $13,000 per month using a large EC2 cluster whereas without EC2 they would have spent upward of $500,000 for the necessary hardware in addition to another systems support employee. There were people interested in combining R with Hadoop in the cloud. I mentioned the packages RHIPE (ree-pay) and HadoopStreaming. Chris Wensel (@cwensel) mentioned that these packages may not be very useful performancewise due to the way that serialization occurs in Hadoop. I may not remember his exact reason, or quote, though. We also heard about some other services such as AsterData and Greenplum. Session 3: Data Mining in R
 The Data Mining in R session was originally planned to be a Bird of a Feather session, but only a few of us in the room had used R for data mining. The room was packed and it seemed that a lot more people had attended than had originally shown interest! Some packages mentioned included: rattle, a GUI for data mining and caret, a package to streamline the creation of predictive models.  Hadley Wickham‘s ggplot2 was the focus of our discussion on visualization of data. For large data, J. Rickert from Revolution Computing gave an interesting bit of advice: “always expect that your size  matrix will require four times as much space.” We also learned a bit about the bigmemory package. Some other time during the day there was an R for Newbies session. I wish I had my slides because I could have assisted. I was thrilled that so many people were interested in R. As I entered the room for the Data Mining in R session, there was a fellow that asked what the next session in the room was. When the presenter said “Data Mining in R”, I expected him to say something like “oh, no, that’s not for me” as I am so used to hearing. Instead, he said “Oh cool, I really need to learn R.” For those that are interested in learning R, see our slides from the UCLA Statistical Consulting Center, where we teach workshops in R several times per quarter. Material from previous quarters is there as well. Session 4: Hadoop Chris Wensel is a Hadoop genius. I envy his attention to every technical detail about the system! At this point in the long day, I attended casually because I was pretty tired. Most of the time was spent in a question and answer forum. The momentum from the presenter and the audience was with Cascading, a workflow system for Hadoop jobs. Wensel’s advice was, “write a map-reduce application, throw it away, and then start using Cascading.” This gave me a lot of motivation to try it now. I just assumed one must master the full art of Hadoop before moving towards Cascading and some of the other projects. One major thing I learned was the Amazon EC2 is not necessary to run Hadoop in the cloud. Amazon Elastic MapReduce accomplishes the same without the need for an AMI! HBase was introduced as a parallel column based key-value store that adheres to much of the BigTable specification; performance is key. On the other hand, Hive was designed for ad-hoc analytics. Pig is a query language for processing large datasets. One participant asked if the user must worry about persistence and file locking. Chris mentioned that Zookeeper allows the user to control some aspects of locking in Hadoop jobs. We did not go into much detail about these subprojects though. In the few minutes remaining, the speaker entertained a question about the NoSQL movement and Hadoop. Relational databases adhere to the CAP theorem: Consistency, Availability, and Partitioning. Wensel stated that with big data, we absolutely must have Partitioning, but for NoSQL, the other two conditions consistency and availability must be relaxed. For example, the filesystem used for S3, which is kind of a database, is eventually consistent, a relaxation of consistency. An ls in an S3 bucket will yield different results when data is being processed. A NoSQL system could also drop availability . This means that at a partition event, the affected services wait until the data is consistent, and the system is unavailable for use during the period of inconsistency. There was also some discussion on Lucene and the companion projects Solr, Katta and Nutch. Rest of Conference At the beginning of the conference, there was a great expert panel that took questions from the audience. There was also time for companies to announce that they are hiring jobs. Despite the terrible US and California economies, there is a ton of momentum in data mining. The best part of the talk was the soundbytes. Joseph Rickert had some real zingers, and I could not agree more. My comments are in parenthesis. Rickert: “The thing about statisticians is that they don’t write good code.” (some of them really think they do; it’s funny)
Rickert: “Ask a statistician about a hash table and they have no idea what you’re talking about.” (yup!)
Dr. Dunning: “For data mining, software engineering is not as important. Working with big data and experience with big data is key.” (I completely agree, but try telling Google that.) There were some door prizes; I did not win anything. Nothing to cry about though: 4 Microsoft 2GB flash drives, certificates for free e-books, a portable hula-hoop, light up bouncing balls, data dictionaries, and some weird snake bracelet that had its tongue sticking out. What was Missing The ACM Data Mining Camp was pretty complete. In hindsight, there were some things that I was expecting to hear about but did not see a session about. Some of these include the visualization system Processing. I also did not see very much about working with network data. I was also expecting to see something about Scala, but it did not seem to come up. In Relection… There is one non-data mining thing I learned this past week. After meeting a Twitter friend at the event, and receiving an email from another Twitter friend over the weekend, it seems that Data Mining and Machine Learning people all have similar feelings about the Statistics and Computer Science “empires.” There is little communication between both fields. It’s a shame because we could conquer the world if we combined minds. This entire four years I’ve been in grad school, I’ve felt all alone and wondering why at UCLA there is such a gap between CS and Statistics with respect to Data Mining. It just happens to be that at UCLA Computer Science gives data mining a better treatment. At many other schools it may be the exact opposite. It is great to know that I am not alone in my frustration!   "	 0 Comments
R: Geometric mean	https://www.r-bloggers.com/2010/03/r-geometric-mean/	March 21, 2010	Tal Galili	 But this requires package heR.Misc so you might as well just use:  	 0 Comments
Returns on Easter week and one week after	https://www.r-bloggers.com/2010/03/returns-on-easter-week-and-one-week-after/	March 21, 2010	kafka	"Inspired by CXO group report, I did a rerun of the same strategy on my data. Easter’s dates can be find at wikipedia. Overall, my results are similar to CXO group’s results. In the graph below, I plotted daily returns on Easter week (Monday to Thursday) from 1982 to 2009. I prefer this way of showing things, where the range, minimum, maximum and mean of returns are presented.
It is clear, that only returns on Thursdays are above zero and t-test confirms that:
t = 2.235, df = 27, p-value = 0.03389. The rest is close to random.  The graph below shows daily returns one week after Easter holidays. Although Monday looks like negative day, but it lacks significance:
t = -1.1517, df = 27, p-value = 0.2595 (should be less that 0.1).
The rest is noise.  In summary, only returns on Thursdays have positive bias. "	 0 Comments
R annoyances	https://www.r-bloggers.com/2010/03/r-annoyances/	March 20, 2010	John Mount	"Readers returning to our blog will know that Win-Vector LLC is fairly “pro-R.”  You can take that to mean “in favor or R” or “professionally using R” (both statements are true).  Some days we really don’t feel that way.  
Consider the following snippet of R code where we create a list with a single element named “x” that refers to a numeric vector.  We start with a demonstration of the hard-coded method of pulling the x-value back out using the “$” operator. But suppose we wanted to automate this; that is pass in the name of the value we want in a variable.  We are after all using a computer, so automating a step seems like a reasonable desire.  R supplies a notation for this using the “[]” operator.  But something slightly different comes out under the “[]” operator than under the “$” operator: Notice that the printed outputs are slightly different (one echoes “$x” and one does not).  Let’s use the “class()” method to see what is actually being returned in each case. Completely different return types are returned (in one case a numeric vector in the other a general list, not interchangeable types).  At this point you may think it is time to turn in our “pro” label and call ourselves “newb” (Internet slang for “newbie” or “idiot”).  But let’s slow down for a bit.   When two views of the same situation disagree (such as the difference in opinion between the authors of R and myself whether the “[]” and “$” operators should return the same type) you at most know that at least one of those views is wrong.  You don’t really know if one view is right or even if one view is right which one it is.  I can, however, bring in some additional argument to try and show the design of R is in fact wrong.  The additional argument is “The Principle of Least Astonishment.”  This principle roughly says that it is a mistake to introduce unnecessary differences in outcomes (which to the unprepared user are unpleasant surprises).  There may be some deep (yet obscure) reasons the two operators prefer to return different results.  But the fact you would have to find a way to document and explain these differences really should make one think that this situation is really a mis-design and the “explanation” is really an attempt at a work around.  Or to put it more rudely: there may be an explanation, but there is no excuse. For another example consider creating a 3 by 3 matrix: Now select the last two rows of the matrix. Now (for the punchline) try to select just the middle row of the matrix.
  Notice that once again (and without warning) the result is subtly different.  I admit that it seems paranoid to worry about such small differences- but when you are debugging a system that should work these are exactly the killing mistakes you are looking for.  In this case the problem is pretty bad.  See what happens if you tried to ask for the dimension of each of these differing returns: The first case works fine (reports 2 rows and 3 columns).  The second case returns “NULL” (instead of 1 row and 3 columns).   In R NULL is sometimes used as an error-value (instead of throwing an exception) and this value will poison any further conditions or calculations it is involved in.  The main way to deal with the arbitrary introduction of such NULLs is the incredibly tedious uncertain defensive coding practices that we argue against in Postel’s Law: Not Sure Who To Be Angry With.  Such code weakens both programs and programmers. But what is going on in this example?  Once again we use the “class()” method to inspect the subtly different results. The result is disappointing.  For a two-row select R returns a matrix (what we would expect).  For a single-row select R does us the “favor” of converting the result into a vector.  This is a disaster.  A single row matrix is similar to a vector, but even R itself does not support the same set of operations and outcomes on vectors as it does on matrices (for example the failure of the “dim()” method).  It is not safe to further calculate with these results (without by-hand converting the result back to a single row matrix which R can in fact represent).  In my case this created crashing bugs deep in a long running analysis (and was hard to diagnose as the bug was in an “innocent operation” not in a “risky calculation”). All of this has to violate John Chambers’ “Prime Directive” for data: “an obligation on all creators of software to program in such a way that the computations can be understood and trusted.”  Chambers’ opinion being relevant as he is the author of the S language (of which R is an open source re-implementation).  We continue to recommend R, but we also recommend being exceptionally careful when using it (which unfortunately adds time to projects). Related posts: "	 0 Comments
R: remove all objects fromt he current workspace	https://www.r-bloggers.com/2010/03/r-remove-all-objects-fromt-he-current-workspace/	March 20, 2010	Tal Galili	 	 0 Comments
Package Releases	https://www.r-bloggers.com/2010/03/package-releases/	March 20, 2010	omegahat	I just put a new version of the XML package on the Omegahat repository. There is a new version of the RKML package which handles large datasets much more rapidly. Also, I put a new package named RJSCanvasDevice which implements and R graphics device that creates JavaScript code that can be subsequently  display on a JavaScript canvas in an HTML document.  	 0 Comments
R: Backwards for loop	https://www.r-bloggers.com/2010/03/r-backwards-for-loop/	March 20, 2010	Tal Galili	 As easy as that. 	 0 Comments
Because it’s Friday: Kittens, beware Tufte	https://www.r-bloggers.com/2010/03/because-its-friday-kittens-beware-tufte-2/	March 19, 2010	David Smith	Edward Tufte has been a tireless promoter of good infographics, and he’s even taken some controversial steps to rid the world of chartjunk. But now he’s gone too far:   Then again, this chart from the Wall Street Journal could lead anyone to felinicide:   What’s wrong with a simple bar chart, WSJ? Mark Goetz: My New Wallpaper (via @sarahd23 and @statalgo) 	 0 Comments
Savage-Dickey [talk]	https://www.r-bloggers.com/2010/03/savage-dickey-talk/	March 19, 2010	xi'an	Here are the slides for the Savage-Dickey paradox paper that I gave in San Antonio this morning:  (Any suspected coincidence of the first part with earlier talks is for real!) I have tried to spell out as clearly as possible in the second part the issues of version choices that are at the core of the “paradox”. 	 0 Comments
Balloon plot using ggplot2	https://www.r-bloggers.com/2010/03/balloon-plot-using-ggplot2/	March 19, 2010	Paolo Sonego		 0 Comments
Senators’ ideal points against Obama vote	https://www.r-bloggers.com/2010/03/senators-ideal-points-against-obama-vote/	March 18, 2010	jackman	"I added another plot to the output generated by my overnight ideal point scripts: a scatterplot of estimated Senate ideal points against Obama vote share in their state (color coded by party, local linear regression overlays by party, labels for some big residuals). I suppose I’m surprised by the way that the loess curve for the Democrats flattens out to the left of 50% Obama vote share.  It is an interesting picture to stare at and speculate as to why that is the case: party whipping by the Dems, agenda control (distorting our estimates of the ideal points of Dems in ranks, say, 35-60), lack of fit by the one-dimensional model, some combination of all this, something else…  Outliers: Coburn (R-OK), Demint (R-SC) and Ensign (R-NV) among the Reps; Lieberman (really a Dem?, and a positive residual), and two big negative Dem residuals, Brown (D OH) and Harkin (D IA). 
> "	 0 Comments
R Project selected for the Google Summer of Code 2010	https://www.r-bloggers.com/2010/03/r-project-selected-for-the-google-summer-of-code-2010/	March 18, 2010	Thinking inside the box	"
An R Wiki page had been created and serves as the central
point of reference for the R Project
and the GSoC 2010. It contains a list of project ideas, currently counting
eleven and spanning everything from research-oriented topics (such as spatial
statistics or automatic differentiation) to R community-support (regarding
CRAN statistics and the CRANtastic site) to extensions (NoSQL, RPy2 data interfaces, Rserve browser integration) and more. I also just created a
mailing list [email protected] where prospective students and mentors can exchange ideas and discuss.  As for other
details, the Google
Summer of Code 2010 site has most of the answers, and we will try to keep
R-related information on the aforementioned 
R Wiki page.


 "	 0 Comments
Create annotated GWAS manhattan plots using ggplot2 in R	https://www.r-bloggers.com/2010/03/create-annotated-gwas-manhattan-plots-using-ggplot2-in-r/	March 18, 2010	Stephen Turner		 0 Comments
Webinar: High-Performance Analytics with R and Microsoft HPC Server	https://www.r-bloggers.com/2010/03/webinar-high-performance-analytics-with-r-and-microsoft-hpc-server/	March 18, 2010	David Smith	On April 14 I’ll be giving a new webinar in partnership with Microsoft on High-Performance Computing with R. I’ll be focusing on the new parallel programming capabilities of REvolution R Enterprise 3.1 for Windows, and how to use the features of Microsoft HPC Server to enable computing on clusters. Here’s the complete agenda, and you can register at the link below. Statistical data analysis is a key part of the operations of just about every business today. But as data sets get larger, analyzing trends or generating predictions becomes more and more of a challenge. If you’re doing predictive modeling today and find that you can no longer use all of your data because of size limitations, or the computations are taking too long for you to take action on the results, then the parallel-processing capabilities of Windows HPC Server can help. In this webinar, we’ll introduce the R language for statistical computing, and show how the easy-to-use parallel programming capabilities of REvolution R Enterprise work with a HPC cluster to cut processing times by an order of magnitude or more. We’ll give practical examples of how to speed up many kinds of analytic computations, from simple summary statistics to cutting-edge tools like ensemble predictive models. Audience: Programmers, researchers and analysts who need to process large volumes of data for data mining, statistical analysis, or predictive analytics REvolution events: High-Performance Analytics with REvolution R and Windows HPC Server 	 0 Comments
Course in San Antonio, Texas	https://www.r-bloggers.com/2010/03/course-in-san-antonio-texas/	March 18, 2010	xi'an	Yesterday, I gave my short (3 hours) introduction to computational Bayesian statistics to a group of 25-30 highly motivated students. I managed to cover “only” the first three chapters, as I included some material on Bayes factor approximation and only barely reached Metropolis-Hastings. Here are the slides, modified from the original Bayesian Core slides:  (It took me close to two hours to download those to Slideshare using the local wireless networks !) Since the slides as registred above seem to be unavailable to some readers, here is another identical version (but with another address):  	 0 Comments
O’Reilly at OSBC: The future’s in the data	https://www.r-bloggers.com/2010/03/oreilly-at-osbc-the-futures-in-the-data/	March 17, 2010	David Smith	Tim O’Reilly’s keynote talk at OSBC this evening was thought-provoking to say the least. The title of the talk was “The Real Open Source Opportunity”, and the surprise for me was that he wasn’t talking about Open Source software. Tim’s insight, and it’s a profound one, is that the next frontier for freedom and openness — and indeed, the way we’ll live our lives — lies with data. Why? The world in which open source software was born is very different from the world we’re heading to. Less than a decade ago, a major concern of the computing world was that much of the capability and innovation was locked up in closed software held by major corporations, like Microsoft. Open-source software addressed that. But look where innovation lies today: companies like Google (and few others) –built on the backs of open-source technology, mind you — can now perform tasks that not long ago were the realm of science fiction. Today, you can speak a question into a tiny handheld gadget, and find out where to get good pizza. But think for a moment how Google can do this reliably and quickly: it’s their data. They’ve amassed a massive, proprietary database of search queries, written text, and voice samples that allow the Google Voice Search app on the iPhone (and algorithms in Google’s cloud servers) to distinguish “pizza” (said in on a noisy street in a Jersey accent) from “piece of” or the city “Pisa”. Tim was careful to point out: it’s not the closed algorithms that make this work. Peter Norvig from Google has said it himself: Google doesn’t have better algorithms than everyone else. They just have more data. Tim asked a question to the audience: “Could anyone in the Open Source community build the infrastructure to deliver Google Voice Search?” The response: a stony silence. The implication? Vendor lock-in is lo longer about proprietary source code. It’s about massive, hard-to-replicate data sets — making Google a potential Microsoft of the next decade. The corollary? The future will be about who has the most data, and who is able to extract meaning from it and deliver it in real time. So how can we avoid data lock-in in the years to come? Tim suggests that it may be the underdogs of these new cloud-based tools that become the allies of open source data applications. Ironically, it may be Microsoft, lagging today in the domains of search, maps, and speech recognition, that may be the biggest ally in making the associated data services available openly. Google certainly has no motivation to do so; on the contrary, in areas like local search where they once linked to third-parties like Yelp, they’re now providing their own data exclusively. Another opening likely comes from open standards for data sharing, like the Gov2.0 initiative. The implications are profound, not just in terms of lock-in but also in the areas of privacy. (Interesting privacy implication: did you know that it’s possible to identify a specific appliance, like a Kenwood dishwasher, from the “DNA” of its power draw signal? Consider that when your power company tracks your power usage with e-meters.) But when the operating system of the future is the entire internet, which license you use for open source software somehow seems like small beans compared to the open data issue. Update: The slides from Tim O’Reilly’s keynote are now available: Open Source in the Cloud Computing Era  	 0 Comments
Tools	https://www.r-bloggers.com/2010/03/tools/	March 17, 2010	The Average Investor	All the tools I am using at the moment are free of charge. The one that comes to mind first is R. It’s a language for statistical computing which comes with a decent GUI. R comes with some time series support out of the box, but there are plenty of packages (R extensions are called packages) which provide powerful functionality. I remember using all of the following (in order of decreasing use): The best part is that I don’t feel like I need any other language at this point! I have considered trying to speed up some R simulations by coding them in C++, but I can wait a few days for them to finish if necessary:). For small scripts I use Python – it’s a great language which I discovered recently and I am trying to learn. And of course, I use spreadsheets and documents (Microsoft Office and OpenOffice) to summarize my results. The other important part is data. My main source is Yahoo Finance. Let me first say, that in my opinion it’s one of the best sites on the Internet in terms of both design and content! It also has a lot of historical data. R comes with at least two interfaces to load historical data from Yahoo transparently, and so does most of other software I have used. Another low cost data source is EODDATA. I have purchased access to some of their data. And that’s all for tonite! 	 0 Comments
Vanilla Rao-Blackwellisation for revision	https://www.r-bloggers.com/2010/03/vanilla-rao-blackwellisation-for-revision/	March 17, 2010	xi'an	The vanilla Rao-Blackwellisation paper with Randal Douc that had been resubmitted to the Annals of Statistics is now back for a revision, with quite encouraging comments: The paper has been reviewed by two referees both of whom comment on the clear exposition and the novelty of the results. Both referees point to the empirical results as being suggestive of a more incremental improvement in practice rather than a major advance. However the approach the authors adopt is novel and I believe may motivate further developments in this area. I cannot but agree on those comments! Since we are reducing the variance of the weights, the overall effect may be difficult to spot in practical applications. In the current version of the paper, we manage 20% reduction in the variance of those weights, but obviously this does not transfer to the same reduction of the variance of the overall estimator! Our vanilla Rao-Blackwellisation does not speed up the Markov chain. 	 0 Comments
OSBC blogging	https://www.r-bloggers.com/2010/03/osbc-blogging/	March 17, 2010	David Smith	I’m at the Open Source Business Conference in San Francisco today and tomorrow; I’ll report in with updates after the talks. I’m particularly looking forward to the panel discussion on The Shifting Open Source Opportunity moderated by Ashlee Vance, the New York Times reporter who wrote the major story on R last year. (Interesting aside: I learned recently that that story was the most-emailed at the NYT the week it was published.) REvolution board member Zack Urlocker is also on the panel. If you’re at OSBC yourself, drop in and say hi. You can find REvolution at the North Bridge Venture Partners booth in the exhibit hall. Update: forgot to mention I’m live-tweeting the conference, too. Follow me at @revodavid and others at the #osbc hashtag.  	 0 Comments
Measuring the length of time to run a function	https://www.r-bloggers.com/2010/03/measuring-the-length-of-time-to-run-a-function-2/	March 17, 2010	Shige		 0 Comments
Omegahat Statistical Computing » R 2010-03-16 19:28:40	https://www.r-bloggers.com/2010/03/omegahat-statistical-computing-r-2010-03-16-192840/	March 16, 2010	omegahat	Hin-Tak Leung mailed me about a problem with certain malformed XML documents from FlowJo.  There are namespace prefixes (prfx:nodeName) with no corresponding namespace declarations (xmlns:prefix=”uri”). How do we fix these?  Well, the XML parser can read this but raises errors. We can do nice things to catch these errors and then post-process them. Then we can fix up the errors, add namespace declarations to the document and then re-parse the resulting document.  Here is the code. It will make it into the XML package. (I’ve made some minor changes thanks to Hin-Tak’s suggestions, but haven’t tested them.) 	 0 Comments
Interrupting R processes in Ubuntu	https://www.r-bloggers.com/2010/03/interrupting-r-processes-in-ubuntu/	March 16, 2010	Samuel Brown		 0 Comments
Validating credit card numbers in SAS	https://www.r-bloggers.com/2010/03/validating-credit-card-numbers-in-sas/	March 16, 2010	heuristicandrew	Major credit card issuing networks (including Visa, MasterCard, Discover, and American Express) allow simple credit card number validation using the Luhn Algorithm (also called the “modulus 10″ or “mod 10″ algorithm).  The following code demonstrates an implementation in SAS.  The code also validates the credit card number by length and by checking against a short of known test account numbers. Patrick’s SAS code was very helpful, but without resetting ChkSum, SAS would fail all credit card numbers following an invalid card number. This code was executed on 177,172 credit card records and found only 40 invalid numbers.  Most of them were obvious like 4000000000000000.  YMMV. 	 0 Comments
In search of a random gamma variate…	https://www.r-bloggers.com/2010/03/in-search-of-a-random-gamma-variate%e2%80%a6/	March 16, 2010	M. Parzakonis	"One of the most common exersices given to Statistical Computing,Simulation or relevant classes is the generation of random numbers from a gamma distribution. At first this might seem straightforward in terms of the lifesaving relation that exponential and gamma random variables share. So, it’s easy to get a gamma random variate using the fact that . The code to do this is the following This works unfortunately only for the case .
 In the general case we got to result to more “complex” (?) simulation, hence programming. The first technique we gonna use is rejection sampling. As the proposal (or proxy or instrumental) density we set the . The key to implementation is to maximise the ratio of the two densities, ie . We find the maximum of the ratio along the next lines. Analytically we can work out that the maximum is achieved at $latex alpha -k$, then the actual value is .  Now, we draw variates from the integer gamma until one is accepted. UPDATED  Not bad! "	 0 Comments
Nutritional supplements, ranked	https://www.r-bloggers.com/2010/03/nutritional-supplements-ranked/	March 16, 2010	David Smith	One of my favourite shows on TV right now is The Big Bang Theory. For those who haven’t seen it: it’s like Friends, except instead of New York yuppies, it’s PhD physicists and engineers at CalTech. It’s nice to see geeks and smart people be the focus (rather than the comic relief) of a sitcom. Also, the equations on the ubiquitous whiteboards on the sets are actually meaningful, instead of the usual scattering of random symbols. Here’s a typical exchange from the show (Penny is, literally, the girl next door; Sheldon is a genius physicist with no sense of sarcasm): (Penny walks to a nearby shelf of vitamins and supplements.) Sheldon: Oh boy. Penny: What now? Sheldon: Well, there’s some value to taking a multi-vitamin but the human body can only absorb so much. What you’re buying here are the ingredients for very expensive urine. Penny: (sarcastically) Well, maybe that’s what I was going for. Sheldon: (trying to be helpful) Well then you’ll want some manganese. I laughed at that exchange: I’ve been saying the same thing as Sheldon for years. And so I was interested to see this of the value of nutritional supplements from Information is Beautiful, even if the eye candy factor rather outweighs the elegance of the information design. (Who needs to use the X axis, anyway?) Fortunately, R blogger Tal Galili has taken the source data (made available as a Google Spreadsheet, so it can be read directly into R) and presented it as a simple bar chart, ranking the supplements by their efficacy score (0 means no evidence of efficacy; 6 means strong evidence of efficacy for the condition listed to the right):  Tal has helpfully made the R code to recreate this plot available in his blog post. R-statistics blog: Nutritional supplements efficacy score  	 0 Comments
DICOM-to-NIfTI Conversion	https://www.r-bloggers.com/2010/03/dicom-to-nifti-conversion/	March 16, 2010	Brandon Whitcher		 0 Comments
Rcpp 0.7.10	https://www.r-bloggers.com/2010/03/rcpp-0-7-10/	March 15, 2010	Thinking inside the box	"

We also put two small improvements in, see the full NEWS entry for this release:
 
As always, even fuller details are in 
Rcpp Changelog page and the 
Rcpp page which also
leads to the downloads, the 
browseable
doxygen docs and zip files of doxygen output for the standard formats.
A local directory  has
source and documentation too. 
Questions, comments etc should go to the
rcpp-devel mailing list
off the R-Forge page

 "	 0 Comments
Solving the rectangle puzzle	https://www.r-bloggers.com/2010/03/solving-the-rectangle-puzzle/	March 15, 2010	xi'an	Given the wrong solution provided in Le Monde and comments from readers, I went to look a bit further on the Web for generic solutions to the rectangle problem. The most satisfactory version I have found so far is Mendelsohn’s in Mathematics Magazine, which gives as the maximal number  for a  grid. His theorem is based on the theory of projective planes and  must be such that a projective plane of order  exists, which seems equivalent to impose that  is a prime number. The following graph plots the pairs  when  along with the known solutions, the fit being perfect for the values of  of Mendelsohn’s form (i.e., 3, 7, 13). Unfortunately, the formula does not extend to other values of , despite Menselsohn’s comment that using for  the positive root of the equation  and then replacing  by nearby integers (in the maximal number) should work. (The first occurrence I found of a solution for a square-free set did not provide a generic solution, but only algorithmic directions. While it is restricted to squares. the link with fractal theory is nonetheless interesting.) 	 0 Comments
Robert Brown and Pollen Particles	https://www.r-bloggers.com/2010/03/robert-brown-and-pollen-particles/	March 15, 2010	Lee	"In 1827, the botanist Robert Brown was studying pollen particles as they floated in water. When viewed through a microscope, he observed that the particles seemed to move around as if the were alive.  Although he couldn’t have known at the time, the seemingly random motion was caused by the collision of water molecules against the pollen particle.  Later on, the random motion he observed would be given the name ‘Brownian Motion’.
 We can model what Brown may have seen by simulating a two dimensional Brownian Motion. Executing the following code in R will produce a chart as if we had recorded the location of the pollen particle every minute (or some other arbitrary time interval) and connected the points in sequence. 
 "	 0 Comments
Visualizing droughts with R	https://www.r-bloggers.com/2010/03/visualizing-droughts-with-r/	March 15, 2010	David Smith	Physicist and weather scientist Joe Wheatley used R to design and create a useful visual representation of how drought affects a region over long time-scales. Instead of charting absolute rainfall (or lack thereof), he instead charts the Standardized Precipitation Index (SPI), where extreme values (above 2 or below -2) indicate extreme wetness or dryness compared to the usual precipitation in that region. He then charts the SPI over time using colors to indicate extreme SPI values: red means dry, blue means wet. Here’s the chart for the Murray Darling basin, an important agricultural zone in Australia (click to enlarge):  That region suffered a drought from 2002-2007, and the chart makes plain that it was the most severe drought event in 60 years. The R code used to normalize the data (by transforming to a shifted Gamma distribution) and create the chart can be found here. Biospherica Earth Vegetation: Visualizing Drought (via @josephwheatley) 	 0 Comments
Weighting model fit with ctree in party	https://www.r-bloggers.com/2010/03/weighting-model-fit-with-ctree-in-party/	March 15, 2010	heuristicandrew		 0 Comments
The Price of Calculation	https://www.r-bloggers.com/2010/03/the-price-of-calculation/	March 15, 2010	John Myles White	"
In a world in which the price of calculation continues to decrease rapidly, but the price of theorem proving continues to hold steady or increase, elementary economics indicates that we ought to spend a larger and larger fraction of our time on calculation.1
 Over the next ten years, I hope that more and more mathematically minded hackers, empowered by open source tools like the R programming language and emboldened by the popularization of statistical analyses by people like Steve Levitt, will follow Tukey’s suggestion. "	 0 Comments
Example 7.27: probability question reconsidered	https://www.r-bloggers.com/2010/03/example-7-27-probability-question-reconsidered/	March 15, 2010	Ken Kleinman		 0 Comments
R Tutorial Series: R Beginner’s Guide and R Bloggers Updates	https://www.r-bloggers.com/2010/03/r-tutorial-series-r-beginners-guide-and-r-bloggers-updates/	March 15, 2010	John Quick	1/1/2011 Update: Tal Galili wrote an article that revisits the first year of R-Bloggers and this post was listed as one of the top 14. Therefore, I decided to make a small update to each section. I start by describing the initial series of tutorials that I wrote. A few more have been added since and even more planned in the upcoming year. As always, an up to date listing of my articles can be found on the R Tutorial Series blog. New posts will also continue to be offered through the R Bloggers network. Since October 2009, I have written 13 articles [many more now, of course] for the R Tutorial Series blog. The first two introduce new users to R. The remaining 11 cover a wide range of topics related to multiple regression and correlation. This collection of tutorials represents my most recent training in statistics. Thus, for the time being, I will not be contributing new articles as frequently as I have over the past few months. However, I will undoubtedly encounter future projects that require new statistical methods and partake in more statistics courses, both of which will provide additional tutorial material. Below is a categorized list of the articles currently offered in the R Tutorial Series. Introduction to R Descriptive Statistics Data Visualization Correlation Regression I also have two additional R-related items to update you on. The first is the R Bloggers website and the second is my R Beginner’s Guide. 1/1/2011 Update: I originally reported that 50 blogs composed the R Bloggers network. Now that number has risen to over 140. I hope that R Bloggers continues to thrive and contribute to the R community. R Bloggers (https://www.r-bloggers.com) is a website that aggregates over 50 different blogs that focus on R. It is an excellent resource for keeping up to date on the many uses of R and for learning about the wide range of work being conducted in R. I recommend using R Bloggers for these purposes. The R Tutorial Series was invited to participate in the R Bloggers collection and is now available to R Bloggers’ readers. 11/1/2010 Update: Statistical Analysis with R is now available! Lastly, I want to let you know that I am working on a beginner’s guide for R. It is primarily focused towards introducing R to information technology, business, and data analyst professionals. The book will be offered through PACKT Publishing (http://www.packtpub.com) and should be available within the next year. If you have enjoyed the R Tutorial Series, then you may be interested in looking for the guide once it is completed. In the meantime, keep reading the R Tutorial Series and R Bloggers and I will keep you updated on the book’s major milestones. 	 0 Comments
t-walk on the banana side	https://www.r-bloggers.com/2010/03/t-walk-on-the-banana-side/	March 14, 2010	xi'an	Following my remarks on the t-walk algorithm in the recent A General Purpose Sampling Algorithm for Continuous Distributions, published by Christen and Fox in Bayesian Analysis that acts like a general purpose MCMC algorithm, Darren Wraith tested it on the generic (10 dimension) banana target we used in the cosmology paper. Here is an output from his comparison R program: The use of the t-walk algorithm (left, with the same number of particles) in this very special case thus produces a wider variability  on the estimated means than the use of adaptive MCMC (center) and our (tuned) PMC algorithm (right). 	 0 Comments
\pi day!	https://www.r-bloggers.com/2010/03/pi-day/	March 14, 2010	M. Parzakonis	"It’s π-day today so we gonna have a little fun today with Buffon’s needle and of course R. A well known approximation to the value of $latex \pi$ is the experiment tha Buffon performed using a needle of length,$latex l$. What I do in the next is only to copy from the following file the function estPi and to use an ergodic sample plot… Lame,huh? So, an estimate would be…
 Ok, not that great but for the whole scene it’s remarkable good! Now, we set some increasing sample sizes to account for the estimation. Which is the best estimate? 
source : [Chiara Sabatti , pdf] Take a look @ + Wiki + An introduction to geometrical probability:  distributional aspects with applications (A. M. Mathai) "	 0 Comments
Google spreadsheets + google forms + R = Easily collecting and importing data for analysis	https://www.r-bloggers.com/2010/03/google-spreadsheets-google-forms-r-easily-collecting-and-importing-data-for-analysis/	March 13, 2010	Tal Galili	"Someone on the R mailing list (link) asked: how can you easily (daily) collect data from many people into a spreadsheet and then analyse it using R. The answer people gave to it where on various ways of using excel.  But excel files (at least for now),  are not “on the cloud”.  A better answer might be to create a google form that will update a google spreadsheet that will then be read by R. If my last sentence wasn’t clear to you, then this post is for you.  First view the following video (2:50 min) which explains what google spreadsheets are  Then view the next video (2 min), which explains what using google forms with google spreadsheet is all about  The next step for you is to access the spreadsheet without having to make it public is to go thorough this tutorial on how to use the R package RGoogleDocs. If you feel comfortable to publish the google spreadsheet, there is a simpler tutorial for you here.  And you could see examples of using this method here and here. Update. Farrel Buchinsky wrote (in the mailing list) the following: 
There is also a package called RGoogleData that can import data from Google
Spreadsheets (even when they are not public). It has one benefit over
RGoogleDocs; RGoogleData can download a spreadsheet as a csv file to your
harddrive. You can then read the csv file into R – read.csv(). Those steps can
easily be written into a script so that you do not have to manually futz with
them each time. The benefit of the csv download is that it is very fast. Both
the RGoogleDocs and the RGoogleData are slow in reading the spreadsheet into R.
I understand, but may be wrong, that Google Stores all the data as HTML and its
API spits it out as xml which these r packages then need to parse out line by
line. The csv treatment by comparison is instantaneous. Only one big problem. As of a few months ago, RGoogleData has stopped working
for me. I think it is still working for its writer, Adrian Dragulescu. Adrian
has been very generous with his time in trying to help me. He predicts that he
will go back to exploring his package once Google updates their API. Try it out. I would love to hear what happens. http://r-forge.r-project.org/projects/rgoogledata/ Do you have any good example for using R with google spreadsheets? please share with us in the comments… "	 0 Comments
Rosetta language popularity	https://www.r-bloggers.com/2010/03/rosetta-language-popularity/	March 13, 2010	Karsten W.	Rosetta Code is a community wiki which presents how to solve various programming tasks by different programming languages. Thus, it serves as a dictionary between programming languages, but also as cookbook of programming recipes for a specific language. One unsolved (until today) programming task for R was to rank languages by popularity. I worked on it using the RJSONIO package from Omegahat and the Mediawiki API. Here I explain the code step by step: First, let us look up the languages which are defined at Rosetta Code. The wiki has a category for solutions by programming languages, which we will use. Now for each programming language, there is a category of the users of the language. We iterate over all languages and count the category members. Now we can print out the top 15 languages: It is very straightforward to work with the Mediawiki API, and it offers many other different features. It would be nice to have a S3 class that does all the URL encoding. There is already a project wikirobot on R-forge, but I did not look into it yet. 	 0 Comments
Rcpp 0.7.9	https://www.r-bloggers.com/2010/03/rcpp-0-7-9/	March 12, 2010	Thinking inside the box	"
So a quick bug-fix release 0.7.9 is now 
in Debian and should be on CRAN shortly.
  
 

The full NEWS entry for this release follows:
 
As always, even fuller details are in 
Rcpp Changelog page and the 
Rcpp page which also
leads to the downloads, the 
browseable
doxygen docs and zip files of doxygen output for the standard formats.
A local directory  has
source and documentation too. 
Questions, comments etc should go to the
rcpp-devel mailing list
off the R-Forge page
 Update: First version number corrected to 0.7.8. "	 0 Comments
Wrong puzzle of the week [w10]?!	https://www.r-bloggers.com/2010/03/wrong-puzzle-of-the-week-w10/	March 12, 2010	xi'an	In the weekend supplement to Le Monde, the solution of the rectangle puzzle is given as 32 black squares. I am thus… puzzled!, since my R program there provides a 34 square solution. Am I missing a hidden rectangle in the above?! Given that the solution in Le Monde is not based on a precise mathematical argument (something to do with graph theory???), it may be that the authors of this puzzle got their reasoning wrong… (The case of the parallelogram is clearer, the argument being that an horizontal distance between two black squares can only occur once.) An open problem (?) is then to figure out a formula for the number of black squares on an nxn grid without any rectangle. (I wonder how this is linked with the friendly queen puzzle set by Gauß…) 	 0 Comments
R workshops in Vancouver	https://www.r-bloggers.com/2010/03/r-workshops-in-vancouver/	March 12, 2010	David Smith	Isabella Ghement of the Ghement Statistical Consulting Company is presenting two R workshops in Vancouver, British Columbia:  “An Introduction to the Statistical Software Package R” (April 15-16,2010; 8:30am-4:30pm; BCIT; Vancouver) “Advanced Statistical Modeling Using the Statistical Software Package R” (May 20-21, 2010; 8:30am-4:30pm; BCIT; Vancouver) Click the links for more information and registration details.  	 0 Comments
Modified Donchian Band Trend Follower using R, Quantmod, TTR	https://www.r-bloggers.com/2010/03/modified-donchian-band-trend-follower-using-r-quantmod-ttr/	March 12, 2010	Intelligent Trading		 0 Comments
Package Update Roundup: Feb 2010	https://www.r-bloggers.com/2010/03/package-update-roundup-feb-2010/	March 12, 2010	David Smith	This is a list of new or updated packages that were released for R in February, as announced on the r-packages mailing list. To include other updates on this list, please email David Smith. For a complete list of all updates on CRAN, see the CRANberries archive for February 2010. Follow package name links for ratings and other information on crantastic.org. ggplot2, the graphics system based on the Grammar of Graphics, has been updated with a new transform and several bug fixes. GWRM, a new package for fitting Generalized Waring Regression Models, has been released. interval, a package that calculates weighted logrank tests and nonparametric maximum likelihood estimates for interval censored data, has been updated to improve performance. mboost, a package for model-based boosting, has been updated with many new features, including new families available for ordinal, expectile and censored regression.  np, a package implementing nonparametric kernel smoothing methods for mixed data types, has been updated with new nonparametric entropy-based testing methods. RFLPtools, a new package providing analysis functions for DNA fragment molecular weights and nucleotide sequence similarities, has been released. rms, the Regression Modeling Strategies package, has been updated to add several new capabilities. solaR, a new package with calculation methods of solar radiation and performance of photovoltaic systems, has been released. survey, a package for the analysis of complex survey samples, has been updated to allow database-backed designs can now use replicate weights and to add some multivariate statistics. tsne, a new package implementing the T-distributed Stochastic Neighbor Embedding algorithm, has been released. 	 0 Comments
Guess the Random Seed	https://www.r-bloggers.com/2010/03/guess-the-random-seed/	March 12, 2010	Yihui Xie	Stephanie asked in 511 today if we were able to get the random seed which was set by set.seed() but we were only given the random numbers (without knowing the seed). This kind of “hacker” questions sound interesting. One dirty solution should be the brute-force method, e.g: Of course it is not a good solution, although it might work: Beside the probably very long computation time, the other disadvantage is we need to know the range of the seed. This could be obtained by some “prior” information. For instance, we may guess Dr Nettleton will not set a random seed greater than one million or less than 0. Usually the random seed is obtained from a certain state of the computer such as the system time. I have an old example in this post: Random Number Generation on the Glasses (A Natural RNG). 	 0 Comments
Because it’s Friday: 3d Mandelbrot	https://www.r-bloggers.com/2010/03/because-its-friday-3d-mandelbrot/	March 12, 2010	David Smith	To while away your Friday afternoon, why not explore the nooks and crannies of a 3-D version of the Mandelbrot set:   And here’s how to create your own 2-D Mandelbrot set in R.   	 0 Comments
Happy Anniversary NYC R Meetup	https://www.r-bloggers.com/2010/03/happy-anniversary-nyc-r-meetup/	March 12, 2010	Drew Conway	"Today is the one year anniversary of the NYC R Statistical Meetup.  Starting as a small group meeting in a crowded conference room, Josh Reich established the meetup as the premiere gathering of data geeks in the tri-state area.  Over the past year we have had 10 meetups, and two more upcoming on the calendar, covering a wonderfully varied set of topics on the application of R. All of this activity has also generated a large set of data, and what better way to celebrate anniversary of our group than to analyze that data using R? 
 First, I downloaded all of the meetup descriptions into a text file (yes, I copied and pasted them) and created a word cloud. This was actually very easy in R, taking only about 40 lines of code.  There is probably a better way to specify the X and Y coordinates in order to minimize word overlap, but after some trial and error runif gives decent results.  The above cloud actually provides interesting insight into our group.  As you can see, we are focused on analysis using specific R packages, particularly with respect to graphics.  Also, the Conways remain a dominate force in all things data and R. Meetup.com provides organizers with lots of data related to a group’s activity.  They provide information on the growth of the group, as well as the number of people joining and RSVPing events. Below is a combined chart of this data. The data was generated by Meetup.com, so here the trick was getting everything into a single data frame and dealing with missing data and formats. UPDATE: I tweaked the chart a bit to show the data more clearly, as well as have the true number of members along with the total number of “active” members.  For some reason the data provided on group size does not start until mid-August 2009, so I have used the number of people joining the group before then as an estimate.  Note that the total number of members has been divided by 10 for scaling.  I am not completely satisfied with this graphic, but it does provide a good illustration of the cyclical nature of member activity and the constant rate of growth for our group.  Also, it is nice to see the number of RSVPs increasing—perhaps we are writing better descriptions. The code and data used to generate these charts can be downloaded in the ZIA Code Repository.  As always, I welcome other analyses and suggestions.   "	 0 Comments
RcppArmadillo 0.1.0	https://www.r-bloggers.com/2010/03/rcpparmadillo-0-1-0/	March 11, 2010	Thinking inside the box	"
Romain and I already had
an example of a simple but fast linear model fit using the (very clever)
Armadillo C++ library by Conrad Sanderson.
In fact, I had used this as a motivational example of why
Rcpp 
rocks in a
recent talk to the ACM chapter at U of Chicago which, 
thanks to David Smith at REvo,
got some further exposure.

 
Now this example is more refined as further glue got added. Given
that both Armadillo and 
Rcpp make use of
C++ templates, the actual amount of code in RcppArmadillo is not that large:
just over 200 lines in a header file, and a little less for some testing accessor
and example functions in a source file.  And this makes for some really nice
example code: the ‘fast regression’ example becomes this (where I simply
removed two blocks with conditional on the Armadillo version):

 
 
No extra copies!  Armadillo instantiates directly from the underlying R
objects for the vector and matrix, solves the regression equations, computes
the standard error of the estimates and returns the two vectors. Leaving us
to write about eleven lines of code.  Moreover, as Armadillo is well designed
and uses template meta-programming to avoid extra copies (see these lecture
notes for details), it is about as efficient as it can be (and will use
Atlas or other BLAS where available).

 
And, this is just one example. Rcpp should be
suitable for other C++ libraries, and provides an easy to use seamless
interface between C++ and R.

 
However, we should note that (at about the last minute) we found out about some unit test failures in OS
X as well as some issues in a Debian chroot —
cran2deb ran into some build
issues on i386 and amd64 in the testing chroot even this ‘it all works’
swimmingly on our Debian, Ubuntu and Fedora build environments.  A follow-up
with fixes for either Rcpp and/or RcppArmadillo appears likely.

 
Update: The build issues seems to be with 64-bit systems and everything appears cool in 32-bit.
 "	 0 Comments
Variations in the literal representation of Pi	https://www.r-bloggers.com/2010/03/variations-in-the-literal-representation-of-pi/	March 11, 2010	Derek-Jones	The numbers system I am developing attempts to match numeric literals contained in a file against a database of interesting numbers.  One of the things I did to quickly build a reasonably sized database of reliable values was to extract numeric literals from a few well known programs that I thought I could trust. R is a widely used statistical package and Maxima is a computer algebra system with a long history.  Both contain a great deal of functionality and are actively maintained. To my surprise the source code of both packages contain a large variety of different literal values for , or to be exact the number of digits contained in the literals varied by more than I expected.  In the following table the value to the left of the  representation is the number of occurrences; values listed in increasing literal order: The comments in the Maxima source led me to believe that some thought had gone into ensuring that the numerical routines were robust.  Over 3/4 of the literal representations of  have a precision comparable to at least that of 64-bit floating-point  (I’m assuming an IEEE 754 representation in this post). In the R source approximately 2/3 of the literal representations of  have a precision comparable to that of 32-bit floating-point. Closer examination of the source suggests one reason for this difference.  Both packages make heavy use of existing code (translated from Fortran to Lisp for Maxima and from Fortran to C for R); using existing code makes good sense and because of its use in scientific and engineering applications many numerical libraries have been written in Fortran.  Maxima has adapted the slatec library, whereas the R developers have used a variety of different libraries (e.g., specfun). How important is variation in the representation of Pi? The obvious solution to this representation issue of creating a file containing definitions of all of the frequently used literal values has possible drawbacks.  For instance, numerical accuracy is a strange beast and increasing the precision of one literal without doing the same for other literals appearing in a calculation can sometimes reduce the accuracy of the final result. Pulling together existing libraries to build a package is often very cost effective, but numerical accuracy is a slippery beast and this inconsistent usage of literals suggests that developers from these two communities have not addressed the system level consequences of software reuse. Update 6 April: After further rummaging around in the R source distribution I found that things are not as bad as they first appear.  Only two of the single precision instances of  listed above occur in the C or Fortran source code, the rest appear in support files (e.g., m4 scripts and R examples).   	 0 Comments
t-walk on the wild side	https://www.r-bloggers.com/2010/03/t-walk-on-the-wild-side/	March 11, 2010	xi'an	When I read in the abstract of the recent A General Purpose Sampling Algorithm for Continuous Distributions, published by Christen and Fox in Bayesian Analysis that We develop a new general purpose MCMC sampler for arbitrary continuous distributions that requires no tuning. I am slightly bemused. The proposal of the authors is certainly interesting and widely applicable but to cover arbitrary distributions in arbitrary dimensions with no tuning and great performances sounds too much like marketing on steroids! The 101 Theorem in MCMC methods is that, no matter how good your sampler is, there exists an exotic distribution out there whose only purpose is to make it crash! The algorithm in A General Purpose Sampling Algorithm for Continuous Distributions is based on two dual and coupled chains which are used towards a double target . Given that only one of the two chains moves at each iteration, according to a random walk, there is a calibration parameter that definitely influences the performances of the method, if not the acceptance probability. This multiple chain approach is reminding me  both of coupled schemes developed by Gareth Roberts in the late 1990′s, along with Laird Breyer, in the wake of the perfect sampling “revolution” and of delayed rejection sampling, as proposed by Antonietta Mira in those years as well. However, there is no particular result in the paper showing an improvement in convergence time over more traditional samplers. (In fact, the random walk nature of the algorithm strongly suggests a lack of uniform ergodicity.) The paper only offers a comparison with an older optimal scaled random walk proposal of Roberts and Rosenthal (Statistical Science, 2001). Rather than with the more recent and effective adaptive Metropolis-Hastings algorithm developed by the same authors. Since the authors developed a complete set of computer packages, including one in R, I figure people will start to test the method to check for possible improvement over the existing solutions. If the t-walk is indeed superior sui generis, we should hear more about it in the near future… 	 0 Comments
Clinical Reporting with R	https://www.r-bloggers.com/2010/03/clinical-reporting-with-r/	March 11, 2010	David Smith	One of the main goals of analyzing clinical data is to produce a report. (What, you thought it was to make the world a better place?) The R Project has, of course, all the tools you need to perform the statistical analysis, calculate the tables of results, and present conclusions graphically. But how can you assemble all of that into a report that someone can, you know, read? You could go the cut-and-paste route: write the text in Word, export the data from R to format the tables in Excel, dress up the saved charts in Photoshop. But that’s a complex, manual process, and manual processes can introduce errors. Worse yet, if the data ever changes, you’ve got to go through the whole process again to update the report. That means no interim reports, and conversely, a big barrier to correcting the data and the report after it’s published. Vanderbilt Biostatistics professor Frank Harrell has a different solution: the rreport package for R. (See an overview slide deck here.) It’s designed to produce statistical reports for clinical trials, and is especially useful for producing interim reports for data monitoring committees (DMCs). You can use it to create a complete report document, fully automating the process of generating tables from your R analyses like this:  and integrating R graphics into the document, like this:   The rreport system is an example of literate programming: the tables and reports are interwoven into the narrative text in a single source document in the open-source LaTeX typesetting language, and the entire report can be redone, with all results recalculated and all charts regenerated from the source data, in a single step. Using LaTeX does take some getting used to — it’s not a WYSIWYG environment like Word — but does make for very attractive reports and the ability to easily typeset the Greek-laden mathematical equations so prevalent in clinical trial reports. You can download rreport from the Vanderbilt CVS repository at the link below. Department of Biostatistics, Vanderbilt University: rreport package  	 0 Comments
Importing Raster Files in NetLogo without the GIS Extension	https://www.r-bloggers.com/2010/03/importing-raster-files-in-netlogo-without-the-gis-extension/	March 11, 2010	E.Crema		 0 Comments
RcppExamples 0.1.0	https://www.r-bloggers.com/2010/03/rcppexamples-0-1-0/	March 10, 2010	Thinking inside the box	"
As mentioned in the
post about release 0.7.8 of Rcpp, 
Romain and I carved this
out of Rcpp 
itself to provide a cleaner separation of code that implements our
R / C++ interfaces (which remain in 
Rcpp)
and code that illustrates how to use it — which is now in RcppExamples.
This also provides an easier template for people wanting to use 
Rcpp 
in their packages as it will be easier to wrap one’s head around the much
smaller RcppExamples package.

 
A simple example (using the newer API) may illustrate this:

 
With essentially five lines of code, we provide a function that takes any
numeric vector and returns both the original vector and a tranformed
version—here by applying a square root operation.  Even the looping along
the vector is implicit thanks to the generic programming idioms of the
Standard Template Library.

 
Nicer still, even on misuse, exceptions get caught cleanly and we get
returned to the R prompt without any explicit coding  on the part of
the user:

 
 
There is also analogous code for the older API in the package, but it is
about three times as long, has to loop over the vector and needs to
set up the execption handling explicitly.

 
As of right now, RcppExamples does not document every class but it should already
provide a fairly decent start for using Rcpp. And many more actual usage
examples are … in the over two-hundred unit 
tests in Rcpp.


 
Update: Now actually showing new rather than classic API.
 "	 0 Comments
Puzzle of the week [w10]	https://www.r-bloggers.com/2010/03/puzzle-of-the-week-w10/	March 10, 2010	xi'an	 The puzzle in last Saturday edition of Le Monde is made of two parts: Given a 10×10 grid, what is the maximum number of nodes one can highlight before creating a parallelogram with one side parallel to one of the axes of the grid? What is the maximum number of nodes one can highlight before creating a rectangle? Given that I was reasonably busy in this intermission week betwen two meetings, I opted for a computer-based solution. My R programs for solving the parallelogram and the rectangle problems are there and there, respectively. They are both based on random hits on the grid that are accepted or rejected depending on whether or not they satisfy the constraint. Obviously simulation only gives a lower bound on the solution, but the parallelogram  program there provides 19 as an answer, which seems to be the correct one. The rectangle R program there produces 34 over iterations but there is no confidence that this is the right number. 	 0 Comments
Stata Fail	https://www.r-bloggers.com/2010/03/stata-fail/	March 10, 2010	jackman	From a recent mailing from Stata (highlighting by me):  Funnily enough, there is a Daniel Rubin, a bio-informatics person here at Stanford. 	 0 Comments
In a nls star things might be different than the lm planet…	https://www.r-bloggers.com/2010/03/in-a-nls-star-things-might-be-different-than-the-lm-planet%e2%80%a6/	March 10, 2010	M. Parzakonis	The nls() function has a well documented (and discussed) different behavior compared to the lm()’s. Specifically you can’t just put an indexed column from a data frame as an input or output of the model. The following will work, when we assign things as vectors. 	 0 Comments
Clustering the world’s diets	https://www.r-bloggers.com/2010/03/clustering-the-worlds-diets/	March 10, 2010	David Smith	Cluster Analysis is a useful technique for classifying the members of a group (people, events, measurements, etc) into “similar” groups. How “similar” is defined depends on the application, but generally involves looking at a number of attributes of the group. For example, we could cluster people by looking at their skin color, hair type, facial features, perhaps even genetic markers and find that we end up with clusters that are somehow associated with ethnicity.  Here’s a fascinating application of cluster analysis: given data on what the citizens of each country eat (on aggregate), can we cluster the countries of the world into groups with similar diets? That’s what Diego Valle did, using the pam (partitioning around medioids) function in R. He presents the six clusters he identifies as a color-coded world map (click to enlarge):  Australia gets grouped with North America and much of Europe and Russia as countries whose citizens enjoy a high-calorie diet with all kinds of foods (except not many beans). Countries in yellow have a cereal-rich diet. The diets of the south-east Asian cluster are heavy on fish and rice, but not dairy foods. See Diego’s blog for the description of the other clusters, and the R code which created the analysis. The code reads the data directly from a Google Spreadsheet in the cloud, so you can easily run it yourself. It also produces an interesting chart comparing the American diet to that of the rest of the world. Diego Valle’s Food & Fishing Blog: Cluster Analysis of What The World Eats 	 0 Comments
Strategy: what if SPY & VIX are up?	https://www.r-bloggers.com/2010/03/strategy-what-if-spy-vix-are-up/	March 10, 2010	kafka	"Recently, I was busy testing the following strategy: If SPY and VIX daily returns are positive, then short SPY at close and keep it for one day. The strategy is dump simple and it has very good feature – short side. There are not so many successful short side strategies. For testing purpose I used daily Yahoo data from 1995 until present. For commissions and bid/ask spread I used 5 $ fee per 10 000 $ trade. Here we go:  Annualized Return                     0.0421
Annualized Std Dev                   0.0488
Annualized Sharpe (Rf=0%)        0.8621
t = 3.3787, df = 3811, p-value = 0.0007356 Up to this point is was relatively easy to make a test (the true is, that I spent some time cracking and hacking blotter package, but I will write about it in separate post). My second objective was the improvement of this strategy. One of the way to understand the strategy is to look how the components are related to each other or correlated. To do that, I took daily returns of SPY and VIX at day 0 and plotted against SPY next day’s (day+1) returns.  What can I tell by looking at this plot? I couldn’t figure out any linear relation between returns of SPY and VIX at day+0 and returns of SPY at day+1. Should I try something like random forest method? I tried to add some TA flavors, like RSI, but the improvements were not very significant. Simplicity is genius! "	 0 Comments
Using Regular Expressions in R: Case Study in Cleaning a BibTeX Database	https://www.r-bloggers.com/2010/03/using-regular-expressions-in-r-case-study-in-cleaning-a-bibtex-database/	March 9, 2010	Jeromy Anglim		 0 Comments
Rcpp 0.7.8	https://www.r-bloggers.com/2010/03/rcpp-0-7-8/	March 9, 2010	Thinking inside the box	"
This is a minor feature release based on a over three weeks of changes that
are summarised below in the extract from the NEWS file.  Some noteworthy
highlights are
 

The full NEWS entry for this release follows:
 
As always, even fuller details are in the ChangeLog on the
Rcpp page which also
leads to the downloads, the 
browseable
doxygen docs and zip files of doxygen output for the standard formats.
A local directory  has
source and documentation too. 
Questions, comments etc should go to the
rcpp-devel mailing list
off the R-Forge page
 Update: Two links corrected. "	 0 Comments
principal components and image reconstruction	https://www.r-bloggers.com/2010/03/principal-components-and-image-reconstruction/	March 9, 2010	jackman	Jeff Lewis at UCLA told me he teaches principal components with an image reconstruction example.  This got me inspired to try it myself. A snapshot appears below, showing how the image quality improves quickly with a relatively small number of principal components.  A full, Sweaved write up is here, making use of the biOps package in R.  	 0 Comments
Learning R by video	https://www.r-bloggers.com/2010/03/learning-r-by-video/	March 9, 2010	Rob J Hyndman	For those people who prefer to be shown how to do something rather than read the instructions, there are some videos on using R available online. Here are the ones I know about. Please add links to other similar resources in the comments.   	 0 Comments
Introducing R on video	https://www.r-bloggers.com/2010/03/introducing-r-on-video/	March 9, 2010	xi'an	Darren Wraith pointed out to me this site proposing a whole series of videos introducing to R. (Unfortunately in a Windows environment.) This can be handy when facing students with no R background… 	 0 Comments
Getting the basics from readAligned	https://www.r-bloggers.com/2010/03/getting-the-basics-from-readaligned/	March 9, 2010	Jeremy Leipzig		 0 Comments
Cluster analysis of what the world eats	https://www.r-bloggers.com/2010/03/cluster-analysis-of-what-the-world-eats/	March 9, 2010	Diego Valle-Jones		 0 Comments
Open Source is Opening Data to Predictive Analytics	https://www.r-bloggers.com/2010/03/open-source-is-opening-data-to-predictive-analytics/	March 9, 2010	David Smith	This article by REvolution Computing CEO Norman Nie is crossposted from the Future of Open Source Forum. The R Project: despite there being over 2 million users of this open-source language for statistical data analysis, you might not have heard of it … yet. You might have seen this feature in the New York Times last year, and you might have heard how REvolution Computing is enhancing and supporting R for commercial use. Because what was once a secret of drug-development statisticians at pharmaceutical companies, quants on Wall Street, and PhD-level statistical researchers around the globe (not to mention pioneers at Web 2.0 companies like Google and Facebook) is suddenly becoming mainstream. The reason? The perfect storm of a deluge of data, open-source technology, and the rise of predictive analytics. Predictive analytics — the process of being able to infer meaningful relationships and predictions from vast quantities of data — is disrupting industries in every sector. You’ve probably seen the impact of predictive analytics yourself: ever been surprised by Amazon apparently “reading your mind” on a suggested purchase, or by LinkedIn being able to figure out who you know, but aren’t yet connected with? That’s predictive analytics in action. By applying advanced statistical models to data, product designers, marketers, sales organizations — basically, anyone who needs to understand the present or predict the future — are able to draw value from the data they’ve collected like never before. Predictive analytics are only possible with data — lots of data. Just last week, the Economist published a nine-part special report on the Data Deluge. Companies like Nestlé and Walmart are collecting reams of data on individual products and consumers. And given that Nestlé (to take just one example) has more than 100,000 products in 200 countries, we’re talking about huge amounts of data being collected. The world has largely solved the problem of how to collect and store these vast quantities of data — see David McFarlane’s post for a great review of the impact of FOSS here. But the real impact of analyzing these data sets is only just now being felt routinely. It truly is a revolution: the information that can be teased out of these data is shaking many industries to their core. This quote from the Economist special report sums it up well: “Revolutions in science have often been preceded by revolutions in measurement,” says Sinan Aral, a business professor at New York University. Just as the microscope transformed biology by exposing germs, and the electron microscope changed physics, all these data are turning the social sciences upside down. Open Source software is playing a key role in this revolution. A noted analyst recently wrote that the most important factor influencing the spread of predictive analytics is the growing popularity of R. And in the Economist’s special report, the combination of R and Hadoop received special attention: A free programming language called R lets companies examine and present big data sets, and free software called Hadoop now allows ordinary PCs to analyse huge quantities of data that previously required a supercomputer. It does this by parcelling out the tasks to numerous computers at once. This saves time and money. For example, the New York Times a few years ago used cloud computing and Hadoop to convert over 400,000 scanned images from its archives, from 1851 to 1922. By harnessing the power of hundreds of computers, it was able to do the job in 36 hours. This revolution fills me with some pride: I started pushing for broad adoption of data analytics as a crucial element in every aspect of science and business decision-making some 40 years ago, when I created SPSS (now part of IBM). The revolution began in scientific practice and now open source R (co-created by REvolution board member Robert Gentleman) represents its future. Today, all of the Fortune 500 companies use R for their data analyses. It’s used in life sciences, financial services, defense technology and other large industries requiring high performance analytical computation.  In the coming months and years, I predict that open-source software will continue to be the driving force in analytical innovation. Open-source platforms like Hadoop, coupled with innovations in open-source file-systems, are able to adapt to the rapidly-evolving data storage and processing requirements. And it’s open-source environments like R, with its world-wide community of researchers collaborating to push the boundaries of statistical analytics, that are most likely provide the novel predictive techniques required to tease yet more accurate predictions from these huge information-age datasets. Tie that with the backing of a commercial company to provide the scalability, usability, and integration into Web-based systems that businesses require to deploy predictive analytics, and you’ve truly got a REvolution in the making. 	 0 Comments
Chinese versus Japanese editions	https://www.r-bloggers.com/2010/03/chinese-versus-japanese-editions/	March 8, 2010	xi'an	Last week, I got news from Springer Verlag about possibly two new editions of my books, one in Chinese and one in Japanese. These were bad news and good news: the bad news was that the Chinese edition was actually a reprint of our original book,  Monte Carlo Statistical Method, by a Chinese publishing company. Supposedly restricted to the Chinese interior market. While this agreement is within the terms of our contract, it will be disastrous for our sales of the original 2004 Springer edition since those cheaper copies have already found their way to American and European markets (I got a copy by the mail only today, but some students in the US do have it!) I actually fail to understand the publisher’s point in giving away sales of a reasonably successful book for a cheaper version with a much lower return. Since this Chinese publisher is also (re)printing Hastie, Tibshirani and Freedman’ The Elements of Statistical Learning: Data Mining, Inference, and Prediction, as well as Erich Lehmann’s Theory of Point Estimation, we are not an isolated case. But this does not make the move less frustrating or more understandable! The good news is about the potential translation of our book Introducing Monte Carlo Methods with R into Japanese, as was recently the case for the book of Phil Spector on Data Manipulation with R. There seems to be a reasonable market for R (and Splus) books in Japan for those translations to take place… 	 0 Comments
White House taps Edward Tufte to explain the stimulus	https://www.r-bloggers.com/2010/03/white-house-taps-edward-tufte-to-explain-the-stimulus/	March 8, 2010	David Smith	Edward Tufte, a pioneer of effective data visualization (and a personal hero) has just been appointed by the White House to the Recovery Independent Advisory Panel. This panel advises The Recovery Accountability and Transparency Board, whose job is to track and explain $787 billion in recovery stimulus funds. Tufte explains: I’m doing this because I like accountability and transparency, and I believe in public service. And it is the complete opposite of everything else I do. Maybe I’ll learn something. The practical consequence is that I will probably go to Washington several days each month, in addition to whatever homework and phone meetings are necessary. This is a great move — while the effects of the stimulus can be debated (and have been ad nauseam), there’s no question that the Administration has had trouble explaining the facts amidst all the noise. It’s possible that Tufte’s had some behind-the-scenes influence already: as Fast Company points out this recent chart has been more successful than many recent attempts to explain the benefits of the stimulus.    Sure, using changes in unemployment instead of absolute numbers certainly belies an agenda, but at least it is clear, meaningful, and based on facts.     	 0 Comments
Weird dietary habits in the US	https://www.r-bloggers.com/2010/03/weird-dietary-habits-in-the-us/	March 8, 2010	Diego Valle-Jones		 0 Comments
Chilean earthquake: impact of the tsunami	https://www.r-bloggers.com/2010/03/chilean-earthquake-impact-of-the-tsunami/	March 8, 2010	David Smith	The National Oceanic and Atmospheric Administration (NOAA) has a page with some interesting information about last week’s earthquake in Chile, but what really stood out for me was this chart of the predicted wave heights around the globe resulting from the associated tsunami:   Click to enlarge: it’s a fascinating chart. Although labelled a forecast, from the explanations on the page it appears to be based on observed wave heights at various monitoring stations (with model-based interpolations between them, I assume). This really was a hemispheric event, with impacts around the entire Pacific basin. Clearly though, the impact on the Chilean coast was extreme. Unfortunately NOAA doesn’t have a similar chart for the devastating Boxing Day 2004 tsunami; it would be interesting to compare them. (By the way, although you can easily create charts like this in R, I’m not sure whether R was used for this one or not.) West Coast/Alaska Tsunami Warning Center, NOAA/NWS: Offshore Maule, Chile Tsunami of 27 February 2010   	 0 Comments
Example 7.26: probability question	https://www.r-bloggers.com/2010/03/example-7-26-probability-question/	March 8, 2010	Ken Kleinman		 0 Comments
R: Eliminating observed values with zero variance	https://www.r-bloggers.com/2010/03/r-eliminating-observed-values-with-zero-variance/	March 8, 2010	Allan Engelhardt	"
I needed a fast way of eliminating observed values with zero variance from large data sets using the R statistical computing and analysis platform.  In other words, I want to find the columns in a data frame that has zero variance.  And as fast as possible, because my data sets are large, many, and changing fast.  The final result surprised me a little.
 
I use the KDD Cup 2009 data sets as my reference for this experiment.  (You will need to register to download the data.)  It is a realistic example of the type of customer data that I usually work with.  It has 50,000 observations of 15,000 variables.  To load it into R you’ll need a reasonably beefy machine.  My workstation has 16GB of memory; if yours have less then use a sample of the data.
 
We load the data into R and propose a few ways in which we may identify the columns we need:
 
Now we just have to load the very useful rbenchmark package and let the machine figure it out:
 
The answer (on my machine) is that it is faster to calculate than to check for equality:
 
The two functions based on the core variance function are easily the fastest (despite having to do arithmetic) while taking out the special case in the equality functions is a Bad Idea.
 
Can you think of an even faster way to do it?
 Jump to comments. 

Area Plots with Intensity Coloring
 I am not sure apeescape’s ggplot2 area plot with intensity colouring is really the best way of presenting the information, but it had me intrigued enough to replicate it using base R graphics. The key technique is to draw a gradient line which R does not support natively so we have to roll our own code for that. Unfortunately, lines(..., type=l) does not recycle the colour col= argument, so we end up with rather more loops than I thought would be necessary. We also get a nice opportunity to use the under-appreciated read.fwf function. 

Employee productivity as function of number of workers revisited
 We have a mild obsession with employee productivity and how that declines as companies get bigger. We have previously found that when you treble the number of workers, you halve their individual productivity which is mildly scary. We revisit the analysis for the FTSE-100 constituent companies and find that the relation still holds four years later and across a continent. 

A warning on the R save format
 The save() function in the R platform for statistical computing is very convenient and I suspect many of us use it a lot. But I was recently bitten by a “feature” of the format which meant I could not recover my data. I recommend that you save data in a data format (e.g. CSV or CDF), not using the save() function which is really for objects (data and code). What is your approach? 

R code for Chapter 2 of Non-Life Insurance Pricing with GLM
 We continue working our way through the examples, case studies, and exercises of what is affectionately known here as “the two bears book” (Swedish björn = bear) and more formally as Non-Life Insurance Pricing with Generalized Linear Models by Esbjörn Ohl… 

R code for Chapter 1 of Non-Life Insurance Pricing with GLM
 Insurance pricing is backwards and primitive, harking back to an era before computers. One standard (and good) textbook on the topic is Non-Life Insurance Pricing with Generalized Linear Models by Esbjorn Ohlsson and Born Johansson. We have been doing som… "	 0 Comments
InfoChimps	https://www.r-bloggers.com/2010/03/infochimps/	March 7, 2010	dylan	"
This looks interesting: http://infochimps.org/search?query=soil "	 0 Comments
ggplot and concepts — what’s right, and what’s wrong	https://www.r-bloggers.com/2010/03/ggplot-and-concepts-whats-right-and-whats-wrong/	March 7, 2010	Harlan	A few months back I gave a presentation to the NYC R Meetup. (R is a statistical programming language. If this means nothing to you, feel free to stop reading now.) The presentation was on ggplot2, a popular package for generating graphs of data and statistics. In the talk (which you can see here, including both my slides and my patter!) I presented both the really great things about ggplot2 and some of its downsides. In this blog post, I wanted to expand a bit on my thinking on ggplot, the Grammar of Graphics, and how peoples’ conceptual representations of graphs, data, ggplot, and R all interact. ggplot is both incredibly elegant and unfortunately difficult to learn to use well, I think as a consequence of the variety of representations. The ggplot package, written by the overachieving and remarkable Hadley Wickham, is based on earlier more theoretical work by Leland Wilkinson. Wilkinson abstracted the process of putting data onto an image, and created a Grammar of Graphics, which describes how the data maps to the parts of a graph, rather than describing the final graph itself. For example, here’s how to create a pie chart, clipped from Wilkinson’s book: Don’t worry about the details, but briefly, a pie chart is just a stacked bar graph (summary.proportion) plotted in polar coordinates (polar.theta). If you took the time to learn this grammar, you would realize that the hierarchical structure of a graph on a page (elements have positions and labels and visual properties like color, each of which have their own abstract structure) maps cleanly to the hierarchical structure of the grammar, and that variables in the grammar map cleanly to the linear structure of the data. As a user of this system, you would be able to see all three key representations at once: the data, the grammatical mapping from data to graph, and the graph itself. Now consider ggplot, the implementation of the Grammar of Graphics in the R programming language. Does ggplot maintain three visible representations, all straightforwardly mappable to each other? Sadly, it does not. Instead, users of ggplot must map among four representations: the data (a standard data.frame object), the R syntax for ggplot2 (which has some quirks), an underlying ggplot object (similar to the Grammar of Graphics, but vastly more complex and impossible to examine directly), and the generated graph. Consider the simple pie graph, below. This chart is generated in ggplot2 by the following R code: The print() function is optional within an R interpreter session, but I include because it illustrates a point that’s not initially obvious to many users. Unlike the built-in R plotting tools, the ggplot() function and its associated functions don’t plot anything on the screen, they just construct an object of type “ggplot”. Almost all of the actual work of mapping the data to stuff on your screen occurs when you print that object, using print() or ggsave(). So what does that object look like? If you type str(pp), you’ll get an answer, but it’s about a hundred lines of undecipherable hierarchical object and list structure, not intended to be examined by mere mortals. But there’s something critically important about that structure — like the original Grammar of Graphics, and unlike the R syntax above, it’s hierarchically structured. In the R syntax, you create a base ggplot structure with the ggplot() call, then you abuse the “+” operator to make changes to that structure. The geom_bar() function adds a layer to the ggplot() object, where a layer is just what it sounds like, a set of information about one of potentially many overlaid layers of content that will be put on the graph. So you construct a ggplot object by first initializing everything about the basic plot, then tack on layers with +, right? Actually no, because the coord_polar() call doesn’t create or modify a layer at all, it modifies the base object! Even if you’ve acquired the nonobvious intuition that ggplot objects are hierarchical and are created by concatenating layers, you now have to break the analogy again to fully understand what + is doing! There is a way to partially see the structure directly, but it’s not well thought-out from the point of view of someone trying to learn how to use the package. The summary() method on ggplot objects tells you about things you didn’t specify (faceting?), it’s incomplete, and it doesn’t map well to the R syntax. If something in your plot isn’t working the way you want it to, summary() won’t help you. Another shortcut that leads to conceptual problems by ggplot beginners is the use of qplot(). The qplot() function is a wrapper around ggplot(). Unlike ggplot(), you can give qplot() data that is not in the form of a data.frame, and the syntax is somewhat different. There’s nothing wrong with some syntactic sugar to make life easier, but in this case, learning ggplot by starting with qplot is like trying to learn a foreign language by starting with contractions and slang. You may be able to say a few essential things on your vacation, but you won’t be able to creatively construct new sentences as new situations arise. The brilliance of the Grammar of Graphics is exactly that it’s a grammar — you can construct new graphs and new types of graphs as new situations arise! But tutorials that start with qplot, with the ggplot book an unfortunate (but in other ways excellent) example, send their learners down a linguistic garden path. To fully use the power of the system requires unlearning the conceptual structures that map the slang to charts on a screen, and starting over with learning the new, more powerful ggplot() grammar and hierarchical representations. I’d like to conclude this overlong rant with two notes. First, just today a new graphics package for R was introduced. jjplot uses many of the ideas of the Grammar of Graphics and ggplot2, but seems to avoid at least a few of the conceptual problems. The + operator is not overloaded in conceptually confusing ways, and there is no distracting qplot function to mislead new users. Additionally, a quick look at the source code finds it much, much simpler than ggplot2′s source, which will likely lead to a more active base of contributors. I look forward to trying jjplot and watching its continuing development, and hope the authors learn from both the remarkable successes and frustrating failures of ggplot. Second, I use ggplot extensively in my work. It’s simply the best available tool for quickly generating elegant graphs of data in R, especially if that generation needs to happen automatically in code. Hadley Wickham deserves extensive praise for the amount of effort he has put into developing and popularizing the Grammar of Graphics. If you want to be maximally effective when visualizing data in R, take the time to learn ggplot2, but do so while keeping in mind that the learning process will be easiest if you skip qplot and other shortcuts, think hierarchically, and prepare for some frustration. Fortunately, the support communities on the ggplot mailing list and Stack Overflow are extremely helpful, as is Hadley himself. 	 0 Comments
A nice link: “Some hints for the R beginner”	https://www.r-bloggers.com/2010/03/a-nice-link-%e2%80%9csome-hints-for-the-r-beginner%e2%80%9d/	March 7, 2010	Tal Galili	"Patrick Burns just posted to the mailing list the following massage: There is now a document called “Some hints for the R beginner” whose purpose is to get people up and running with R as quickly as possible. Direct access to it is:
http://www.burns-stat.com/pages/Tutor/hints_R_begin.html JRR Tolkien wrote a story (sans hobbits) called ‘Leaf by Niggle’ that has always resonated with me.  I offer you an imperfect, incomplete tree (but my roof is intact). Suggestions for improvements are encouraged. And here is the link tree for the document (for your easy reviewing of the offered content) :  This page has several sections, they can be put into the four categories: General, Objects, Actions, Help. General
Introduction 
Blank screen syndrome 
Misconceptions because of a previous language 
Helpful computer environments 
R vocabulary 
Epilogue  Objects
Key objects 
Reading data into R 
Seeing objects 
Saving objects 
Magic functions, magic objects 
Some file types 
Packages  Actions
What happens at R startup 
Key actions 
Errors and such 
Graphics 
Vectorization 
Make mistakes on purpose   Help
How to read a help file 
Searching for functionality 
Some other documents 
R-help mailing list  "	 0 Comments
One R Tip A Day meets Tecnica Arcana	https://www.r-bloggers.com/2010/03/one-r-tip-a-day-meets-tecnica-arcana-2/	March 7, 2010	Paolo Sonego		 0 Comments
Ecological Modelling with “R”	https://www.r-bloggers.com/2010/03/ecological-modelling-with-r/	March 7, 2010	» R	"Here i present some Books and Articles about Ecological Modelling and “R”. Since “R” is integrated in Bio7 all
the presented methods in the Books and Articles can also be useful together with Bio7. Books: Ellner, Stephen P. & Guckenheimer, John (2006). Dynamic Models in Biology.
Princeton University Press Bolker B (2008) Ecological Models and Data in R. Princeton University Press. Soetaert K, Herman P (2009) A Practical Guide to Ecological Modelling. Using R as a Simulation Platform. Springer-Verlag, New York. Stevens M. Henry H. (2009). A Primer of Ecology with R.
Springer. And Chapter 26 in:
Crawley MJ (2007) The R book. Wiley, Chichester. Articles:
 Petzoldt, T. (2003). R as a Simulation Platform in Ecological Modelling. R-News 3(3), 8–16 Journal of Statistical Software: Volume 22 : Ecology and Ecological Modelling in R  "	 0 Comments
Intermarket Whac-A-Mole	https://www.r-bloggers.com/2010/03/intermarket-whac-a-mole/	March 6, 2010	Milk Trader		 0 Comments
schoolmath	https://www.r-bloggers.com/2010/03/schoolmath/	March 6, 2010	xi'an	"In connection with the Le Monde puzzle of last week, I was looking for an R function that would give me the prime factor decomposition of any integer. Such a function exists within the package schoolmath, developped by Joerg Schlarmann and Josef Wienand. It is called prime.factor and it returns the prime factors of any integer: > prime.factor(2016)
[1] 2 2 2 2 2 3 3 7
> prime.factor(2032)
[1]   2   2   2   2 127
> prime.factor(2031)
[1]   3 677
> prime.factor(2039)
2039 is a prime!
[1] 2039 "	 0 Comments
Visualizing Drought	https://www.r-bloggers.com/2010/03/visualizing-drought/	March 6, 2010	joe	"The impacts of drought depend on time-scale. On short time-scales, drought means dry soil. On long time-scales, it means dry rivers and empty reservoirs. A region may simultaneously experience dry conditions on one time-scale and wet conditions on another e.g. wet soil but low streamflow or visa versa. Standardized Precipitation Index (SPI) is a widely used measure of drought which can be defined for any time-scale of interest. For any location, SPI is normally distributed with zero mean and unit standard deviation. Index values > 2 indicate exceptionally wet conditions for that location, values < -2 indicate exceptionally dry conditions for that location, etc. Historical precipitation is the only input needed to compute SPI. Australia experienced drought between 2002 and 2007. The image below shows SPI computed for a location in the drought-prone Murray-Darling basin of New South Wales. The time-series run from Jan 1948 to Jan 2010 and the index was calculated for time-scales from 1 to 12 months. Precipitation data is from NCEP Reanalysis [1] in a 1.875° × 1.875° grid cell centred at 30°S 145°E.        The drought of 2002 to 2007 shows up very clearly. It was preceeded by a wet period between 2005 and 2001. While 2009 showed an episode of severe drought at short time-scales, SPI at was normal/wet at longer time-scales during 2009. Agricultural yields recovered.     Empirical rainfall probability distributions are far from normal (gaussian) and often approximate a shifted gamma distribution. The empirical cumulative probability distributions are used to transform the rainfall time-series into time-series of percentile probabilities. A normally distributed precipitation index is found by pretending that these percentile probabilities derive from a standard cumulative normal distribution and inverting to find the index values. This is simple in R. If the vector data contains rainfall infall data, then: fit.cdf <- ecdf(data)

cdfs <- sapply(data,fit.cdf)

SPI <- qnorm(cdfs)

 Tha rainfall data are M-month moving averages (current and previous months). A separate index is calculated for each calendar month to remove seasonality. The R code used to compute SPI values (based in NCEP Reanalysis or other data sets such as GCPC) is here. [1] The NCEP/NCAR 40-year reanalysis project, Bull. Amer. Meteor. Soc., 77,  437-470, 1996 Noted Added 11 October 2011: I have uploaded a slightly improved SPI R script here. The function getPrecOnTimescale(precipitation,k) takes a vector of monthly precipitation values and returns a k-month average (i.e current month and prior k-1 months). getSPIfromPrec(precip.k) takes k-month precipitation values and returns the corresponding vector of SPI values. "	 0 Comments
Contingency Tables – Fisher’s Exact Test	https://www.r-bloggers.com/2010/03/contingency-tables-%e2%80%93-fisher%e2%80%99s-exact-test/	March 6, 2010	Ralph	A contingency table is used in statistics to provide a tabular summary of categorical data and the cells in the table are the number of occassions that a particular combination of variables occur together in a set of data. The relationship between variables in a contingency table are often investigated using Chi-squared tests. The simplest contingency table with two variables has two levels for each of the variables. Consider a trial comparing the performance of two challengers. Each of the challengers undertook the trial eight times and the number of successful trials was recorded. The hypothesis under investigation in this experiment is that the performance of the two challengers is similar. If the first challenger was only successful on one trial and the second challenger was successful on four of the eight trials then can we discriminate between their peformance? The function fisher.test is used to perform Fisher’s exact test when the sample size is small to avoid using an approximation that is known to be unrealiable for sample samples. The data is setup in a matrix: The function is then called using this data to produce the test summary information: The p-value calculated for the test does not provide any evidence against the assumption of independence. In this example this means that we cannot confidently claim any difference in performance for the two challengers. 	 0 Comments
Posterior likelihood	https://www.r-bloggers.com/2010/03/posterior-likelihood/	March 6, 2010	xi'an	At the Edinburgh mixture estimation workshop, Murray Aitkin presented his proposal to compare models via the posterior distribution of the likelihood ratio.  As already commented in a post last July, the positive aspect of looking at this quantity rather than at the Bayes factor is that the priors are then allowed to be improper if one simulates from the posteriors for each model, as in Aitkin et al. (2007). My overall feeling has not changed though, namely the ratio should be instead considered under the joint posterior of , which is [proportional to]  instead of the product of both posteriors. This of course makes a whole difference, as shown on the next R graph that compares the distribution of the likelihood ratio under the true posterior and under the product of posteriors (when comparing a Poisson model against a negative binomial with  successes, when ). The joint simulation produces a much more supportive argument in favour of the negative binomial model, when compared with the product of the posteriors. Obviously, this joint perspective also cancels the appeal of the approach under improper priors. 	 0 Comments
oro.nifti 0.1.3	https://www.r-bloggers.com/2010/03/oro-nifti-0-1-3/	March 5, 2010	Brandon Whitcher		 0 Comments
InformationWeek on Urlocker	https://www.r-bloggers.com/2010/03/informationweek-on-urlocker/	March 5, 2010	David Smith	InformationWeek published today a profile of Zack Urlocker, the former MySQL executive who recently joined REvolution’s board: Former MySQL staffer Zack Urlocker is going to try to do for predictive analytics what he once did for relational database systems: bring open source code to a user population that hasn’t necessarily had access to the technology before. REvolution Computing of Palo Alto aims to move the R open source code beyond use by researchers and academics into the enterprise, challenging other suppliers of predictive analytics with open source as a disruptive force. Urlocker was a key member of the MySQL AB team that built the open source firm up from a penniless free download agency to a company with $100 million in revenues. The rest of the article provides more details of Zack’s history with Oracle née Sun née MySQL, and the experiences there he brings to bear here at REvolution. InformationWeek: MySQL’s Urlocker Joins Revolution Board   	 0 Comments
Because it’s Friday: Why a Salad Costs More than a Big Mac	https://www.r-bloggers.com/2010/03/because-its-friday-why-a-salad-costs-more-than-a-big-mac/	March 5, 2010	David Smith	In the US, at least. Via The Consumerist:   Incidentally, the US FDA doesn’t publish pyramids like this any more: it’s now a garish personalized 2-d triangle with stripes. But at least it doesn’t make the error of dimension committed by the left-hand pyramid: that orange section is a hell of a lot larger than 74% of the volume. The Consumerist: Why A Salad Costs More Than A Big Mac 	 0 Comments
GLMM revisted	https://www.r-bloggers.com/2010/03/glmm-revisted/	March 5, 2010	Shige		 0 Comments
R amusements	https://www.r-bloggers.com/2010/03/r-amusements/	March 5, 2010	Abhijit	"On a lark, and to kill a bit of time, I was running the R fortune command looking for references to SAS. Here’s what two successive random fortunes turned up. Can there be two more antipodal opinions about the same product? I laughed out loud. 
> fortune(‘SAS’) There are companies whose yearly license fees to SAS total millions of dollars.
Then those companies hire armies of SAS programmers to program an archaic macro
language using old statistical methods to produce ugly tables and the worst
graphics in the statistical software world.
– Frank Harrell
R-help (November 2004) > fortune(‘SAS’) For almost 40 years SAS has been the primary tool for statisticians worldwide
and its easy-to-learn syntax, unsurpassed graphical system, powerful macro
language and recent graphical user interfaces have made SAS the number one
statistical software choice for both beginners and advanced users.
– Rolf Poalis, Biostatistics Denmark (announcement of the SAS to R parser
sas2R)
R-help (April 1, 2004) "	 0 Comments
Example 7.25: compare draws with distribution	https://www.r-bloggers.com/2010/03/example-7-25-compare-draws-with-distribution/	March 5, 2010	Ken Kleinman		 0 Comments
Getting data from an image (introductory post)	https://www.r-bloggers.com/2010/03/getting-data-from-an-image-introductory-post/	March 5, 2010	Timothée	Hi there! This blog will be dedicated to data visualization in R. Why? Two reasons. First, when it comes to statistics, I am always starting by some exploratory analyses, mostly with plots. And when I handle large quantities of data, it’s nice to make some graphs to get a grasp about what is going on. Second, I have been a teacher as part of my PhD, and I was quite appaled to see that even Masters students have very bad visualization practices. My goal with this blog is to share ideas/code with the R community, and more broadly, with anybody with an interest in data visualization. Updates will not be regular. This first post will be dedicated to the building of a plot digitizer in R, i.e. a small function to get the data from a plot in graphic format. I have recently been using programs such as GraphClick and PlotDigitizer to gather data from graphs, in order to include them in future analyses (in R). While both programs are truly excellent and highly intuitive (with a special mention to GraphClick), I found myself wondering if it was not possible to digitize a plot directly in R. And yes, we can. Let’s think about the steps to digitize a plot. The first step is obviously to load the image in the background of the plot. The second is to set calibration points. The third step is boring as hell, as we need to click the points we cant to get the data from. Finally, we just need to transform the coordinates in values, with the help of very simple maths. And this is it! OK, let’s get this started. We will try to get the data from this graph: First, we will be needign the ReadImages library, that we can install by typing : This packages provides the read.jpeg function, that we will use to read a jpeg file containing our graph : I strongly recommend that before that step, you start by creating a new window (dev.new()), and expand it to full size, as it will be far easier to click the points later on. So far, so good. The next step is to calibrate the graphic, by adding four calibration points of known coordinates. Because it is not always easy to know both coordinates of a point, we will use four calibration points. For the first pair, we will know the x position, and for the second pair, the y position. That allows us to place the points directly on the axis. We can see the current value of the calibration points : The third step is to click each point, individually, in order to get the data. After clicking all the points, you should have the following graph : Our data are, so far : OK, this is nearly what we want. What is left is just to write a function that will convert our data into the true coordinates. It seems straightforward that the relationship between the actual scale and the scale measured on the graphic is linear, so that  and as such, both a and b can be simply obtained by a linear regression. We can write the very simple function calibrate : And apply it to our data : Which give us : And we can plot the data :  Not so bad! With the simple use of R, we were able to construct a “poor man’s data extraction system” (PMDES, ©), based on the incorporation of graphics in the plot zone, and the locator capacity of R. We can wrap-up everything in functions for better usability : Do you have any ideas to improve these functions? Let’s discuss them in the comments! 	 0 Comments
Accessing Climate Change Data and a Custom Panel Function for Filled Polygons	https://www.r-bloggers.com/2010/03/accessing-climate-change-data-and-a-custom-panel-function-for-filled-polygons/	March 4, 2010	dylan	GCS Model Grids Recently finished some collaborative work with Vishal, related to visualizing climate change data for the SEI.  This project was funded in part by the California Energy Commission, with additional technical support from the Google Earth Team. One of the final products was an interactive, multi-scale Google Earth application, based on PostGIS, PHP, and R. Interaction with the KMZ application results in several presentations of climate projections, fire risk projections, urban population growth projections, and other related information. Charts are dynamically generated from the PostGIS database, and returned to the web browser. In addition, an HTTP-based interface makes it simple to download CSV-formatted data directly from the CEC server. Some of our R code seemed like a good candidate for sharing, so I have posted a complete example below– illustrating how to access climate projection data from the CEC server, a couple custom functions for fancy lattice graphics, and more. read more 	 0 Comments
An email about mixtures	https://www.r-bloggers.com/2010/03/an-email-about-mixtures/	March 4, 2010	xi'an	"As a coincidence, or not, I received the following email just before starting our mixture estimation workshop (the above is Ben Nevis on Monday, whose skyline really looks like a three component mixture!) and giving a discussion on label switching: I am implementing a Markov-Chain Monte Carlo method for Gibbs sampling from a simple mixture of normal model. I am using a decision-theoretic approach per Stephens (2000), but am also looking at simpler methods. Specifically, I have used:
i) mean ordering
and also tried
ii) ordering by the mixture component weights
I have gotten much better results with the former approach than the later. I was wondering if (ii) was, in fact, an accepted approach? Per Stephens (2000), it seems ordering is only done by means or variances. PS- By ordering by the cluster weights w (taken from Dirichlet distribution as I believe is standard I mean):
i) discard burn-in
ii) for each iteration check order of $ latex w$’s. if order satisfies our condition (e.g.,  keep the iteration; otherwise, discard this iteration MIsha xxxx Obviously, any ordering creates an identifiability constraint and is equally acceptable. Or not. Indeed, my opinion on ordering is now the same as it was at the time our 2000 JASA paper got published: the cut (or more exactly quotienting) of the parameter space created by the ordering is not tuned to the topology of the likelihood/posterior surface. Therefore, the resulting subset may well contain incomplete parts of several modes, instead of concentrating on one of the  modes. The right identifiability constraint should provide a single modal region, but I am unclear whether or not this is at all possible… Anyway, with a large enough number of components, I believe any ordering device, whether it is on mean, variance or weight, will eventually fail. 
 Note that the above email implements the ordering by discarding wrongly ordered ‘s. This is unnecessary: when running the MCMC sampler, wrongly ordered ‘s can simply be reordered by the appropriate permutation. (In R, just use the function order().) "	 0 Comments
Yet Another plyr Example	https://www.r-bloggers.com/2010/03/yet-another-plyr-example/	March 4, 2010	dylan	another plyr example quantiles (0.05, 0.25, 0.5, 0.75, 0.95) of DSC by temperature bin There are plenty of good examples on how to use functions from the plyr package. Here is one more, demonstrating how to use ddply with a custom function. Note that there are two places where the example function may blow up if you pass in poorly formatted or strange data: calls to 1) t.test() and 2) quantile(). Also note the use of the transpose function, t(), for converting column-wise data into row-wise data– suitable for inclusion into a dataframe containing a single row. read more 	 0 Comments
More on the Economist’s special report on big data	https://www.r-bloggers.com/2010/03/more-on-the-economists-special-report-on-big-data/	March 4, 2010	David Smith	I totally missed this the other day, but there’s much more to that special report on the data deluge in The Economist. (Thanks to readers SB and DN for pointing this out.) There’s an total of nine articles in the report (you can find them all in the Related Items box on this page), including a section on business intelligence analytics: “A different game: Information is transforming traditional business“. It offers anecdotes of how companies like Cablecom, Best Buy and Wal-Mart have used predictive analytics to better manage their businesses. The article also has a short section on technology used for such applications, including this on R: It’s nice to see the open-source combination of Hadoop and R getting recognition as the engine behind many such large-scale problems in predictive modeling. The Economist: A different game 	 0 Comments
New Le Monde puzzle	https://www.r-bloggers.com/2010/03/new-le-monde-puzzle/	March 3, 2010	xi'an	When I first read Le Monde puzzle this weekend, I though it was even less exciting than the previous one: find , such that  is a multiple of . The solution is obtained by brute-force checking through an R program:  and then the a next solution is  (with several values for N).    However, while waiting in the plane to Edinburgh, I thought more about it and found that the problem can be solved with paper and pencil. It goes like this. There exists an integer  such that  Hence, solving the second degree equation,  which implies that  is one of the integral factors of . If we write  with  we get  and thus . We thus deduce that  is an integer, meaning that  and thus that  is an integer factor of . This obviously restricts the choice of , especially when considering that  implies . Furthermore, the solutions to the second degree equations are then given by . The conclusion is thus that any  which can be divided by a squared integer larger than or equal to  provides a solution.  Now, if we look at the decomposition of , , , , , , , we see (without any R programming) that  is the smallest solution (in ).  is the smallest solution with  a possible solution in  ( being another one). Which makes (at least) for a more bearable diversion in the plane… An approach avoiding the second degree equation is to notice that  implies that  and  share a largest common divider  which implies , thus that  divides  (because both other terms are prime with ). Eliminating dividers common to  and  actually leads to , hence to the same conclusion as before. 	 0 Comments
Quality trimming in R using ShortRead and Biostrings	https://www.r-bloggers.com/2010/03/quality-trimming-in-r-using-shortread-and-biostrings/	March 3, 2010	Jeremy Leipzig		 0 Comments
Example of plotting a serial position curve in R	https://www.r-bloggers.com/2010/03/example-of-plotting-a-serial-position-curve-in-r/	March 3, 2010	Thom Baguley		 0 Comments
R-bloggers (with ~50 blogs) has just crossed the 1000 subscribers mark!	https://www.r-bloggers.com/2010/03/r-bloggers-with-50-blogs-has-just-crossed-the-1000-subscribers-mark/	March 3, 2010	Tal Galili		 0 Comments
Intelligent Enterprise: You Can Predict that R Will Succeed	https://www.r-bloggers.com/2010/03/intelligent-enterprise-you-can-predict-that-r-will-succeed/	March 3, 2010	David Smith	Analyst David Stodder at Intelligent Enterprise also noted the activity around R at the recent Predictive Analytics World conference in San Francisco, and he reviews his impressions in a column today. In fact, he attributes the increasing prominence of predictive analytics to R: David also reported on John Chamber’s talk at the Bay Area User Group meeting, and the REvolution-sponsored social beforehand. On REvolution Computing itself, David had the following to say: He concludes: “What remains to be seen is how soon predictive analytics tools will become pervasive outside the confines of the statistics and data mining community.” With respect to R, I think that depends on two key items: being able to easily apply R to truly large data sets, and making it easier to use for people outside the stats and data mining domains. Those are both areas we’re actively working on, and about which you’ll be hearing more quite soon. Intelligent Enterprise: Expert Analysis: You Can Predict That R Will Succeed  	 0 Comments
Arrange multiple ggplot2 plots in the same image window	https://www.r-bloggers.com/2010/03/arrange-multiple-ggplot2-plots-in-the-same-image-window/	March 3, 2010	Stephen Turner		 0 Comments
Augmented support for complex survey designs in R	https://www.r-bloggers.com/2010/03/augmented-support-for-complex-survey-designs-in-r/	March 3, 2010	Nick Horton		 0 Comments
Analyzing Google’s Winter Olympics Search Traffic with R	https://www.r-bloggers.com/2010/03/analyzing-googles-winter-olympics-search-traffic-with-r/	March 2, 2010	David Smith	The Official Google Blog today includes an analysis of Google’s search traffic related to the recently-concluded Winter Olympics, correlating various high-profile events with searches from particular countries. For example, traffic from the United States shows the expected diurnal cycle but with promintent peaks for the opening ceremony and the hockey matches featuring the USA team:  It’s not specifically stated in the post, but it’s a good bet that R was used to create this analysis. It’s well known that R is in widespread use at Google, but the telltale sign here is the Y axis label. Google’s traditionally coy about giving exact figures for search numbers, and in this chart the search volumes have been suppressed. That’s easy to do in R, by passing the yaxt=”n” option (“Y axis type: none”) to the plot command. Here’s an example, using the built-in lynx data set: > plot(lynx,yaxt=”n”) That tends to leave the Y axis label a bit too far off the axis itself though (it leaves room for number labels that are no longer there), but you can move it into place with an explicit call to the mtext function to place the label 1 line of text from the axis (and suppressing the original axis label with ylab=”” when calling plot):  > plot(lynx,yaxt=”n”,ylab=””)> mtext(“Lynx pelts”,side=2,line=1)  I’d bet good money that similar code was used for these plots. Now if only that could recoup my losses from the USA-Canada hockey final… Official Google Blog: Searching for gold during the Games    	 0 Comments
MySQL alum Zack Urlocker join’s REvolution’s board	https://www.r-bloggers.com/2010/03/mysql-alum-zack-urlocker-joins-revolutions-board/	March 2, 2010	David Smith	As you might have heard from this morning’s press release, we’ve just welcomed a new member to REvolution’s board of directors: Zack Urlocker. Zack has an impeccable open-source pedigree: until recently, he was responsible for engineering and marketing at MySQL, the wildly successful open-source database company recently acquired by Oracle (via its acquisition of Sun). Zack is also a frequent guest speaker at major business summits on using open source technology, and blogs at InfoWorld’s Open Sources blog and his own widely-read Open Force blog (and a regular Twitterer, too). With Zack and the recent appointments of Robert Gentleman and Donald Nickelson (not to mention Tex Hull joining Engineering), REvolution is getting some serious guidance from its Board of Directors. We’re glad to have such a strong team. REvolution Computing: REvolution Computing Names Former MySQL Executive Zack Urlocker to Board 	 0 Comments
ACM Data Mining Camp, March 20	https://www.r-bloggers.com/2010/03/acm-data-mining-camp-march-20/	March 2, 2010	David Smith	Following last year’s successful unconference on data mining, the Bay Area Association for Computing Machinery (ACM) will again host the 2010 ACM Data Mining Camp on March 20 in San Jose, CA. The event is free and runs from 11:15am – 7:30pm, with an optional 2-hour pre-camp training in the morning. (REvolution Computing is a proud sponsor of this event.) An unconference is an event where users suggest topics, get together and discuss them in detail. This camp is focused on Data Mining, Analytics, Cloud Computing and the various applications of these technologies. I went along last year, and found it to be a great venue not just to learn about applications of data mining, but also to get a sense of where the action was focused in applications of analytics to large data sets. There were also a lot of interest in, and several presentations about, the R project. Should be a more comfortable venue this year too, with eBay offering space to meet. Use the link below to RSVP and suggest topics for discussion. San Francisco Bay Area ACM: ACM Data Mining Camp, March 20, 2010 	 0 Comments
The Economist reports on the information explosion	https://www.r-bloggers.com/2010/03/the-economist-reports-on-the-information-explosion/	March 1, 2010	David Smith	The current edition of The Economist includes a “special report on managing information“, targeting the issue of the information explosion / data deluge / whatever you want to call it these days. It includes the usual attributes of the problem: data is being collected faster than we can store it, astronomers are creating petabytes of data daily, the usual. But unusually, the Economist looks at some of the implications of all of this data being collected and analyzed:  What happens when the data deluge evolves from a flood into a source of information hydro-power? We’ll be finding out, sooner rather than later I suspect. The Economist: Data, data everywhere   	 0 Comments
REvolution Computing hiring parallel computing developer	https://www.r-bloggers.com/2010/03/revolution-computing-hiring-parallel-computing-developer/	March 1, 2010	David Smith	We’re looking for a programmer with experience in high-performance computing and the R system to work on the ParallelR suite and other data-analysis projects. Sound like anyone you know? Check out the details at the link below. REvolution Computing careers: Parallel Computing Developer 	 0 Comments
Example 7.24:  Sampling from a pathological distribution	https://www.r-bloggers.com/2010/03/example-7-24-sampling-from-a-pathological-distribution/	March 1, 2010	Nick Horton		 0 Comments
Bayes fits the data less closely than maximum likelihood	https://www.r-bloggers.com/2010/03/bayes-fits-the-data-less-closely-than-maximum-likelihood-2/	March 1, 2010	Andrew Gelman		 0 Comments
End of the month investment	https://www.r-bloggers.com/2010/03/end-of-the-month-investment/	March 1, 2010	kafka	"It is know, that the first day of the month provides bullish edge. According to Quantifiable edges  not all the months are equal. So, I made a test on S&P500 index, from January, 1980 until February, 2010. It is true, March isn’t the best month to run this strategy.  Only 3 months have significant results based on p-values:
“month  5,  p-value  0.0399233570186162″
“month  7,  p-value  0.0466800163648646″
“month  11,  p-value  0.0218919220125013″ p.s. if somebody is interested in R-Language code to repeat this test, then let me know. "	 0 Comments
Quick and dirty parallel processing in R	https://www.r-bloggers.com/2010/04/quick-and-dirty-parallel-processing-in%c2%a0r/	April 30, 2010	Abhijit	"R has some powerful tools for parallel processing, which I discovered while searching for ways to fully utilize my 8-core computer at work. What surprised me is how easy it is…about 6 lines of code, if that. Given that I wasn’t allowed to install heavy duty parallel-processing systems like MPICH on the computer, I found that the library SNOW fit the bill nicely through its use of sockets. I also discovered the libraries foreach and iterators, which were released to the community by the development team at Revolution R. Using these 3 libraries, I could easily parallelize a transformation of my dataset where the transformations happened within each unique ID. The following code did the trick:  library(foreach) library(doSNOW)
 cl <- makeCluster(6, type=""SOCK"") # using 6 nodes registerDoSNOW(cl) uID <- unique(ID) foreach(i=icount(length(uID)) %dopar% {     transformData(dat[dat$ID==uID[i],]) } stopCluster(cl)  Note that this is for a multiprocessor single computer. Doing this on a cluster may be more complicated, but this serves my purposes quite nicely. There are other choices for this, including the multicore library and others described in the CRAN Task View  Update: I found that this strategy did not work for R 2.11 Windows versions, since snow is not properly spawning processes. However, there is a library  doSMP provided by Revolution Analytics which gets around this problem. So replacing doSNOW with doSMP should do the trick. "	 0 Comments
How many girls, how many boys?	https://www.r-bloggers.com/2010/04/how-many-girls-how-many-boys/	April 30, 2010	Matt Asher	" I found this interesting question over here at mathoverflow.net. Here’s the question: If you have a country where every family will continue to have children until they get a boy, then they will stop. What is the proportion of boys to girls in the country. First off, there are some assumptions you need to make that aren’t stated in the problem. The most important one is that boys are just as likely as girls to be born. This is empirically false, but there’s nothing wrong with assuming it for the problem, so long as the assumption is acknowledged. My first thought about solving the problem was to think about Martingales and stopping times, but that’s more complication than you need. If you look at it from the point of view of expectation things are simpler.The probability of having a boy first is 1/2, at which point the family stops and you have a ratio of 1 boy per 1 births. The probability of having one girl, then one boy is 1/4, at which point you have a ratio of 1 boy per 2 births. Multiplying the probabilities by the ratios and summing from 1 birth to infinity, you get an expectation of approximately %69.31 boys. Problem is, this is the expectation for a single family. Because families who have more children (and thus more girls) contribute disproportionately to the pool of children, one family is a biased estimator for the proportion in the entire population. Douglas Zare at the above-linked mathoverflow question does a good job of working out the details for a country with an arbitrary number of families. Here is what he comes up with for the percentage of girls:  Where k is the number of families in the country, and  is the digamma function. To be true to the new motto of this site, I decided to test this out in R using a Monte Carlo method. Here is my code:  And here is the resulting graph: Looks like a good match.  Maybe you noticed that at the beginning I mentioned assumptions, as in more than one. We are also assuming that that all of the boys and girls, no matter how old, are still considered boys and girls. All of the parents were already in the country at the beginning of the problem, and then they all started having children until they had a boy and stopped. None of these children have had any children. The process is complete, and the new generation is the last. Obviously even if parents in a country did follow the rule of ""babies until boy then stop"", the results wouldn't match the theoretical because at any given moment there are many families in the process of still having kids. This is where, if I were so inclined or needed a more accurate model, I would dive back into the Martingale issue and things would get messy. "	 0 Comments
Which font uses the most ink?	https://www.r-bloggers.com/2010/04/which-font-uses-the-most-ink/	April 30, 2010	David Smith	"If you’re being particularly cost-conscious about your use of printer ink or toner, you may be wondering which font you should choose to minimize ink use. Here’s an infographic with the answer: 
 This is an interesting infographic in its own right, but what makes it cool is that these are not photoshopped images of Bic biros. Matt Robinson created this chart by writing the word “Sample” on a wall using a brand new biro for each font; the amount of ink remaining indicates the density of the type. In other words, the data is the chart. Brilliant! Click the link below to see how he did it. Matt Robinson: Measuring Type   "	 0 Comments
Data Manipulation with R – Spector (2008)	https://www.r-bloggers.com/2010/04/data-manipulation-with-r-spector-2008-2/	April 30, 2010	bryan	" If there is one book that every beginning R user coming from a programming background should have, it is Spector’s Data Manipulation with R.  New R users with analytic backgrounds and experience with software packages such as SAS and SPSS will do well to start with Muenchen’s R for SPSS and SAS users, especially given that a free abbreviated version is available, but those users should also make Data Manipulation with R a quick second addition to their library.

The text of this book is as concise and to the point as its title. It covers almost every relevant data manipulation topic in R, from modes and classes, through accessing data via database connections, to complex reshaping and aggregating functions. It has copious examples and the text hits just the right level of sophistication for the individual who has some experience with programming, but little experience with R idioms and data manipulation techniques. My only critique of this book is that it skips over the basics of creating user-defined functions for data manipulation tasks. Spector addresses mapping functions to various data structures, but it seems likely that, at this level, the average R analyst would be better served by a discussion of how to simply create a function in R. Keep in mind that if you are looking for that type of information, you will need to look elsewhere. The same is true if you are looking for any sort of statistical instruction, as Data Manipulation with R focuses almost exclusively on programming. Overall, I highly recommend this book. At around $45 USD, it is well worth the price. You’ll breeze through it on your first pass, but if you’re new to R you will get your money’s worth out of it as a reference text.  The post Data Manipulation with R – Spector (2008) appeared first on ProgrammingR. 
 "	 0 Comments
Data Manipulation with R – Spector (2008)	https://www.r-bloggers.com/2010/04/data-manipulation-with-r-spector-2008/	April 30, 2010	bryan	 If there is one book that every beginning R user coming from a programming background should have, it is Spector’s Data Manipulation with R.  New R users with analytic backgrounds and experience with software packages such as SAS and SPSS will do well to start with Muenchen’s R for SPSS and SAS users, especially given that a free abbreviated version is available, but those users should also make Data Manipulation with R a quick second addition to their library. read more 	 0 Comments
hash-2.0.0	https://www.r-bloggers.com/2010/04/hash-2-0-0/	April 30, 2010	Christopher Brown	The hash-2.0.0 package has been uploaded to CRAN.  This version was developed in conjunction with R-2.11.0 and was refactored for performance.   hash-2.0.0 requires R-2.10.0 or later and will not be supported on earlier versions of R.  This is a result of recent changes to the language itself. Importantly: Understand that hash-2.0.0, breaks backward compatibility; code written with previous versions of the hash package are not guaranteed to work with this or future versions. This is due to changes made in order to achieve much higher performance.  Assignments and look-ups are achieved more quickly through  direct inheritance of environments, stripping of non-essential customizations  and reliance on core and primitive functions. Here is a summary of major changes: ChangeLog and TODO track many technical details; here I will discuss only the more  important changes: Included in this version is a demo script that runs benchmarks (demo(hash-benchmarks).  One of the questions that has been repeatedly posed, often in the context of look-up, is:  how does this compare to native R named lists and vectors? In other words, how much quicker is accessing a value on a hash / environment as opposed to a list (or vector)?  This is a difficult questions, and generally depends on the size of the hash or list.  My rule of thumb is that it is quicker to look-up elements on lists and vectors less than about 500 elements.  After ~500 elements, hashes and environments greatly outperform lists.  The difference increases relative to the size of the object.  However, look-ups for all these objects are very fast if objects are small  ( >120,000 / sec ).  So unless you are doing many serial look-ups, hashes are likely the better option. I have written previously about hashes in R [1]  [2], and will continue to  discuss the  evolution of R hashes on this blog.  Additionally I will be speaking on this and related work at useR!2010 (July 20-23.) 	 0 Comments
5 Minute Analysis in R: Case-Shiller Indices	https://www.r-bloggers.com/2010/04/5-minute-analysis-in-r-case-shiller-indices/	April 29, 2010	Lee	The Case-Shiller Home Price Indices measure residential home values for 20 cities in the US, with some indices going all the way back to the 80s. With housing prices all the rage these days, we should perform a quick-and-dirty analysis using R to see what we can glean from this rich dataset.  First things first, the data needs to be downloaded from S&P’s website, converted into a CSV format, and then imported into R.  Now that the data is loaded, lets start by simply plotting the time series of the Indices.  There’s alot of ’stuff’ going on which makes it hard to distinguish one index from another. To simplify things, lets just plot a subset of the indices. For no particular reason, I’ll pick New York, Las Vegas, and San Francisco.  Much better, but all this really shows us is that there was a pretty substantial run-up in home values starting in the late 90s, followed by a bust in 2006 (not exactly new news). What would be more interesting would be to analyze the monthly returns in the indices, which I suspect would be somewhat stationary. If we define  as the monthly return in the form , we can calculate it as . At this point we haven’t made any assumption about the distribution of .  Now things are starting to get interesting.  Clearly there is some seasonality going on and the returns appear to be correlated. To investigate the correlation a bit more, lets do a pairs plot.  This confirms our suspicions about correlation. The monthly return almost appear bivariate normal.  Lets produce some boxplots to investigate the distribution of .  It appears that the returns are roughly normal, with the mean return just above 0, but some appear to have much fatter tails than others (compare New York to Las Vegas for instance). We should perform some QQ Normal plots to see how normal the monthly returns really are.   This confirms our suspicions that the return are ‘normal like’, but have some pretty fat tails (as do most financial assets).  Although, New York and Boston appear to be much more normal than the rest. This analysis really begs for an ARMA model that incorporates the correlation across housing markets. 	 0 Comments
Research in pair next summer	https://www.r-bloggers.com/2010/04/research-in-pair-next-summer/	April 29, 2010	xi'an	  Today I received the very good news that our proposal with Jean-Michel Marin to undertake “research in pair” in CIRM, Luminy, a fortnight next summer was accepted! This research centre in Mathematics is a southern and French version of the renowned German centre of Oberwolfach and, while I would have prefered the cool Black Forest to the burning rocks of the nearby calanques, I am very grateful for this support from the sponsors of the  CIRM centre. We aim at revising the book Bayesian Core towards a Use R! version during this fortnight (if the heat does not kill our legendary productivity!).  The CIRM centre is located in a nicely renovated bastide within a small park, and the famous climbing cliffs of the calanques are within walking distance. (I just need to find a climbing partner!) I have organised several meetings there along the years and the atmosphere there is always propitious for research. (There is also a well-provided library, if not comparable to Oberwolfach.)  	 0 Comments
Getting a Web application to talk to R	https://www.r-bloggers.com/2010/04/getting-a-web-application-to-talk-to-r/	April 29, 2010	David Smith	Let’s face it: you can do some pretty awesome things with R — statistical models, beautiful charts, you name it — but if the only way to do those things is from the R command line you’re limiting the audience of people who might make use of all this awesomeness to a limited subset: R programmers. What if you could get the results of R programs, charts and data, into a Web application? R’s an open system, so it’s definitely possible. To get started, Neil Saunders shows us how to build a simple Web application that displays data and a chart from an R script, by serving JSON or CSV from Rails to RApache and sending a result back to Rails. As he points out, it’s not a complete solution — “a real application would require more model or controller methods, views, R functions, error checks and prettier views, perhaps with a dash of AJAX thrown in”, but it’s a great starting point. What You’re Doing Is Rather Desperate: Getting your web application and R(Apache) to talk to each other 	 0 Comments
Response to Flowingdata Challenge: Graphing obesity trends	https://www.r-bloggers.com/2010/04/response-to-flowingdata-challenge-graphing-obesity-trends/	April 29, 2010	Hrishi Mittal	"Nathan at Flowingata put up another interesting challenge today to improve the following graphic showing obesity trends in America.  Here’s my attempt:  I transposed the data so that the cohorts are on the X axis and each separate line represents an age group. So each line shows the percentage of obese people in a particular age group. This way the graph tells you the probability of you being obese at a given age in a particular decade. For each age group, the line roughly trends upwards. For example, the lavender (violet? 3rd from bottom) line shows that if you were in your twenties during the World War II chances of you being obese were just below 10%, but if you were in your twenties during the mid 60s-70s (rocking out to The Doors?) you were more than twice as likely to be obese. So, from a cursory look, it does seem that Americans have been getting obese faster. For those interested, here’s my modified data file and the R code: o colnames(o) library(RColorBrewer)
pal plot(o[,2],pch=19,xaxt=”n”,col=pal[2],type=”o”,ylim=c(0,max(o[,-1],na.rm=T)),xlab=”Cohort by Decade”,ylab=”Percentage of Obese People”,main=”Obesity trends by Age Group”)
 for(i in 3:length(colnames(o))) {
points(o[,i],pch=19,xaxt=”n”,col=pal[i])
lines(o[,i],pch=19,xaxt=”n”,col=pal[i])
} axis(1,at=1:length(o[,1]),labels=o[,1],cex.axis=0.75) legend(“topright”,legend=colnames(o)[-1],col=pal[-1],lty=1,pch=19,bty=”n”) "	 0 Comments
JAGS 2.0	https://www.r-bloggers.com/2010/04/jags-2-0/	April 29, 2010	jackman	Is out.  On sourceforge.  Along with a new rjags.  Thanks Martyn. I’m looking forward to working my way through some of the improvements, which include  	 0 Comments
Tipping heuristics	https://www.r-bloggers.com/2010/04/tipping-heuristics/	April 28, 2010	dan	INCREDIBLY SIMPLE CALCULATIONS MADE SIMPLE  Yes, we all know how to calculate 15% or 20% exactly, but it’s fun to use tipping heuristics and even more fun to make crowded graphs of how they compare to each other. (Sorry for the junky chart. Open for suggestions, in the words of Tom Waits.) Here are a few tipping heuristics compared to a 15% baseline (which some claim to be 15-20% in NYC): – Round to the nearest $10, then double the number on the left – Round to the nearest $5 and throw in $1 for every $5 – Double the tax There is also the notorious “double the number on the left”, which a friend’s father described as “sometimes they win, sometimes they lose.” DSN doesn’t like this one as it inflicts its damage on small checks, which often require as much waitstaff effort as large ones. If you’re a high roller, it looks pretty safe, however.  Whatever you do, please advocate smart heuristics instead of those undeservedly popular iPhone tipping apps. What tipping rule of thumb do you use? Note: Tax figure is New York City restaurant tax, which is something like 8.875%. I regret doing this in Excel instead of R, but it seemed like it would be faster and prettier. 	 0 Comments
Wavelet Spectrogram Non-Stationary Financial Time Series analysis using R (TTR/Quantmod/dPlR) with USDEUR	https://www.r-bloggers.com/2010/04/wavelet-spectrogram-non-stationary-financial-time-series-analysis-using-r-ttrquantmoddplr-with-usdeur/	April 28, 2010	Intelligent Trading		 0 Comments
R Beginner’s Guide Book Update 4/28/2010	https://www.r-bloggers.com/2010/04/r-beginners-guide-book-update-4282010/	April 28, 2010	John Quick	Update: Statistical Analysis with R is now available! I am writing to update you on the progress of my R Beginner’s Guide book, which is to be published through Packt. I have really gotten to work over the past couple months and have recently completed the first draft of the first half of the book. Right now, I am operating a few weeks ahead of our planned schedule, which calls for the first draft of all ten chapters by mid-August. To give you an idea of its content, the book focuses on most of the topics covered in this blog as well as many more, such as data visualization, custom functions, and online resources. The topics are covered in great depth and numerous opportunities for practice and exploration are offered. The book’s theme centers around the Three Kingdoms period of ancient China. The reader takes on the role of the lead strategist for the Shu kingdom at a pivotal point in history. Throughout the book, the reader uses R to help devise a course of action for the Shu forces. I will continue to make steady progress on this book over the summer months. I am also excited to be able to share more R tutorials and knowledge in the near future through this project. 	 1 Comment
Sweave vs. pgfSweave	https://www.r-bloggers.com/2010/04/sweave-vs-pgfsweave/	April 28, 2010	Shige		 0 Comments
Eclipse and StatET – a working environment for R	https://www.r-bloggers.com/2010/04/eclipse-and-statet-%e2%80%93-a-working-environment-for-r/	April 28, 2010	Luke Miller		 0 Comments
Annotating Lattice Box and Whisker Plots	https://www.r-bloggers.com/2010/04/annotating-lattice-box-and-whisker-plots/	April 28, 2010	dylan	bwplot annotation example Sometimes you want to add a little text to box and whisker plots produced by the lattice function bwplot(). Here is one approach. Could be optimized a bit more to reduce manual specification of some elements. Suggestions welcomed. read more 	 0 Comments
Analysis of Covariance – Extending Simple Linear Regression	https://www.r-bloggers.com/2010/04/analysis-of-covariance-%e2%80%93-extending-simple-linear-regression/	April 28, 2010	Ralph	The simple linear regression model considers the relationship between two variables and in many cases more information will be available that can be used to extend the model. For example, there might be a categorical variable (sometimes known as a covariate) that can be used to divide the data set to fit a separate linear regression to each of the subsets. We will consider how to handle this extension using one of the data sets available within the R software package. There is a set of data relating trunk circumference (in mm) to the age of Orange trees where data was recorded for five trees. This data is available in the data frame Orange and we make a copy of this data set so that we can remove the ordering that is recorded for the Tree identifier variable. We create a new factor after converting the old factor to a numeric string: The purpose of this step is to set up the variable for use in the linear model. The simplest model assumes that the relationship between circumference and age is the same for all five trees and we fit this model as follows: The summary of the fitted model is shown here: The test on the age parameter provides very strong evidence of an increase in circumference with age, as would be expected. The next stage is to consider how this model can be extended – one idea is to have a separate intercept for each of the five trees. This new model assumes that the increase in circumference is consistent between the trees but that the growth starts at different rates. We fit this model and get the summary as follows: The additional term is appended to the simple model using the + in the formula part of the call to lm. The first tree is used as the baseline to compare the other four trees against and the model summary shows that tree 2 is similar to tree 1 (no real need for a different offset) but that there is evidence that the offset for the other three trees is significantly larger than tree 1 (and tree 2). We can compare the two models using an F-test for nested models using the anova function: Here there are four degrees of freedom used up by the more complicated model (four parameters for the different trees) and the test comparing the two models is highly significant. There is very strong evidence of a difference in starting circumference (for the data that was collected) between the trees. We can extended this model further by allowing the rate of increase in circumference to vary between the five trees. This additional term can be included in the linear model as an interaction term, assuming that tree 1 is the baseline. An interaction term is included in the model formula with a : between the name of two variables. For the Orange tree data the new model is fitted thus: Interesting we see that there is strong evidence of a difference in the rate of change in circumference for the five trees. The previously observed difference in intercepts is now longer as strong but this parameter is kept in the model – there are plenty of books/websites that discuss this marginality restrictin on statistical models. The fitted model described above can be created using lattice graphics with a custom panel function making use of available panel functions for fitting and drawing a linear regression line for each panel of a Trellis display. The function call is shown below: The panel.xyplot and panel.lmline functions are part of the lattice package along with many other panel functions and can be built up to create a display that differs from the standard. The graph that is produced: Analysis of Covariance Model fitted to the Orange Tree data This graph clearly shows the different relationships between circumference and age for the five trees. The residuals from the model can be plotted against fitted values, divided by tree, to investigate the model assumptions: The residual diagnostic plot is: Residual diagnostic plot for the analysis of covariance model fitted to the Orange Tree data There are no obvious problematic patterns in this graph so we conclude that this model is a reasonable representation of the relationship between circumference and age. Additional: The analysis of variance table comparing the second and third models shows an improvement by moving to the more complicated model with different slopes: 	 0 Comments
Interview with Revolution CEO Norman Nie	https://www.r-bloggers.com/2010/04/interview-with-revolution-ceo-norman-nie/	April 28, 2010	David Smith	Steve Miller has posted his interview with Revolution’s CEO Norman Nie at Information Management blogs. In the interview, Steve digs into Norman’s motivations for taking on a new venture around R after his successes with SPSS and how what he learned there applies to Revolution Computing. Also up for discussion: the benefits and challenges of an open-source business model; the relationship between academia and R; and how analytics can be relevant for a non-expert audience. Read Part 1 of interview at the link below; a follow-up will appear next week. Information Management: Revolutionary R, An Interview with REvolution Computing CEO Norman Nie – Part 1 	 0 Comments
Bhapkar V test	https://www.r-bloggers.com/2010/04/bhapkar-v-test/	April 28, 2010	Todos Logos		 0 Comments
Social Network Analysis using R and Gephis	https://www.r-bloggers.com/2010/04/social-network-analysis-using-r-and-gephis/	April 28, 2010	prasoonsharma		 0 Comments
Transitions in R redux	https://www.r-bloggers.com/2010/04/transitions-in-r-redux/	April 28, 2010	Samuel Brown		 0 Comments
Using R for Introductory Statistics, Chapters 1 and 2	https://www.r-bloggers.com/2010/04/using-r-for-introductory-statistics-chapters-1-and-2/	April 27, 2010	Chris	"I’m working my way through Using R for Introductory Statistics, by John Verzani, a free version of which is available as  SimpleR. …covers basics of R such as arithmetic, loading libraries and reading data. We also get an introduction to vectors and indexing. The book divides data into three types: categorical, discrete numerical and continuous numerical. Other books talk about levels or scales of measurement: nominal (same as categorical), ordinal (rank), interval (arbitrary zero), and ratio (true zero).

 The table command tabulates categorical observations. We can use cut to bin numeric data. For summarizing a data series, use the summary command, or its cousin fivenum. Fivenum gives the Tukey five number summary (minimum, lower-hinge, median, upper-hinge, maximum). Hinges are the medians of the left and right halves of the data, which is only slightly different than quartiles. The two most common measures of central tendency are mean and median. Variance and standard deviation measure how much variation there is from the mean. They are measures of dispersion or spread.
The standard deviation, the square root of the variance, has the same units as the original data. I've personally always wondered why we square the differences rather than take the distance or mean absolute deviation. Apparently, it's a matter of some debate. Other measures of variability or dispersion are quantiles (quantile) and inter-quartile range (IQR). Histograms are a graphical way to look at how data points are distributed over a range. To construct a histogram, we first divide the data into bins. Then, for each bin, we draw a rectangle whose area is proportional to the frequency of data that falls into that bin. Drawing histograms in R is done with the hist command.  Boxplots give another way of viewing the shape of data which works for comparing several distributions, although this example shows only one.  "	 0 Comments
How to do this graph using R	https://www.r-bloggers.com/2010/04/how-to-do-this-graph-using-r/	April 26, 2010	enayet	" I am showing three examples that will help an average R user to create beautiful graphs. The interesting (may be useful) parts of these examples are the use of some very smart but tricky functions to, for example, add a Greek symbol on a plot, add a title to a plot with some mathematical symbols.  I am showing three examples that will help an average R user to create beautiful graphs. The interesting (may be useful) parts of these examples are the use of some very smart but tricky functions to, for example, add a Greek symbol on a plot, add a title to a plot with some mathematical symbols. In these examples I used expression and paste functions  repeatedly. For more examples please see this site.    
x 
hx 
 gshape 
colors 
 plot(x, hx, type=”n”, lty=2, lwd=2, xlab=”x”, ylab=expression(f(x)), main=expression(theta == 1), ylim=c(0,0.4), frame.plot=F) for (i in 1:4){
  lines(x, dgamma(x,shape=gshape[i], scale=1), lwd=2, col=colors[i])
} # Inserting mathematical expressions text(3.5,.35 , expression(paste(alpha==2)))
text(6.5,.18 , expression(paste(alpha==5)))
text(11.5,.13 , expression(paste(alpha==10)))
text(23,.05 , expression(paste(alpha==15)))
  
# Generating Poisson random numbers with rates 1, 2, 3, and 4
# and drawing the histograms and arranging them on a 2 x 2 matrix par(mfrow=c(2,2))
lambda 
x 
 for (i in lambda)
  {
    x1 
    barplot(x1, names.arg=c(“0”, “1”, “2”, “3”, “4”, “5”, “6”, “7”, “8”, “9”, “10”),  ylab=expression(P(x)), ylim=c(0,.4), col=”lightgreen”)
    title(main = substitute(lambda == i,list(i=i)))
  }
  
# Binomial probability plot with vertical lines par(mfrow=c(2,2))
n 
p 
x 
for (i in 1:4)
  {
    x1 
    plot(x1, type=”h”, xlab=”x”, ylab=expression(P(x)),
         ylim=c(0,.4),lwd=2, col=”red”)
    j 
    title(main = substitute(p == j,list(j=j)))
  }
 If you have an idea of a complicated graph, please submit your code  to me. I will add your contribution on this page.  If you find this page useful, please link this site from your blog/website or refer it in your report/thesis. Thank you. "	 0 Comments
R Project and Google Summer of Code: Welcome to our students!	https://www.r-bloggers.com/2010/04/r-project-and-google-summer-of-code-welcome-to-our-students/	April 26, 2010	Thinking inside the box		 0 Comments
R is going to have a GUI to ggplot2! (by the end of this years google-summer-of-code)	https://www.r-bloggers.com/2010/04/r-is-going-to-have-a-gui-to-ggplot2-by-the-end-of-this-years-google-summer-of-code/	April 26, 2010	Tal Galili	"I was delighted to see the following e-mail post from Dirk Eddelbuettel regarding the google-summer-of-code R google group:
*  *  * Earlier today Google finalised student / mentor pairings and allocations for
the Google Summer of Code 2010 (GSoC 2010).  The R Project is happy to
announce that the following students have been accepted:   Colin Rundel, “rgeos – an R wrapper for GEOS”, mentored by Roger Bivand of
     the Norges Handelshoyskole, Norway   Ian Fellows, “A GUI for Graphics using ggplot2 and Deducer”, mentored by
     Hadley Wickham of Rice University, USA   Chidambaram Annamalai, “rdx – Automatic Differentiation in R”, mentored by
     John Nash of University of Ottawa, Canada   Yasuhisa Yoshida, “NoSQL interface for R”, mentored by Dirk Eddelbuettel,
     Chicago, USA   Felix Schoenbrodt, “Social Relations Analyses in R”, mentored by Stefan
     Schmukle, Universitaet Muenster, Germany   Details about all proposals are on the R Wiki page for the GSoC 2010 at
http://rwiki.sciviews.org/doku.php?id=developers:projects:gsoc2010 The R Project is honoured to have received its highest number of student
allocations yet, and looks forward to an exciting Summer of Code.  Please
join me in welcoming our new students. At this time, I would also like to thank all the other students who have
applied for working with R in this Summer of Code. With a limited number of
available slots, not all proposals can be accepted — but I hope that those
not lucky enough to have been granted a slot will continue to work with R and
towards making contributions within the R world. I would also like to express my thanks to all other mentors who provided for
a record number of proposals.  Without mentors and their project ideas we
would not have a Summer of Code — so hopefully we will see you again next
year.   Regards,   Dirk (acting as R/GSoC 2010 admin) *  *  * From all the projects, the one I am most excited about is:
Ian Fellows, “A GUI for Graphics using ggplot2 and Deducer”, mentored by Hadley Wickham of Rice University, USA Deducer  (text from the website) attempts to be a free easy to use alternative to proprietary data analysis software such as SPSS, JMP, and Minitab. It has a menu system to do common data manipulation and analysis tasks, and an excel-like spreadsheet in which to view and edit data frames. The goal of the project is to two-fold. Deducer is designed to be used with the Java based R console JGR, though it supports a number of other R environments (e.g. Windows RGUI and RTerm). This combination (of Deducer and ggplot2) might finally provide the bridge to the layman-statistician that some people recently wrote to be one of R’s weak spots (while other bloogers wrote back that this is o.k., still no one refuted that R doesn’t compete with the point-and-click of softwares like SPSS or JMP.)
I came across Ian in the discussion forums, where he provided very kind help to his package “deducer”.  Coupled with having Hadley as his mentor, I am very optimistic about the prospects of seeing this project reaching very high standards.
Very exciting development indeed! Update: Ian’s proposal is available to view here. p.s: for some intuition about how a GUI for ggplot2 can look like, have a look at this video of Jeroen Ooms’s ggplot2 web interface "	 0 Comments
R Project websites down	https://www.r-bloggers.com/2010/04/r-project-websites-down/	April 26, 2010	David Smith	The main R Project website, www.r-project.org, many of the primary CRAN mirrors (including cran.r-project.org), and r-forge.r-project.org are currently unavailable following a power failure at the master host in Austria. The US CRAN mirror, cran.us.r-project.org, and other mirrors not under the r-project.org domain (including cran.revolution-computing.com) are still accessible. In addition, the following services are not affected: bugs.r-project.org, developer.r-project.org, ess.r-project.org, search.r-project.org, svn.r-project.org, wiki.r-project.org, win-builder.r-project.org. Downtime of at least 24 hours is expected. Update Apr 27 09:34: The R Project websites appear to be back online. 	 0 Comments
A serial Connection for R	https://www.r-bloggers.com/2010/04/a-serial-connection-for-r/	April 26, 2010	Matt Shotwell	"***UPDATED: June 3, 2010 – revert name from “tty” to “serial“, R version (2.11.1)*** I’m working on a patch for R (currently 2.11.1)  that adds a serial connection feature for POSIX systems (i.e. Linux, Mac OS X, …). The serial connection works like the other connections. For example, the following code opens, writes a single byte, and closes a serial device The serial connection supports modifying serial port parameters, including input/output baud rates, character size, number of stop bits, parity, blocking, and start/stop flow control. For instance, to open a serial connection with input and output baud rate set to 38400 use The rest of this post is a HOW TO for applying, configuring, and compiling the patch on a POSIX system so that you can use the serial connection! There are two ways to apply the patch, the second (Elegant) requires a little more work than the first (Hack) , but is the recommended route. This patch is free software: you can redistribute it and/or modify
it under the terms of the GNU General Public License as published by
the Free Software Foundation, either version 2 of the License, or
(at your option) any later version. This patch is distributed in the hope that it will be useful,
but WITHOUT ANY WARRANTY; without even the implied warranty of
MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
GNU General Public License for more details. You should have received a copy of the GNU General Public License
along with this patch.  If not, see http://www.gnu.org/licenses/. "	 0 Comments
Example 7.34: Propensity scores and causal inference from observational studies	https://www.r-bloggers.com/2010/04/example-7-34-propensity-scores-and-causal-inference-from-observational-studies/	April 26, 2010	Ken Kleinman		 0 Comments
R : NA vs. NULL	https://www.r-bloggers.com/2010/04/r-na-vs-null/	April 25, 2010	Christopher Brown	" It is common for programming languages to have a NULL value.  What often leads to confusion is the fact NULL can have two distinct meanings.  In the first, NULL is used to represent missing or undefined values.  This is well appreciated in SQL. In the second case, NULL is the logical representation a statement that is neither TRUE nor FALSE.  This indeterminacy is the basis for ternary logic.  While these meanings are distinct, they are very often related.  When missing values (the first meaning) are evaluated, the desired result is often an ambiguous result (the second).  That is, the former implies the latter.  In programming, the distinction is often unnecessary and glossed over and the concepts become confounded.  The R language has two closely related NULL-like values, NA and NULL.  Both are fully support in the language by core functions (e.g, is.na, is.null, as.null, etc.). And, while NA is used exclusively in the logical sense, both are used to represent missing or undefined values.  This has lead to much confusion.  Here’s what the R documentation has to say: NULL represents the null object in R: it is a reserved word.
NULL is often returned by expressions and functions whose values are
undefined. NA is a logical constant of length 1 which contains a missing
value indicator. NA can be freely coerced to any other vector
type except raw.  There are also constants NA_integer_,
NA_real_, NA_complex_ and NA_character_ of the other atomic
vector types which support missing values: all of these are
reserved words in the R language. There is a lot of subtlety in the treatment of these values.  A good way to understand the distinction between  NA and NULL is through some examples: The important distinction is that NA is a ‘logical’ value that when evaluated in an expression, yields NA.  This is the expected behavior of a value that handles logical indeterminacy.   NULL is its own thing and does not yield any response when evaluated in an expression, which is not how we would want or expect NA to work. To delve deeper into the behavior we must look at how R’s basic data structures, vectors (including matrices and arrays) and lists (including data.frames) behave.  Vectors and lists are similar structures, both allow for multiple values with similar accessors.  There are subtle differences in the treatment of NA and NULL.  Let’s take a look at how they compare:  Vectors ( inc. Matrices and Arrays ) What happened?  NULL is not allowed in a vector.  When you attempt to set it as a value in a vector, it is it is quietly ignored.  This is because NULL is an object and type of its own.  NULL does not have various types such as NULL_integer_.  There is just NULL. By contrast, NA has NA_integer, etc. and happily coexists with any of the basic vector types vector.  So for any vector (matrix or array), NA represents a missing value.  NULL does not. Now, let’s look at the lists example. This is interesting! Unlike the vector, the list can hold objects and values other than the basic types.  This includes the NULL value/object.  Perhaps a little inconsistent and not what we would expect.  But from here, things get a little quirky, let’s try value assignment: Sure enough NULL cannot be assigned to a vector.  So for all purposes, NA with respect to the basic vector behaves like NULL in other languages.  NULL is almost never what you want.  On the list side, however, we see an idiom of NULL. Assigning NULL to list items, removes them.  This behavior is a bit unexpected, but it is the idiom. There is one final idiom to know about NULL and lists. Namely, that trying to access a list element by a non-existing name yields a NULL value. ( Note: the same is true for trying to access non-existing objects on an environment ) R does not have a consistent or intuitive way of dealing with missing and logically ambiguous values, i.e. addressing the two meanings from the beginning of this post.  For vectors and basic variables, R mimics other languages and uses NA.  For lists however, the syntax is more idiomatic.  It is this latter case that presents difficulty.  R has other quirks too.  But all languages have quirks, and given R’s strength for statistical analysis, I have found no better tool for this.   >Here c( 1, NA, NULL) [1]  1 NA "	 0 Comments
Summarising data using box and whisker plots	https://www.r-bloggers.com/2010/04/summarising-data-using-box-and-whisker-plots/	April 25, 2010	Ralph	A box and whisker plot is a type of graphical display that can be used to summarise a set of data based on the five number summary of this data. The summary statistics used to create a box and whisker plot are the median of the data, the lower and upper quartiles (25% and 75%) and the minimum and maximum values. The box and whisker plot is an effective way to investigate the distribution of a set of data. For example, skewness can be identified from the box and whisker as the display does not make any assumptions about the underlying distribution of the data. The extreme values at either end of the scale are sometimes included on the display to show how far they extend beyond the majority of the data. To illustrate creating box and whisker plots we consider UK meteorological data that has been collected on a monthly basis at Southampton, UK between 1950 and 1999 and is publicly available. This data is available from the UK Met Office and we will compare the range of temperatures recorded in each month of the year over this period by creating box and whisker plots with the different packages. The data is assumed to have been imported into R and stored in a data frame called soton.df. An extract of the data is shown here: Base Graphics Fast Tube by Casper The base graphics approach makes use of the boxplot function to create box and whisker plots. In this situation the function can be used with a formula rather than specifying two separate vectors of data – we can specify a data frame to point towards a source of data to be used in the graph. For the temperature data we use this code: The horizontal and vertical axes labels are specified using the xlab and ylab arguments respectively and the title of the plot is created using the main argument. The box and whisker plot is shown here: Base Graphics Box and Whisker Plot The function boxplot makes it easy to create a reasonably attractive box and whisker plot. The variation in the distribution of temperatures across the year can be seen from the graph. Lattice Graphics Fast Tube by Casper In the lattice graphics package there is a function bwplot which is used to create box and whisker plots. The function call also uses a formula to specify the x and y variables to use on the graph. The function call arguments are identical to the boxplot function in base graphics: The variable Month is categorical so a separate box and whisker summary is created for each month separately. The lattice version of the graph is shown here: Lattice Graphics Box and Whisker Plot This is very similar to the box and whisker plot created by base graphics with a similar level of effort required. The main difference is the use of a circle rather than a line to identify the location of the median of the data. ggplot2 Fast Tube by Casper In the ggplot2 package there is a general function ggplot that is used to create graphs of any type. We make use of the boxplot geom to create a box and whisker plot following the standard approach. The first step is to specify a data frame to use to create the graph and then map the columns of this data frame, via the texttt{aes} argument, to the different axes or other aesthetics (such as colour or symbol shape). The particular geom is used to specify the type of plot that we want to create. Our final step is to add on the various axes labels and an overall title to the graph. The ggplot2 version of box and whisker plots is shown here: ggplot2 Graphics Box and Whisker Plot The distinctive gray background used by ggplot2 is an obvious visual difference compared to the default clear background used in the other two approaches. The boxes themselves have a cleaner look in this graph than the other two methods and the overall look is slick. This blog post is summarised in a pdf leaflet on the Supplementary Material page. 	 0 Comments
How to upgrade R on windows – another strategy (and the R code to do it)	https://www.r-bloggers.com/2010/04/how-to-upgrade-r-on-windows-%e2%80%93-another-strategy-and-the-r-code-to-do-it/	April 23, 2010	Tal Galili	"Update: In the end of the post I added simple step by step instruction on how to move to the new system.  I STRONGLY suggest using the code only after you read the entire post. If you didn’t hear it by now – R 2.11.0 is out with a bunch of new features. After Andrew Gelman recently lamented the lack of an easy upgrade process for R, a Stackoverflow thread (by JD Long) invited R users to share their strategies for easily upgrading R. In that thread, Dirk Eddelbuettel suggested another idea for upgrading R.  His idea is of using a folder for R’s packages which is outside the standard directory tree of the installation (a different strategy then the one offered on the R FAQ). The idea of this upgrading strategy is to save us steps in upgrading.  So when you wish to upgrade R, instead of doing the following three steps:
1) download new R and install
2) copy the “library” content from the old R to the new R
3) upgrade all of the packages (in the library folder) to the new version of R.
You could instead just have steps 1 and 3, and skip step 2. For example, under windows, you might have R installed on:
C:Program FilesRR-2.11.0
But (in this alternative model for upgrading) you will have your packages library on a “global library folder” (global in the sense of independent of a specific R version):
C:Program FilesRlibrary So in order to use this strategy, you will need to do the following steps – Thanks to help from Dirk, David Winsemius and Uwe Ligges, I was able to write the following R code to perform all the tasks I described   So first you will need to run the following code:
 Then you will want to run, on your old R installation, this: And on your new R installation, this: (Please do not try the following code before reading this post and understanding what it does) In order to move your R upgrade to the new (simpler) system, do the following:
1) Download and install the new version of R
2) Open your old R and run –  (wait until it finishes)
3) Open your new R and run (wait until it finishes)  Once you do this, then from now on, whenever you will upgrade to a new R, all you will need to do only the following TWO (instead of three) steps:
1) Download and install the new version of R
2) Open your new R and run (wait until it finishes)  And that’s it. If you have any more suggestions on how to make this code better – please do share.
(After some measure of review will be given to this code, I would upload it to a file for easy running through “source(…)” ) "	 0 Comments
Some LaTeX Gems – Part 1: TikZ, Loops and more	https://www.r-bloggers.com/2010/04/some-latex-gems-%e2%80%93-part-1-tikz-loops-and-more/	April 23, 2010	Ryan	" This logo means that the blog post is about something I have found interesting, but does not apply directly to the exact purpose of this blog.
 Note: These commands have been tested in pdflatex. I am not sure if they work in other distributions. Over the past couple of months, I have been assisting with editing some papers and also doing some projects in LaTeX. Seeing other peoples’ code has taught me some interesting things. Here are a few of them. Arrays, Lists, and Loops Indeed, it is possible to write loops in LaTeX, with a few limitations. I have used this in some of the following ways Some other ways arrays and loops can be used with some clever programming: Arrays. To create an array in LaTeX, you will need the arrayjob package that can be downloaded from here. Then, to create the array we use the command newarrayArrayName where ArrayName is the name of the array we want to create. Then, we use the readarray{arrayname}{elements separated by &} to actually populate the array with entries. The entries must be separated by ampersands (&) though. If we want to split entries across lines, we just end each line with a percent sign (%). Lists. [just found today] We can also create a list in LaTeX, and this is actually easier than creating an array with arrayjob! We simply use the def command followed by the name of the list and its arguments in {}. Loops. We can iterate through the array or list using a foreach loop from the TikZ package (surprisingly). There are actually a couple of different ways to do this, such as using a counter with a loop. Here I am just going to use the simplest method. As an example, consider some pseudocode to create several midterms, all containing the same questions, but each using a different dataset file. (This is a real example) To do this, we must wrap all of the code that defines the midterm page within the loop, so that the entire midterm is generated on each iteration of the loop, and then use newpage to force a new page for the next iteration. Or, using an array we can access the th element: Figure Directories and Extensions For graphics, we can tell LaTeX to look in a particular directory to find our graphics files. Then, it is no longer necessary to reference the directory Figures/ in the filename passed to includegraphics. For more information about graphicspath, check here. A matter of fact, we can even tell LaTeX what extensions to expect for our graphics files. Below, if we give LaTeX a graphic’s filename without the extension, it will search for files of the given types, in the order that they are passed to the command: Alternate Directories Not all of your LaTeX code needs to be in the same file. In fact, it is much better to spread out your code into logical units such as sections and chapters into different directories. Then, create one master file that references the files for each section or chapter. In the main file, we just call input and pass the path to the corresponding file as an argument. Suppose I have 5 chapters, and each chapter has its own directory containing a LaTeX file and some graphics. We can combine all of these chapters into one document as follows: Some Plots TikZ is an awesome syntax for drawing vector graphics in LaTeX by Till Tantau. It is set of high-level TeX macros for PGF. I must warn readers that this section is for hardcore LaTeX folks because a lot of the syntax is obscure and takes some getting used to. I apologize if my explanation is not incredibly clear…I am new to this as well. I wanted to plot the piecewise function induced by the following discrete probability distribution. The exercise for the students was to generate random numbers from the following probability distribution:  First, we open the tikzpicture environment. Since random uniform numbers range from 0 to 1, the  axis will go from 0 to 1. That is, the domain is from 0 to 1, so we specify domain=0:1. We will also want to magnify the image a bit so we set xscale=2, yscale=2. In this plot, each 0.25 units on the  axis represents 0.1, and each 0.25 units on the  axis represents 0.5 units. Next, we generate the gray grid. There is a command in TikZ called grid that constructs a grid, but I am choosing not to use it for reasons I will explain a bit later. In line 4, we use a foreach loop to generate a series of horizontal lines. This is a special foreach loop in that it uses two counters, separated by a slash (/). The first number will be used as a plotting coordinate (in user defined graphics units) and the second number will serve as a tick label on the axis. These values form a key/value pair if you will. On line 5 we use the draw command to actually draw the lines. The shift parameter tells TikZ to move the pen to a new location (in this case, (0,y)) to begin drawing. We specify that we want thick, dashed lines that are colored gray with 40% opacity. Then, with respect to the pen’s location, we draw a line from (2.5, 0) to (0, 0), where (0, 0) is the location of then pen before drawing the line. Nodes allow us to annotate objects that we draw, and do other cool things. After drawing the line, we want to put a node to the left of the line, and the node is going to contain the text specified in the foreach loop. In this particular example, I want to label the tick marks on the axis. We do this by specifying a node to the left of the line, containing the text in the braces. Since the grid lines are gray, we must explicitly specify that we want the tick labels to be black. Note that the line was drawn from the right endpoint to the left endpoint. If we want to draw the line from left to right, we would place the node statement before we specify the coordinates for the line. Note that I did not include a line at 0. This is for aesthetic purposes so that the grid line does not blend with the  axis line. On lines 6 and 7 I do the same for the vertical lines. On lines 9 and 10, I create some fake vertical lines that all start at (0, u), so I can use the nodes feature to annotate the  axis. If there is a better way to do this, please comment! This time, I pass below as a parameter to node because I want the annotations to print below the line I just drew. Next, on lines 12-15 I draw the axis. Using -> tells TikZ that the right endpoint of the  axis should have an arrow and < -> draws an arrow on both endpoints for the  axis. On line 12 I specify that a node is to be drawn to the left of the line that is about the drawn. The node should contain the number 0 denoting the origin. Then, I draw the axis from (0,0) to (2.5,0) and then draw a node containing the label  to the bottom and right of the axis we just drew. Lines 14 and 15 are similar, but for the  axis. Then we draw the actual function (three lines) on lines 16-18 with respect to the user’s coordinate system. We want to make the lines a little thicker than the grid so that they will stand out. I accomplished this by passing line width=0.5mm as an option to draw. On lines 19-24 We draw the little circles that denote inclusion/exclusion in the domain for each piece of the function. First we specify the fill color, white for exclusion (an open dot), and black for inclusion (a solid dot). Then we specify where the pen should move to, and tell TikZ to draw a circle with a particular radius (0.25mm). The result:  I’m out of breath. You can also plot functions symbolically if you have gnuplot installed. You can download gnuplot for Linux and Windows systems here, and for Mac here. There is a ton more to learn about TikZ syntax. Check out the fairly comprehensive PGF/TikZ manual or the PGF/TikZ gallery at TeXample.net. There is not much out there in terms of documentation yet, but this package is surely going to blow up on the Internet, as R and GRASS GIS have. "	 0 Comments
Because it’s Friday: Four chords, and the truth	https://www.r-bloggers.com/2010/04/because-its-friday-four-chords-and-the-truth/	April 23, 2010	David Smith	  	 0 Comments
R/Finance 2010 … and unicorns	https://www.r-bloggers.com/2010/04/rfinance-2010-and-unicorns/	April 23, 2010	David Smith	At the Information Management blogs, Steve Miller has posted a great roundup of last weekend’s R/Finance 2010 conference in Chicago. Here’s Steve’s overall take:  This year’s conference was even better than the 2009 inaugural, the in-excess-of-200 participants consumed by more than 20 consecutive high-powered presentations over the fast-paced day and a half. And while I’m a quantitative finance welterweight at best, there was plenty to pique my interest, including the latest developments to scale R for size and performance. Check of the rest of Steve’s post for a great review of the other talks at the conference. As Steve mentions, analysis of large data sets was a big focus of the conference with at least six presentations on the topic, including my own. I talked about a research project we’ve been working on at REvolution for a while, to make data processing and statistical analysis techniques for huge data sets available in REvolution R, breaking the bottlenecks of single-CPU processing, slow disk I/O processing, and being limited to RAM on just one machine. I deviated from the pre-advertised title, and the title in the slides, “A Herd of Unicorns” (download as PDF), may require a little explanation out of context. The “unicorn” here is something powerful and (at least today) mythical: the combination of analytic algorithms for really large data sets, and a flexible programming environment that enables modern statistical analysis: exploration, data manipulation, visualization, model evaluation. In other words, the R environment. And if you had the freedom to do large-scale data analysis in R, while making the use of the power of multiple machine in a cluster or in the cloud then that would be, well, a herd of unicorns. We’re working hard to make that fantasy a reality, soon.  Information Management: R/Finance 2010: Applied Finance with R 	 0 Comments
R 2.11.0 just landed…	https://www.r-bloggers.com/2010/04/r-2-11-0-just-landed%e2%80%a6/	April 23, 2010	M. Parzakonis	The new version is here. 	 0 Comments
Top 10 Algorithms in Data Mining	https://www.r-bloggers.com/2010/04/top-10-algorithms-in-data-mining/	April 23, 2010	Stephen Turner		 0 Comments
Trouble with ESS and Sweave	https://www.r-bloggers.com/2010/04/trouble-with-ess-and-sweave/	April 23, 2010	Shige		 0 Comments
Simple Linear Regression	https://www.r-bloggers.com/2010/04/simple-linear-regression-2/	April 23, 2010	Ralph	One of the most frequent used techniques in statistics is linear regression where we investigate the potential relationship between a variable of interest (often called the response variable but there are many other names in use) and a set of one of more variables (known as the independent variables or some other term). Unsurprisingly there are flexible facilities in R for fitting a range of linear models from the simple case of a single variable to more complex relationships. In this post we will consider the case of simple linear regression with one response variable and a single independent variable. For this example we will use some data from the book Mathematical Statistics with Applications by Mendenhall, Wackerly and Scheaffer (Fourth Edition – Duxbury 1990). This data is for a study in central Florida where 15 alligators were captured and two measurements were made on each of the alligators. The weight (in pounds) was recorded with the snout vent length (in inches – this is the distance between the back of the head to the end of the nose). The purpose of using this data is to determine whether there is a relationship, described by a simple linear regression model, between the weight and snout vent length. The authors analysed the data on the log scale (natural logarithms) and we will follow their approach for consistency. We first create a data frame for this study: As with most analysis the first step is to perform some exploratory data analysis to get a visual impression of whether there is a relationship between weight and snout vent length and what form it is likely to take. We create a scatter plot of the data as follows: The scatter plot is shown here: Scatter plot of the weight and snout vent length for alligators caught in central Florida The graph suggests that weight (on the log scale) increases linearly with snout vent length (again on the log scale) so we will fit a simple linear regression model to the data and save the fitted model to an object for further analysis: The function lm fits a linear model to data are we specify the model using a formula where the response variable is on the left hand side separated by a ~ from the explanatory variables. The formula provides a flexible way to specify various different functional forms for the relationship. The data argument is used to tell R where to look for the variables used in the formula. Now that the model is saved as an object we can use some of the general purpose functions for extracting information from this object about the linear model, e.g. the parameters or residuals. The big plus with R is that there are functions defined for different types of model, using the same name such as summary, and the system works out what function we intended to use based on the type of object saved. To create a summary of the fitted model: We get a lot of useful information here without being too overwhelmed by pages of output. The estimates for the model intercept is -8.4761 and the coefficient measuring the slope of the relationship with snout vent length is 3.4311 and information about standard errors of these estimates is also provided in the Coefficients table. We see that the test of significance of the model coefficients is also summarised in that table so we can see that there is strong evidence that the coefficient is significantly different to zero – as the snout vent length increases so does the weight. Rather than stopping here we perform some investigations using residual diagnostics to determine whether the various assumptions that underpin linear regression are reasonable for our data or if there is evidence to suggest that additional variables are required in the model or some other alterations to identify a better description of the variables that determine how weight changes. A plot of the residuals against fitted values is used to determine whether there are any systematic patterns, such as over estimation for most of the large values or increasing spread as the model fitted values increase. To create this plot we could use the following code: We create our own custom panel function using the buliding blocks provided by the lattice package. We start by creating a set of grid lines as the base layer and the h=-1 and v=-1 tell lattice to align these with the labels on the axes. We then create a solid horizontal line to help distinguish between positive and negative residuals. Finally we get the points plotted on the top layer. The residual diagnostic plot is shown below: Residual Diagnostics Plot for the Linear Regression Model The plot is probably ok but there are more cases of positive residuals and when we consider a normal probability plot we see that there are some deficiencies with the model: The function resid extracts the model residuals from the fitted model object. The plot is shown here: Quantile-Quantile Plot for the Linear Regression Model We would hope that this plot showed something approaching a straight line to support the model assumption about the distribution of the residuals. This and the other plots suggest that further tweaking to the model is required to improve the model or a decision would need to be made about whether to report the model as is with some caveats about its usage. I am interested in the thoughts/comments/suggestions from how other people would proceed when faced with this situation – feel free to add in the comments. Related posts: 	 0 Comments
Fun with the Vasicek Interest Rate Model	https://www.r-bloggers.com/2010/04/fun-with-the-vasicek-interest-rate-model/	April 22, 2010	Lee	"A common model used in the financial industry for modelling the short rate (think overnight rate, but actually an infinitesimally short amount of time) is the Vasicek model.  Although it is unlikely to perfectly fit the yield curve, it has some nice properties that make it a good model to work with.  The dynamics of the Vasicek model are describe below.


In this model, the parameters  are constants, and the random motion is generated by the Q measure Brownian motion .  An important property of the Vasicek model is that the interest rate is mean reverting to , and the tendency to revert is controlled by .  Also, this process is a diffusion process, hence Markovian, which will lead to some nice closed form formulas.  Finally, the future value of the interest rate is normally distributed with the distribution .   To give you an idea of what this process looks like, I have generate some sample paths using the Euler discretization method.  One important things you will notice is that this process starts at 0.03, but is pulled towards 0.10, which is the value of . I’ve added dashed lines to highlight this, as well as lines for the expected value of the process and confidence bands representing two standard deviations. Also, you may notice the major problem with this process is that the interest rate can drop below 0%. In the real world, this shouldn’t happen.  The Cox-Ross-Ingersoll model, or CIR model, actually corrects for this, but the process is no longer normally distributed. The following is the R code used to generate the chart above. One can show that a zero coupon bond with a maturity at time T can be found by calculating the following expectation under the risk neutral measure  where the short rate process  can be pretty much any process.  We could estimate this expectation using Monte Carlo simulation, but the Vasicek model allows us to calculate the value of the zero coupon bond by using the Markov property and PDE techniques.  Using this method, we can price the bond by the following  equations.

This closed form solution for a zero coupon bond makes our lives much easier since we don’t need to compute the expectation under the martingale measure to find the price of a bond.  Additionally, it will allow us to easily calculate the yield curve implied by the model.  If we note that  where  is the continuous time yield for a zero coupon bond with maturity T, then we can use our model estimate of the bond price to rearrange this formula and solve for the yield by . Finally, to demonstrate some of the possible curve shapes allowed by the Vasicek model, I produced the following chart.  Notice that the yield curve can be both increasing or decreasing with maturity.  The following R code implements the closed form pricing function for a bond under the Vasicek model, and then uses the pricing function to generate the chart above. Just to make sure the bond pricing formula was implemented correctly, I compared the price using the formula versus the price using Monte Carlo simulation to estimate the expectation under the martingale measure.  Below are the results and R code.  Seems everything is in order, although the Euler discretization method seems to be causing some error in the Monte Carlo results. 

Exact Vasicek Price: 0.9614

MC Price: 0.9623

MC Standard Error: 0.0005

 "	 0 Comments
The Bernoulli factory	https://www.r-bloggers.com/2010/04/the-bernoulli-factory/	April 22, 2010	xi'an	A few months ago, Latuszyński, Kosmidis, Papaspiliopoulos and Roberts arXived a paper I should have noticed earlier as its topic is very much related to our paper with Randal Douc on the vanilla Rao-Blackwellisation scheme. It is motivated by the Bernoulli factory problem, which aims at (unbiasedly) estimating f(p) from an iid sequence of Bernoulli B(p). (The paper only considers functions f valued in [0,1]. In our case, the function is f(p)=1/p.) It appears that this problem has been studied for quite a while, in particular by Yuval Peres. Being in a train to Marseille (thanks to Eyjafjallajökull!), I do not have access to those earlier papers of Peres’, but Latuszyński et al. mentions that there are conditions on f such that it is sufficient to generate a Bernoulli event with probability  where  does not seem to be achievable (Nacu and Peres, 2005). (The way it is rephrased in Latuszyński et al. does not seem correct, though, as they state that f(p)=2p cannot be estimated in an unbiased manner, missing the constraint that the estimator must belong to [0,1], I think.) The paper by Latuszyński et al. develops an original scheme to achieve simulation from B(f(p)) through the simulation of two bounding sequences that are respectively super- and submartingales and that both converge to f(p). (But their simulation scheme does not have to wait for the two sequences to coalesce.) This idea presumably (?) stemmed from the experience of the authors, in particular Gareth Roberts, in perfect sampling, since the most advanced perfect samplers made intensive use of this sandwiching idea of Kendall and Møller (2000, Advances in Applied Probability). The whole thing is also related to the famous Series B paper of Beskos et al. (2006). The method of Latuszyński et al. then builds the upper and lower processes via a truncated series decomposion of f(p), whose availability induces constraints on f. The first application illustrated in Latuszyński et al. is the unbiased estimation of a transform f(p) that has a known series expansion  with  In that case, we could use the scheme of our paper with Randal, estimating  by  The probability of using at least n simulations is then , while the scheme of Latuszyński et al. leads to a probability of  . (Note however that the direct approach above allows to handle any series decomposition, alternating or not, with no constraint on the ‘s.) What I find exciting about this Bernoulli factory problem is that the well-known issue of the absence of unbiased estimators for most transforms of a parameter p (Lehmann and Casella, 1998) vanishes when an unlimited number of iid simulations with mean p is available. Here are the slides of the talk given by Omiros last week at the Big’ MC seminar:  	 0 Comments
New R User Group in San Diego	https://www.r-bloggers.com/2010/04/new-r-user-group-in-san-diego/	April 22, 2010	David Smith	There’s a new local R User Group in San Diego (CA, USA), and they’re meeting tonight. If you’re in the area, why not RSVP and come along? The topic looks great: Our speaker, Scott Wallihan, will be covering how to expand R’s functionality through custom packages. This topic will be covered over two meetings. In our April meeting, we will cover the basics of creating R packages in standard R code, and also in C and C++. In the following meeting, we will cover how to write high-performance R packages using multithreaded C/C++ programs, and also CUDA, enabling R programs to leverage the highly parallel computational power of NVIDEA’s GPU compute devices, including ordinary video cards. Meetup.com: San Diego R Users Group   	 0 Comments
R 2.11.0 released	https://www.r-bloggers.com/2010/04/r-2-11-0-released/	April 22, 2010	David Smith	The latest version of R from the R Project, R 2.11.0, is now available in source code form. Binaries for Windows, Mac and Linux will appear in your local CRAN mirror in the next few days. Some new features include:   This release also marks support of the 64-bit Windows platform by the R Project for the first time. With the assistance of members of the R Core Group. REvolution Computing pioneered support for R on 64-bit Windows: it’s major platform of our REvolution R Enterprise product and has been for almost 2 years. While R 2.11.0 is built using the free mingw compiler, REvolution R Enterprise is built using a commercial toolchain, and links to multi-threaded libraries for improved processing speed on multi-core systems. R Project: R 2.11.0 announcement 	 0 Comments
R 2.11.0 is released!	https://www.r-bloggers.com/2010/04/r-2-11-0-is-released/	April 22, 2010	Paolo Sonego		 0 Comments
The difference between “letters[c(1,NA)]” and “letters[c(NA,NA)]“	https://www.r-bloggers.com/2010/04/the-difference-between-%e2%80%9clettersc1na%e2%80%9d-and-%e2%80%9cletterscnana%e2%80%9c/	April 22, 2010	Tal Galili	"In David Smith’s latest blog post (which, in a sense, is a continued response to the latest public attack on R), there was a comment by Barry that caught my eye.  Barry wrote: Even I get caught out on R quirks after 20 years of using it. Compare letters[c(12,NA)] and letters[c(NA,NA)] for the most recent thing that made me bang my head against the wall. So I did, and here’s the output: Interesting isn’t it?
I had no clue why this had happened but luckily for us, Barry gave a follow-up reply with an explanation.  And here is what he wrote:

My example with ‘letters’ comes from a collision of three features: […] to really understand that letters[c(1,NA)] is different from letters[c(NA,NA)] you have to see that: To make sure I understood Barry correctly, I tried the following code: "	 0 Comments
Free Video Courses on R, Structural Equation Modelling, Causal Inference, and Regression from Uni Jena	https://www.r-bloggers.com/2010/04/free-video-courses-on-r-structural-equation-modelling-causal-inference-and-regression-from-uni-jena/	April 22, 2010	Jeromy Anglim		 1 Comment
R: more plotting fun, this time with the Poisson	https://www.r-bloggers.com/2010/04/r-more-plotting-fun-this-time-with-the-poisson/	April 21, 2010	Matt Asher	" Click on image for a larger version. Here is the code: 
 "	 0 Comments
Automated way to check for PGF version	https://www.r-bloggers.com/2010/04/automated-way-to-check-for-pgf-version/	April 21, 2010	cameron	This is one way to check for the version of PGF that is installed in an automated way.  First create a tex file with the following contents:  Say you named it test-pgf-version.tex.  Then:  should display the version number.  I happen to need to do this from R so using similar logic:  which accomplishes the same task as the second line above. 	 0 Comments
Why use R? Because that’s what the pros use	https://www.r-bloggers.com/2010/04/why-use-r-because-thats-what-the-pros-use/	April 21, 2010	David Smith	I had the great pleasure of sitting down for a beer with Steve O’Grady (from the open-source analyst group RedMonk), at the MySQL conference last week. It was great to get the perspective of someone who knows the tech industry so well, sees predictive analytics as a hot area, and is taking an active interest in statistics and R (Steve has been getting into R programming recently). I asked him, amongst all the software tools available, why choose to learn R for predictive analytics? He answered with a great analogy to scuba diving, which he just shared on his blog: I had the opportunity to dive in a lot of interesting places, from Key West to Cayman Brac to Bonaire to plain old Rockport, MA. One of the things I noticed was that most of the professionals, pretty much to a person, used the same BCD [scuba equipment]: workman-like, beat up Scubapro designs. Ugly, even industrial-looking, but functional. Day after day, dive after dive. Which begged the question that so many ask themselves in so many industries: what did I know about diving that the professionals did not? Exactly. My next BCD, which I still own today, was a Scubapro. I relate this story here because I told it to REvolution Computing’s David Smith last week to explain our interest in the R language. I love this analogy. R today may not be as pretty as some of the alternatives (though we do have big plans for REvolution R), but it sure is functional, reliable and powerful. And that’s why the professionals are using it.  tecosystems: Watch the Professionals  	 0 Comments
Doing Maximum Likelihood Estimation by Hand in R	https://www.r-bloggers.com/2010/04/doing-maximum-likelihood-estimation-by-hand-in-r/	April 21, 2010	John Myles White	Lately I’ve been writing maximum likelihood estimation code by hand for some economic models that I’m working with. It’s actually a fairly simple task, so I thought that I would write up the basic approach in case there are readers who haven’t built a generic estimation system before. First, let’s start with a toy example for which there is a closed-form analytic solution. We’ll ignore that solution and use optimization functions to do the estimation. Starting with this toy example makes it easy to see how well an approximation system can be expected to perform under the best circumstances — and also where it goes wrong if you make poor programming decisions. Suppose that you’ve got a sequence of values from an unknown Bernoulli variable like so: Given the sequence, we want to estimate the value of the parameter, p, which is not known to us. The maximum likelihood approach says that we should select the parameter that makes the data most probable. For a Bernoulli variable, this is simply a search through the space of values for p (i.e [0, 1]) that makes the data most probable to have observed. It’s worth pointing out that the analytic solution to the maximum likelihood estimation problem is to use the sample mean. We’ll therefore use mean(sequence) as a measure of the accuracy of our approximation algorithm. How do we find the parameter numerically? First, we want to define a function that specifies the probability of our entire data set. We assume that each observation in the data is independently and identically distributed, so that the probability of the sequence is the product of the probabilities of each value. For the Bernoulli variables, this becomes the following function: To do maximum likelihood estimation, we therefore only need to use an optimization function to maximize this function. A quick examination of the likelihood function as a function of p makes it clear that any decent optimization algorithm should be able to find the maximum: For single variable cases, I find that it’s easiest to use R’s base function optimize to solve the optimization problem: Here I’ve used an anonymous function that returns the likelihood of our current data given a value of p; I’ve also specified that the values of p must lie in the interval [0, 1] and asked optimize to maximize the result, rather than minimize, which is the default behavior. Examining the output of optimize, we can see that the likelihood of the data set was maximized very near 0.7, the sample mean. This suggests that the optimization approximation can work. It’s worth noting that the objective value is the likelihood of the data set for the specified value of p. The smallness of the objective for large problems can become a major problem. To understand why, it’s worth seeing what happens as the size of the sample grows from 10 to 2500 samples: As you can see, our approximation approach works great until our data set grows, and then it falls apart. This is exactly the opposite of what asymptotical statistical theory tells us should be happening, so it’s clear that something is going very wrong. A quick examination of the results from the last pass through our loop makes clear what’s wrong: The likelihood of our data is numerically indistinguishable from 0 given the precision of my machine’s floating point values. Multiplying thousands of probabilities together is simply not a viable approach without infinite precision. Thankfully, there’s a very simple solution: replace all of the probabilities with their logarithms. Instead of maximizing the likelihood, we maximize the log likelihood, which involves summing rather than multiplying, and therefore stays numerically stable: You can check that this problem is as easily solved numerically as the original problem by graphing the log likelihood: And then you can rerun our error diagnostics using both approaches to confirm that the log likelihood approach does not suffer from the same numerical problems: More generally, given any data set and any model, you can — at least in principle — solve the maximum likelihood estimation problem using numerical optimization algorithms. The general algorithm requires that you specify a more general log likelihood function analogous to the R-like pseudocode below: Then you need to apply multivariable, constrained optimization tools to find your maximum likelihood estimates. This actually turns out to be a hard problem in general, so I’m going to bail out on the topic here. 	 0 Comments
Parallel Multicore Processing with R (on Windows)	https://www.r-bloggers.com/2010/04/parallel-multicore-processing-with-r-on-windows/	April 21, 2010	Tal Galili	"This post offers simple example and installation tips for “doSMP” the new Parallel Processing backend package for R under windows.
*  *  * Update:
The required packages are not yet available on CRAN, but until they will get online, you can download them from here:
REvolution foreach windows bundle
(Simply unzip the folders inside your R library folder) *  *  * Recently, REvolution blog announced the release of “doSMP”, an R package which offers support for symmetric multicore processing (SMP) on Windows.
This means you can now speed up loops in R code running iterations in parallel on a multi-core or multi-processor machine, thus offering windows users what was until recently available for only Linux/Mac users through the doMC package. For now, doSMP is not available on CRAN, so in order to get it you will need to download the REvolution R distribution “R Community 3.2” (they will ask you to supply your e-mail, but I trust REvolution won’t do anything too bad with it…)
If you already have R installed, and want to keep using it (and not the REvolution distribution, as was the case with me), you can navigate to the library folder inside the REvolution distribution it, and copy all the folders (package folders) from there to the library folder in your own R installation. If you are using R 2.11.0, you will also need to download (and install) the revoIPC package from here:
revoIPC package – download link (required for running doSMP on windows)
(Thanks to Tao Shi for making this available!) Once you got the folders in place, you can then load the packages and do something like this: Points to notice: The new R version (2.11.0) doesn’t work with doSMP, and will return you with the following error: Loading required package: revoIPC
Error: package ‘revoIPC’ was built for i386-pc-intel32
 So far, a solution is not found, except using REvolution R distribution, or using R 2.10
A thread on the subject was started recently to report the problem.  Updates will be given in case someone would come up with better solutions. Thanks to Tao Shi, there is now a solution to the problem.  You’ll need to download the revoIPC package from here:
revoIPC package – download link (required for running doSMP on windows)
Install the package on your R distribution, and follow all of the other steps detailed earlier in this post.  It will now work fine on R 2.11.0 Update 2: Notice that I added, in the beginning of the post, a download link to all the packages required for running parallel foreach with R 2.11.0 on windows. (That is until they will be uploaded to CRAN) "	 0 Comments
Little R == r	https://www.r-bloggers.com/2010/04/little-r-r/	April 21, 2010	xi'an		 0 Comments
Experiments with igraph	https://www.r-bloggers.com/2010/04/experiments-with-igraph/	April 21, 2010	nsaunders	"Networks – social and biological – are all the rage, just now.  Indeed, a recent entry at Duncan’s QOTD described the “hairball” network representation as the dominant cultural icon in molecular biology. I’ve not had occasion to explore networks “professionally”, but have always been fascinated by both networks and the tools used to analyse them.  My grasp of graph theory, the mathematics behind networks, is more or less summarised by this Wikipedia page.  I’ve also been exploring the igraph library and thought I’d share a few of my “experiments with igraph”.  As I say, I’m learning myself as I go along, so none of this should be taken as professional advice. Let’s start with my favourite network – FriendFeed of course – and ask a few questions about everyone’s favourite group, The Life Scientists (TLS).

My initial thought was:  to what extent is TLS a community?  In other words, do people who subscribe to TLS also tend to subscribe to one another? 1. Fetching the raw data from FriendFeed
To begin, I used the FriendFeed API to fetch subscribers to TLS and then subscriptions within TLS subscribers.  It’s becoming confusing already – roll the Ruby: Lines 6-15 fetch information about the TLS feed.  A few words about privacy.  If you uncomment line 8 and substitute your FriendFeed user name and remote key, line 9 will make an authenticated request which returns information for users with private feeds, if you are subscribed to them.  However, those people may not want their data exposed to non-subscribers.  So, in lines 11-15, we take user names and only store them in an array if their feed is public.  The upshot of all that is that of 1 323 users that I can access, we use only 1 128 with public feeds. Lines 17-30 process each TLS subscriber and their subscriptions, storing the results in the network array.  Only subscriptions to users who are also subscribed to TLS are stored (lines 24-28), plus the subscription to TLS (line 23).  There’s also a small pause between feeds, just in case we’re hammering the API too hard. At the end, each user-subscription pair is written to a CSV file, one pair per line.  Total = 17 468 lines, some of which look like this: 2. Using igraph in R
Igraph can be used in C, Python, Ruby or R – let’s go with R.  Reading in the file and converting to an igraph object is straightforward.  I’m assuming that the subscriptions graph is directed, in that you subscribing to me has direction you –> me and vice versa, me –> you. Voilà – a graph with 1 129 vertices (nodes) and 17 468 edges (connections).  What can we do with that?  Well, there are a few simple summary metrics, such as graph diameter: That tells us that the average minimum distance between a pair of vertices in the TLS network is 7 people – no surprise there. However, the real meat of igraph comes with methods that describe edges, vertices, subgraphs, communities and so on.  First example:  the largest “clique”.  A clique is a maximally-connected subgraph in which every vertex connects to every other vertex. I know that you want the identity of these individuals, so here’s a quick and dirty plot. The result is the left figure, below; click on the image for a larger version.
Adding properties to vertices and edges is easy, using the V or E functions, respectively.  You can add whatever you like:  for example, V(g)$foo , provided that bar is a vector of length equal to number of vertices.  Here’s an example in which we try to find communities in the TLS network using a model from statistical mechanics called spin-glass [1, 2], label the vertices and change vertex colour and size: Result at far-right. Community detection in the TLS network TLS largest clique subgraph I still have plenty to learn about both igraph and network analysis; hopefully, this is a useful initial exploration. References
1. J. Reichardt and S. Bornholdt (2006)
Statistical Mechanics of Community Detection
Phys. Rev. E (74), 016110 2. M. E. J. Newman and M. Girvan (2004)
Finding and evaluating community structure in networks
Phys. Rev. E (69), 026113 "	 0 Comments
R / Finance 2010 presentations	https://www.r-bloggers.com/2010/04/r-finance-2010-presentations/	April 20, 2010	Thinking inside the box	" As a co-organizer, it was a great pleasure to see so many users of R in Finance—from both industry and academia—come to
Chicago to discuss and share recent work.  There is a lot going on, and it is always good to exchange ideas with others sharing the same
infrastructure.  Participants appeared to enjoy the conference.  My thanks to everybody who helped to put it together, from the
local committee to the helping hands at UIC and of course the sponsors.

 
I just put my slides from the
Extending and Embedding R with C++
tutorial preceding the conference, as well as the RQuantLib: Interfacing QuantLin from R
presentation (with Khanh Nguyen), up onto my 
presentations page.   I do have a usb-drive with all conference
presentations and will provide them via the R / Finance site in a few days.

 
The only truly sour note is the fact that several presenters from Europe had their travels schedules turned
upside down by the disruption to international air travel caused by the Icelandic volcano eruption and the resulting ash clouds.
While we are glad to have had them for a little longer in Chicago, we understand that they are getting eager to return home. I
hope this extended stay in the Windy City does not take away from the overall usefulness of the trip.


 "	 0 Comments
Book Review – ggplot 2: Elegant Graphics for Data Analysis by Hadley Wickham (Springer 2009)	https://www.r-bloggers.com/2010/04/book-review-%e2%80%93-ggplot-2-elegant-graphics-for-data-analysis-by-hadley-wickham-springer-2009/	April 20, 2010	Ralph	   Order this book from Amazon This book is written by the author of the ggplot2 package for R, which is a package with a design inspired by the grammar of graphics and can remove some of the effort required to put together impressive graphs. The book is just under 200 pages and covers a decent range of material to introduce new and experienced R users to the ggplot2 package. The first chapter is a short introduction to the ggplot2 package and discusses how it fits in with the other approaches to creating graphics in R. The second chapter covers usage of the qplot function and is intended to allow people to hit the ground running. As mentioned by the author a large amount of functionality is available through this function and it shields the inexperienced users from the full details of the grammar of graphics. The running example using data on diamond quality covers various common components of a graphical displays that most users would be interesting in reading about. The chapter is a good introductory tour of the facilities available with ggplot2. Chapter three moves from using qplot to the ggplot function for creating graphics and the concept of adding components to the graph by the + operator, and the nice feature that a graph can be saved as an object and added to at a later day piece by piece. The automatic creation of legends is one area where ggplot2 scores highly compared to other graphics system although it will not be clear to novice users how to fine tune various aspects of the display – though it is questionable whether they should be tweaking ever last detail. The next chapter (four) condesense a large amount of information about the layers that are used to make up different display types into a short space via tables. The concepts of geoms and stats are important for the system and the chapter might possible be best read after working through other examples in the book. Chapter five, titled the toolbox, discusses a wide range of graphics that a user might be interested in creating ranging from distributions to surface plots providing a good description and reference point for determining how to create the type of graph of interest. The chapter continues the process of demonstrating how to build plots up layer by layer using the grammar of graphics approach. In chapter six there is a long and detailed coverage of the setting up the axis scales and how to customise them to avoid over plotting with too many numbers of the axes as well as transformations such as logarithms which are commonly used in various applications. The chapter ends with a discussion of the automatic creation of legends by ggplot2 and highlights the fact that not having too much control over the appearance simplifies creation of a legend rather than introducing undesirable restrictions on the user. In most cases the defaults will be sufficient and compared to other graphics approaches in R is something to be applauded. The use of facets is covered in chapter seven which is the equivalent of trellising in the lattice graphics.  There are some good examples provided to show how to create facets with one or two variables and coverage of working with scales and axes over multiple facets. Chapter eight provides a brief summary of the use of themes to determine the visual style of ggplot2 graphs and also discusses exporting graphs to be included in documents outside of R. This chapter might benefit form more examples of customising the themes as this is an area where people might want to know how to do more. In chapter nine there is a short introduction to another package created by the author of ggplot2, the plyr package for manipulating data more easily than with standard R approaches. There is a good example for plotting fitted models on top of data and including confidence limits on the graph as well. The book ends with a short chapter ten providing a few tips on reducing duplication when coding in R and feels slightly out of place. Overall Comment: This is a well presented book that provides a good introduction to the ggplot2 package for R and is a good compliment to the online help provided by the author. 	 0 Comments
Data I/O performance tips	https://www.r-bloggers.com/2010/04/data-io-performance-tips/	April 20, 2010	David Smith	The R tag on StackOverflow recently topped 1000 questions, and continues to be a great community resource for practical tips on using the R language for data analysis and visualization. To take one example, “Efficiency of operations on R data structures” has been answered with some great tips on efficiently getting data in and out of the R system. Here’s three quick tips excerpted from user “doug”‘s response:  See the post linked below for the complete details on how to implement these tips and power up the process of reading data into R. StackOverflow.com: Efficiency of operations on R data structures  	 0 Comments
R and the Next Big Thing	https://www.r-bloggers.com/2010/04/r-and-the-next-big-thing/	April 19, 2010	David Smith	I’ve been travelling for the past few days (for the R/Finance 2010 conference in Chicago), so I’d missed much of the reaction to AnnMaria De Mars’ article last week where she claimed that “R is an epic fail”. Understandably, that inflammatory statement provoked many reactions from the R community on Twitter and in the blogosphere. (I suspect the fact that she was attending a SAS conference when writing the post only added fuel to the fire.) Yihui Xie was the first to bring attention to the article, and Drew Conway followed up with a detailed and well-reasoned response, and Tal Galili provides a great round-up of other responses along with his own commentary. Now that I’m back at my desk and have had a chance to reflect on the post and its responses, it seems like the disconnect is less around what R does, but rather who uses it. According to De Mars, “The vast majority of people are NOT programmers. They are used to looking at things and clicking on things.” And for the most part, I think that’s true: most of the people who use R are statisticians, and there are more non-statisticians than statisticians in the world. And R does have a steep learning curve today. But I think the point that’s being missed here is that different communities have a different concept of what statistical analysis is. It’s almost a generational difference: if your statistical education was based around SAS or SPSS, it seems that your view of statistics is a very procedural one: for this kind of problem, fit that kind of model, and look at these results. That’s a worldview that’s easily accommodated by the procedural nature of SAS programming, or the regimented nature of point-and-click GUIs such as one finds in SPSS. But there’s a newer — and, for the most part, younger — cohort of statisticians out there now, who have a different view of statistics. For them, statistics is less of a set of rigid procedures and more of a fluid process. A process where trying out different data transformations and models is encouraged. Where innovative visualizations of data are created. Where cross-discipline fertilization is commonplace, and models from (say) biostatistics are successfully applied to marketing data. I meet the members of this cohort mainly within industries where innovation is particularly valued: places like Web 2.0 and social-media companies; hedge funds; drug discovery; and marketing optimization at a greater rate than I do in “traditional” venues for statisticians. And most of them have been trained in R. Can these two communities ever be bridged? I think it’s inevitable. The need for data analysts is only going to grow further, but given that the way we think about data analysis and statistics has changed, so the tools we use in the future are going to need to accommodate that change. R’s flexibility (the programming language) and innovation (the exponential growth in add-on packages) driven by the open-source community is what makes it attractive to those organizations where innovation is paramount. (And given competitive forces, innovation in data analysis will soon be paramount for most.)  But it’s true: there will always be more non-programmers than programmers, and even they are going to need a software platform that supports a freer, less procedural style of data analysis. “Statistics 2.0”, if you will. So why can’t R be that platform? What if we could bring the innovation and flexibility of R, and make it available to non-programmers? I think it’s possible: in fact, making R accessible to more users (and therefore customers, natch) has been a focus area for REvolution since its inception. As it happens, I’m looking forward to sharing some exciting developments in an extensible and flexible user-interface for large-scale data analysis that we’ve been prototyping for R. R is a lot more than a programming language, and it will certainly become much more in the future. In fact, I’d second Joe Dunn in saying: “I will not be surprised if in ten years R is the standard for statistical data analysis, much as Linux has supplanted commercial UNIX and gone on to explore territory that its predecessor never touched (look at Ubuntu). R may not be the next big thing, but R is certainly a big thing that is forthcoming.” AnnMaria’s blog: The Next Big Thing   	 0 Comments
A stateful C function for R: parsing Fasta sequences	https://www.r-bloggers.com/2010/04/a-stateful-c-function-for-r-parsing-fasta-sequences/	April 19, 2010	Pierre Lindenbaum		 0 Comments
Converting Alpha-Shapes into SP Objects	https://www.r-bloggers.com/2010/04/converting-alpha-shapes-into-sp-objects/	April 19, 2010	dylan	Just read about a new R package called alphahull (paper) that sounds like it might be a good candidate for addressing this request regarding concave hulls. Below are some notes on computing alpha-shapes and alpha-hulls from spatial data and converting the results returned by ashape() and ahull() into SP-class objects. Note that the functions are attached at the bottom of the page. Be sure to read the license for the alphahull package if you plan to use it in your work. Alpha-Shape Example read more 	 0 Comments
R and Tolerance Intervals	https://www.r-bloggers.com/2010/04/r-and-tolerance-intervals/	April 19, 2010	Ralph	Confidence intervals and prediction intervals are used by statisticians on a regular basis. Another useful interval is the tolerance interval that describes the range of values for a distribution with confidence limits calculated to a particular percentile of the distribution. The R package tolerance can be used to create a variety of tolerance intervals of interest. These tolerance limits, taken from the estimated interval, are limits within which a stated proportion of the population is expected to occur. The function normtol.int from the tolerance package can be used to calculate a tolerance interval for data from a normal distribution. The function arguments include the data itself in a vector denoted x. The confidence level associated with the tolerance interval is specified by alpha, where alpha is the difference between 100% and the confidence level – alpha is 0.05 for 95% confidence. The argument P is the proportion of the data to be included in the tolerance interval. The side argument determines whether a one-sided or two-sided interval is required. Consider a simulated set of data from a manufacturing process loaded into R, stored as vector object obs, as follows: A 95% tolerance interval for 90% of data of this type, based on the 25 observations above is created with this code: The alpha and P are as noted above and the average of the data is reported along with the lower and upper tolerance intervals in this case as we asked for a two-sided interval. This can be easily changed to cover 95% rather than 90% of the data: The package tolerance can create intervals for other data distributions. 	 0 Comments
Estimating Missing Data with aregImpute() {R}	https://www.r-bloggers.com/2010/04/estimating-missing-data-with-aregimpute-r/	April 19, 2010	dylan	" 
Missing Data
Soil scientists routinely sample, characterize, and summarize patterns in soil properties in space, with depth, and through time. Invariably, some samples will be lost or sufficient funds required for complete characterization can run out. In these cases the scientist is left with a data table that contains holes (so to speak) in the rows/columns that are missing data. If the data are used within a regression, missing values in any of the predictor or the response variable result in row-wise deletion– even if 9/10 variables are present. Furthermore, common multivariate methods (PCA, RDA, dissimilarity metrics, etc.) cannot effectively deal with missing data. The scientist is left with a couple options: 1) row-wise deletion of cases missing any variable, 2) re-sampling or re-characterizing the missing samples, or 3) estimating the missing values from other variables in the dataset. This last option is called missing data imputation. This is a broad topic with countless books and scientific papers written about it. Here is a fairly simple introduction to the topic of imputation. Fortunately for us non-experts, there is an excellent function (aregImpute()) in the Hmisc package for R. read more "	 0 Comments
R tip: Maximum screen width	https://www.r-bloggers.com/2010/04/r-tip-maximum-screen-width/	April 19, 2010	Stewart MacArthur		 0 Comments
Example 7.33: Specifying fonts in graphics	https://www.r-bloggers.com/2010/04/example-7-33-specifying-fonts-in-graphics/	April 19, 2010	Ken Kleinman		 0 Comments
Getting your web application and R(Apache) to talk to each other	https://www.r-bloggers.com/2010/04/getting-your-web-application-and-rapache-to-talk-to-each-other/	April 19, 2010	nsaunders	"Here’s the situation.  Web applications, built using a framework (e.g. Rails, Django) are great for fetching data from a database and rendering it.  They’re not so great for crunching and charting the data.  Conversely, R is great for crunching and charting, but doesn’t make for a great web application.  Index view for values As a first step in understanding all of this, we can build a small demo application using Rails (version 2.3.5), which serves both JSON and CSV.  We’ll see if we can get that into R, then see if R can return results back to Rails, via RApache.  Baby steps, so we’ll avoid the AJAX stuff for now and just use Rails rendering methods to serve JSON from a controller.

1. Generate the Rails application
First, generate the skeleton.  I’m calling the application “rapache” and scaffolding for a simple model, named Value, containing 2 integer fields named “x” and “y”: Next, edit config/environment.rb to contain these lines, required for Rails to render CSV: Install those gems if you don’t have them and optionally, unpack them into your application tree.  I like to freeze the Rails gems too: We also need to edit models/values.rb to provide the comma method: So far, so good.  Now, we could seed the database with some test data but for just a few values, it’s as easy to use the forms generated by the scaffolding and enter some X and Y values.  I made my Y = X, fired up ./script/server and then opened up “http://localhost:3000/values” in the browser, where I saw a view like that in the image, above-right in this blog post. Generating the JSON and CSV end-points couldn’t be easier.  Just edit the index method in controllers/values_controller.rb to look like this: That gives us JSON in the browser when we go to “http://localhost:3000/values.json” and a CSV file when we go to “http://localhost:3000/values.csv”.  Depending on your browser setup, the browser may download the CSV, offer to download it or offer to open it in a spreadsheet application. 2. Experiments in R
With the Rails development environment still running, I opened an R console.  First, fetching the data from the Rails application via JSON: Not too bad.  The warning message arises because readLines() can’t detect an end of line character; the message could be hidden using suppressWarnings().  We could use either rjson or RJSONIO – both of them have the fromJSON() method, but neither can read directly from a URL connection, hence the requirement for readLines(url(…)).
R read the JSON into a list of lists.  That could be transformed into a dataframe, but it’s a little unwieldy.  Let’s have a look at the CSV approach: Perfect – read.csv() can read the Rails-rendered CSV directly into a data frame. 3. Calling R from within Rails and displaying the results
The last step is to call R and retrieve results from within the Rails application.  For this, you’ll need to have installed RApache.  That’s beyond the scope of this post – it’s not difficult, under Ubuntu at least.  I assume that R scripts will be served from /var/www/R, which is configured using the Apache configuration file /etc/apache2/conf.d/rapache.conf: What we should really do is write a Rails controller method to retrieve the appropriate data and then write a generic R function to accept CSV from any URL and return the results.  However, for demonstration purposes, I’m just going to write a test R script, /var/www/R/plot.R, to plot a scatterplot of the X versus Y data and then wrap the result inside a Rails image_tag.  Here’s the R: And here’s the Rails – I just added this to the bottom of views/values/index.html.erb: The result – see image, right. Index view for values + plot from RApache "	 0 Comments
Thoughts on LSPM from R/Finance 2010	https://www.r-bloggers.com/2010/04/thoughts-on-lspm-from-rfinance-2010/	April 18, 2010	Joshua Ulrich	"
 "	 0 Comments
Sudokus more random than random!	https://www.r-bloggers.com/2010/04/sudokus-more-random-than-random/	April 18, 2010	xi'an	Darren Wraith pointed out this column about sudokus to me. It analyses the paper by Newton and De Salvo published in the Proceedings of the Royal Academy of Sciences A that I cannot access from home. The discussion contains this absurd sentence “Sudoku matrices are actually more random than randomly-generated matrices” which shows how mistreated the word “random” can be! The point is absurd because any random sequence is random, no more, no less than others. It is only the distribution behind the randomness that changes. I presume the author of the column interpreted a large entropy value as a sign of higher randomness and I would guess the authors did not make this claim. Although we are talking about distributions over a finite set, the uniform distribution over the set of all 1,2,…,9 valued matrices has simply nothing to do with the uniform distribution of the set of all sudoku matrices. Up to a factor , the later set is negligible within the first one. (It is impossible to create a sudoku by pulling 81 integers at random.) Therefore, comparing the entropies of both distributions hardly makes sense… This other remark in the column “Newton and DeSalvo predict that this greater understanding of Sudoku could lead to better Sudoku-generating algorithms that create more difficult puzzles. Currently, Sudoku puzzles require at least 17 numbers to be given in their correct boxes in order for the puzzle solver to find a unique solution. The new study could decrease that number, making it more difficult to solve the puzzles.” does not seem more relevant. Why should the randomness of sudokus be related to their difficulty or to the minimal number of entries for a unique solution? 	 0 Comments
Summarising data using scatter plots	https://www.r-bloggers.com/2010/04/summarising-data-using-scatter-plots/	April 18, 2010	Ralph	A scatter plot is a graph used to investigate the relationship between two variables in a data set. The x and y axes are used for the values of the two variables and a symbol on the graph represents the combination for each pair of values in the data set. This type of graph is used in many common situations and can convey a lot of useful information. To illustrate creating a scatter plot we will use a simple data set for the population of the UK between 1992 and 2009. This data is saved in a data frame uk.df using the following command: For this example the data is recorded in thousands to make the graph easier to read and there is no benefit or noticeable improvement to be seen by using greater detail. Base Graphics In the base graphics system the general purpose plot function can be used to create a scatter plot for the UK population data set that we created. The first two arguments to the plot function are the x and y variables respectively. The following code will create a scatter plot, including various labels: The labels for the x and y axes are specified via the xlab and ylab arguments to the plot function and the main argument specifies the title for the plot. Base Graphics Histogram The graph itself is plain and functional which solid circles indicating the population (in thousands) for each of the years covered by the data. Lattice Graphics The lattice graphics package provides a function xyplot specifically to create scatter plots and the function is used in a similar way to the base graphics approach. The first argument to the function is a formula describing the relationship to be plotted on the graph, with the y variable preceding the x variable as we are used to when describing mathematical fomula such as y=a+bx. The data frame is specified with the data argument to simplify the expression in the formula. The code used is as follows: The axis labels and the overall title for the graph are specified in the same way as the base graphics system. We indulge in some fine tuning of the labels on the x axis via the scales argument – here we indicate that every second year should be included on the label starting in 1992 and running until 2009. The lattice graph is shown here for comparison with the graphs created using the other two packages: Lattice Graphics Scatter Plot There are very few visual differences between the lattice and base graphics. In lattice graphics an object is created that can be edited to add or remove components and then printed to the screen. This approach is more flexible than the base graphics where the components are painted on top of each other and the use of themes in lattice will make it easier to keep a consistent look to all graphs in a document. ggplot2 In the ggplot2 package the ggplot function is used to create graphs of all types rather than having a separate function defined for each type of graph. The first argument is adata frame with the data to be plotted and the aes argument specifies the aesthetics associated with the graph such as the point symbol, size or colour. In this case the Year variable appears on the x axis and the Population variable on the y axis. The code to create the scatter plot is shown here: The geom_point specifies the type of graph to create (a scatter plot in this situation and this highlights the flexibility of the ggplot2 package as changing the geom will create a new type of graph) and the labels for the graph are created by adding them to the graph with the xlab, ylab and opts functions. The graph is shown below: ggplot2 Scatter plot This graph is not greatly different to the scatter plot created using the base and lattice packages. The default theme in the ggplot2 package has a gray background with white grid lines that allows easy visual recognition of graphs created using this package. This blog post is summarised in a pdf leaflet on the Supplementary Material page. 	 1 Comment
alphahull: an R Package for Alpha-Convex Hull	https://www.r-bloggers.com/2010/04/alphahull-an-r-package-for-alpha-convex-hull/	April 16, 2010	Yihui Xie	 alpha-convex hull with different alpha's The above animation can be reproduced with the code below (uncomment the lines to create a GIF animation with the animation package): Again, you may interactively play with the convex hull using the gWidgets package: 	 0 Comments
Significant Figures in R and Rounding	https://www.r-bloggers.com/2010/04/significant-figures-in-r-and-rounding/	April 16, 2010	Neil Gunther		 0 Comments
Simulating Dart Throws in R	https://www.r-bloggers.com/2010/04/simulating-dart-throws-in-r/	April 16, 2010	JD Long	Back in November 2009 Wired wrote an article about some grad students who decided to try to stochastically model throwing darts. Because I don’t actually read printed material I didn’t see the article until a couple of months ago. My immediate thought was, “hey, I drink beer. I throw darts. I build stochastic models. Why haven’t I done this?” Well we all know why I haven’t done this. I have a job and a 2 year old daughter and I like my wife. Well a funny thing happened a few weeks ago. I sat down and was thinking about this problem and then 5 hours later I had a working dart simulator in my text editor. I don’t remember writing this. So Occam’s Razor says that the most likely explanation is the simplest explanation. So clearly I was abducted by aliens and someone broke into my office and built a dart simulator. I do reinsurance modeling to pay the bills and it immediacy hit me that this type of modeling is very similar to what I do for work. This similarity became the impetus for my presentation at R in Finance 2010 which starts today. I dumped the dart board code into a github gist which can be found here: If the embedded code is not showing up, you can get to it directly on Github. 	 0 Comments
R Command Line	https://www.r-bloggers.com/2010/04/r-command-line/	April 16, 2010	Lee	I am an R user! And I see a whole army of R users, here in defiance of tyranny. You’ve come to use R as free men… and free men you are. What will you do with that freedom? Will you use R?  Use R and you may use the command line. Use SAS, and you’ll use a GUI… at least for a while. And dying in your beds, many years from now, would you be willing to trade ALL the GUIs, from this day to that, for one chance, just one chance, to come back here and tell our enemies that they may take our lives, but they’ll never take… OUR COMMAND LINE! The Next Big Thing  	 0 Comments
Because it’s Friday: When infographics go bad	https://www.r-bloggers.com/2010/04/because-its-friday-when-infographics-go-bad/	April 16, 2010	David Smith	Phil Gyford laments crappy infographics swamping the good:   Phil Gyford on Flickr: Infographic (used under Creative Commons license)  	 0 Comments
R – not the epic fail we thought	https://www.r-bloggers.com/2010/04/r-not-the-epic-fail-we-thought/	April 16, 2010	John Johnson		 0 Comments
Rcpp 0.7.12	https://www.r-bloggers.com/2010/04/rcpp-0-7-12/	April 16, 2010	Thinking inside the box	"
This is another bug-fix version related solely to a build failure on
Windows. Trying to protect paths with spaces has the side-effect of breaking
backticks use, which unfortunately is already in use by a number of package
that since broke during CRAN autobuilds.  No other changes were made.
 
 
As always, full details are in 
Rcpp Changelog page and the 
Rcpp page which also
leads to the downloads, the 
browseable
doxygen docs and zip files of doxygen output for the standard formats.
A local directory  has
source and documentation too. 
Questions, comments etc should go to the
rcpp-devel mailing list
off the R-Forge page

 "	 0 Comments
An article attacking R gets responses from the R blogosphere – some reflections	https://www.r-bloggers.com/2010/04/an-article-attacking-r-gets-responses-from-the-r-blogosphere-%e2%80%93-some-reflections/	April 16, 2010	Tal Galili	"In this post I reflect on the current state of the R blogosphere, and share my hopes for it’s future. *  *  * I am very grateful to Dr. AnnMaria De Mars for writing her post “The Next Big Thing”.
In her post,  Dr. De Mars attacked R by accusing it of being “an epic fail” (in being user-friendly) and “NOT the next big thing”.  Of course one should look at Dr. De Mars claims in their context.  She is talking about particular aspects in which R fails (the lacking of a mature GUI for non-statisticians), and had her own (very legitimate) take on where to look for “the next big thing”.  All in all, her post was decent, and worth contemplating upon respectfully (even if one, me for example, doesn’t agree with all of Dr. De Mars claims.) But Dr. De Mars post is (very) important for a different reason.  Not because her claims are true or false, but because her writing angered people who love and care for R (whether legitimately or not, it doesn’t matter).  Anger, being a very powerful emotion, can reveal interesting things.  In our case, it just showed that R bloggers are connected to each other. So far there are 69 R bloggers who wrote in reply to  Dr. De Mars post (some more kind then others), they are: This is good news, since it shows that R has a community of people (not “just people”) who write about it.
In one of the posts, someone commented about how R current stage reminds him of how linux was in 1998, and how he believes R will grow to be amazingly dominant in the next 10 years.
In the same way, I feel the R blogosphere is just now starting to “wake up” and become aware that it exists.  Already 6 bloggers found they can write not just about R code, but also reply to does who “attack” R (in their view).  Imagine how the R blogosphere might look in a few years from now… I would like to end with a more general note about the importance of R bloggers collaboration to the R ecosystem.  In his wonderful (3 minutes) TED talk, Derek Sivers talks about “how to start a movement”:  One of his most interesting conclusions (in my opinion) is that a movement is not just thanks to it’s leader, but also (if not more) thanks to it’s first followers – the one who make the first guy a leader. The implication of that is that if you are a bloggers, and you find someones work (articles) worth while – “follow them”.  Write about it (in twitter/facebook or your own blog), support that blogger by commenting.  Doing that will only strengthen the impact of the thing you care about. I think and believe that Bloggers collaboration is synergistic.
Bloggers who cross link to each other gain more respect (and thus, traffic and influence) from both search engines (e.g: google) and the traditional media.
Bloggers coming together, supporting each other with their words, can sometimes make a “news story” suddenly important for the media to report.
I hope as time will progress, we will have a more interconnected R blogosphere.  One that will enable the R community to reach a wider circle of people and influence (in the public and private sector). In conclusions the case of the bloggers reply to Dr. De Mars article is (I believe) a sign to whats coming a head – and I feel very optimistic about it   "	 3 Comments
The Next Big Thing: SAS and SPSS!…wait, what?	https://www.r-bloggers.com/2010/04/the-next-big-thing-sas-and-spss%e2%80%a6wait-what/	April 15, 2010	Drew Conway	"Thanks to the R Bloggers aggregator I came across Yihui Xie’s post on a piece currently making the rounds about statistical analysis platforms.  In  The Next Big Thing, AnnMaria De Mars makes the argument that R—as a statistical computing platform—is not well suited for what she views as the next big things in data analytics: dealing with very large data sets, and creative visualization.  She goes so far as to say that in this respect, R is an epic fail (emphasis below mine): Contrary to what some people seem to think, R is definitely not the next big thing, either. I am always surprised when people ask me why I think that, because to my mind it is obvious…I know that R is free and I am actually a Unix fan and think Open Source software is a great idea. However, for me personally and for most users, both individual and organizational, the much greater cost of software is the time it takes to install it, maintain it, learn it and document it. On that, R is an epic fail. It does NOT fit with the way the vast majority of people in the world use computers. The vast majority of people are NOT programmers. They are used to looking at things and clicking on things. I have edited out a bit of R code that De Mars uses to illustrate her points; first because the code itself has nothing to do with either big data or creative visualization; and second, it contains errors and does not run.  This, however, is rather beside the point.  De Mars’s main point is that commercial statistical platforms, such as SAS, STATA and SPSS, are better suited to handle large data and visualization.  Where to begin… First, yes, R is a difficult language to learn.  Even for those that have an extensive programming background its syntactical peculiarities and functional foundation can be a difficult hurdles to climb for many.  That said, R is a Turing complete language, which means once you learn the language,  data analytics are bounded only by your imagination (and NP-completeness).  To De Mars’s point then, it would seem a tautological fallacy that endeavors into the “next big thing” would be best nurtured in the fully limited environment of point-and-click commercial analysis platforms.  Byron Ellis summarized this sentiment quite nicely on Twitter: [R] is for making new things. Point and click is for redoing old things. I often need to make new things to analyze my data. Well put.  R allows users to build their own methods for analysis and feast on an ever expanding catalog of libraries for any number of analytical needs, commercial products provide users with the set of functionality they deem fit.  Next, with respect to the specific big things De Mars is concerned with—big data and visualization—R appears to be the hands down winner. Brendan O’Connor provided an excellent, though somewhat dated now, side-by-side comparison of several open-source and commercial data analytic platforms.  One his main complaints about all of these platforms is that they cannot handle data sets that do not fit on a single hard drive, i.e., really big.  Since his writing, however, R now supports computing over a cluster with MPI or SNOW, and streaming to various map/reduce frameworks such as Hadoop.  In addition, the sqldf library enhances R to manipulate large relational databases. Regarding R specifically, O’Connor notes that one of its main weaknesses is, “visualizations are great for exploratory analysis, but you want something else for very high-quality graphs.”  Now, however, there are several libraries that empower users to create extremely high-quality, and publication ready visualization.  To the latter, both lattice and ggplot2 provide unique visualization power, and are fully extensible to the needs of their users—again, unlike the commercial platforms.  In addition, R can be extended to work with Processing to generate extremely high-end interactive visualization.  There are also several libraries that allow R to generate web-ready visualization with protovis.  Some commercial platforms are able perform high-performance computing, but to my knowledge, none have the flexibility and quality of visualization as R. I am wholesale perplexed by De Mars’s argument.  While most software users are more comfortable with GUI platforms, it seems entirely unlikely that the next big data analysis “thing” would come from a world catering to the lowest-common denominator.  While clearly I am biased, that bias comes from experience on all of these platforms and dealing with problems of big data and visualization. For those looking for the next big thing, I highly recommend following the adoption of R and its relatives, and spending very little concerned with the commercial platforms. Finally, Matt Blackwell of the Social Science Statistics Blog has also weighed in, and I recommend his post.  UPDATE: Others have weighed in as well: Tal Galili – “The next big thing”, R, and Statistics in the cloud
Joe Dunn – R Is (Not) the Next Big Thing Photo: Social Media Law Student "	 0 Comments
Solving optimization problems numerically in R with optim()	https://www.r-bloggers.com/2010/04/solving-optimization-problems-numerically-in-r-with-optim/	April 15, 2010	VCASMO - drewconway		 0 Comments
Saving the world with R	https://www.r-bloggers.com/2010/04/saving-the-world-with-r/	April 15, 2010	David Smith	Tuesday’s meeting of the Bay Area R UseR Group at the LinkedIn offices was a great event. The headline speaker was Joe Adler, author of the excellent R reference manual, R in a Nutshell. Joe’s presentation was an in-depth look at the relative speed of various options in R for looking up values from a key in a key-value pair sequence. One of the simplest ways of doing this is to assign a names to a vector with the names function, and then you can look up the value associated with name “Australia” (if, say, you’d named your vector population with country names) with the statement population[“Australia”] or population[[“Australia”]]. With simulations, Joe revealed that the latter version is a bit faster than the former, but both have lookup times proportional to the length of the vector (i.e. lookups get slower for longer vectors). An faster (but more complicated) option is to use environments, where lookups can be done via a hash table in constant time. Joe provides all the details in this blog post. He did offer some good advice: even though the simpler constructs are slower, in general it’s best to program for clarity, and only optimize for speed when really necessary. A surprise addition to the program — and a very pleasant surprise, at that — was a lightning talk from Megan Price at Benetech. Benetech is a non-profit organization contracted by the likes of Amnesty International and Human Rights Watch to answer thorny geopolitical questions through the use of data and science. For example: “Were acts of genocide committed against the Mayan people in Guatemala?” (the answer, sadly, is yes.) Megan opened her talk by saying that she “uses R to save the world” — and I think the was only half-joking. As Megan explained in a fascinating presentation, using statistical techniques to address these questions can “transform emotional political debates into debates about methodology and science”. Specifically, she uses Multiple Systems Estimation techniques (from the rcapture package) to count things that are otherwise difficult to quantify. The process is related to capture-recapture methods used, for example, to count the number of animals in the wild. Megan also offered some great advice for creating scientific reports with R: her process of building reports dynamically with xtable and Sweave means that when something changes at the last minute — and “something always changes at the last minute!”, says Megan — it’s a simple process to recreate the report to incorporate the changes, without having to reformat and cut-and-paste everything together again. Bay Area UseR Group: April 13 2010   	 0 Comments
R is an Epic Fail?	https://www.r-bloggers.com/2010/04/r-is-an-epic-fail/	April 15, 2010	Yihui Xie	I came across this blog post just now: The  Next Big Thing, and of course these words caught my attention: […] However, for me personally and for most users, both individual and organizational, the much greater cost of software is the time it takes to install it, maintain it, learn it and document it. On that, R is an epic fail. I don’t really understand how (much more?) difficult will it be to install and maintain R. Usually it takes about one minute to install it from the binary (and SAS? SPSS? buy it, find a technician, install it, maintain according to different licenses – single PC or server or other types, continue to pay only tens of thousand dollars next year, …). For learning, it depends. I don’t think it is too difficult for people who know well about statistics, and for the rest of people, do they really feel safe to do something they do not understand? For the documentation, some people prefer simple ones and some prefer handbooks (of SAS-style). In all, I cannot see why R is an epic fail for the above reasons… What? Data visualization?… The R community must have been tired of comparing SAS with R. Please don’t tell Prof Frank Harrell about this post… 	 0 Comments
I’d be more than happy with the unlinked data web	https://www.r-bloggers.com/2010/04/i%e2%80%99d-be-more-than-happy-with-the-unlinked-data-web/	April 14, 2010	nsaunders	Visit this URL and you’ll find a perfectly-formatted CSV file containing information about recent earthquakes.  A nice feature of R is the ability to slurp such a URL straight into a data frame: I hear a lot about the “web of data” and the “linked data web” but honestly, I’ll be happy the day people start posting data as delimited, plain text instead of HTML and PDF files. 	 0 Comments
Zelig and Matching in R with an Application to Conflict and Leader Tenure	https://www.r-bloggers.com/2010/04/zelig-and-matching-in-r-with-an-application-to-conflict-and-leader-tenure/	April 14, 2010	VCASMO - drewconway		 0 Comments
Lots of new Videos in Rchive	https://www.r-bloggers.com/2010/04/lots-of-new-videos-in-rchive/	April 14, 2010	Drew Conway	I have just uploaded a bunch of new videos the Rchive (yea, that’s what I am calling it now).  Most of the videos are from the April NYC meetup, which include the following talks: As an added bonus, I came across the original full lenfth video of Andrew Little’s talk on Zelig and Matching in R from the August 2009 meetup, so I added it the Rchive (said it again)—enjoy! 	 0 Comments
Portfolio Correlation Analysis Tool	https://www.r-bloggers.com/2010/04/portfolio-correlation-analysis-tool/	April 14, 2010	VCASMO - drewconway		 0 Comments
Biomarker detection in cancer (gene expression analysis)	https://www.r-bloggers.com/2010/04/biomarker-detection-in-cancer-gene-expression-analysis/	April 14, 2010	VCASMO - drewconway		 0 Comments
New York Pizza – How to Find the Best	https://www.r-bloggers.com/2010/04/new-york-pizza-how-to-find-the-best/	April 14, 2010	VCASMO - drewconway		 0 Comments
Object Oriented Programming with R: My notebook	https://www.r-bloggers.com/2010/04/object-oriented-programming-with-r-my-notebook/	April 14, 2010	Pierre Lindenbaum		 0 Comments
Slides from High-Performance Analytics webinar now available	https://www.r-bloggers.com/2010/04/slides-from-high-performance-analytics-webinar-now-available/	April 14, 2010	David Smith	Thanks to everyone who attended the webinar I presented this morning, High-Performance Analytics with REvolution R and Windows HPC Server. My slides are now available for download at the link below; even if you’re not using Windows, I hope the slides are a useful introduction to the foreach parallel programming construct in general. If you do use R on Windows, there’s some new material on using the new doSMP package (included in current versions of REvolution R Community and REvolution R Enterprise) for local parallel processing on multi-core workstations, and a guide for distributing those computation to a Windows HPC Server cluster using the doNWS package (included with REvolution R Enterprise). In addition to the slides, a replay of the webinar with audio (in Windows WMV format[*]) will appear in the next couple of days at the link below. REvolution Events: High-Performance Analytics with REvolution R and Windows HPC Server [*] Apologies to folks not on Windows for whom that format is a pain – it’s an artifact of the NetMeeting software we used for the webinar, which also caused audio problems for some viewers. We’ll return to using GoToWebinar (with cross-platform support) for future webinars. 	 0 Comments
Get at least 12 observations before making a confidence interval?	https://www.r-bloggers.com/2010/04/get-at-least-12-observations-before-making-a-confidence-interval/	April 14, 2010	dan	"GET CONFIDENT ABOUT YOUR INTERVALS  Decision Science News is happy with its purchase of Statistical Rules of Thumb by Gerald van Belle many years ago. It’s full of examples in which math can surprise.  The first example in the book is titled “use at least 12 observations in constructing a confidence interval”. When people first hear this they think, nonsense, there’s nothing magic about the number twelve.  And then they think that confidence interval sizes have to do with the square root of the sample size, but that still doesn’t do it. Thinking harder, one realizes that the half-width confidence interval for a sample of size n is t(n-1,1-alpha)/sqrt(n). One plots this out for 90% and 95% CIs and one sees that the first intuition was right, there is nothing magic about 12, but the plot above sure does seem to stop dropping in width somewhere around there.  Maybe 15 is a safer number. To make it easier to see, here are the points on the above graph from the value 15 and greater.  We love heuristics for statistics, but do not promote following rules of thumb without reflection. We do promote playing with such rules of thumb as a way to become aware of the tradeoffs one makes in designing experiments. To encourage such play, we post the R code behind the above graphs here. R CODE
(Don’t know R yet? Learn by watching: R Video Tutorial 1, R Video Tutorial 2) 
n=seq(3,30,.1)
alpha=.1
y90=qt(1-alpha/2,n-1)/sqrt(n)
alpha=.05
y95=qt(1-alpha/2,n-1)/sqrt(n) plot.new()
plot(n,y90,type=”l”,xlim=c(0,30),ylim=c(0,3),ylab=”Half-Width Confidence Interval Size”, xlab=”Sample Size”)
lines(n,y95,type=”l”)
text(15,y95[which(n==15)]+.15,labels=”95%”)
text(15,y90[which(n==15)]-.15,labels=”90%”) #second plot
plot.new()
a=min(which(n>=15))
b=max(which(n>=15))
plot(n[a:b],y90[a:b],type=”l”,xlim=c(0,30),ylim=c(0,3),ylab=”Half-Width Confidence Interval Size”, xlab=”Sample Size”)
lines(n[a:b],y95[a:b],type=”l”)
text(15,y95[which(n==15)]+.15,labels=”95%”)
text(15,y90[which(n==15)]-.15,labels=”90%”)
 Update: After Arjan’s comment, I tried to figure out if Van Belle is Dutch. I didn’t figure that out, but I did learn that he keeps a lot of these tips on his site. There’s even one on the 12 observation rule and some information added by others, including this figure:  "	 0 Comments
“The next big thing”, R, and Statistics in the cloud	https://www.r-bloggers.com/2010/04/%e2%80%9cthe-next-big-thing%e2%80%9d-r-and-statistics-in-the-cloud/	April 14, 2010	Tal Galili	"A friend just e-mailed me about a blog post by Dr. AnnMaria De Mars titled “The Next Big Thing”. In it Dr. De Mars wrote (I allowed myself to emphasize some parts of the text): Contrary to what some people seem to think, R is definitely not the next big thing, either. I am always surprised when people ask me why I think that, because to my mind it is obvious. […]
for me personally and for most users, both individual and organizational, the much greater cost of software is the time it takes to install it, maintain it, learn it and document it. On that, R is an epic fail. It does NOT fit with the way the vast majority of people in the world use computers. The vast majority of people are NOT programmers. They are used to looking at things and clicking on things. Here are my two cents on the subject:
 First, I agree with Dr. De Mars that R (out of the box) is not very (non programmer) user friendly – there is (almost) no point and click capabilities.  And while there are several projects offering a GUI layer interface to R (a good list of them can be found here), still non of them is in the level of refinement of what softwares like SPSS, JMP or SAS offers to users today. But is traditional “point and click” the next “big thing”?  My suspicion is that the answer is – no.
Neither does Dr. De Mars thinks so, since her predictions for the next big thing are “Data visualization” and “Analyzing enormous quantities of unstructured data”.  Both of which R is offering quite powerful solutions to (assuming that you will go through the learning curve). Dr. De Mars question is a fascinating one – what IS going to be the next big thing? I think that the next BIG thing is (becoming to be) “Statistics in the Cloud“.  This intuition came from (among other things) my review of the “Future of Open Source” Survey (see “conclusion 3″). In the near future, I believe, we will see more statisticians and data analysts tapping into the opportunities that cloud computing offers them.  Here are some examples of what I came a cross (or covered) lately in the topic of cloud computing and R: All of these are well connected to the emerging trend of “web of data”/“linked data web” that some are talking about. For example, here is a good Ted talk by Tim Berners-Lee (the inventor of the World Wide Web). Talking about building a web for open, linked data that could do for numbers what the Web did for words, pictures, video: unlock our data and reframe the way we use it together.

The same plea is given by Hans rosling in his famous Ted talk showing GapMinder.
Although at he same time, some R users are saying – “You don’t have to bother linking the data. I’ll do with just the data, really, just release it…” In conclusion, I don’t know what capabilities other projects/products offer for doing statistics in the cloud.  But it is clear to me that the R community is (not surprisingly) bringing very diverse and innovative solutions to the world. Is R the next big thing?  I don’t think so.  But I do think that some of the next big things will be built with R.
*  *  *
I would love to know your thought about Dr. De Mars post, and also about what the “next big thing” is going to be (and what role will R have in it). "	 0 Comments
R: parallel processing using multicore package	https://www.r-bloggers.com/2010/04/r-parallel-processing-using-multicore-package/	April 14, 2010	Stewart MacArthur		 0 Comments
Plotting “time of day” data using ggplot2	https://www.r-bloggers.com/2010/04/plotting-%e2%80%9ctime-of-day%e2%80%9d-data-using-ggplot2/	April 14, 2010	nsaunders	"William asks: 
How can I make a graph that looks like this, “tweet density” style, showing time intervals?
 He then helpfully describes his input data:  a CSV file with headers “time started, time finished, date”.

Here’s a simple CSV file, tasks.csv: Read into R, calculate the weekday and reorder the weekday factors from Sunday, Monday…to Saturday: Convert the start and end times to decimal hours.  I’m not very familiar with the as.POSIX… functions, so I’m sure that there’s a more elegant way to do this: We’re going to plot task duration as a horizontal rectangle.  If there is more than one task per day, we need to offset the rectangle vertically, so as they don’t overlap. Finally, call ggplot with the rectangle geom plus a bunch of options to colour the rectangles (by task), facet the plot (by day) and clean up, rescale and label the axes: Result:
 "	 0 Comments
formatR: farewell to ugly R code	https://www.r-bloggers.com/2010/04/formatr-farewell-to-ugly-r-code/	April 13, 2010	Yihui Xie	It is not uncommon to see messy R code which is almost not human-readable like this: Apparently it is pain reading unformatted R code, but on the other hand, it is natural for us to be lazy. I don’t care about adding spaces or indent to my raw R code — I’ll concentrate on programming first and format my code later. The R package ‘formatR‘ is intended to help us format our messy R code. Two lines of R code will show you the graphical interface of formatR: Then you can either paste your code into the text box or click the “Open” button to open an existing R code file. Click the “Convert” button and you are done! formatR: unformatted R code formatR: tidy R code There are several options in the “Preferences” panel, e.g. you can specify whether to keep comments or blank lines, or specify the width of the formatted R code. No matter how messy your code looks like, formatR can make it tidy and structured as long as there are no syntax errors in your R code. If you prefer the command line interface, you may want to take a look at the function tidy.source() in the animation package. Currently there are problems with the encoding of multi-byte characters, and I have not figured out how to deal with them. 	 0 Comments
Efficient Mixed-Model Association in GWAS using R	https://www.r-bloggers.com/2010/04/efficient-mixed-model-association-in-gwas-using-r/	April 13, 2010	Stephen Turner		 0 Comments
Repeated measures ANOVA with R (tutorials)	https://www.r-bloggers.com/2010/04/repeated-measures-anova-with-r-tutorials/	April 13, 2010	Tal Galili	"Repeated measures ANOVA is a common task for the data analyst. There are (at least) two ways of performing “repeated measures ANOVA” using R but none is really trivial, and each way has it’s own complication/pitfalls (explanation/solution to which I was usually able to find through searching in the R-help mailing list). So for future reference, I am starting this page to document links I find to tutorials, explanations (and troubleshooting) of “repeated measure ANOVA” done with R Unbalanced design
Unbalanced design doesn’t work when doing  repeated measures ANOVA with aov, it just doesn’t.  This situation occurs if there are missing values in the data or that the data is not from a fully balanced design.  The way this will show up in your output is that you will see the between subject section showing withing subject variables. A solution for this might be to use the Anova function from library car with parameter type=”III”.  But before doing that, first make sure you understand the difference between SS type I, II and III. Here is a good tutorial for helping you out with that.
By the way, these links are also useful in case you want to do a simple two way ANOVA for unbalanced design I will “later” add R-help mailing list discussions that I found helpful on the subject. If you come across good resources, please let me know about them in the comments. "	 0 Comments
Cherry Picking to Generalize ~ NASA Global Temperature Trends ~ enhanced w/ ggplot2	https://www.r-bloggers.com/2010/04/cherry-picking-to-generalize-nasa-global-temperature-trends-enhanced-w%c2%a0ggplot2/	April 12, 2010	apeescape	"In a prior article, I tried to visualize the linear global temperatures trends for a grid of start and end years. The visual I created was confusing in that the specification of color scale was interdependent with the data values. I wanted a blue -> white -> red scale of the temperatures indicating cool -> neutral -> warm temperatures, with the extremums using the same “darkness” (e.g. +3C will be rgb(1,0,0) and -3C will be rgb(0,0,1)). But the maximum temperatures were higher than the minimum, so I had to artificially include the negative of the maximum temp. in the lower triangle of the data (or else 0C won’t be white). I have recreated this plot in the ggplot2 package because of the handy scale_colour_gradient2 function to set the color scale, and the ability to elegantly extend the features of the graph by calling the existing data frame. The graph now adds identification of statistically insignificant temperature slopes marked as “x”. After setting the color scale, getting rid of the extra space in the panel, adding a stair graph (geom_step) and changing colors of the panel and background, I have the following:  None of the blue colored boxes are statistically significant at the 5% level. code: 
Filed under: Climate Change, ggplot2, R        

 "	 0 Comments
Using MKL-Linked R in Eclipse	https://www.r-bloggers.com/2010/04/using-mkl-linked-r-in-eclipse/	April 12, 2010	Michael		 0 Comments
Jeroen Ooms’s ggplot2 web interface – a new version released (V0.2)	https://www.r-bloggers.com/2010/04/jeroen-ooms%e2%80%99s-ggplot2-web-interface-%e2%80%93-a-new-version-released-v0-2/	April 12, 2010	Tal Galili	"Good news. Jeroen Ooms released a new version of his (amazing) online ggplot2 web interface: yeroon.net/ggplot2 is a web interface for Hadley Wickham’s R package ggplot2. It is used as a tool for rapid prototyping, exploratory graphical analysis and education of statistics and R. The interface is written completely in javascript, therefore there is no need to install anything on the client side: a standard browser will do. The new version has a lot of cool new features, like advanced data import, integration with Google docs, converting variables from numeric to factor to dates and vice versa, and a lot of new geom’s. Some of which you can watch in his new video demo of the application:
 The application is on:
http://www.yeroon.net/ggplot2/ p.s: other posts about this (including videos explaining how some of this was done) can be views on the category page: R and the web "	 0 Comments
pgfSweave version 1.0.5 released	https://www.r-bloggers.com/2010/04/pgfsweave-version-1-0-5-released/	April 12, 2010	cameron	Version 1.0.5 is now on CRAN. This version brings some bug fixes as well as two new features: http://cran.r-project.org/web/packages/pgfSweave/index.html 	 0 Comments
Arizona court rules statistical sampling is legal	https://www.r-bloggers.com/2010/04/arizona-court-rules-statistical-sampling-is-legal/	April 12, 2010	David Smith	"A court in Arizona has ruled that statistical sampling is legal for determining damages awarded to individual claimants when there are thousands of similar cases to be assessed simultaneously. In a case where 30,000 claims were filed Maricopa County, AZ by hospitals for improper reimbursement, the trial judge appointed a former judge as a special master in the case to resolve all the claims within 24 months. The special master in turn hired a statistician who drew a sample of 1,150 claims that would be used as the basis for all claims. The county appealed this process, claiming due process rights of the hospitals were violated. The appellate court ruled that sampling was valid, but sent the case back to the trial court to validate that an appropriate sampling process was used. that the viability of the sampling methods or even whether the methods chosen were properly implemented and complies with all the necessary legal principles. The case is Scottsdale Memorial et al. v Maricopa County CA-CV 07-0150.  http://www.azcentral.com/business/abg/articles/2010/04/08/20100408abg-fischer0408.html  Posted on April 12, 2010  by David Smith  in R bloggers | 0 Comments 
 A court in Arizona has ruled that statistical sampling is legal for determining damages awarded to individual claimants when there are thousands of similar cases to be assessed simultaneously. In a case where 30,000 claims were filed Maricopa County, AZ by hospitals for improper reimbursement, the trial judge appointed a former judge as a special master in the case to resolve all the claims within 24 months. The special master in turn hired a statistician who drew a sample of 1,150 claims that would be used as the basis for all claims. The county appealed this process, claiming due process rights of the hospitals were violated. The appellate court ruled that sampling was valid, but sent the case back to the trial court to validate that an appropriate sampling process was used. that the viability of the sampling methods or even whether the methods chosen were properly implemented and complies with all the necessary legal principles. The case is Scottsdale Memorial et al. v Maricopa County CA-CV 07-0150.  http://www.azcentral.com/business/abg/articles/2010/04/08/20100408abg-fischer0408.html "	 0 Comments
Working with themes in Lattice Graphics	https://www.r-bloggers.com/2010/04/working-with-themes-in-lattice-graphics/	April 12, 2010	Ralph	The Trellis graphics approach provides facilities for creating effective graphs with a consistent look and feel and one of the good things about the system is the use of themes to define the colour, size and other features of the components that make up a graph. The lattice package in R is an implementation of the approach and in this post we will consider how to change the default settings. The main functions of interest are the pair trellis.par.get and trellis.par.set that are used to get hold of the settings for the current graphics device or to set a list of new parameters. The parameters themselves are described by a list object with information about a large range of possible options. To extract and save this information we would do the following: To get a feel for how many options can be specified we can look at the names of the components that make up this list: There are a total of 34 components of this list that each have a set of other parameters that can be set by the user. For example, if we wanted to find out how lattice will draw a scatter plot then the plot.symbol component of the list is where we should be looking. We extract this information as follows: The plot symbol is 80% of the overall size specified for the system and the empty circle is the first shape symbol available – set by the pch component. When there are groups in the data being plotted different shape symbols, colours etc. can be specified as part of the superpose.symbol component of the theme list. By default the settings are: There are seven possible groups that are specified in the default theme and the various options are in many cases the same name as for the base graphics system. If we wanted to make the symbols large then the cex option specifies a multiplier relative to the base size for symbols. The pch is used to indicate which plot symbol to use, and these are again the same as those provided when using base graphics. So if we wanted to change the symbols to solid circles then we would adjust the my.theme list object to indicate a different plot symbol shape: To update the graphics settings we would need to use the trellis.par.set function: The new settings can be accessed via: They are now as shown here: Lattice Graphics Settings The plot symbols are now solid rather than empty circles. 	 0 Comments
Example 7.32: Add reference lines to a plot; fine control of tick marks	https://www.r-bloggers.com/2010/04/example-7-32-add-reference-lines-to-a-plot-fine-control-of-tick-marks/	April 12, 2010	Nick Horton		 0 Comments
Anecdotal Evidence that Facebook Stores all Clicks?	https://www.r-bloggers.com/2010/04/anecdotal-evidence-that-facebook-stores-all-clicks/	April 11, 2010	Ryan	 This is not really news. A few months ago, news broke that Facebook recorded each user’s clicks and profile views in a database. Of course, I am not at all surprised. I would be more surprised if they didn’t store every single click. By now, most people have some sense as to how Facebook’s recommendation system works. It typically performs what one of my professors called “completing the triangle.” If users  and  are friends, and users  and  are friends, Facebook may hypothesize that  and  should also be friends. Of course, Facebook’s algorithm is not that naive. Consider a slightly more realistic example in the graph below. I must provide a picture, otherwise I will end up using “recursive language” (i.e. “friends of a friend of a friend that’s friends with…”). The red lines represent existing friendships. This graph consists of two triangles, one containing one man and the two women, and another containing one woman and the two men. Facebook would most likely conclude that the two people with spiky hair should be friends, denoted by the green dashed line.  On Facebook, I am a member of several different network clusters, as most people are. Some include high school friends, dormmates from two different schools, colleagues in the same college major, graduate school friends, and coworkers from a job I worked at in college. I will focus my discussion on this last group. This job was a very “social” job in that it was a lot of fun, and socializing with patrons and coworkers was essential. In other words, it was not a cubicle, professional type position. Anyway, most of us are friends on Facebook, and this group is/was very tight in a social networking sense. On last check, a random coworker and I have 50 Facebook friends in common. In this subgraph, a lack of a connection is more telling than a connection. Typically, if someone is not friends with another person in the cluster, there is a story behind it. For me, there was one person in particular, a supervisor, so we were not friends on Facebook. This guy and I shared not only the friends from this job, but also several other friends from the guys I lived with my senior year in college. Of anybody out there on Facebook, he and I probably have the most friends in common. Naturally, he should have been the first person displayed to me in the “People you May Know” system; his ranking should have been sky high. So what does the ranking take into account? I have no clue. Of course network position and common ties are both very important. Two important questions come to mind: I am not blocked, so why did he not show up for so long? Why did he show up on that fateful day? Or, The Plot Thickens (the point of this post) One typical day I logged onto Facebook and saw his face in the “People You May Know” section of the screen and the fact that we had 60 something friends in common. I will call him Bill. I saw that he now how works for the same employer as my father. I did not know this until now, and neither did my dad. My dad’s coworkers are very tight knit despite the size of the organization. I told him about Bill. The next day when my dad called me, he said, “you are never going to guess who I just met today. Bill!” In this profession, people frequently move around and work at other locations, particularly for overtime days. They typically discuss shift information with each other, and know who is the “supervisor” at each location. More likely than not, Bill put 2 and 2 together. My last name isn’t terribly common (as a last name) in the US. He thought perhaps we are from the same family. He then goes on to Facebook, looks at my profile and see if he can find any information matching my dad and I to confirm his suspicion. Facebook then recorded this click/profile view, combined it with network position metrics, and bingo he ended up being recommended to me. This is no Earth-shattering blog post, but it was definitely an “Oh wow!” moment with respect to Facebook. The icons used in the social graph in this post are from Ikonka. 	 0 Comments
Significant Figures in R and Info Zeros	https://www.r-bloggers.com/2010/04/significant-figures-in-r-and-info-zeros/	April 11, 2010	Neil Gunther		 0 Comments
R frustration of the day	https://www.r-bloggers.com/2010/04/r-frustration-of-the-day/	April 11, 2010	Tal Galili	Whenever you take a 1 column slice of a matrix, that gets automatically converted into a vector. But if you take a slice of several columns, it remains a matrix. The problem is you don’t always know in advance how big the slice will be, so if you do this:  You'll get an error if x is 1. This creates the worst kind of bug: an intermittent one that will hide until the right (wrong?) value of x occurs. To fix the problem you need to RE-declare the slice to be a matrix with ncol=x after you take the slice. 	 0 Comments
Historical / Future Volatility Correlation Stability	https://www.r-bloggers.com/2010/04/historical-future-volatility-correlation-stability/	April 11, 2010	Joshua Ulrich	"
 "	 0 Comments
Poor man’s pairs trading…	https://www.r-bloggers.com/2010/04/poor-man%e2%80%99s-pairs-trading%e2%80%a6/	April 11, 2010	M. Parzakonis	"There is a central notion in Time Series Econometrics, cointegration. Loosely it refers to finding the long run equilibrium of two non-stationary series. As the most know non-stationary series examples comes from finance, cointegration is nowadays a tool for traders (not a common one though!). They use it as the theory behind pairs trading (aka Statistical Arbitrage). In the following lines we use a simple pairs trading technique, studying the ratio of the two price evolution series. We use the New York versions of two of the greatest players in ATHEX, NBG and OTE. 
Now, let’s plot the ratio of the two. We locate a traing opportunity when there is an exceedence of the 2sigmas. Then we go short on the OTE and long on the NBG and close the positions on when the ratio is approaching the mean.  Looking at the above there was a opportunity to start a pair trading at the end of the 2009… "	 0 Comments
Summarising data using histograms	https://www.r-bloggers.com/2010/04/summarising-data-using-histograms/	April 11, 2010	Ralph	The histogram is a standard type of graphic used to summarise univariate data where the range of values in the data set is divided into regions and a bar (usually vertical) is plotted in each of these regions with height proportional to the frequency of observations in that region. In some cases the proportion of data points in each region is shown instead of counts. The shape of the histogram is determined by the width and number of regions that divided up the data. A histogram provides an indication the following features of a set of data: the general shape, symmetry or skewness of data and modality (uni-, bi- or multi-modal). There are some situations where a different type of graph would be preferable but histograms are useful for describing the general features of the distribution of a set of data. To illustrate creating a histogram we consider data from the AFL sports league in Australia and the total number of points scored by the home team in each fixture. If we assume that the data is in a comma separated text file, called afl_2003_2007.csv, then we would import that data using the following command saving the results in a data frame: Base Graphics In base graphics the function hist is used to create a histogram with the first argument being the name of the vector that contains the data to be plotted. The x-axis is given a label using the xlab argument and the main argument is used to add a title to the graph. Code to create a histogram of home points is shown below: The default option is to display bars representing the frequency of data values in each of the ranges and the overall look of the graph is basic as shown here: Base Graphics Histogram The default algorithm for selecting number of bins to use for the histogram usually makes a sensible selection but this can be specified if required. Lattice Graphics In the lattice graphics package there is a function histogram and we make use of the formula to specify a single variable for the number of points scored by the home team. The specification for the axis labels and graph title are the same as for the base graphics package. The equivalent graph is created using the following code: Here the default option is the work with proportions of the total number of data points rather than counts so the shape of the distribution is slightly different when compared to the base graphics plot. The lattice version is shown below: Lattice Graphics Histogram The main other difference is the choice of colour for the bars in the histogram and these can be adjusted by changing the global theme for lattice. ggplot2 The ggplot2 library uses a general purpose graphics function called ggplot to create graphs of all types and the geom specifies the type of display to create, in this case a histogram. Components that make up the graph are added sequentially to build up the whole plot and in the example below we add axis labels and a main title. The default theme for ggplot2 is distinctive and the histogram is shown in the graph below: ggplot 2 Histogram The default number of bins is larger compared to base and lattice graphics which provides a rough distribution in this particular case. The online ggplot2 manual is a good source of information about customising graphs created using this approach. This blog post is summarised in a pdf leaflet on the Supplementary Material page. 	 0 Comments
Compiling 64-bit R 2.10.1 with MKL in Linux	https://www.r-bloggers.com/2010/04/compiling-64-bit-r-2-10-1-with-mkl-in-linux/	April 10, 2010	Michael		 1 Comment
Where do you sit?  Author position and the h-index	https://www.r-bloggers.com/2010/04/where-do-you-sit-author-position-and-the-h-index/	April 10, 2010	Brandon Whitcher		 0 Comments
Because it’s Friday: Pixels invade New York	https://www.r-bloggers.com/2010/04/because-its-friday-pixels-invade-new-york/	April 9, 2010	David Smith	Posted for no other reason than it warms my gamer-geek heart to see NYC taken over by 8-bit video game characters. The Tetris sequence is particularly cool.     	 0 Comments
REvolution R Community 3.2 now available	https://www.r-bloggers.com/2010/04/revolution-r-community-3-2-now-available/	April 9, 2010	David Smith	REvolution R Community, REvolution’s free distribution based on R from the R Project, has been updated to version 3.2 and is now available for download for Windows and MacOS. Some features of this release include: Upgraded R engine. This release is based on R 2.10.1, the latest release (as of this writing). This brings many new features to the R language as detailed in the NEWS file (or you can read the highlights here and here). Multi-threaded math libraries. The Windows version is compiled to link with the Intel MKL (Math Kernel Libraries). This means that many computations in R — especially linear algebra functions like matrix multiply, inverse, and decompositions — have been dramatically optimized for performance. (Optimizations are tuned for Intel chipsets, but improvements are apparent on AMD systems, too.) This is really noticeable on multi-core and/or multi-processor systems, where multi-threaded code uses all available cores for computations. (R for Windows typically only uses one core. Conversely, the Mac version has always used multi-threaded libraries.) For example, matrix multiply runs about 6x faster (32x on a quad-core box), and principal components analysis (PCA) runs about 4x faster (9x on a quad-core). (See this page for the specific benchmarks.) You don’t need to change any code to benefit from these speedups — it all happens automatically when you use those standard R functions that linear-algebra computations. On the other hand, it doesn’t help with general R code that doesn’t make use of math libraries, but that’s where parallel programming comes in (see below).  New parallel backend for Windows. This release includes new open-source libraries from REvolution Computing to support symmetric multicore processing (SMP) on Windows machines, which enables you to speed up loops in R code running iterations in parallel on a multi-core or multi-processor machine. This is similar to using the doMC parallel backend for foreach on Mac or Linux. The new doSMP package acts as a replacement for doMC on Windows. For example, on a 4-core box, you’d register the doSMP backend like this: require(doSMP)workers registerDoSMP(workers) and from then on, foreach loops would run four iterations in parallel (one for each core) for up to a 4x speedup. (You get the most benefit when the body of the loop is performing time-consuming operations.) There’s more info in the ParallelR Lite User’s Guide, included with REvolution R Community 3.2. Improved Installer: We’ve streamlined the install process to make it faster and require fewer clicks. For one thing, all of the added REvolution components are now open-source, so there’s no longer any click-through license. (Of course, you’re still bound by the terms of the respective open-source licenses, including the GPL for R itself.) REvolution R Community 3.2 is available now from the link below. REvolution Computing: Download REvolution R  	 0 Comments
Chicago R User Group… It’s for the sexy people!	https://www.r-bloggers.com/2010/04/chicago-r-user-group%e2%80%a6-it%e2%80%99s-for-the-sexy-people/	April 9, 2010	JD Long	"Morris Day, y'all!  I think we all know that Morris Day was talking about when he wrote the lyrics to “The Bird”: Yes! Hold on now, this dance ain’t for everybody.
Just the sexy people.
White folks, you’re much too tight.
You gotta shake your head like the black folks.
You might get some tonight.
Look out! That’s right, he was talking about the new R User Group in Chicago! a.k.a Chicago RUG! We know that R is sexy because statistical analysis is sexy. That is, if you’re doing it right! Even Mike Driscol at Dataspora knows that Data Geeks have to get their sexy on.  There is no doubt that Chicago is sexy. The second city is so damned sexy that Karen Abbott wrote Sin in the Second City and managed to get it on the NYT best sellers list. She makes me reconsider my agrarian interpretation of Chicago’s “meat packing” heritage. *rim shot* Thank you, thank you. I’ll be here all week. Try the veal!
 If you’re in Chicagoland and reading this blog then you have every reason to get over to the Chicago R User Group web site and sign up! I’m looking forward to meeting all the Chicago R users in the near future. In case you’re afraid you won’t recognize me I’ll be the one that looks just like Morris Day… only white… and not as well dressed… and kinda nerdy. But otherwise, just like Morris. Now shut up and dance! Morris Day and the Time on Grooveshark!  "	 0 Comments
The Future of Math is Statistics	https://www.r-bloggers.com/2010/04/the-future-of-math-is-statistics/	April 9, 2010	JD Long	The future of math is statistics… and the language of that future is R:  I’ve often thought there was way too little “statistical intuition” in the workplace. I think Author Benjamin would agree.  	 0 Comments
Maximum Probability of Profit	https://www.r-bloggers.com/2010/04/maximum-probability-of-profit/	April 9, 2010	Joshua Ulrich	"
 "	 0 Comments
GLMM using DPpackage	https://www.r-bloggers.com/2010/04/glmm-using-dppackage/	April 9, 2010	Shige		 0 Comments
Gravity Game in R	https://www.r-bloggers.com/2010/04/gravity-game-in-r/	April 8, 2010	Lee	"So why should R only be used for ’serious’ stuff?  No longer!  I’ve written the following code in R which executes a little gravitational physics game.  The goal of the game is simple.  You supply a velocity and direction to a spaceship with the goal of getting the ship to the winning area without crashing into a planet.

To give you an idea of how this game works, below is a screenshot of the 3rd level.

In this screenshot, the blue dot represents the starting position of the ship.  The black curve is the ships path.  The red circles are the planets where the number represents the mass.  Finally, the green circle is the winning area. In this example, the ship made it to the winning area safe and sound! Just copy and paste the code below into R and make a call to the function  gravity()  and enjoy! Also, you can easily add your own levels by putting your own game parameters in the ‘define level’ code blocks. "	 0 Comments
R: heatmaps with gplots	https://www.r-bloggers.com/2010/04/r-heatmaps-with-gplots/	April 8, 2010	Stewart MacArthur		 0 Comments
New R User Group in Dallas	https://www.r-bloggers.com/2010/04/new-r-user-group-in-dallas/	April 8, 2010	David Smith	Wow, it’s a big week for new R User Groups. Larry D’Agostino has started up a new R user group based in Dallas, Texas (USA). It’s just getting started, and Larry posted the following request on the r-help mailing list:  I would like to know if there is anyone like me interested in an R User Group in Dallas, TX. David Smith at REvolutions was kind enough to help getting it started. My first thought would to have some informal meet ups at some local Dallas locations to discuss overall goals, ideas, wishes of the RUG.  The next step would be to nominate and elect a leadership team.  Then get the ball rolling to a more formal meet up process with presentations, workshops, and tutorial sessions. If you’re in the Dallas area, why not join in and help Larry get the ball rolling? Sign up at the link below.  Yahoo! Groups: Dallas R Users Group 	 0 Comments
R: another nifty graph	https://www.r-bloggers.com/2010/04/r-another-nifty-graph/	April 8, 2010	Tal Galili	 Make sure to click on the image to see the large version. Code for this graph:  	 0 Comments
Video of UCLA / LA RUG talk on R and C++ integration	https://www.r-bloggers.com/2010/04/video-of-ucla-la-rug-talk-on-r-and-c-integration/	April 7, 2010	Thinking inside the box	"
Thanks also to David Smith (at the REvolutions blog) and 
Drew Conway (at his blog) for spreading
the word about the presentation video and slides — quite a few folks have come to my
presentations page to get them.


 "	 0 Comments
An obscure integral	https://www.r-bloggers.com/2010/04/an-obscure-integral/	April 7, 2010	xi'an	Here is an email from Thomas I received yesterday about a computation in our book Introducing Monte Carlo Methods with R: I’m currently reading your book “Introduction to Monte Carlo Methods with R” and I quite highly appreciate your work. I’m not able to see how the integral on page 74, that describes the marginal likelihood, simplifies to the fraction on the second line. If I’m not asking too much, could you confirm to me whether the fraction is as is given in the text. Because the transform of the integral  into the ratio of two integrals  may sound curious (and possibly wrong) to many readers besides Thomas, let me explain that the bottom integral is the normalisation constant of the prior  while the top integral is the product of the observation density:  and of the prior (minus the normalisation constant). Nothing wrong then with the formula at the bottom of page 74, but this is a bit short on explanations! 	 0 Comments
Correlation scatter-plot matrix for ordered-categorical data	https://www.r-bloggers.com/2010/04/correlation-scatter-plot-matrix-for-ordered-categorical-data/	April 7, 2010	Tal Galili	"When analyzing a questionnaire, one often wants to view the correlation between two or more Likert questionnaire item’s (for example: two ordered categorical vectors ranging from 1 to 5). When dealing with several such Likert variable’s, a clear presentation of all the pairwise relation’s between our variable can be achieved by inspecting the (Spearman) correlation matrix (easily achieved in R by using the “cor.test” command on a matrix of variables).
Yet, a challenge appears once we wish to plot this correlation matrix.  The challenge stems from the fact that the classic presentation for a correlation matrix is a scatter plot matrix – but scatter plots don’t (usually) work well for ordered categorical vectors since the dots on the scatter plot often overlap each other. There are four solution for the point-overlap problem that I know of: In this post I will offer the code for the  a solution that uses solution 3-4 (and possibly 2, please read this post comments). Here is the output (click to see a larger image):  And here is the code to produce this plot:  Note that this code will work fine for continues data points (although I might suggest to enlarge the “point.size.rescale” parameter to something bigger then 1.5 in the “panel.smooth.ordered.categorical” function) If you got ideas on how to improve this code (or reproducing it with ggplot2 or lattice), please do so in the comments (or on your own blog, but be sure to let me know    ) "	 0 Comments
Video: Seamless R Extensions using Rcpp and RInside	https://www.r-bloggers.com/2010/04/video-seamless-r-extensions-using-rcpp-and-rinside/	April 7, 2010	David Smith	Dirk Eddelbuettel presented joint work with Romain François on calling C++ from R at the LA R User Group meeting last week. Now, with thanks to Drew Conway of the NY R User Group, video of the presentation is now available. It’s also embedded below — click on it for a larger view. Dirk’s slides are also available for download in handout and slide format.   Vcasmo: Seamless R Extensions using Rcpp and RInside 	 0 Comments
Seamless R Extensions using Rcpp and RInside	https://www.r-bloggers.com/2010/04/seamless-r-extensions-using-rcpp-and-rinside-2/	April 7, 2010	Drew Conway	I just added a new video to the R repository, and this one comes from the Los Angeles R Meetup.  The folks in LA were fortunate enough to have Dirk Eddelbuettel—renowned R expert and StackOverflow super-user—discuss his joint work with Romain François for interfacing C++ and R code using the Rcpp package.  For those looking to get a performance boost in their R code the Rcpp package, and this introductory talk by Dirk, will be extremely useful.  	 0 Comments
Matrix determinant with the Lapack routine dspsv	https://www.r-bloggers.com/2010/04/matrix-determinant-with-the-lapack-routine-dspsv/	April 6, 2010	Matt Shotwell	The Lapack routine dspsv solves the linear system of equations Ax=b, where A is a symmetric matrix in packed storage format. However, there appear to be no Lapack functions that compute the determinant of such a matrix. We need to compute the determinant, for instance, in order to compute the multivariate normal density function. The dspsv function performs the factorization A=UDU’, where U is a unitriangular matrix and D is a block diagonal matrix where the blocks are of dimension 1×1 or 2×2. In addition to the solution for x, the dspsv function also returns the matrices U and D. The matrix D may then be used to compute the determinant of A. Recall from linear algebra that det(A) = det(UDU’) = det(U)det(D)det(U’). Since U is unitriangular, det(U) = 1. The determinant of D is the product of the determinants of the diagonal blocks. If a diagonal block is of dimension 1×1, then the determinant of the block is simply the value of the single element in the block. If the diagonal block is of dimension 2×2 then the determinant of the block may be computed according to the well-known formula b11*b22-b12*b21, where bij is the value in the i’th row and j’th column of the block. The following C code snip demonstrates the procedure. 	 0 Comments
correlograms are correlicious	https://www.r-bloggers.com/2010/04/correlograms-are-correlicious/	April 6, 2010	Tal Yarkoni	In the last year or so, I’ve been experimenting with different ways of displaying correlation matrices, and have gotten very fond of color-coded correlograms. Here’s one from a paper I wrote investigating the relationship between personality and word use among bloggers (click to enlarge):  The rows reflect language categories from Jamie Pennebaker’s Linguistic Inquiry and Word Count (LIWC) dictionary; the columns reflect Extraversion scores (first column) or scores on the lower-order “facets” of Extraversion (as measured by the IPIP version of the NEO-PI-R). The plot was generated in R using code adapted from the corrgram package (R really does have contributed packages for everything). Positive correlations are in blue, negative ones are in red. The thing I really like about these figures is that the colors instantly orient you to the most important features of the correlation matrix, instead of having to inspect every cell for the all-important ***magical***asterisks***of***statistical***significance***. For instance, a cursory glance tells you that even though Excitement-Seeking and Cheerfulness are both nominally facets of Extraversion, they’re associated with very different patterns of word use. And then a slightly less cursory glance tells you that that’s because people with high Excitement-Seeking scores like to swear a lot and use negative emotion words, while Cheerful people like to talk about friends, music, and use positive emotional language. You’d get the same information without the color, of course, but it’d take much longer to extract,  and then you’d have to struggle to keep all of the relevant numbers in mind while you mull them over. The colors do a lot to reduce cognitive load, and also have the secondary benefit of looking pretty. If you’re interested in using correlograms, a good place to start is the Quick-R tutorial on correlograms in R. The documentation for the corrgram package is here, and there’s a nice discussion of the principles behind the visual display of correlation matrices in this article. p.s. I’m aware this post has the worst title ever; the sign-up sheet for copy editing duties is in the comment box (hint hint). 	 0 Comments
New version of R package futile released	https://www.r-bloggers.com/2010/04/new-version-of-r-package-futile-released/	April 6, 2010	Brian Lee Yung Rowe	The latest version of futile was released to CRAN yesterday. This release broke out the various functions into self-contained sub-packages with the base futile package hanging around as a wrapper to ensure loading of the most recent stable sub-packages (similar to the relationship that kde has to kdegraphics, kdepim, etc.). Current sub-systems avaiable in futile include All packages are available in R via the standard installation method. More information is available on the futile home page and at CRAN. 	 0 Comments
Cherry Picking to Generalize ~ NASA Global Temperature Trends	https://www.r-bloggers.com/2010/04/cherry-picking-to-generalize-nasa-global-temperature-trends/	April 6, 2010	apeescape	The relatively (to this decade) cool 2008 global temperatures spurred talks of a warming pause, or even global cooling. The claim usually comes from people who cherry picked either data sets and(!)/or start and end points of the global temperature trends to back up their allegation. The blogosphere already has a lot on this: Basically, you really need to cherry pick to show a declining, or even a neutral trend. Below is the most recent data taken from NASA GISS:  We see generally an increasing trend, and it flat lines at the end. Depending on where you pick your end points, it may not be too out of the ordinary to claim global cooling. I decided to really take this exercise further, to cherry pick starting and ending years of temperature trends for a large portion of the data. What I did was to calculate simple linear regressions (just a plain lm() in R) for virtually all combinations of starting and ending points between 1970 and 2009 (where the trend is more or less linear). I chose a a minimum lag (min.lag) to “cushion” the regression so it’s actually meaningful. I then extracted the estimated slope parameter (Celsius/yr), plot them in a 2D grid (of start and end years). I chose a lag of 4 years (anything smaller makes things a little unstable but still works). That means I have (2009-1970-4+1)^2/2 = 648 linear regressions to calculate. Even though each of the regressions shouldn’t determine a trend, the aggregate of it should. The grids are calibrated so positive temp. trends are red, and negative temp. trends are blue. The x-axis is the start year, while the y-axis is the end year. The bottom right triangle portion of this matrix was forced to be the negative of the maximum value so R can get the color right.  We see a lot of red especially for longer lags. There are a few trends that go down for really short lags (along the diagonal), but even then it is not a common occurrence. The temperature has increased by something like 0.13 ℃ / decade for the last 50 years or so, we do generally see that except for latest short trends, which are obviously noisy. It looks like, at least from 1970 on, there are times of decline during short periods but flattens out over time. The most recent data show a slight decline. The plot uses the fields package to attach the color legend to the grid. The code is below (there has to be a better way to rotate axis labels…): Add to: Facebook | Digg | Del.icio.us | Stumbleupon | Reddit | Blinklist | Twitter | Technorati | Yahoo Buzz | Newsvine 	 0 Comments
Le Monde rank test (corr’d)	https://www.r-bloggers.com/2010/04/le-monde-rank-test-corr%e2%80%99d/	April 6, 2010	xi'an	Since my first representation of the rank statistic as paired was incorrect, here is the histogram produced by the simulation when . It is obviously much closer to zero than previously. An interesting change is that the regression of the log-mean on  produces meaning that the mean is in  rather than in  or : with a very good fit.  	 0 Comments
R package Blotter	https://www.r-bloggers.com/2010/04/r-package-blotter/	April 6, 2010	kafka	"How many times have you been disappointed by nice trading system, because neither trading cost or slippage or bid/ask spread were included into back-test results? Did you find difficult to back-test a portfolio in R or many portfolios with different stocks? Blotter package is supposed to solve these problems. In really – it is complicated. I spent a couple of days to start using it. There was one bug in the demo example, but the rest was in my code. You should remember:
1. Time zone must be specified: 2. xts object must be created with explicit index class: 3. When the test is conducted all blotter related values are written into .blotter environment. If you want repeat the same test, then you need to get rid of all .blotter values. So, you have to run something like this: You can find the author’s comment about this issue here: http://n4.nabble.com/Blotter-package-problem-with-example-tp1018634p1572911.html
4. It is slow. If you want to find the fittest rule from a bunch of them – try avoid using blotter, instead, use pure cumulative return and later on, include blotter. Let’s test this rule with blotter as an example:
short SPY if today’s low is higher than yesterday’s close and close next day. The result (it will take time to get it…):
 "	 0 Comments
ProbABEL – R package for GWAS data imputation	https://www.r-bloggers.com/2010/04/probabel-r-package-for-gwas-data-imputation/	April 6, 2010	Stephen Turner		 0 Comments
New R User Group in Chicago	https://www.r-bloggers.com/2010/04/new-r-user-group-in-chicago/	April 6, 2010	David Smith	While there’s been an informal coterie of R users in the Chicago area for some time (notably the fine folks behind the successful R/Finance conferences) there hasn’t been a formal R User Group. Until now, that is. JD Long has taken the plunge and announced the new Chicago R User Group on meetup.com. If you’re in the Chicagoland area, why not sign up? MeetUp.com: Chicago R User Group   	 0 Comments
Rules of Thumb to Meet R Gurus in the Help List	https://www.r-bloggers.com/2010/04/rules-of-thumb-to-meet-r-gurus-in-the-help-list/	April 5, 2010	Yihui Xie	Here is my personal list of rules of thumb for people who want to meet some R gurus (quickly) in the R help mailing list ([email protected]): I’ve been reading the mailing list for about 2 years, so I may not know enough about all the gurus. Let me know if I missed anyone. The above list is not given for serious purpose, and my real point is I learned a lot from their advice and arguments. 	 0 Comments
R Tools for Dynamical Systems ~ R pplane to draw phase planes	https://www.r-bloggers.com/2010/04/r-tools-for-dynamical-systems-r-pplane-to%c2%a0draw%c2%a0phase%c2%a0planes/	April 5, 2010	apeescape	MATLAB has a nice program called pplane that draws phase planes of differential equations models. pplane on MATLAB is an elaborate program with an interactive GUI where you can just type the model to draw the phase planes. The rest you fidget by clicking (to grab the initial conditions) and it draws the dynamics automatically. As far as I know, R doesn’t have a program of equal stature. R’s GUI itself is non-interactive (maybe because creating a good GUI require money), and you can’t fiddle around with the axes graphically, for example. The closest I could find was code from Prof. Kaplan from Macalester College in his program, pplane.r. Below is a slight modification of his program that uses the deSolve package for a more robust approximation of the trajectory, and I made it so you can draw the trajectories by clicking, using the locator() function. The pplane.r program takes in a 2D differential equation model, initial values and parameter value specifications to draw the dynamics on a plane. It draws arrows at evenly spaced out points at a certain resolution to see the general shape of the dynamics. This is done by using a crude method to create the Jacobian matrix. The next step is to give in initial values to draw the trajectory. The only changes that made were to change the phasetraj() function, which draws the trajectories after you’ve made the arrow plot. Instead of using a self-made Runge Kutta method, I replaced it with a more robust ode() from the deSolve package. I also made it possible to point click multiple points (initial values) to draw the trajectories from. The code is shown below and it’s a little bit redundant because of different model specifications between pplane.r and deSolve, but it works. Also, the nullclines() function that draws the nullclines seems to not be working for whatever reason. I could make the code more coherent, but I am lazy. Point is, the code can reproduce the pplane package in MATLAB to the best of my knowledge. When I run draw.traj(), it asks for the number of initial points you’d like to give (denoted by loc.num). If that number is 5, I click the graph 5 times, and it automatically runs the model. I run a predator-prey model from a previous post with parameters: .  I could specify the color of the trajectory, and its time range. As analyzed by linearization, the predator-prey dynamics of this model is a center (you could see the dynamics go in a circle with the initial value up top). One can imagine running all kinds of 2D models. It’s not as interactive as the MATLAB version, but I think it works well enough as a first step. Code is follows (please source in pplane.r and deSolve package): 	 0 Comments
Example 7.31: Contour plot of BMI by weight and height	https://www.r-bloggers.com/2010/04/example-7-31-contour-plot-of-bmi-by-weight-and-height/	April 5, 2010	Ken Kleinman		 1 Comment
Le Monde rank test (cont’d)	https://www.r-bloggers.com/2010/04/le-monde-rank-test-cont%e2%80%99d/	April 5, 2010	xi'an	Following a comment from efrique pointing out that this statistic is called Spearman footrule, I want to clarify the notation in  namely (a) that the ranks of  and  are considered for the whole sample, i.e.  instead of being computed separately for the ‘s and the ‘s, and then (b) that the ranks are reordered for each group (meaning that the groups could be of different sizes). This statistics is therefore different from the Spearman footrule studied by Persi Diaconis and R. Graham in a 1977 JRSS paper,  where  and  are permutations from . The mean of  is approximately . I mistakenly referred to Spearman’s  rank correlation test in the previous post. It is actually much more related to the Siegel-Tukey test, even though I think there exists a non-parametric test of iid-ness for paired observations… The ‘s and the ‘s are thus not paired, despite what I wrote previously. This distance must be related to some non-parametric test for checking the equality of location parameters. 	 0 Comments
UCLA and LA RUG talks on R and C++ integration	https://www.r-bloggers.com/2010/04/ucla-and-la-rug-talks-on-r-and-c-integration/	April 4, 2010	Thinking inside the box	"
The talks centered around R and C++ integration using both Rcpp and
RInside and
summarise where both projects stand after all the recent work
Romain and I put in over
the last few months. The presentations went fairly well; I received some
favourable comments.  

 
Szilard and the R User Group had also suggested a group discussion about
CRAN, its growth and how to maximise
its usefulness. Given my CRANberries feed,
my work on the CRAN Task Views
for Empirical Finance and
High-Performance
Computing with R as well as our
cran2deb binary package
generator, I had some views and ideas that helped frame the discussion which
turned out to very useful and informed.  So maybe we should do this User
Group thing in Chicago too!

 
Special thanks to Jan de Leeuw and Szilard Pafka for organising the meeting,
talks and discussion. 

 "	 0 Comments
Le Monde rank test	https://www.r-bloggers.com/2010/04/le-monde-rank-test/	April 4, 2010	xi'an	"In the puzzle found in Le Monde of this weekend, the mathematical object behind the silly story is defined as a pseudo-Spearman rank correlation test statistic,  where the difference between the ranks of the paired random variables  and  is in absolute value instead of being squared as in the Spearman rank test statistic. I don’t know whether or not this measure of distance has been studied in the statistics literature (although I’d be surprised has it not been studied!). Here is an histogram of the distribution of the new statistics for  under the null hypothesis that both samples are uncorrelated (i.e. that the sequence of ranks is a random permutation). Each point in the sample was obtained by 
 When regressing the mean of this statistic  against the covariates  and , I obtain the uninspiring formula  which does not translate into a nice polynomial in ! Another interesting probabilistic/combinatorial problem issued from an earlier Le Monde puzzle: given an urn with  white balls and  black balls that is sampled without replacement, what is the probability that there exists a sequence of length  with the same number of white and black balls for ? If , the answer is obviously one (1), but for some values of , it is less than one. When  goes to infinity, this is somehow related to the probability that a Brownian bridge crosses the axis in-between  and  but I have no clue whether this helps or not! Robin Ryder solved the question for the values  and  by establishing that the probability is still one. Ps- The same math tribune in Le Monde coincidently advertises a book, Le Mythe Climatique, by Benoît Rittaud that adresses … climate change issues and the “statistical mistakes made by climatologists”. The interesting point (if any) is that Benoît Rittaud is a “mathematician not a statistician”, with a few papers in ergodic theory, but this advocated climatoskeptic nonetheless criticises the use of both statistical and simulation tools in climate modeling. (“Simulation has only been around for a few dozen years, a very short span in the history of sciences. The climate debate may be an opportunity to reassess the role of simulation in the scientific process.”) "	 0 Comments
Why isn’t my 2X Ultra ETF keeping pace with the market and what is path asymmetry (R ex)? Part 2	https://www.r-bloggers.com/2010/04/why-isnt-my-2x-ultra-etf-keeping-pace-with-the-market-and-what-is-path-asymmetry-r-ex-part-2/	April 3, 2010	Intelligent Trading		 0 Comments
R-Node: a web front-end to R with Protovis	https://www.r-bloggers.com/2010/04/r-node-a-web-front-end-to-r-with-protovis/	April 3, 2010	Tal Galili	"Update (April 6 – 2010) : R-Node now has it’s own a website, with a dedicated google group (you can join it here) *  *  *  * The integration of R into online web services is (for me) one of the more exciting prospects in R’s future.  That is way I was very excited coming across Jamie Love’s recent creation: R-Node. R-Node is a (open source) web front-end to R (the statistical analysis package). Using this front-end, you can from any web browser connect to an R instance running on a remote (or local) server, and interact with it, sending commands and receiving the responses. In particular, graphing commands such as plot() and hist() will execute in the browser, drawing the graph as an SVG image. You can see a live demonstration of this interface by visiting:
http://69.164.204.238:2904/ 
And using the following user/password login info:
User: pvdemouser
Password: svL35NmPwMnt
(This link was originally posted here) Here are some screenshots: 





In the second screenshot you see the results of the R command ‘plot(x, y)’ (with the reimplementation of plot doing the actual plotting), and in the fourth screenshot you see a similar plot command along with a subsequent best fit line (data points calculated with ‘lowess()’) drawn in.  Once in, you can try out R by typing something like: The plot and lines commands will bring up a graph – you can escape out of it, download the graph as a SVG file, and change the graph type (e.g. do: plot (x, type=”o”) ).
Many R commands will work, though only the hist(), plot() and lines() work for graphing.
Please don’t type the R command q() – it will quit the server, stopping it working for everyone! Also, as everyone shares the same session for now, using more unique variable name than ‘x’ and ‘l’ will help you.  Currently there is only limited error checking but the code continues to be improved and developed. You can download it from:
http://gitorious.org/r-node  How do you may imagine yourself using something like this?  Feel invited to share with me and everyone else in the comments. Here are some of the more technical details of R-Node:
 (Credit: The following text is based on this forum thread) R-node, uses protovis for drawing graphs. Protovis is a visualization toolkit written in JavaScript using the canvas element. Using simple graphical marks, like boxes and dots, one can construct custom views to present or explore data. Besides Protovis, R-node also uses jquery and ExtJS core on the front-end.  Most R commands are passed back to the server and their results returned to the client. Some, such as the graph commands, are parsed and the arguments used in javascript re-implementations of the R commands (e.g. the R command ‘plot’ has a protovis equivalent).  The server side is R+Rserve, and to connect the browser client to the R server Jamie used a nodejs based application server.  Projects utilised in this include:  I would love to read your thoughts about this in the comments. "	 4 Comments
embed images in Rd documents	https://www.r-bloggers.com/2010/04/embed-images-in-rd-documents/	April 3, 2010	romain francois	The new help system that was introduced in R 2.10.0 and documented in an article of the R journal is very promising.  One thing that is planned for future versions of R (maybe 2.12.0) is some way to include images into Rd documents using the fig option of the Sexpr macro Another way is to use data uri and embed the image directly inside the html code, so this morning I played with this and wrapped up this little c library into an R package called base64 and hosted in the Rcpp project at r-forge. The package allows encoding and decoding files using the Base64 format. It currently has three functions: encode, decode and img. encode and decode do what their name implies, and img produces the html code suitable for embedding the image into an html document.  The help page for img actually contains an image, here is it:   and here is how it is produced:  	 0 Comments
Demonstrating the Power of F Test with gWidgets	https://www.r-bloggers.com/2010/04/demonstrating-the-power-of-f-test-with-gwidgets/	April 2, 2010	Yihui Xie	We know the real distribution of the F statistic in linear models — it is a non-central F distribution. Under H0, we have a central F distribution. Given 1 – α, we can compute the probability of (correctly) rejecting H0. I created a simple demo to illustrate how the power changes as other parameters vary, e.g. the degrees of freedoms, the non-central parameter and alpha. Here is the video:  The Power of F Test And for those who might be interested, here is the code (you need to install the gWidgets package first and I recommend the RGtk2 interface). Have fun: 	 0 Comments
Because it’s Friday: Chatroulette	https://www.r-bloggers.com/2010/04/because-its-friday-chatroulette/	April 2, 2010	David Smith	Yesterday, Drew Conway posted an analysis of the survival time to events on Chatroulette. If you’re familiar with Chatroulette, you’ll know what kind of events you can expect to occur when using it. (If you’re not, here’s a hint: don’t try it now if you’re at work.) Sadly, it was all an April Fool’s Day joke. But Drew takes the opportunity to teach an important lesson: it’s really really easy to simulate some data (Drew provides all the necessary code to do so in R), create some convincing-looking charts, and publish an academic-sounding review of a “paper”. If Drew’s article hadn’t been posted on April 1, I’d bet good money that a news site somewhere would have linked to it. Drew’s data was simulated, but Casey Neistat did actually collect real data (example: average time-to-pervert) from a Chatroulette session, and even did a controlled experiment (time-to-next for guy versus pretty girl). He made a movie about it, it’s kinda cute (especially the data presentation) but also slightly NSFW. Enjoy.    	 0 Comments
A free book on Geostatistical Mapping with R	https://www.r-bloggers.com/2010/04/a-free-book-on-geostatistical-mapping-with-r/	April 2, 2010	David Smith	Tomislav Hengl of the University of Amsterdam has published new book, A Practical Guide to Geostatistical Mapping. It’s jam-packed with 291 pages on mapping and analyzing spatial data using free software including R, SAGA, GRASS, ILWIS and Google Earth, and freely-available map data. The book itself is also available for free, as an Open Access Publication. You can order the book in printed form for US$12.78, or download it for free as a PDF. Surprisingly (given the title), this book isn’t just about visual displays of spatial data. In fact, the first two chapters offer a nice overview of statistical analysis of spatial data (although with a greater focus on continuous-field models than point-process models). If you want a concise overview of regression-kriging, this is a great resource. Chapter 3 addresses the various software tools you’ll use to analyze the data and create the maps. Some care has been taken in considering how the software elements should be integrated, and Hengl recommends a “R on top” model, where R scripts drive the other tools.  This is a clever move: making use of the scripting capabilities of R means you can avoid much of the tedious manual back-and-forth activities that are usually associated with working with several software tools. Hengl offers some other reasons for working with R, too (p. 90):  Chapter 4 covers the various auxiliary data sources available, listing sources global environmental and socio-economic data, and sources of maps and satellite imagery like GADM, Google Earth and MODIS.  The remaining chapters are devoted to worked examples of spatial data analysis and mapping. By working through the examples, you can recreate charts like these (click to enlarge):  One minor complaint: most of the images in the book are in black-and-white (most likely to facilitate the printing process). But at least you have the R scripts and data for all exercises (these, plus updated maps, are available from the book’s website), so at least you can re-run the examples in R to recreate them in color.  Tomislav Hengl: A Practical Guide to Geostatistical Mapping (via @fernando_mayer) 	 0 Comments
How to Produce Fake Data Analysis in R: 3 Easy Steps	https://www.r-bloggers.com/2010/04/how-to-produce-fake-data-analysis-in-r-3-easy-steps/	April 2, 2010	Drew Conway	"Did you really think that a team of researchers spent their weekends counting the number of shirtless adolescent men and exposed penises they could find on charoulette.com?  Perhaps you should not answer that, as it may be a better measure of your opinion of sociologist than gullibility.  It is true, sociologist do say the darndest things, but c’mon, some of my best friends are sociologist! The truth is, yesterday’s post was an April Fools joke, and one that I thought was fairly obvious (who’s that guy in the bottom panel of the chat roulette window?).  However, given the level of traffic, comments, and chatter on Twitter (even by some prolific Tweeters), it seems that many people were seduced by the what seemed to be legitimate data analysis.  In fact, the analysis was real—albeit rather light on detail—what was fake were the data.  In a world where data manipulation in scientific endeavor can rise to the level of international scandal, and data analytics are more frequently being used as a means to promote various political agenda, it is important to understand just how easy the process of generating fake data is. Below I describe this process in three easy steps, using the process of generating fake time-series data from chat roulette as an example. First, a disclaimer: I do not endorse actually producing fake data analysis.  This is to be used either for your own April Fools proclivities, or perhaps as a way to help you recognize real scientific shenanigans. While it is a bit of an existential quandary, when producing fake data analysis you need to generate “good” random values for your data.  Whatever phenomenon you are alleging to analyze, people will not be convinced if the values do not match their preconceived bias about that process.  In the case of survival times to seeing various events on chat roulette, my assumption (after toying around on the service a bit) was that seeing lonely men and penises were highly probable; therefore, I needed to generate random time values with relatively low means. Fortunately, R provides random number generators for nearly every distribution, thus making it trivial to generate data from any number of functional forms.  For the purposes of creating random time values with low means I choose the chi-square distribution.  Through rounding, the continuous random values of the chi-square can be converted into discrete times, and by adjusting the k parameter we can get mean values that seem to reasonably approximate my assumptions.  To test, simply generate a large sample of random values and plot:  

  For this analysis, I used k=1 for the time to seeing a lonely man, and k=2 for the time to seeing a penis.  On the other hand, I assumed that the time to seeing drunk people and a woman would be uniformly distributed over different intervals.  I believed it was reasonable to see a group of drunk people sometime before your first 20 minutes on chat roulette, while women were much rarer; you would be very lucky to see one even after your first hour. This is trivial.  We have assumed functional forms, now all we have to do is turn that intoan R data frame.  As I was faking a survival analysis, I had to create additional data specific  to this type of analysis, which simply involved creating a bunch of 1′s to go with the times as observation indicators and identification values.  Creating quality visualization is critical to real analysis, so it follows that it would be equally important in fake analysis as well.  People read titles and axis labels, so be sure to make them very descriptive.  Finally, it is crucial that you also provide the data with the analysis.  While most people will not actually bother to download the data, the fact that is available makes the whole thing seem more legitimate.  Output a CSV file, upload it, and you are all set.   Congratulations, you have now produced fake data analysis in three easy steps.  Now, do not every actually do this, but recognize how easy it is. "	 0 Comments
CLT Standard Normal Generator	https://www.r-bloggers.com/2010/04/clt-standard-normal-generator/	April 2, 2010	Lee	"I’ve found this standard normal random number generator in a number of places, one of which being from one of Paul Wilmott’s books.  The idea is that we can use the Central Limit Theorem (CLT) to easily generate values distributed according to a standard normal distribution by using the sum of 12 uniform random variables and subtracting 6.  In Excel, the implementation looks like this:


=RAND()+RAND()+RAND()+RAND()+RAND()+RAND()+RAND()+RAND()+RAND()+RAND()+RAND()+RAND()-6


By doing a simple cut-and-paste, we can stick this formula in an Excel cell and go on with our merry way assuming we have generated values from a standard normal distribution. But what is really going on here, and how good does this generator work?
 The idea behind this standard normal generator is simple and is based on the Central Limit Theorem.  In a nut shell, if we define the following random variables.



Then we can approximate the distribution of  using the CLT.



Since we know that the mean and variance of  is  and  respectivly.



Finally, if we ‘standardize’  by subtracting the mean and dividing by its standard deviation we get a standard normal random variable.



So we have essentially taken the sum of uniform random variables and used them to approximate a standard normal random variable by applying the CLT. The important thing to keep in mind is that the more uniforms we use to do this, the better the approximation. You may be asking yourself why this looks nothing like the simple Excel formula I showed earlier. Well, something special happens when we use 12 uniforms; things start to simplify!



Voila! We have an easy to implement standard normal random number generator.  We should still be a little concerned about the CLT approximation and we should probably ask ourselves if using only 12 uniform random variables is ‘good enough’.  Now to the fun part! I’ve written the following function which implements the above method in R. 
Using the generated values, we can perform a visual inspection using QQ normal plots for various values of m.  I also generated results using m as 30 since 30 is often used as a rule-of-thumb for applying the CLT.   Based on this output, the generated values have lighter tails than a normal distribution, but using 12 uniforms seems to be ok if one was performing a ‘quick and dirty’ analysis in Excel.  30 uniforms obviously performs better, but things start to slow down considerably and it would probably be better to write a function using the Box-Muller method if better accuracy in the tails was needed.  "	 0 Comments
Lookup Performance in R	https://www.r-bloggers.com/2010/04/lookup-performance-in-r/	April 2, 2010	JD Long	Rumor has it that Joe Adler, author of the O’Reilly Book R in a Nutshell, has joined Linked In as a data scientist.  But that does not keep him from still pumping out some interesting content over at OReilly.com. His latest article is about lookup performance in R. He does a great job giving code samples and explaining what he is doing. Worth reading, for sure. 	 0 Comments
Opening Statements on Markov Chain Monte Carlo	https://www.r-bloggers.com/2010/04/opening-statements-on-markov-chain-monte-carlo/	April 1, 2010	Ryan Rosario	This quarter I am TAing UCLA’s Statistics 102C. Introduction to Monte Carlo Methods for Professor Qing Zhou. This course did not exist when I was an undergraduate, and I think it is pretty rare to teach Monte Carlo (minus the bootstrap if you count that) or MCMC to undergrads. I am excited about this class because to me, MCMC turns Statistics on its head. It felt like a totally different paradigm compared to the regression and data analysis paradigm that I was used to at the time. It also exposes students to the connection between Statistics/MCMC and other fields such as Computer Science, Genetics/Biology, etc. I usually do not have much to talk about during week 1, especially if my class is the second day of the quarter. Today was an exception because I wanted to excite the class about this topic.  Some examples I discussed: You can see my handout here. 	 0 Comments
Frank Harrell’s Regression Modeling Strategies Course Handouts	https://www.r-bloggers.com/2010/04/frank-harrells-regression-modeling-strategies-course-handouts/	April 1, 2010	Stephen Turner		 0 Comments
Quantile LOESS – Combining a moving quantile window with LOESS (R function)	https://www.r-bloggers.com/2010/04/quantile-loess-%e2%80%93-combining-a-moving-quantile-window-with-loess-r-function/	April 1, 2010	Tal Galili	"In this post I will provide R code that implement’s the combination of repeated running quantile with the LOESS smoother to create a type of “quantile LOESS” (e.g:  “Local Quantile Regression”). This method is useful when the need arise to fit robust and resistant (Need to be verified) a smoothed line for a quantile (an example for such a case is provided at the end of this post). If you wish to use the function in your own code, simply run inside your R console the following line: I came a cross this idea in an article titled “High throughput data analysis in behavioral genetics” by Anat Sakov, Ilan Golani, Dina Lipkind and my advisor Yoav Benjamini.  From the abstract: In recent years, a growing need has arisen in different fields, for the development of computational systems for automated analysis of large amounts of data (high-throughput). Dealing with non-standard noise structure and outliers, that could have been detected and corrected in manual analysis, must now be built into the system with the aid of robust methods. […]  we use a non-standard mix of robust and resistant methods: LOWESS and repeated running median. The motivation for this technique came from “Path data” (of mice) which is prone to suffer from noise and outliers. During progression a tracking system might lose track of the animal, inserting (occasionally very large) outliers into the data. During lingering, and even more so during arrests, outliers are rare, but the recording noise is large relative to the actual size of the movement. The statistical implications are that the two types of behavior require different degrees of smoothing and resistance. An additional complication is that the two interchange many times throughout a session. As a result, the statistical solution adopted needs not only to smooth the data, but also to recognize, adaptively, when there are arrests. To the best of our knowledge, no single existing smoothing technique has yet been able to fulfill this dual task. We elaborate on the sources of noise, and propose a mix of LOWESS (Cleveland, 1977) and the repeated running median (RRM; Tukey, 1977) to cope with these challenges If all we wanted to do was to perform moving average (running average)  on the data, using R, we could simply use the rollmean function from the zoo package.
But since we wanted also to allow quantile smoothing, we turned to use the rollapply function. Here is the R function that implements the LOESS smoothed repeated running quantile (with implementation for using this with a simple implementation for using average instead of quantile):  More on the math of the algorithm can be found in the original article.  The following example uses the “airquality” dataset which gives us “Daily air quality measurements in New York, May to September 1973.” With several variables, we will only look at Ozone level and Temperature.
Since high Ozone levels reduces the air quality we breath, I would like to give a prediction of the predicted “worst case” Ozone level (e.g: 95% Ozone level) using to the temperature of the same day. How would you try to do something like that? The first solution would be to use the “rq” function from the Quantile Regression R package, but if we where to look at the data, we would see that fitting a straight line is not suitable for our data (since we have a sharp change in slope around the temperature of 80 degrees).
This is a situation where Quantile LOESS (of 95%) might prove to be useful. Here is the code to produce the above plot. After A considerate e-mail from Dirk Eddelbuettel I corrected myself from using LOWESS to LOESS throughout the article. Here’s an explanation to why I did it and also why I corrected it - Dirk wrote to me: You have a post entitled ‘quantile lowess’ but you then (correctly) use loess.  Do you understand that there are two functions lowess() and loess()?
The former is sort-of a predecessor but nobody but really old books still talks about it.  Google for (maybe) ‘Brian Ripley lowess loess’ as he drove
that point home a few times on r-help. My answer was:

 After Nicholas’s comment I went checking and came across a R-help thread by
Martin Maechler explaining how to update my code from above so that the system will be robust. Martin wrote (My notes are added in []):
One gotcha [when comparing lowess to loess is]– particularly if you were used to the fact that lowess() by default is resistant to outliers {well, in many cases at least} :  I.e., loess() by default is not resistant/robust where as lowess() is. [...] I would however recommend using loess(….., family = “sym”) routinely.  *  *  * If you find this code useful, please let me know about it in the comments. "	 0 Comments
Because it’s Thursday: Epidemiology of the Undead	https://www.r-bloggers.com/2010/04/because-its-thursday-epidemiology-of-the-undead/	April 1, 2010	David Smith	Noted statistician Andrew Gelman has teamed up with occultist George Romero to address the most serious public-health threat of out time: Zombies. They’ve published a paper in the journal Biomastika, “How many zombies do you know?” to propose the use of indirect survey methods to measure outbreaks of the undead: Abstract: The zombie menace has so far been studied only qualitatively or through the use of mathematical models without empirical content. We propose to use a new tool in survey research to allow zombies to be studied indirectly without risk to the interviewers. Building on the research of Lakeland, Gelman & Romero identify a simple solution for tracking the scale of the Zombie menace: surveying non-Zombies (for Zombies are reluctant telephone-users, and face-to-face interviews are completely out of the question) and asking respondents, “How may Zombies do you know?”. Statistical inference can then be used to estimate the number of zombies lurking in the population. Gelman & Romero note that the methodology can be extended to other hidden menaces like aliens and Frequentists. (Okay, Gelman might not have mentioned that last one. Phew.)  You can find a link to the published version of the paper below. (An interesting note, although the paper was written in Word, it was published in PDF via LaTeX, “to make it look more like science”. I applaud the choice.)  Biomastika: Gelman & Romero, 12 Mar 2010 	 0 Comments
Plots in R and the ImageJ visualization	https://www.r-bloggers.com/2010/04/plots-in-r-and-the-imagej-visualization/	April 1, 2010	» R	"If you plot data in R and you would like to display the same data in the ImageJ view it is necessary to transfer the data matrix to ImageJ. The first thing which can be noticed is that the image data is displayed rotated because of the Bio7 approach to transfer data forth and back in the following way: imagedata(x,y) whereas (x0,y0) can be found in the upper left corner (Java coordinates in images!). In R values in a matrix are referenced with matrix(rows, columns) thats why images plotted with R and images (matrix data) displayed in ImageJ are different oriented. Plot created with R using a spatstat (spatial analysis package) example and displayed in ImageJ  Pure R matrix data transferred as Floats, coloured with a LUT (LookUpTable) and displayed in ImageJ (Scalebar could be added, too).
  If you would like to get the same view in ImageJ as the R plot simply transform the image (for the spatstat example rotate the image 90° in the left direction; for an image plot view like R flip the image vertically).  Of course another possibility to get the same result would be to adjust the coordinate system in the R plot. But this example illustrates the differences between the R coordinates and the ImageJ (Java) coordinates. "	 0 Comments
Vanilla Rao-Blackwellisation [re]revised	https://www.r-bloggers.com/2010/05/vanilla-rao-blackwellisation-rerevised/	May 31, 2010	xi'an	Although the revision is quite minor, it took us two months to complete from the time I received the news in the Atlanta airport lounge… The vanilla Rao-Blackwellisation paper with Randal Douc has thus been resubmitted to the Annals of Statistics. And rearXived. The only significant change is the inclusion of two tables detailing computing time, like the one below  which provides different evaluations of the additional computing effort due to the use of the Rao–Blackwellisation: median and mean numbers of additional iterations, $80%$ and $90%$ quantiles for the additional iterations, and ratio of the average R computing times obtained over $10^5$ simulations. (Turning the above table into a formula acceptable by WordPress took me for ever, as any additional white space between the terms of the matrix is mis-interpreted!) Now, the mean time column does not look very supportive of the Rao-Blackwellisation technique, but this is due to the presence of a few outlying runs that required many iterations before hitting an acceptance probability of one. Excessive computing time can be curbed by using a pre-set number of iterations, as described in the paper… 	 0 Comments
MLB Baseball Pitching Matchups ~ manipulating pitch f/x data using the RMySQL package in R	https://www.r-bloggers.com/2010/05/mlb-baseball-pitching-matchups-manipulating-pitch-fx-data-using-the-rmysql-package%c2%a0in%c2%a0r/	May 31, 2010	apeescape	"After downloading some pitch f/x data using my R script, we can finally have some fun. But because the pitch f/x data is very elaborate, R can easily get overwhelmed by copying the dataset back and forth in memory, as you manipulate the data. So the natural progression is to use relational database systems. Here, I show a simple example of using the RMySQL package to plot the pitch types of Tim Lincecum’s 2010 season, extracted from the source pitch f/x data from MLB Gameday. So first, I downloaded the 2010 data using the following script (with a weird error, I was only able to download till 5/4/10, but I can easily concatenate the dataset by running the script again for later dates): Then, I start MySQL from the BASH shell to create a database called “pitchfx” within the MySQL shell (this on Mac OS 10.6, not really familiar w/ Windows). Then we connect to the MySQL database (“pitchfx”) from R from root. From there, I can write the text file to the MySQL database by specifying a table (“2010Apr04May04″) and the file location (“./pitch2010Apr04May04.txt”). It’s important to know dbWriteTable() can include certain input arguments from read.table() so you can specify how the data will be translated into a MySQL database. We can get the pitcher code for Tim Lincecum by looking online to then call a SQL query to get all the pitch info from Lincecum (hopefully I will have script to get pitcher/batter codes shortly). It’s somewhat of a pain to go through extracting info from the data frame, but here it is:  Where “FF” is the four seam fastball, “FT” the two seam fastball and “FA” when Gameday can’t figure out which. 
Filed under: Baseball, R, RMySQL        

 "	 0 Comments
R 2.11.1 released	https://www.r-bloggers.com/2010/05/r-2-11-1-released/	May 31, 2010	David Smith	It’s official: R 2.11.1 is out. Source code and binaries for Windows and MacOS are available at the master CRAN mirror, and will be available for download from your local mirror soon. As anticipated, this is an update release focussing mainly on bugfixes and with just one new feature. According to R core team member Peter Dalgaard one fix in particular may be significant if you work with large quantities of dates: a problem with format.POSIXlt that could cause a crash when working with long date vectors has been fixed. You can see the complete list of changes in the current NEWS file.  R-announce mailing list: R 2.11.1 is released 	 0 Comments
A logo for R?	https://www.r-bloggers.com/2010/05/a-logo-for-r/	May 31, 2010	Matt Asher	" In light of my recent attempt at aRt, Tal from R bloggers suggested I submit a T-shirt design for this contest. That got me thinking that R needs a logo freshining in general, so I dusted off my technical pens and drafted something. I’ll explain why I think this makes a good logo for R, but before I defend the “Rtichoke”, perhaps you could come up with some of your own reasons why it works? I’ll give you a moment…. OK. Here’s my justification for the logo: My submission doesn’t quite meet the T-shirt requirement (it’s more that one color), but if folks like it I can create a one-color version and submit it properly to the contest and the general R community for considering. Cheers. UPDATE:
I created an all-blue version so you can get a feel for what it would look like with reduced colors:  UPDADE 2: I made the lines a bit bolder and made a minor tweak. I think this is the best “all blue” version yet. Opinions?  "	 0 Comments
highlight 0.2-0	https://www.r-bloggers.com/2010/05/highlight-0-2-0/	May 31, 2010	romain francois	"I’ve released version 0.2-0 of highlight to CRAN This version brings some more additions to the sweave driver that uses highlight to produce nice looking vignettes with color coded R chunks The driver gains new arguments boxes, bg and border
to control the appearance of the code chunks. When boxes is set to TRUE, the R code chunks are surrounded in boxes, and the arguments bg and border control the background color and the color of the box Also, when the other highlight is available, the driver will also color code example code in any language that highlight supports. To use this, just surorund the code with  for the language foo. For example:  will output the content of the code chunk as highlighted c++. The Rcpp-modules vignette in the next version of Rcpp uses both these new features. (see the vignette source in r-forge. The vignette is rendered into latex using :  "	 0 Comments
Bike The Drive 2010	https://www.r-bloggers.com/2010/05/bike-the-drive-2010/	May 31, 2010	Thinking inside the box		 0 Comments
Betting on Pi	https://www.r-bloggers.com/2010/05/betting-on-pi/	May 31, 2010	Matt Asher	 I was reading over at math-blog.com about a concept called numeri ritardatari. This sounds a lot like “retarded numbers” in Italian, but apparently “retarded” here is used in the sense of “late” or “behind” and not in the short bus sense. I barely scanned the page, but I think I got the gist of it: You can make money by betting on numbers which are late, ie numbers that haven’t shown up in a while. After all, if the numbers in a lottery or casino are really random, that means it’s highly unlikely that any one number would go a long time without appearing. The “later” the number, the more likely it is to appear. Makes sense, right? Before plunking down my hard(ly) earned cash at a casino, I decided to test out the theory first with the prototypical random number: Pi. Legend has it that casinos once used digits from Pi to generate their winning numbers. Legend also has it that the digits of Pi are so random that they each appear with almost exactly 1 in 10 frequency. So, given this prior knowledge that people believe Pi to be random, with uniform distribution of digits and no discernible pattern, I can conclude that no one digit should go too long without appearing. I pulled down the first 10 million digits from here (warning, if you really want this data, right click the link and “save as”). Then I coded up a program in the computer language R to scan through the digits of Pi, one by one, making a series of “fair” bets (1:9 odds) that the next number to appear in the sequence would be the one that had gone longest without appearing. My code is shown below. I had to truncate the data to 1 million digits, and even then this code will take your Cray a good while to process, most likely because I have yet to master the use of R’s faster “apply” functions with complicated loops. So what was the result? How good was my strategy? After an initial 100 digits to build up data about which digits were latest, I placed a total of 999,900 bets at $1 each. Final earnings: $180. That’s so close to breaking even that it’s almost inconceivable. I had 100,008 wins and 899,892 losses. My winning percentage was 10.0018% percent. On the face of it, this result seemed almost a little too good, dare I say even suspiciously good, if you know what I mean. How rare is it to get this close (or closer) to the exact perfect proportions after so many trials? Assuming that the number of wins followed a binomial distribution with , my total wins should follow a Normal distribution with mean 99,990 and variance  (for an “n” of almost a million and non-minuscule “p”, the Normal approximation to the Binomial should be essentially perfect). Taking the square root of the result, and we get almost exactly 300 as our standard deviation. That’s much larger than the 18 extra wins I had. In fact, the chances that you will land within  standard deviations on either side of the Normal’s mean are less than 5%. Before getting too worked up over this result, I decided to take a look at the graph. Using the code: I got this:  The graph looks pretty much like any random walk, doesn’t it? So the fact that I ended up breaking almost exactly even had to do with the stopping point, not any “unusual” regularity. Just to see if I might salvage any mystery, I tested the very lowest point, -$2,453, which occurred after 202,133 trails. Even that falls within 2 standard deviations of the expected mean for that number of trials, and of course cherry picking the most extreme point to stop at isn’t a fair way to go about this. Any last hope that the graph might be unusual? I plotted a couple random walks using numbers generated in R. Most of them looked like this:  This looks to have the same level of “jaggedness” as  the results of my bet on Pi. Unfortunately, I am forced to conclude that the promising strategy of “late number” gambling turned out to be fairly retarded after all, at least so far as it applies to the digits of Pi. 	 0 Comments
A data visualization manifesto	https://www.r-bloggers.com/2010/05/a-data-visualization-manifesto/	May 31, 2010	Andrew Gelman		 0 Comments
JPM Chase Corporate Challenge 2010	https://www.r-bloggers.com/2010/05/jpm-chase-corporate-challenge-2010/	May 31, 2010	Thinking inside the box	"
This time we all got chip-timing via a small (rfid ?) strip tagged to back of
the bib number.  Which is handy as I managed to not stop my time by hand
correctly. Given that I am still nursing a sore Achilles tendon and don’t
train well or much, the time of 23:51 (or 6:48 min/mile) was ok
compared
to
the
other
seven
previous
times
I have run this.


 "	 0 Comments
Example 7.39: Nelson-Aalen estimate of cumulative hazard	https://www.r-bloggers.com/2010/05/example-7-39-nelson-aalen-estimate-of-cumulative-hazard/	May 31, 2010	Nick Horton		 0 Comments
Simulating a Queue in R	https://www.r-bloggers.com/2010/05/simulating-a-queue-in-r/	May 30, 2010	Neil Gunther		 0 Comments
Talk at CRiSM	https://www.r-bloggers.com/2010/05/talk-at-crism/	May 30, 2010	xi'an	This is the talk I am giving at the workshop on model uncertainty organised by the Centre for Research in Statistical Methodology (CRiSM) at the University of Warwick, on May 30-June 1. Careful readers will notice there is not much difference with my previous talk on the topic, as I only included the Savage-Dickey slides from the talk in San Antonio!  	 1 Comment
Dynamic Modeling 2: Our First Substantive Model	https://www.r-bloggers.com/2010/05/dynamic-modeling-2-our-first-substantive-model/	May 30, 2010	Tony	"(This is the second of a series of ongoing posts on using Graph Algebra in the Social Sciences.) First-order linear difference equations are powerful, yet simple modeling tools.  They can provide access to useful substantive insights to real-world phenomena.  They can have powerful predictive ability when used appropriately.  Additionally, they may be classified in any number of ways in accordance with the parameters by which they are defined.  And though they are not immune to any of a host of issues, a thoughtful approach to their application can always yield meaningful information, if not for discussion then for further refinement of the model. Let’s look at that example from the last post: OK, time to reveal the secret.  Here’s how to do it: df <- read.csv(file=""http://nortalktoowise.com/code/datasets/Electricity.csv"", head=TRUE, sep="","")

attach(df)

lagvar <- function(x,y){return(c(rep(NA,y),x[-((length(x)-y+1):length(x))]))}

lagvar1 <- lagvar(electricalusage,1)

model <- lm(electricalusage ~ lagvar1)

y1 <- 290

y2 <- 0

t <- 0

a <- model$coefficients[[2]]

b <- model$coefficients[[1]]

timeserieslength <- nrow(df)

for (i in 1:timeserieslength) {

y2[i] <- (a*y1[i])+b

t[i] <- i

if (i < timeserieslength) y1[i+1]=y2[i]}

plot(year, electricalusage, xlab=""Year"", ylab=""Electricity Usage (in per capita KWh)"", main=""Figure 1: Annual Electricity Usage, 1920-70"", pch=19)

lines(year, y2, lwd=2) Alright, this probably needs a little bit of explaining.  This first few lines are just getting the data from the URL, and attaching the set so we can call the variable names directly.  I’ll have to justify that weird function.  Instead of breaking it down character for character, I’m just going to get away with explaining what it does.  lagvar is function that takes a table, looks at the variable you tell it to (in this case, electricalusage), and copies it into a new array.  Why would you want that?  Well, it has the added flexibility of delaying the variable by a number of rows (which you can readily specify.  This one, for instance, takes our dataset: > df

year electricalusage

1  1920             339

2  1921             347

3  1922             359

4  1923             368

5  1924             378

...

47 1966            5265

48 1967            5577

49 1968            6057

50 1969            6571

51 1970            7066 and generates lagvar1, which “lags” electricalusage by one row: > newdata

year electricalusage lagvar1

1  1920             339      NA

2  1921             347     339

3  1922             359     347

4  1923             368     359

5  1924             378     368

...

47 1966            5265    4933

48 1967            5577    5265

49 1968            6057    5577

50 1969            6571    6057

51 1970            7066    6571 This is extremely useful for calculating differences over time.  Then we run a simple linear regression: electricalusage as explained by lagvar1.  However, the regression is not the end-all of this process (and a million freshman statistics students gasp in horror).  We just run the regression to estimate our slope and intercept (a and b respectively).  The rest of the code if effectively identical to the example from my last post, though I must make one small confession: you might have noticed that y1 is a constant, while the other parameter variables are called from the dataset or linear model.  The truth is that I don’t know an easy mathematical way to pick a perfect y1.  It should be pretty close to electricalusage at the beginning of the graph, which means 339-ish.  It’s actually a bit lower (290) because you must pick a value which represents the electricalusage in a place where the line doesn’t go, just the tiniest bit behind the graph.  In other words, just estimate it.  If your curve is off-target, just fiddle around with the y1 value until it looks right.  But if your y1 is crappy, your prediction curve is still going to beat the hell out of the linear model: abline(model, lwd=2) 
 So, what’s Y* in this graph?  Mathematically it is b/(1-a) = 64, but the substantive implication is silly.  The close fit seen in figure 1 is compelling for predictive accuracy (and this may indeed be a fine graph for future predictions), but taking this model at face value still leaves us with a completely incorrect substantive conclusion: Human beings back to the beginning of time have had about 64 kWhrs at their disposal annually.  While this may not play a meaningful part in any model which incorporates these data, it is an example of the need to at least be aware of the substantive implications of a model. Next time: When a first-order linear difference equation doesn’t cut it. References: Code adapted from http://www.courtneybrown.com/classes/ModelingSocialPhenomena/Assignments/Assignment2CourtneyBrownMathModeling.htm Dataset from http://www.courtneybrown.com/classes/ModelingSocialPhenomena/Assignments/Assignment2ElectricEnergyCourtneyBrownMathModeling.htm "	 0 Comments
Notice that even though output is in a log scale, output is…	https://www.r-bloggers.com/2010/05/notice-that-even-though-output-is-in-a-log-scale-output-is/	May 29, 2010	Human Mathematics	Notice that even though output is in a log scale, output is shooting up in an exponential way. DATA from Brad DeLong 	 0 Comments
Source Code Files in R	https://www.r-bloggers.com/2010/05/source-code-files-in-r/	May 29, 2010	C		 0 Comments
Weekend art in R (part 1?)	https://www.r-bloggers.com/2010/05/weekend-art-in-r-part-1/	May 29, 2010	Matt Asher	"
As usual click on the image for a full-size version. Code: "	 1 Comment
highlight 0.1-9	https://www.r-bloggers.com/2010/05/highlight-0-1-9/	May 29, 2010	romain francois	"The version 0.1-8 of highlight 
introduced a small bug in the latex renderer.  This is now fixed in version 0.1-9 and the latex renderer also gains an argument “minipage” which wraps the latex code in a minipage environment. I’ve used this to make this vignette for an upcoming feature of Rcpp "	 0 Comments
Syncing files across computers using DropBox	https://www.r-bloggers.com/2010/05/syncing-files-across-computers-using-dropbox/	May 29, 2010	Tal Galili	" In the past few months I have been using DropBox for syncing my work files between my home and work computer.  It has saved me from numerous mistakes and from sending the files to myself via e-mail. Recently I found this service highly useful for sharing files with 4 other people with whom I am working on a data analysis project.  Being so happy with it (and also by gaining more storage space by inviting friends to use it), I thought of sharing my experience here with other R users that might benefit from this cool (free) service. Dropbox is a Software/Web2.0 file hosting service which enable users to synchronize files and folders between computers across the internet.
This is done by installing a software and then picking a “shared folder” on your computer.  From that moment on, that folder will be synced with any computer you choose to install the software on (for example, your home/work computer, your laptop – and so on)  DropBox also enables users to share some of their folders with other DropBox users.  This seamless integration of the service with your OS file system (Windows, Mac or Linux) is what’s making this service so comfortable, by allowing me to work with co-workers and have the same “project tree” of folders, all of which are always synced. You could also share a file “online”, by getting a link to it which you could share with others.  So for example, you could write an R code, share it online, and call to it later with source().  This is the easiest way I know of how to do this.  Dropbox is a “cloud computing” Web2.0 file hosting service offering both free and paid services.  The free version (which I use) offers 2GB of “shared storage” (unless you invite other users, in which case you get some extended storage space.  Which is one of my motivations in writing this post). Dropbox has other non-trivial uses allowing one to: The service’s major competitors are Box.net, Sugarsync and Mozy, non of which I have had the chance of trying. Simply go to: DropBox.com
Sign up, install the software, use the new shared folder, and let me know if it helped you   "	 0 Comments
An XML Representation of the Keys to Soil Taxonomy?	https://www.r-bloggers.com/2010/05/an-xml-representation-of-the-keys-to-soil-taxonomy/	May 28, 2010	dylan	Western Fresno Soil Hierarchy: partial view of the hierarchy within the US Soil Taxonomic system Maybe this is just craziness, but wouldn’t be neat to have an XML formatted version of the Keys to Soil Taxonomy? The format might look something like the following code snippet, although there may be more efficient uses of XML… The only problem I can see is that it would take a hell of a long time to type in the entire 300+ page document. A complete document of this nature would support all kinds of new and creative uses for the ‘keys– electronic look-up, automated generation of a PDA-ready version, an awesome teaching tool, or just something that could be used to generate cool figures. Anyone know of a quick way to get this put together, or of any similar document that has already been published? Anyone want to help type-in the data?  read more 	 0 Comments
R: More plotting fun with Poission	https://www.r-bloggers.com/2010/05/r-more-plotting-fun-with-poission/	May 28, 2010	Matt Asher	 Coded as follows: 	 0 Comments
Tuesday’s child is full of probability puzzles	https://www.r-bloggers.com/2010/05/tuesday%e2%80%99s-child-is-full-of-probability-puzzles/	May 28, 2010	dan	"COUNTERINTUITIVE PROBLEM, INTUITIVE REPRESENTATION  Blog posts about counterintuitive probability problems generate lots of opinions with a high probability. Andrew Gelman and readers have been having a lot of fun with the following probability problem: I have two children. One is a boy born on a Tuesday. What is the probability I have two boys? The first thing you think is “What has Tuesday got to do with it?” Well, it has everything to do with it. DSN agrees with Andrew that one virtue of the “population-distribution” method is that it forces one to be explicit about various aspects of the problem, and in so doing, causes much confusion to disappear. As a public service this week, Decision Science News presents the population-distribution representation of the problem (what it thinks of as the Gigerenzerian / Hoffragian / Peter Sedlmeier-ian representation of the problem) in a visual form. To follow the logic, see Andrew’s post on how he solved the problem. Voila:  Red means “outside the reference class”. Yellow means “in the reference class but not boy-boy”. Green means “inside the reference class and boy-boy”. Boy-boy in the reference class occurs with probability Green / (Green + Yellow) or 13 /27 NOTE
To see why DSN calls these Gigerenzerian / Hoffragian / Sedlmeierian representations, see: Sedlmeier, P. (1997). BasicBayes: A tutor system for simple Bayesian inference.
Behavior Research Methods, Instruments & Computers, 29(3), 328-336. Gigerenzer, G., & Hoffrage, U. (1995). How to improve Bayesian reasoning without instruction: Frequency formats. Psychological Review, 102,, 684–704. (Sorry for not using R, excel is just darn fast for some things) "	 0 Comments
Dynamic Modeling 1: Linear Difference Equations	https://www.r-bloggers.com/2010/05/dynamic-modeling-1-linear-difference-equations/	May 28, 2010	Tony	"(This is the first in a series on the use of Graph Algebraic models for Social Science.) Linear Difference models are a hugely important first step in learning Graph Algebraic modeling.  That said, linear difference equations are a completely independent thing from Graph Algebra.  I’ll get into the Graph algebra stuff in the next post or two, but for now bear with me.  Linear Difference models are very cool. Linear difference models are basically recursively-calculated functions.  They generally take the form: Y(t+1) = aY(t) + b Where t indicates a period in time, t+1 is the next period in time, and a and b are constants.  This is strikingly similar to a much more familiar equation to anyone with high-school Algebra I.  Does Y = mx + b ring any bells?  It’s the same idea here.  However, here’s the weird thing: this method draws curves. Wait, What? Yup, curves.  Despite the name, we can use linear difference equations to generate the above graph (and many other cool ones).  The idea is that because you’re recursively calculating the operative equation, the equation represents a straight line between two discrete points, t and t+1.  And the line between the next two points will be different.  Thus, when we literally connect the dots, we extrapolate a curve-like picture.  (Those with a background in Calculus will recognize how this is simply an approximation, and that as the distance between the points is shrunk towards zero, a true curve will emerge.  However, in order to do this, we’ll need to talk a little about the calculus-equivalent of Difference equations, which are of course Differential Equations–another topic for later). Here’s how you do it in R. If you run the code as-is, you’ll notice something interesting:  The code doesn’t match the graph shown above.  In fact, this is a completely manufactured example, with no substantive basis.  This is a by-product of the parameters you set the model to run with.  The variables you can alter to play around with different graphs are pretty obvious: a, b, timeserieslength, and y1. So this is a good start.  But let’s think about a a little bit.  If a is negative, what happens?  Well, if a > -1, you’ll have an interesting little oscillation that eventually hits an equilibrium.  If a < -1, the oscillation increases as t increases, meaning you can follow t backwards to equilibrium, or forwards to increasing divergence.  Now, the special case: what about a =  -1?  This one is cool.  It bounces back and forth between two constant values.  Very unusual, very useful. Here are some examples with their parameters: a = -.6 a = -1.6 a = -1 A note about that equilibrium I just mentioned.  We can calculate it deterministically!  Check this out: Equilibrium is the value Y* such that Y* = Y(t) = Y(t+1).
Substituting Y* into the first equation, Y* = aY* + b
Solving for Y* gives us Y* = b/(1-a)
Thus, for any linear difference equation (such that a﻿ ≠ 1)﻿, the limit as Y(t) approaches equilibrium is the Y* shown above. Stay tuned.  Next time I’ll show how to parse useful information out of a bivariate regression to make substantive models. References: Code Adapted from http://courtneybrown.com/classes/ModelingSocialPhenomena/Assignments/Assignment1CourtneyBrownMathModeling.htm Brown, Courtney. “An Introduction to Linear Difference Equations.” Presentation for POLS 490, Emory University, Spring 2010.  Available Online, http://courtneybrown.com/classes/ModelingSocialPhenomena/Presentations/DifferenceEquationsIntroduction.ppt Huckfeldt, R. Robert, C. W. Kohfeld, and Thomas W. Likens. 1982. Dynamic Modeling: An Introduction. Newbury Park, California: Sage  Publications.          Series: Quantitative Applications in the Social Sciences,  Number          27. "	 0 Comments
Must Have Software	https://www.r-bloggers.com/2010/05/must-have-software/	May 28, 2010	John Mount	Having worked with Unix (BSD, HPUX, IRIX, Linux and OSX), Windows (NT4, 2000, XP, Vista and 7) for quite a while I have seen a lot of different software tools.  I would like to quickly exhibit my “must have” list.  These are the packages that I find to be the single “must have offerings” in a number of categories.  I have avoided some categories (such as editors, email programs, programing language, IDEs, photo editors, backup solutions, databases, database tools and web tools) where I have no feeling of having seen a single absolute best offering. The spirit of the list is to pick items such that: if you disagree with an item in this list then either you are wrong or you know something I would really like to hear about.   I look forward to learning which of my choices are considered poor and what your must-haves are. Related posts: 	 0 Comments
Creating surface plots	https://www.r-bloggers.com/2010/05/creating-surface-plots/	May 28, 2010	Ralph	A 3d wireframe plot is a type of graph that is used to display a surface – geographic data is an example of where this type of graph would be used or it could be used to display a fitted model with more than one explanatory variable. These plots are related to contour plots which are the two dimensional equivalent. To illustrate this type of graph we will consider some surface elevation data that is available in the geoR package and was used in the blog post on level plots. The data set in this package is called elevation and stores the elevation height in feet (as multiples of ten feet) for a grid region of x and y coordinates (recorded as multiples of 50 feet). This post has details of the various operations that are undertaken to prepare the data for graphing. Base Graphics Fast Tube by Casper The function persp is the base graphics function for creating wireframe surface plots. The persp function requires a list of x and y values covering the grid of vertical values which is specified as the z variable. The heights for the display are specified as a table of values which we saved previously as the object z during the calculations when the local trend surface model was fitted to the data. The text on the axis labels are specified by the xlab and ylab function arguments and the main argument determines the overall title for the graph. The function arguments phi and theta are used to rotate the viewing angle of the surface. Trial and error is probably the way to go when setting these as good choices depend entirely on the shape of the surface being displayed. Base Graphics Surface Plot The surface is clear and easy to determine the shape and variation in height across the x and y grid coordinates. Lattice Graphics Fast Tube by Casper The lattice graphics package has a function wireframe and we use the data in the object elevation.fit to create the graph. We use the formula interface to specify first the z axis data (the heights) followed by the two variables specifying the x and y axis coordinates for the data. The axes labels and title are specified in the same way as the base graphics with the xlab, ylab and main function arguments. A colour key is added using the colorkey function argument and setting it to TRUE. Lattice Graphics Surface Plot The surface produced by the wireframe function is similar to the persp function with the main difference between the colours used on the surface. This blog post is summarised in a pdf leaflet on the Supplementary Material page. 	 0 Comments
Because it’s Friday: The dating equation	https://www.r-bloggers.com/2010/05/because-its-friday-the-dating-equation/	May 28, 2010	David Smith	"According to internet lore, there’s a mathematical equation that governs the lower bound for the socially acceptable age of a potential dating partner: half your age plus 7, or, in mathematical terms, if x is your age then the lower bound is f(x) = x/2 + 7. Seems simple, right? if you’re 20, then the minimum socially acceptable age for a date is 17. But it was The Ragbag who pointed out that this rule has a duality: for the date to be socially acceptable, the rule must be adhered to by your date as well. In other words, there’s a socially acceptable maximum too, given by inverting the equation: It was only yesterday that i realised that the rule of thumb for dating people of different ages (the “half your age plus 7” rule) determines not only the lower bounds for dating but the upper bounds as well—that for each ½x + 7, there is a corresponding 2(x-7). For the last 15 years of my life, i have been ignoring an entire market segment, namely those of the genus cougar. The Ragbag handily plotted out the socially acceptable upper and lower bounds for dates at various ages in the chart below: 
  Notably, the chart also implies that there is a socially acceptable lower bound for your own age: below the age of 14, by this rule, you shouldn’t be dating at all. Incidentally, if you’re wondering how well is this rule adhered to in practice, the dating website okcupid.com has done an extensive analysis of age preferences amongst their members. A really interesting analysis (and quite sophisticated – maybe done in R?), especially for heterosexual males looking for the ideal age-ranges for potential dates.  The Ragbag: The half your age plus 7 rule   "	 0 Comments
Follow Rmetrics on Twitter	https://www.r-bloggers.com/2010/05/follow-rmetrics-on-twitter/	May 28, 2010	ellis		 0 Comments
Rmetrics 2010	https://www.r-bloggers.com/2010/05/rmetrics-2010/	May 28, 2010	romain francois	The 4th User/Developer Meeting on computational Finance and Financial Engineering (Rmetrics 2010) will take place once again in Meielisalp. This is the first time I’ll attend the conference, but I’m not coming empty handed. I’ll present the work Dirk and I have done on Rcpp since version 0.7.0. See the abstract for my talk.  	 0 Comments
A repulsive random walk	https://www.r-bloggers.com/2010/05/a-repulsive-random-walk/	May 28, 2010	xi'an	"Matt Asher posted an R experiment on R-bloggers yesterday simulating the random walk  which has the property of avoiding zero by quickly switching to a large value as soon as  is small. He was then wondering about the “convergence” of the random walk given that it moves very little once  is large enough. The values he found for various horizons t seemed to indicate a stable regime. I reran the same experiment as Matt in a Monte Carlo perspective, using the R program The outcome of this R code plotted above shows that the range and the average of the 100 replications is increasing with t. This behaviour indicates a transient behaviour of the Markov chain, which almost surely goes to infinity and never comes back (because at infinity the variance is zero). Another indication for transience is shown by the fact that  comes back to the interval (-1,1) with probability , a probability which goes to zero with . As suggested to me by Randal Douc, this transience can be established rigorously by considering 

Related
ShareTweet




To leave a comment for the author, please follow the link and comment on their blog:  Xi'an's Og » R.

R-bloggers.com offers daily e-mail updates about R news and tutorials about learning R and many other topics. Click here if you're looking to post or find an R/data-science job.

Want to share your content on R-bloggers? click here if you have a blog, or  here if you don't.
 "	 0 Comments
The guessing game in R (with a twist, of course)	https://www.r-bloggers.com/2010/05/the-guessing-game-in-r-with-a-twist-of-course/	May 27, 2010	Matt Asher	Maybe you remember playing this one as a kid. If you are about my age, you may have even created a version of this game as one of your first computer programs. You guess a number, the computer tells you if you if you are too low or high. I’ve limited the number of maximum guesses, and randomized the computer’s choice based on the Poisson distribution (more on that later). Here’s the code. This was part of my attempt to understand how R reads input from the command line. One of the things I learned: you may need to save this to a file and run it with “source()”, instead of running it directly from the console, line by line. Note 1: My code makes a couple uses of the aparently controversial “break” function. I can still recall a heated debate I had with a CS professor who believed that calling “break” (in Python) was as bad as crossing the streams of your Proton Pack. That said, I have sucessfully used it on several occasions now without any appearance by Stay Puft Marshmallow Man or changing the natural order between dogs and cats. In R, the biggest problem with using constructs like “break” and “while” is that, for reasons clear only to readers of this blog but not myself, if you ask R for help about either of these tokens using  you get an sent an error or to purgatory, respectively. Hint: Because the random guesses are Poisson based, using a “half the distance” strategy for guessing may not be the best way to go. The hardcore amongst yourselves might want to calculate the median of the expected value conditional on having guessed too low or high. Note 2: The Poisson isn’t a very good distribution for for this. Maybe you can find a better one, or at least jack up the dispersion like an overzealous offroader tweaking the suspension of his 4Runner.  	 0 Comments
Getting Parent Material Data out of SSURGO	https://www.r-bloggers.com/2010/05/getting-parent-material-data-out-of-ssurgo/	May 27, 2010	dylan	" 
Parent material data is stored within the copm and copmgrp tables. The copm table can be linked to the copmgrp table via the ‘copmgrpkey’ field, and the copmgrp table can be linked to the component table via the ‘cokey’ field. The following queries illustrate these table relationships, and show one possible strategy for extracting the parent material information associated with the largest component of each map unit.   
Several of the example queries are based on this map unit:

 read more "	 0 Comments
Canadian CPI: Visualization Brainstorm	https://www.r-bloggers.com/2010/05/canadian-cpi-visualization-brainstorm/	May 27, 2010	Wojciech Gryc	After finishing the R prototype for data visualization, I’ve started abstracting the various methods necessary to create beautiful graphs. While there’s no preliminary version of the R package yet, I think I’ve taken a number of exciting steps. These include: I chose to present data on the Canadian consumer price index (CPI). This is freely available data and serves as a reminder of the major political issue of our time… While I don’t want to make this post political, the ultimate goal of this blog is to use such visualizations and mathematical models to better understand public policy and the role of data mining therein. Might as well start referencing useful data in this regard. So without further ado, here’s the graph… The next step is fairly clear: making the above possible in R! 	 0 Comments
Solving Factor Models	https://www.r-bloggers.com/2010/05/solving-factor-models/	May 27, 2010	rmetrics		 0 Comments
WU Wien presentations	https://www.r-bloggers.com/2010/05/wu-wien-presentations/	May 27, 2010	Thinking inside the box	"
On Friday, I also gave an informal lecture / tutorial / workshop to some of the
Stats and Finance Ph.D. students, drawing largely from the section on
parallel computing of the most recent

Introduction to High-Performance Computing with R tutorial.

 
My sincere thanks to Kurt Hornik and Stefan Theussl for the invite — it was
a great trip, notwithstanding the mostly unseasonally cold and wet weather.


 "	 0 Comments
How to map your Twitter social network	https://www.r-bloggers.com/2010/05/how-to-map-your-twitter-social-network/	May 26, 2010	David Smith	"Ever wondered which Twitterers you and a friend share? Using R and the twitteR package, there’s an easy way to find out. Cornelius Puschmann hacked together some R code to do just that for the Humanities and Technology Camp and it seems to work pretty well. Just replace ‘coffee001’ with the your Twitter username, ‘mypassword’ with your Twitter password, and ‘Cornelius’ with your friend’s Twitter name, and the code does the rest. The only problem I had was that my graph was very cluttered, and despite the comments indicating you can set n to a smaller number, it didn’t work for me. (The code is very clearly commented, by the way.) I had to add the following two lines to make a readable chart: friends.object followers.object  The chart of followers I (@revodavid) share with the RevolutionR twitter account is below. 
  Although to be honest, looking at the chart now, I’m not actually sure this is our shared followers. I’ll have to look at the code more closely to figure out what’s going on; in any case it’s a neat example of integrating R with the Twitter API. Cornelius Puschmann’s Blog: Code and brief instruction for graphing Twitter with R "	 0 Comments
Zone of instability	https://www.r-bloggers.com/2010/05/zone-of-instability/	May 26, 2010	Matt Asher	 I woke up from my afternoon nap feeling a bit off-kilter, so I decided to go for another random walk. In particular, I wanted a journey that avoided the center, but didn’t just run for an exit either. After playing around for a while I came up with this: Basically, during each iteration the program samples from a normal distribution centered at the same location as the previous iteration, with standard deviation equal to the inverse of the previous location. So if the sequence is at 5, the next number will be sampled from the  distribution. Run it a few times and you’ll see how the blue dot bounces around for a bit near 0, then shoots off to one side or the other, where it will most likely hang out for the rest of its life. There are a number of interesting questions about this sequence which, sadly, will remain unanswered. Among these are: For a given number of iterations, how many times is this sequence expected to cross zero? What is the maximum (or minimum) value the sequence is expected to obtain over a fixed number of iterations? Will the sequence ever diverge to some flavor of infinity?  My hunch for this last question is to say no, since the normal distribution is thin-tailed, and the standard deviation is set to converge to 0 (slowly) as the value of the sequence gets larger and larger. At the same time, I suspect that the higher the number of iterations, the larger (in absolute terms) the final number in the sequence. This makes general sense, as the farther you get from 0, the harder it is to return to 0. During testing, I saw a lot of plots that wiggled back and forth, getting closer to the edges of the plot with each wiggle. Since I’m never content to just have a thought without actually testing it out, I plotted the final value in the sequence after  iterations, where x went from 1 to 20. Here’s the result:  Sure enough, as a general trend, the more iterations you run, the farther you are from zero. It would have been interesting to see how the 8th trial ended up north of 300, but I only tracked the final result for these. I suspect that it made up most of the ground in a single leap while sampling from a Normal with extremely high variance (ie when the previous number was very close to 0). Here’s the extra bit of code for comparing final location to number of iterations: These results, I should note, provide very little evidence that the sequence, if extended out to infinite length, will have to converge or diverge. Weird things happen when you start to consider random walks of infinite length, and the one sure limitation of Monte Carlo testing is that no matter how long let a computer simulation run, your PC will crash well before it performs an infinite number of calculations, and most likely before you finish your coffee.  	 0 Comments
Voter targeting with R	https://www.r-bloggers.com/2010/05/voter-targeting-with-r/	May 26, 2010	jjh	"Voter targeting for turnout is the process of scoring registered voters using demographic and electoral variables taken from voter lists and commercial databases. The score of all voters together is used to predict overall turnout, which determines the allocation of campaign resources and directs strategy for voter contact and communication.  Targeting for turnout is a three-step process:  Depending on his or her turnout percentage – high, middling, or low – a voter will be ignored, targeted for persuasion, or targeted for get-out-the-vote (GOTV) efforts by a campaign. Targeting for turnout, along with almost every other type of political targeting, is explained in detail in Political Targeting by Hal Malchow (2008).  In this post, I recreate parts of the regression analysis from Chapter Nine (Targeting for Turnout) of Political Targeting (Malchow 2008), using the free R Project for Statistical Computing. R is a programming environment that excels at data manipulation and statistical analysis, making it an interesting alternative to traditional statistical tools, like SPSS or web-based voter management software. The analysis will be performed against the full voters list from Ohio’s 1st congressional district, with the intention of predicting turnout for the 2010 congressional midterm elections. This analysis is similar or identical to what a candidate for Ohio’s 1st district would perform throughout the election year. The R code for each step in the analysis will be provided inline so a reader can perform the same operations.  A voter file is a list containing electoral and demographic data on registered voters, maintained by state boards of elections, political parties, PACs, or private companies. My voter file was downloaded from the Ohio Secretary of State in late 2009 and contains: name, address, age, registration date, voting history, and party affiliation (primary voters only).  To better simulate what a political campaign would use, I’ve appended the following fields:  Email me here for the code used to scrub and augment the voter file.  I am using the R environment to perform this analysis. To download and install R, go to CRAN homepage and follow the instructions for your platform. Once R us up and running, execute the following to get the required libraries installed and load the voter file into memory: Now the dependencies are installed and the voter file is read into the vfs variable. According to Political Targeting (Malchow 2008), the strongest indicators of participation in a future election are age and previous participation. Malchow also says  participation tends to be consistent between similar elections in different years. I am looking at the 2010 General election, a congressional midterm, so I used the 2006 General election as my guide. The first step is to generate a turnout table. Political campaigns use a tool called “last 4″, which measures a voter’s recent participation. A voter’s last 4 score represents how many of the previous four elections he or she cast a ballot in. A standard is to use both the primary and general elections for the previous two major election years. My data set contains last 4 calculations for the 2010, 2008, and 2006 elections.  The 2006 last 4 calculation looks at elections as far back as the primary in 2002, but a percentage of voters in the list weren’t eligible or registered to vote in some or all of these elections. These voters have an incomplete last 4 score, and need to be evaluated separately so their scores don’t influence voters with a complete history. As such I created two turnout tables: one for voters eligible for all elections (full), and one for voters eligible for at least one of the last four elections (partial). The turnout tables below show a turnout percentage for every combination of age group and participation score for 2006 voters: Turnout percentage for turnout.full:
  Turnout percentage for turnout.partial:
 For each table, we see that turnout percentage increases as previous participation increases for every age group, but it is pretty difficult to compare more than two age groups at once using this table. There are also several anomalous groups with 100% turnout, indicating a small population in that group. We’ll use the R library ggplot2 to create a simple visualization of each table to help interpret the turnout values: Figure 1: 2006 Turnout Percentage by Age as a Function of Previous Participation (full) Figure 2: 2006 Turnout Percentage by Age as a Function of Previous Participation (partial) Figure 1 shows turnout for voters with a complete last 4 score, and tells us that for all age groups except 18-21, turnout increases with previous participation, until turnout reaches a maximum of 85%-90%. The rate at which turnout increases is similar between age groups, suggesting previous participation may have more predictive value than age. Figure 2 is a representation of all voters who were registered in time for the the 2006 general but not for the 2002 primary. Figure 2 shows a relationship between turnout and previous participation, but there is substantially more noise than in Figure 1. Taken together we can verify the hypothesis put forth by Malchow that age and previous participation seem to have a positive influence on future participation.  The 2006 turnout tables are useful but they don’t represent a formal model of turnout the 2006 election. A formal model will measure the interactions between the predictor variables (participation & age) and the intended outcome (turnout) of 2006 voters. This model can be applied to 2010 voters to project turnout.  The model can include the other voter file variables with potential predictive qualities like gender, party affiliation, and martial status. A campaign will traditionally build a linear regression model to project turnout, but linear regression doesn’t support categorical variables and can produce values that don’t make sense for turnout, so I won’t be using that type of regression here.  Instead, I’ll use a generalized linear model to perform a binomial regression with a logit link function (logistic regression). Logistic regression estimates a binary variable given an intercept and a number of independent continuous or categorical predictor variables. R has terrific support for defining and evaluating these models using the base glm package. The goal is to fit a logistic regression on voter data from 2006, and then use that regression to project turnout for 2010. I actually create two regressions, one for voters with at least a 4-year voting eligibility (full model), and one for all other voters (partial model). This is identical to the segmentation used when creating the turnout tables. The output of these regressions is the probability that a voter will turn out in the given year. A campaign can use this figure to estimate total turnout in an election, and to allocate resources to different geographic and demographic segments. The R function glm is used to create two models of 2006 turnout based on last 4 participation, age group, gender, party affiliation, and martial status. Now that I have fitted models, I’ll use the predict function to capture the model output. The output is the likelihood that a voter turned out in 2006 given his last4.2006 score, age group, gender, martial status, and party affiliation. Then I can compare the predicted turnout probability with the actual turnout to determine the effectiveness of each model. This isn’t a valid statistical measure of accuracy but merely a smell test. The prediction rates for our regressions aren’t spectacular: 80% for the full model and 77% for the partial model. Given the limited information in our voter file, though, they aren’t that bad. Additionally, a political campaign would have access to other data like detailed demographics, financial data, and more accurate lifestyle or ideological information. Extending the regression with these variables might increase the predictive power of the system.  Now I’ll apply the regression equations to project turnout in 2010. First, I determine which regression (partial or full) to apply to current voters by their registration date: Next I prepare the data and project 2010 turnout for each model using the predict function: According to pred.g10.full and pred.g10.partial, OH-01 will see 63% overall turnout for voters from the full model and 18% overall turnout for voters from the partial model. To determine the validity of the 2010 projections, I plotted 2006 actual turnout against the 2010 projected turnout for every age group. As stated in the introduction, Malchow says the turnout rates for 2010 should be similar to the 2006 election, so I expect no large unexplainable deviations in the chart. A separate chart is created for each participation model (full, partial): Figure 3: OH-01 2006 Actual Turnout and 2010 Projected Turnout (full) Figure 4: OH-01 2006 Actual Turnout and 2010 Projected Turnout (partial) Figure 3 suggests larger 2010 turnout for all groups as compared to 2006. The projected increase in the 22-29 age group seems unlikely, but can probably be explained by the higher turnout among younger voters in 2008. In addition to 2008 being a presidential election year, the Obama for America campaign focused on registering new voters and activating dormant voters, both of which increased turnout among younger voters. That higher 2008 turnout inflated the last 4 measure for new voters, which pushed up the projected 2010 turnout. Figure 4 exhibits a similar projected increase for the younger age groups, which is probably due to the same increase in 2008 turnout. Before using these models in an actual election, the projections would need to be scaled based on some other turnout estimate. Despite the inflated values, however, this is a strong system for turnout prediction and could be used by almost any congressional campaign.  In addition to scaling, the regression models would need to be improved in several ways before being put into production. The predictor variables are currently considered independently, which effectively discounts any interactive effects that may exist. Turnout for younger married females or unmarried democrats may be better modeled using compound variables, for example. Also, the model makes no use of demographic or opinion survey information available to political campaigns. Finally, the projection isn’t limited to two regressions; a campaign could create regressions by county or school district, or based on marriage status, or any other combination.  While not specifically related to turnout, I produced several simple visualizations that explore the rest of the voter file. The full power of R can be applied using the same voter data from the turnout projections. Figure 5: OH-01 2010 Precinct Summary Figure 6: OH-01 2008 General Turnout by Gender, Age Figure 7: OH-01 2008 Newly Registered Voters by Age Figure 8: OH-01 2008 Newly Registered Voters by Age, Turnout Figure 5 shows the 2008 Democratic turnout percentage and 2008 general turnout percentage for each precinct, and each bubble is scaled to the registered voter population of the precinct it represents. Figure 6 shows 2008 general turnout by gender and age group. Figure 7 is the raw count of voters registered between the day after the 2006 general election and the day of the 2008 general election, broken down by age group. Figure 8 is a variation of Figure 7, showing 2008 turnout of voters registered after the 2006 election. None of these charts took more than 5 minutes to create from concept to output, and ggplot did almost all of the heavy lifting. I believe the ease with which these charts were built highlights the utility of having your data analysis tool also be your visualization tool.  In this short example I’ve analyzed, visualized, and modeled electoral data using R and a few add-on packages. These are standard techniques used by any congressional campaign, but they are usually performed by some by combination of Excel, SPSS, or SQL. By using R, I avoided the compatibility issues usually encountered when transferring data between tools. R would have also allowed me to perform clustering, component analysis, or Bayesian inference on the same data from the same R interface. All together, these reasons make R a good addition to the political analysis toolbox for a campaign or campaign consultant. If you would like to discuss how advanced statistical analysis can help your Democratic campaign model turnout, increase fund-raising, or benchmark field operations, please don’t hesitate to contact me. Click here (14MB) to download the R scripts and data associated with this post. The voter file data has been scrubbed to remove the VoterID, name, and address components.  "	 0 Comments
Testing Out my Pitch F/X Data	https://www.r-bloggers.com/2010/05/testing-out-my-pitch-fx-data/	May 25, 2010	Millsy		 0 Comments
Use SQL queries to manipulate data frames in R with sqldf package	https://www.r-bloggers.com/2010/05/use-sql-queries-to-manipulate-data-frames-in-r-with-sqldf-package/	May 25, 2010	Stephen Turner		 0 Comments
German Tanks, Statistical Intelligence	https://www.r-bloggers.com/2010/05/german-tanks-statistical-intelligence/	May 25, 2010	David Smith	"In World War II, the Allies had a problem: German tanks were often captured, but how many more did the Nazis have in reserve? Allied intelligence estimated around 1400 Panther tanks were being produced a month: a formidable arsenal, and perhaps an insurmountable one given the much smaller numbers being captured or destroyed. But those captured tanks provided exactly the clue needed to get a more realistic assessment of the German production rates: serial numbers. Assuming that the serial numbers on the tanks were assigned in sequence, and looking at the serial numbers of those captured, Allied statisticians came up with simple arithmetic formula  to come up with a less daunting estimate of the production rate: 256 per month. (Production data revealed after the war revealed the true figure to be 255 per month.) Suppose N tanks were actually produced, and k were captured with the highest serial number observed being m. The formula (rather intuitively) estimates the actual number of tanks as the maximum serial number plus the average gap between serial numbers observed, or in formula form, m + m/k – 1. But how well does this work in practice for different values of N (actual tanks)? In a statistical version of a World War II reenactment, the Statistics Blog has recreated the German Tank Problem using R, simulating captured serial numbers for various values of N, and comparing this “true” value with the estimate obtained. The process was repeated for various values of N, and the results plotted against each other. Here are the results: 
 The MLE for the number of German tanks is the highest serial number observed. This is because MLE works backwards, finding the parameter which makes our observation most likely in terms of joint conditional probability. As a result, the MLE for this problem is not only biased (since it will always be less than or equal to the true number of tanks), but dumb as well. Statistics Blog: How many tanks? MC testing the GTP  "	 0 Comments
The Kalman Filter For Financial Time Series	https://www.r-bloggers.com/2010/05/the-kalman-filter-for-financial-time-series/	May 25, 2010	Intelligent Trading		 0 Comments
How many tanks? MC testing the GTP	https://www.r-bloggers.com/2010/05/how-many-tanks-mc-testing-the-gtp/	May 25, 2010	Matt Asher	It’s 1943 and you work for the good guys. A handful of German tanks have been captured, and each one has a serial number. This is back when serial numbers were still presumed to come in serial, one right after the other. Given your collection of numbered tanks, and assuming that any existing tank was just as likely to be captured as any other, how many tanks would you guess that the Krauts have? By luck, you have a time machine, so you jump forward in time, check out the Wikipedia entry, and copy down the formula , noting that  should be replaced with the highest serial number encountered, while  represents the number of tanks captured. Reading on, you see that Wikipedia has provided a rare nugget of actual understanding for a formula. This estimate represents “the sample maximum plus the average gap between observations in the sample”. So you’re done, right? Just plug in to the formula, hand in your estimate to the commanding officer, go enjoy some R&R. Not so fast. Here at StatisticsBlog.com, nothing is believed to work until it passes the Monte Carlo test. To test out the formula I coded a simulation in R: Which produces the following log-log plot:  Gratuitous clip art was added with the “chartJunk()” function. Looks pretty good, no? Especially given that the sample size for each of these tests was just 20. To make sure everything was OK, I plotted the residuals as well:  Make sure to click on the images above to see larger versions. Bigger really is better when it comes to viewing charts. Looks good too, no? So, German tank problem estimate? Confirmed. Just don’t dig too deep into the assumption that all tanks had an equal chance of being captured; common sense goes against that one (ask yourself if there might be a relationship between length of time a tank is in the field of battle and the likelihood it will be captured). Speaking of likelihood… this problem gives a nice example of how maximum likelihood estimation (MLE) can fail in spectacular form, like a bomb whose innards have been replaced by sawdust (alright, I promise, last military analogy). The MLE for the number of German tanks is the highest serial number observed. This is because MLE works backwards, finding the parameter which makes our observation most likely in terms of joint conditional probability. As a result, the MLE for this problem is not only biased (since it will always be less than or equal to the true number of tanks), but dumb as well. How likely is it (in the common sense usage of the term) that your captured tanks will include the highest-numbered one? If the distribution is truly uniform, the chance that you have to top one is  where  is the true, unknown number of tanks. You don’t know , but you do know that it’s at least  (the highest number observed). For small samples, where , the probability that you have captured the very top-numbered tank is quite small indeed, no larger than  at best. Just how bad is the MLE? I compared the mean absolute residuals from the two different methods. Using the formula from at the beginning of this post gives 6,047. Using MLE, the average residual was 8,175, or 35% worse. Standard deviation for the MLE method is also higher, by about 27%. Back to boot camp for the MLE. (I know, I know, I promised). 	 0 Comments
"extrapolation and interpolation
The most important lesson I…"	https://www.r-bloggers.com/2010/05/extrapolation-and-interpolationthe-most-important-lesson-i/	May 25, 2010	Isomorphismes	The most important lesson I learned from this book:  regression is reliable for interpolation, but not for extrapolation.  Even further, your observations really need to cover the whole gamut of causal variables, intersections included, to justify faith in your regressions. Imagine you have two causal variables, A and B, that are causing X.  Maybe your data cover a wide range of observations of A — some high, some low, some in-between.  And you have, too, the whole gamut of observations of B — high, low, and medium.  It might still be the case that you haven’t observed A and B together (not seen ).  Or that you’ve only observed them together (not seen ).  In either case, your regression is effectively extrapolating to the other causal region and you should not trust it. Let’s keep the math sexy.  Say you meet an attractive member of your favorite sex.  This person A) likes to hunt, and B) is otherwise vegetarian.  Your prejudices  are that you don’t like hunters () and you do like vegetarians ().  By comparing the magnitudes of these preferences, you deduce that you should not get along with this attractive person, because the bad A part outweighs the good B part.  However, since you haven’t observed both A and B positive at once, your preconceptions are not to be trusted.  Despite your instincts , you go out on a date with Mr or Ms (A>0, B>0) and have a fantastic time.  Turns out there was a positive interaction term in the  range, it also correlates positively with the noise (it wasn’t noise, just unknown knowledge), and you’ve found your soul mate.  	 0 Comments
SQLite as an alternative to shapefiles, and some GPS fun in R	https://www.r-bloggers.com/2010/05/sqlite-as-an-alternative-to-shapefiles-and-some-gps-fun-in-r/	May 24, 2010	dylan	Finally made it out to Folsom Lake for a fine day of sailing and GPS track collecting. Once I was back in the lab, I downloaded the track data with gpsbabel, and was ready to import the data into GRASS.   I was particularly interested in how fast we were able to get the boat going, however my GPS does not keep track of its speed in the track log. Also, I had forgotten to set GPS up for a constant time interval between track points. Dang. In order to compute a velocity between sequential points from the track log I would need to first do two things: 1) convert the geographic coordinates into projected coordinates, and 2) compute the associated time interval between points. read more 	 0 Comments
Chicago R Meetup: Healthier than Drinking Alone	https://www.r-bloggers.com/2010/05/chicago-r-meetup-healthier-than-drinking-alone/	May 24, 2010	JD Long	I’m kinda blown away by the number of folks who have joined the Chicago R User Group (RUG) in the last few weeks. As of this morning we have 65 people signed up for the group and 25 who have said that they are planning on attending the meetup this Thursday (yes, only 3 days away!) I’m very pleased that this many people in Chicago find the R language interesting and/or valuable. Of course, there is the possibility that some of the 25 who are attending are simply hoping for some free beer. I was a member of a vegan society for 2 years because they had free beer. The week I accidentally showed up with a six pack of White Castle sliders really blew my cover. That’s how I discovered that you can scare off angry vegans by waving a steaming hot onion covered meat-like patty in their face. True story. And when I say “true story” I mean “total lie”. By the way, I’m already recruiting presenters for next month’s RUG meetup. And I’m also looking for locations. So if you have an idea for either, let me know. I promise to not throw any mini burgers at you. 	 0 Comments
Webinar: R analytics and Business Intelligence dashboards	https://www.r-bloggers.com/2010/05/webinar-r-analytics-and-business-intelligence-dashboards/	May 24, 2010	David Smith	On Wednesday next week, Revolution will be hosting a joint webinar with open-source Business Intelligence software maker Jaspersoft and open-source Business Intelligence services company OpenBI. Together, we’ll be talking about making BI dashboards even more powerful by integrating custom visualizations and advanced predictive models from R. If you create advanced analytics in R and are looking for a way to make the results available to business users in an easy-to-use dashboard environment, this is a great webinar to check out. The complete agenda is below, and you can register for free for the live event at 9AM (Pacific Time) on June 2. SUPERCHARGE BI AND DASHBOARDS WITH PREDICTIVE ANALYTICS WEBINARPowered by Jaspersoft, Revolution Analytics, and OpenBI Date:    Wednesday, June 2, 2010Time:    9am – 10am Pacific Presenters: David Smith, Vice President of Marketing, Revolution AnalyticsSteve Miller, President, OpenBI, LLCAndrew Lampitt, ISV/Technology Alliances, Senior Director, Jaspersoft Business intelligence is about providing reporting and analysis solutions that show business users what happened and why. On the other hand, advanced analytics solutions deliver deeper insight into what might happen in the future, based upon high volumes of historical data and sophisticated modeling techniques. Traditionally, advanced analytics has been reserved for a highly technical audience in fields such as life sciences and academia. However, with the explosion in data in nearly every sector, advanced analytics is now becoming useful to more mainstream business users. This webinar introduces commercial open source business intelligence solutions from Jaspersoft, advanced analytics solutions with the popular open source project R and Revolution Analytics, and how those separate products mesh together in a demonstration by OpenBI, the expert commercial open source system integrator. Target Audience:  Level: No background knowledge required.  Revolution Analytics: Supercharge BI and Dashboards with Predictive Analytics 	 1 Comment
Example 7.38: Kaplan-Meier survival estimates	https://www.r-bloggers.com/2010/05/example-7-38-kaplan-meier-survival-estimates/	May 24, 2010	Nick Horton		 0 Comments
Rmetrics AMPL Interface	https://www.r-bloggers.com/2010/05/rmetrics-ampl-interface/	May 24, 2010	rmetrics		 0 Comments
THETA AG uses Rmetrics	https://www.r-bloggers.com/2010/05/theta-ag-uses-rmetrics/	May 24, 2010	rmetrics		 0 Comments
Introduction to using R with org-babel, Part 1	https://www.r-bloggers.com/2010/05/introduction-to-using-r-with-org-babel-part-1-2/	May 23, 2010	erik	"
In my opinion, the description of orgmode by its creator as a tool “for keeping notes, maintaining ToDo lists, doing project planning, and authoring with a fast and effective plain-text system” is a little like describing Emacs as a “text editor”.  While technically accurate, one not acquainted with orgmode might not be immediately persuaded to learn it based on its pedestrian description.  Needless to say, I think it is worthwhile. 
 
While there are plenty of tutorials and a great introduction to orgmode on its site, there exists a relatively new orgmode extension called org-babel whose documentation, although complete and accurate, might benefit from a high-level overview showing how it can be used to write an R program in an orgmode buffer. 
 
Even without org-babel, you can create a source code block in an orgmode buffer.  Why would you want to do this?  Mostly so that when you export the orgmode buffer to HTML, that the source code looks like source code.  Source code will be displayed in a monospace font, and colored to look just like it does in your Emacs buffer.  
 
To accomplish this, you would put something like the following in your org-mode document. I will use the programming language R as an example.  To define a source code block, simply use the #+BEGIN_SRC syntax along with the major mode of the language, in this case ‘R’. 
 
Then, when you export the orgmode document to, say, HTML, your R code will look just as it does in Emacs with syntax highlighting, and will be offset from the text surrounding it, like this:
 Ideally, when you entered into a code block, Emacs would recognize this, and the syntax highlighting within the code block would reflect whatever language you had specified as you typed it. Unfortunately, it is difficult to get Emacs to behave this way, at least with orgmode.  I have investigated several options for doing this, but have run into problems with all of them. Orgmode's solution to this is to create an indirect buffer containing the source code you're editing when you type C-c ' (i.e., Control-C, then a single quote) in a source code block.  You don't have to do this, but it is nice to get your program in a temporary buffer that is set to the right mode. In R, this also means you can use all the usual ESS shortcut keys.  
 
Org-babel lets you execute source code blocks in an orgmode buffer.  Well, what does that mean?  At its simplest, org-babel lets you submit a source code block, like the one above, to an R process, and places the results in the actual orgmode buffer.
 
You activate the additional features that org-babel provides by giving the #+BEGIN_SRC line in an orgmode buffer special arguments, some of which I describe below.  All the available options are described in the org-babel documentation. I will show some basic examples of how to add arguments to a #+BEGIN_SRC line. 
 
However, since submitting a code block to a new R process and placing the results in the orgmode buffer is the default behavior of org-babel, you don't need to supply any arguments to the source code block for this simple operation.  You simply carry out this process by typing C-c C-c in a source code block. 
 
In your org-mode buffer, with point in the code block, you now type C-c C-c, an R process is started, and the following is placed in the orgmode buffer. 
 
Since we used the default value for the results argument (i.e., we didn't specify anything), org-babel collected the output and put it in an org-table, as shown above.  If you just want the output like it would appear in your ESS R process buffer, you can use the value 'output' to the results argument, as shown below.  You specify an argument by typing a ':', and then using one of the valid parameter names, typing a space, and then giving the value of the parameter.  So, in the example below, results is the name of the parameter, and we're setting its value to 'output'.  The default value for the results parameter is 'value', and gives you the same results as above, i.e., a table of the last results returned by the code block.  
 
Since we set results to 'output', we see the familiar R notation for printing a vector.  The default value, where an org-table is created, would be useful for passing the resulting table to perhaps a different function within the orgmode buffer, even one programmed in another language.  That is a  feature that org-babel supports and encourages, but I will not describe that further here. 
 
Personally, I set results to 'output' to write this entries for this blog, since it shows users what the actual R output will look like.  This is nice, because there is no cutting and pasting of the results!  I can just write my R code in a source code block, and then add a special argument to export the code and results to an HTML file upon exporting in orgmode.  My code block would look like this to accomplish that.  Notice the new argument, exports, and its value 'both', as in both the code and its results. 
 
Putting the above code in an orgmode buffer and exporting to HTML will show the following: 
 Indeed, using org-babel in this fashion is how I now generate content for this site.  Compare this approach to what I used to be doing with Sweave.  The two approaches are  similar, but I now have a completely contained orgmode approach, where as before I had to preprocess the orgmode buffer with an Sweave function before export.  You can see the difference not only in how I specify code blocks, but also how the appearance of the output has changed. 
 
Note that the last thing I still have to do in an Elisp function is to insert inline CSS code to generate the background color and outline of the code and results blocks.  As far as I know, there is no way to do this in orgmode yet, as the HTML output it contains the CSS style information in the HTML header. 
 
I think org-babel is interesting for several reasons.  First, as R was one of the first languages supported by it, and I use R as my main programming language at my job, I was naturally interested in it.  Org-babel is very useful for me because I am used to working in orgmode, so I can add links, tags, TODO items, create notes, and more right in my code.  It's like having a hyper-commenting system that can be crosslinked with everything.  Using orgmode's intuitive visibility cycling system results in a powerful code-folding feature.  Orgmode properties allow me to collect meta-information about my code, such as what objects a code block create. 
 
The export feature of orgmode is very useful for producing this blog, and producing readable documentation for my source code. Also, if I am writing a program to be run in a script, a very common task, org-babel can handle that through a feature called tangling, which I will cover in a future article. The tangling feature turns orgmode into a tool to perform literate programming. 
 
In addition to tangling mentioned above, there are several important features that I have not even mentioned in this short introduction.  In the subsequent article, I want to show how I use R with org-babel to include inline images created with R, and how to generate $latex \LaTeX$ output that can be previewed inline in orgmode documents.  Using org-babel in this manner closely mirrors a 'notebook' style interactive session such as Mathematica provides.  The other main feature that org-babel provides is using it as a meta-programming language to, say, call R functions using data generated from a shell script or Python program.  Org-babel is a very interesting project that is definitely worth your time to check out, especially if you're already an Emacs or orgmode user.  If you've read through this post, you can get started by reading the official org-babel introduction, which will describe what to include in your .emacs file to setup org-babel. 
 "	 0 Comments
R Function of the Day: sample	https://www.r-bloggers.com/2010/05/r-function-of-the-day-sample-2/	May 23, 2010	erik	"

This post originally appeared on my WordPress blog on May 23, 2010. I present it here in its original form.
 
The R Function of the Day series will focus on describing in plain language how certain R functions work, focusing on simple examples
that you can apply to gain insight into your own data. 
 
Today, I will discuss the sample function. 
 
In its simplest form, the sample function can be used to return a
random permutation of a vector.  To illustrate this, let’s create a
vector of the integers from 1 to 10 and assign it to a variable x. 
 
Now, use sample to create a random permutation of the vector x. 
 Note that if you give sample a vector of length 1 (e.g., just the
number 10) that it will do the exact same thing as above, that is,
create a random permutation of the integers from 1 to 10. 
 
This can be a source of confusion if you're not careful.  Consider the
following example from the sample help file. 
 Notice how the first output is of length 2, since only two numbers are
greater than eight in our vector. But, because of the fact that only
one number (that is, 10) is greater than nine in our vector, sample
thinks we want a sample of the numbers from 1 to 10, and therefore
returns a vector of length 10. 
 
Often, it is useful to not simply take a random permutation of a
vector, but rather sample independent draws of the same vector.  For
instance, we can simulate a Bernoulli trial, the result of the flip of
a fair coin.  First, using our previous vector, note that we can tell
sample the size of the sample we want, using the size argument. 
 Now, let's perform our coin-flipping experiment just once. 
 And now, let's try it 100 times. 
 Oops, we can't take a sample of size 100 from a vector of size 2,
unless we set the replace argument to TRUE.  
 
The sample function can be used to perform a simple bootstrap. 
Let's use it to estimate the 95% confidence interval for the mean of a
population.  First, generate a random sample from a normal
distribution. 
 
Then, use sample multiple times using the replicate function to
get our bootstrap resamples. The defining feature of this technique is
that replace = TRUE.  We then take the mean of each new sample, gather them, and finally compute the relevant quantiles. 
 Compare this to the standard parametric technique.
 "	 0 Comments
Code and brief instruction for graphing Twitter with R	https://www.r-bloggers.com/2010/05/code-and-brief-instruction-for-graphing-twitter-with-r/	May 23, 2010	cornelius	"Edit: I’ve posted an updated version of the script here. It is not quite as compressed as Anatol’s version, but I think it’s a decent compromise between readability and efficiency.   Edit #2 And yet another update, this one contributed by Kai Heinrich. I hacked together some code for R last night to visualize a Twitter graph (=who you are following and who is following you) that I briefly showed at the session on visualizing text today at THATCamp and that I wanted to share. My comments in the code are very basic and there is much to improve, but in the spirit of “release early, release often”, I think it’s better to get it out there right away. Ingredients: Note that packages are most easily installed with the install.packages() function inside of R, so R is really the only thing you need to download initially. Code: # Load twitteR package

library(twitteR) # Load igraph package

library(igraph)


# Set up friends and followers as vectors. This, along with some stuff below, is not really necessary, but the result of my relative inability to deal with the twitter user object in an elegant way. I'm hopeful that I will figure out a way of shortening this in the future friends <- as.character()

followers <- as.character() # Start an Twitter session. Note that the user through whom the session is started doesn't have to be the one that your search for in the next step. I'm using myself (coffee001) in the code below, but you could authenticate with your username and then search for somebody else. sess <- initSession('coffee001', 'mypassword')


# Retrieve a maximum of 500 friends for user 'coffee001'. friends.object <- userFriends('coffee001', n=500, sess) # Retrieve a maximum of 500 followers for 'coffee001'. Note that retrieving many/all of your followers will create a very busy graph, so if you are experimenting it's better to start with a small number of people (I used 25 for the graph below). followers.object <- userFollowers('coffee001', n=500, sess) # This code is necessary at the moment, but only because I don't know how to slice just the ""name"" field for friends and followers from the list of user objects that twitteR retrieves. I am 100% sure there is an alternative to looping over the objects, I just haven't found it yet. Let me know if you do... for (i in 1:length(friends.object))

{

	friends <- c(friends, friends.object[[i]]@name);

}


for (i in 1:length(followers.object))

{

	followers <- c(followers, followers.object[[i]]@name);

} 

# Create data frames that relate friends and followers to the user you search for and merge them. relations.1 <- data.frame(User='Cornelius', Follower=friends)

relations.2 <- data.frame(User=followers, Follower='Cornelius')

relations <- merge(relations.1, relations.2, all=T) # Create graph from relations. g <- graph.data.frame(relations, directed = T) # Assign labels to the graph (=people's names) V(g)$label <- V(g)$name # Plot the graph. plot(g) For the screenshot below I've used the tkplot() method instead of plot(), which allows you to move around and highlight elements interactively with the mouse after plotting them. The graph only shows 20 people in order to keep the complexity manageable.   "	 0 Comments
S4 timeDate Package	https://www.r-bloggers.com/2010/05/s4-timedate-package/	May 23, 2010	rmetrics		 0 Comments
Portfolio Risk Surfaces	https://www.r-bloggers.com/2010/05/portfolio-risk-surfaces/	May 23, 2010	rmetrics		 0 Comments
Stability Watch Views	https://www.r-bloggers.com/2010/05/stability-watch-views/	May 22, 2010	rmetrics		 0 Comments
Variable selection using automatic methods	https://www.r-bloggers.com/2010/05/variable-selection-using-automatic-methods/	May 22, 2010	Ralph	When we have a set of data with a small number of variables we can easily use a manual approach to identifying a good set of variables and the form they take in our statistical model. In other situations we may have a large number of potentially important variables and it soon becomes a time consuming effort to follow a manual variable selection process. In this case we may consider using automatic subset selection tools to remove some of the burden of the task. It should be noted that there is some disagreement about whether it is desirable to use an automated method but this post will focus on the mechanics of doing it rather than the debate about whether to be doing it at all. The R package leaps has a function regsubsets that can be used for best subsets, forward selection and backwards elimination depending on which approach is considered most appropriate for the application under consideration. In previous post we considered using data on CPU performance to illustrate the variable selection process. We load the required packages: First up we consider selecting the best subset of a particular size, say four variables for illustrative purposes (nvmax argument), and we specify the largest possible model which in this example has six variables: A summary for the output from this function is shown here: The function regsubsets identifies the variables mmin, mmax, cach and chmax as the best four. Alternatively we could perform a backwards elimination and the function will indicate the best subset of a particular size, from one to six variables in this example: The subset of four variables is the same for this example as the best subsets approach. The third approach if forward selection: For this data set, as there are only six variables, we do not see divergence between the subsets chosen by the different methods. 	 0 Comments
Helping the blind use R – by exporting R console to Word	https://www.r-bloggers.com/2010/05/helping-the-blind-use-r-%e2%80%93-by-exporting-r-console-to-word/	May 22, 2010	Tal Galili	"For blind people who wish to do statistics, R can be ideal.  R command line interface offers straight forward statistical scripting in the form of question (what is the mean of x) followed by an answer (0.2).  That is, instead of point-and-click dialog boxes with jumping windows of results that GUI statistical systems offer. But there are still more hurdles to face before R can offer a perfect solution to the blind.
In this post I would like to address just one such problem – reading R console output. Recently, a question was posed in the R-help mailing list by a guy names Faiz, a blind new user of R.  Faiz wants to direct R output into word, to allow him to be able to read it.  Here is what he wrote: I would like to read the results of the commands type in the terminal window in Microsoft Word. As a blind user my options are somewhat limited and are time consuming if I want to see the results of the commands that I have type earlier. for example if my first two commands were
 x
mean(x)
and I have typed ten more commands after the first two commands it is not easy for me to see that what was the result of mean(x)
but if I can somehow divert the results of the commands to Microsoft Word it is comparatively easy for me to see what was the result of mean(x) and what were the results of other commands. One another advantage of diverting R’s output to Microsoft Word for me is that from there they can be easily copied into assignments as well.
 Faiz later elaborated more on his issue: I am using Windows XP, and using a screen reader called JAWS. When I type something at the console, I hear once what I have typed, and then the focus is on the next line. Then if I press the up arrow key I get to hear the function I just typed, not its output. For example if I type mean(x) and then I press enter I will hear “[5]” if it is the mean of x. Then I will hear “>”. Now if I want to find out what was the mean of x by pressing the
up arrow key, I will only hear mean(x) and I will not hear [5].
My screen reader does provide options to use different cursors to read command lines.
but if I have typed median(x) sd(x) var(x) length(x) after typing mean(x), it takes a long time before I can move my cursor to the location where I can hear the mean of x. If the results of the commands can be diverted to MS Word it becomes comparatively easy for me to quickly move forward and backward in the document. Any ideas and suggestions are appreciated.
 Since recently I reviewed how one could export R output to MS-Word with R2wd, It was only fitting to try and implement R2wd for this problem.
I went looking on how to direct R console into a txt file, so I could later dump it into word.  I found that two commands gave me half of what I wanted.  sink() allows me to direct R output to a txt file, and savehistory() can save the command history into a txt file.  But I needed something that combines the two and captures all of R console output into a file.
Failing to locate one, I turned to the R mailing list.  Among the kind people trying to help (Thank you David Winsemius, Bert Gunter and Duncan Murdoch) Greg Snow came through in supplying the help (not surprisingly…).
Greg directed me to a function he wrote called txtStart() (from the TeachingDemos package), which operates in a similar way as sink(), only it also captures the R commands that where used – exactly what I was looking for! Based on this, I devised two functions that can be used to redirect R output into word. Here is how to use them: For me, this worked… If you would like R to automatically run in the startup the code needed to get the two functions: txtStart.2wd and txtStop.2wd , you can run this in your R console: (once is enough) Until this point, it didn’t cross my mind to ask how can R be used by the blind.  But once  this question was raised – it brings with it many more questions.
Can R be adjusted to easily be read by known aids to sight impaired people? (I am sure Linux users here will have much to add)
Can people in the community think of writing function to turn R output into a more easily read text for the blind?
For example – the summary() command is wonderful for me.  But I am trying to imagine how it would look like in the “eyes” of a person who can’t see.  Surly there could be some way to turn the wide summary format into a long format.
Perhaps there is room for a more general approach to the question of how to help blind people to be able to use R.
And is there a need?  How many blind people choose to pursue studying statistics (or disciplines for which they would need to know statistics/R)?
I hope to read your thoughts on the matter. On a personal note:  My father was on the verge of blindness, prior to his cataract surgery.  I saw first hand how the life of the sight-impaired can look like.  Giving people in that situation help is a great MITZVA (a.k.a: “good deed” in Hebrew). "	 0 Comments
Using R for Introductory Statistics, 3.1	https://www.r-bloggers.com/2010/05/using-r-for-introductory-statistics-3-1/	May 21, 2010	Chris	The grades data.frame holds two columns of letter grades, giving pairs of categorical data, like so: This type of data can be summarized by the table function, which counts the occurrence of each possible pair of letter grades. But first, I was never a fan of plus-minus grading, so lets do away with that. You might want to compute row (1) or column (2) sums, using margin.table: Of the students who got an A on the first test, what proportion also got an A on the second test? Those types of questions are answered by prop.table(). Finally, this type of data can be displayed as a stacked barplot.  	 0 Comments
The only thing smiling today is Volatility	https://www.r-bloggers.com/2010/05/the-only-thing-smiling-today-is-volatility/	May 21, 2010	Lee	With the stock market freaking out and all, I figured I should take a look at how volatility was being priced in the option market.  The CBOE generously provides snapshots of market data for anyone interested to download.  By using this data, we can calculate the markets ‘implied volatility’, or level of ‘freaking out’.  For those not familiar with the concept of implied volatility, essentially we can take the prices of options in the market and back out the volatility implied by those prices using the Black-Scholes formula.  Its been shown over and over again that the assumptions of the Black-Scholes model don’t hold up to empirical data; but its an easy calculation to perform, and so implied volatility is a widely used metric.  Anyway, below is my Black-Scholes option pricing function and the function used to back out implied volatility (written in R of course).  Since implied volatility can only be found numerically, I used the Bisection Method to calculate it since it was easy to implement, but there are faster methods out there.  To apply the calculations above, I downloaded option data on the S&P 500 (SPX) as of midday Friday, May 21st.  Data is available for multiple expiration dates, but I narrowed my analysis to only the near dated contract, which was the June contract.  As with all market data, I had to do a bit of cleaning first, but you can find my clean data here.  Once cleaned, the data was read into R and cranked though the implied volatility function.  Note that the S&P 500 was at 1082 at the time of these quotes. Finally the fun part, I plotted the implied volatility vs the strikes for both the calls and puts.  I also plotted the open interest (number of open contracts held in the market) vs strike.  For reference, I put the current S&P 500 index value (at the time of these quotes) as a vertical dashed red line.  The most obvious thing you should notice is the phenomenon known as the ‘volatility smile’. The assumptions of the Black-Scholes model tell us that volatility should be constant for all strikes.  Clearly the market doesn’t believe this assumption and there are numerous theories on why this happens. Something you may have missed, but is rather shocking, most of the put contracts held are at a strike of 800.  These are essential insurance contracts for people who are worried about the S&P 500 dropping over 25% within the next month. Scary! 	 0 Comments
Because it’s Friday: Ash	https://www.r-bloggers.com/2010/05/because-its-friday-ash/	May 21, 2010	David Smith	I lived for 10 years within sight (on a clear day, anyway) of Mount St. Helens, and had seen and heard a lot about the devastation caused by the eruption and pyroclastic flow 30 years ago. But I’d heard relatively little about the effects of the ash cloud settling on land. I was surprised to learn about the effect it had on the city of Spokane: basically immobilizing everyone there and every vehicle for days. If you thought the effect of the Eyjafjallajökull eruption on air traffic was serious, imagine what would happen if the ash cloud settled on a city like London or Paris. Check out the slide show below for what happened to Spokane, and extrapolate.   Spokane Spokesman-Review: Mount St. Helens 	 0 Comments
Color choosing in R made easy	https://www.r-bloggers.com/2010/05/color-choosing-in-r-made-easy/	May 21, 2010	Aviad Klein	I don’t know about you, but when I want to make a graph in R, I handpick the colors, line widths etc… to produce awesome output. A lot of my time is spent on color choosing, I had to find a more convenient way of doing so. Earl F. Glynn’s “Chart of R colors”  posted on  http://research.stowers-institute.org/efg/R/Color/Chart/ gave me the idea to the following function. R has an Internal called colors(), it’s output is a 657 long character vector of reserved names for colors. The names can be used directly as in : Alternatively, they can be referred to from colors() The color() function has a two arguments (notice it is not plural… I chose this unreserved name because it’s easy to remember) : The call color() is the same as colors(). Calling color(plot.it=T) or color(T) gives the following output (very similar to Earl F. Glynn’s “Chart of R colors”) :  You can choose and remember the numbers, or print and stick above your working area… but the following makes color() more useful : Specifying locate = k > 0 will plot the chart above and this time will use the locator() function in a loop to choose colors you want. After choosing k colors the output will be  a k-long character vector of the chosen colors. You can use it directly in a plot function, the palette of colors will plot first, choose your colors, the plot you called for will be followed by it:  The function is given by : You can also write it to variable for further use: Of course it’s not perfect, I still have not solved issues of working with Devices such as pdf() jpeg() and such… Your comments are welcome. 	 0 Comments
R 2.11.1 scheduled for May 31	https://www.r-bloggers.com/2010/05/r-2-11-1-scheduled-for-may-31/	May 21, 2010	David Smith	As announced by the R Core Team, the next update to R will be 2.11.1 to be released on May 31. Despite being a minor-minor version increment, this release is expected to sport at least one new feature: BIC (in package stats4) will work with multiple fitted models, like AIC does. There will also be some improvements to the installation process, and a few bug fixes (mainly for some pretty obscure cases: the level of detail these fixes go to is a testament to the quality of R).  Look for the updated sources on May 31, with binaries to follow soon after. R-announce mailing list: R 2.11.1 Scheduled for May 31 	 1 Comment
Tip of the day: Keep the console active in R Productivity Environment	https://www.r-bloggers.com/2010/05/tip-of-the-day-keep-the-console-active-in-r-productivity-environment/	May 21, 2010	David Smith	Jared Lander on Twitter asks: When I alt-tab into [Revolution] R Enterprise how do I make the #rstats [R] console the active window by default. Now it goes to solution explorer. Revolution’s crack Support engineer Stephen Weller offers this solution:  The best way to do this is to right-click on the R-Console window and make it a ‘Tabbed Document’. If you then make the ‘R-Console’ window the active window and exit Revolution R, it will remain the active window the next time you start the program (or press ‘Alt-Tab’). In in addition to the R console, the  R Productivity Environment includes a smart R code editor and interactive debugger. It’s included with Revolution R Enterprise for Windows, available by subscription for businesses, and for free download for academic users.  	 1 Comment
Random sudokus [p-values]	https://www.r-bloggers.com/2010/05/random-sudokus-p-values/	May 21, 2010	xi'an	I reran the program checking the distribution of the digits over 9 “diagonals” (obtained by acceptable permutations of rows and column) and this test again results in mostly small p-values. Over a million iterations, and the nine (dependent) diagonals, four p-values were below 0.01, three were below 0.1, and two were above (0.21 and 0.42). So I conclude in a discrepancy between my (full) sudoku generator and the hypothesised distribution of the (number of different) digits over the diagonal. Assuming my generator is a faithful reproduction of the one used in the paper by Newton and DeSalvo, this discrepancy suggests that their distribution over the sudoku grids do not agree with this diagonal distribution, either because it is actually different from uniform or, more likely, because the uniform distribution I use over the (groups of three over the) diagonal is not compatible with a uniform distribution over all sudokus… 	 0 Comments
highlight 0.1-8	https://www.r-bloggers.com/2010/05/highlight-0-1-8/	May 21, 2010	romain francois	I’ve pushed version 0.1-8 of highlight to CRAN. highlight is a syntax highlighter for R that renders R source code into some markup language, the package ships html and latex renderers but is flexible enough to handle other formats. Syntax highlighting is based on information about the code gathered by a slightly modified version of the R parser, available in the separate parser package. Internal code has been modified to take advantage of new features of Rcpp such as the DataFrame c++ class.  Since R 2.11.0, it is possible to install custom handlers to respond to http request (GET, POST, …). highlight takes advantage of this and responds to urls with html syntax highlighted functions. So if the httpd port used by the dynamic help system is 9000 (hint: tools:::httpdPort) :  	 0 Comments
ACM Transactions on Modeling and Computer Simulation	https://www.r-bloggers.com/2010/05/acm-transactions-on-modeling-and-computer%c2%a0simulation/	May 20, 2010	xi'an	Pierre Lecuyer is the new editor of the ACM Transactions on Modeling and Computer Simulation (TOMACS) and he has asked me to become an Area Editor for the new area of simulation in Statistics. I am quite excited by this new Æditor’s hat, since this is a cross-disciplinary journal: The ACM Transactions on Modeling and Computer Simulation (TOMACS) provides a single archival source for the publication of high-quality research and developmental results in computer simulation. The subjects of emphasis are discrete event simulation, combined discrete and continuous simulation, as well as Monte Carlo methods. Papers in continuous simulation will also receive serious consideration if their contributions to modeling and simulation in general are substantial. The use of simulation techniques is pervasive, extending to virtually all the sciences. TOMACS serves to enhance the understanding, improve the practice, and increase the utilization of computer simulation. Submissions should contribute to the realization of these objectives, and papers treating applications should stress their contributions vis-a-vis these objectives. As an indication of this cross-disciplinarity, I note that most Area Editors and Associate Editors are unknown to me (except for Luc Devroye, of course!). In addition, I savour the irony of being associated with a journal of the Association for Computer Machinery (ACM), given my complete lack of practical skills! So, if you have relevant papers to submit in the field, please consider the ACM Transactions on Modeling and Computer Simulation (TOMACS) as a possible outlet. 	 0 Comments
R: A random walk though OOP land.	https://www.r-bloggers.com/2010/05/r-a-random-walk-though-oop-land/	May 20, 2010	Matt Asher	If you are used to object oriented programing in a different language, the way R does things can seem a little strange and backwards. “proto” to the rescue. With this library you can simulate “normal” OOP. I found the examples for proto not so helpful, so to figure out how the package works I sent one lonely red ant on a drunken walk. Here’s my code: 	 0 Comments
Load Testing Think Time Distributions	https://www.r-bloggers.com/2010/05/load-testing-think-time-distributions/	May 20, 2010	Neil Gunther		 0 Comments
Calling all T-shirt designers	https://www.r-bloggers.com/2010/05/calling-all-t-shirt-designers/	May 20, 2010	David Smith	Got design skills as well as R skills? The organizers of the useR! 2010 conference are looking for a design to be used on the conference T-shirts. Mango Solutions (who are sponsoring the T-shirt) will select the winner, but personally I’d give extra points for designs that use R itself — it’s been done before! R-statistics blog: useR-2010 is looking for a T-shirt design 	 1 Comment
Tutorial: Principal Components Analysis (PCA) in R	https://www.r-bloggers.com/2010/05/tutorial-principal-components-analysis-pca-in-r/	May 20, 2010	Stephen Turner		 0 Comments
RcppArmadillo 0.2.0 (and 0.2.1)	https://www.r-bloggers.com/2010/05/rcpparmadillo-0-2-0-and-0-2-1/	May 20, 2010	Thinking inside the box	"
This new release offers a number of key improvements:
 
While we had tested this quite rigourously, the combination of some last
minute changes that were meant to be stylistic-only, some troubles with the
tests and builds at CRAN that were not apparent in all our tests (hint: do
not yet use dynamic help features referencing other packages even if you have
a Depends: on them) and an upcoming travel deadline meant that we missed a
gotcha on Windows—so release 0.2.1 had to follow a few hours after the
short-lived 0.2.0.

 
More information is on the 
RcppArmadillo page.
Questions, comments etc should go to the
rcpp-devel mailing list
off the R-Forge page.


 "	 0 Comments
Prediction in the cloud: turbulent	https://www.r-bloggers.com/2010/05/prediction-in-the-cloud-turbulent/	May 19, 2010	David Smith	While Microsoft rolled out its Technical Computing Initiative — promising new tools for distributed parallel computing on large data sets in the cloud — with much fanfare earlier this week, Google made a rather more understated response. In a post to the developer-focused Google Code Blog, they quietly announced two new, but potentially disruptive, products. Google BigQuery promises super-fast SQL-like queries on massive data sets (provided the data has been uploaded to a Google host). And the Google Prediction API apparently offers a machine learning black-box: pump some (again, Google-hosted) data in, and “advanced machine learning algorithms” will then give a service that predicts outcomes from new data. Presumably, it does something like using a subset of the data to train a number of machine learning algorithms (or an ensemble of them) and then chooses the best one for prediction. A little like the (now-discontinued) machine-learning service based on R, but obviously at Google scale. The Google products, for now, are only available to invitees as part of Google Labs, but it’s certainly an interesting development: essentially making machine learning a commodity. If it works (and if the intended business users don’t mind having their private data hosted by Google), it could certainly add some turbulence to the nascent market of machine learning in the cloud. I’ll be following tomorrow’s keynote on the topic (via Google Wave) with much interest. Google Code blog: BigQuery and Prediction API: Get more from your data with Google 	 0 Comments
Introduction to Revolution R webinar tomorrow, May 20	https://www.r-bloggers.com/2010/05/introduction-to-revolution-r-webinar-tomorrow-may-20/	May 19, 2010	David Smith	A quick reminder that I’ll be hosting a free webinar tomorrow. Mainly intended for those new to R and Revolution, An Introduction to Revolution R will give an overview and history of the R Project and the additional features of the Revolution R products. But even if you’ve used R before, there will be lots of useful tips and pointers to R resources that might come in handy. The agenda will be:   There will also be a live Q&A session at the end. Everyone is welcome — to register, click the link below, and the live session is at 9AM Pacific tomorrow. Revolution Analytics: Introduction to Revolution R webinar 	 1 Comment
useR-2010 is looking for a T-shirt design	https://www.r-bloggers.com/2010/05/user-2010-is-looking-for-a-t-shirt-design/	May 19, 2010	Tal Galili	"Katharine Mullen has just published on the R mailing list a call for designeRs who might be willing to design a T-shirt aRt design for the shirt that will be given in useR 2010.  I consider such contests as one of those good-for-the-community things, and hope regular useRs, R bloggers, and companies that are based on R – will consider spreading the word, participating in it (and maybe even offer more bonuses to the designers). If you design something and put it on picasa or flickr, please tag it with “useR2010Tshirt” (and consider leaving a comment with a link to the design), so there could later be a follow up on your work.  Even if you don’t “win” you will get positive “karma points” from the community   . Here are the competition details, as published in the mailing list:

Participants in the R User Conference, useR! 2010, July 21-23,
(http://R-project.org/useR-2010) will each receive a t-shirt, thanks to
the sponsorship of Mango Solutions (http://www.mango-solutions.com/). This email is a call for designs for the front of the t-shirt.  The design
should be made using a single color of your choice.  The design should be
in the form of a high-resolution (at least 300 dpi) jpeg, png or gif file,
and will be printed on a 13” x 13” area on the shirt.  There are no
rules for the content. The shirts will be white.  The back of the t-shirt will include the useR!
logo (in blue and black), and the logo of Mango Solutions (in orange and
black). The staff of Mango Solutions will decide which design to use.  The creator
of the chosen design will receive 5 free t-shirts plus a book token (i.e.,
a gift certificate for a book).  You don’t have to be registered for the
conference to enter a design. Please email your design proposal by Sunday, June 6, to
[email protected].  The designer that submits the chosen
design will be notified by June 15th. Thanks in advance for enriching the conference and the wardrobes of useR!
participants! Kate Mullen, for the useR! 2010 Organizing Committee "	 0 Comments
Updated R code and data for ARM	https://www.r-bloggers.com/2010/05/updated-r-code-and-data-for-arm/	May 19, 2010	Andrew Gelman		 0 Comments
Mining and Analyzing Online Social Graph Data	https://www.r-bloggers.com/2010/05/mining-and-analyzing-online-social-graph-data/	May 19, 2010	VCASMO - drewconway		 0 Comments
Random [uniform?] sudokus [corrected]	https://www.r-bloggers.com/2010/05/random-uniform-sudokus-corrected/	May 19, 2010	xi'an	"As the discrepancy [from 1] in the sum of the nine probabilities seemed too blatant to be attributed to numerical error given the problem scale, I went and checked my R code for the probabilities and found a choose(9,3) instead of a choose(6,3) in the last line… The fit between the true distribution and the observed frequencies is now much better but the chi-square test remains suspicious of the uniform assumption (or again of my programming abilities): > chisq.test(obs,p=pdiag)
Chi-squared test for given probabilities
data:  obs
X-squared = 16.378, df = 6, p-value = 0.01186 since a p-value of 1% is a bit in the far tail of the distribution. "	 0 Comments
RcppArmadillo 0.2.1	https://www.r-bloggers.com/2010/05/rcpparmadillo-0-2-1/	May 19, 2010	romain francois	"Armadillo is a C++ linear algebra library aiming towards a good balance
between speed and ease of use. Integer, floating point and complex numbers
are supported, as well as a subset of trigonometric and statistics
functions. Various matrix decompositions are provided through optional
integration with LAPACK and ATLAS libraries. A delayed evaluation approach is employed (during compile time) to combine
several operations into one and reduce (or eliminate) the need for
temporaries.  This is accomplished through recursive templates and template
meta-programming. This library is useful if C++ has been decided as the language of choice 
(due to speed and/or integration capabilities), rather than another language 
like Matlab or Octave. It is distributed under a license that is useful in 
both open-source and commercial contexts. Armadillo is primarily developed by 
Conrad Sanderson at 
NICTA (Australia),
with contributions from around the world. RcppArmadillo 
is an R package that facilitates using Armadillo classes
in R packages through Rcpp. 
It achieves the integration by extending Rcpp’s
data interchange concepts to Armadillo classes. Here is a simple implementation of a fast linear regression (provided by
RcppArmadillo via the
fastLm() function): Note however that you may not want to compute a linear regression fit this
way in order to protect from numerical inaccuracies on rank-deficient
problems. The help page for 
fastLm() 
provides an example. RcppArmadillo 
is designed so that its classes can be used from other packages.  Using RcppArmadillo requires:  Using the header files provided by Rcpp and RcppArmadillo. This is 
   typically achieved by adding this line in the DESCRIPTION file of the 
   client package: and the following line in the package code: Linking against Rcpp dynamic or shared library and librairies needed
   by Armadillo, which is achieved by adding this line in the src/Makevars
   file of the client package and this line in the file src/Makevars.win: RcppArmadillo contains a function 
RcppArmadillo.package.skeleton, modelled
after package.skeleton from the utils package in base R, that creates a 
skeleton of a package using RcppArmadillo, including example code. RcppArmadillo uses the RUnit package by Matthias Burger et al to provide 
unit testing. RcppArmadillo currently has 19 unit tests (called from 8 unit 
test functions).  Source code for unit test functions are stored in the unitTests directory 
of the installed package and the results are collected in the 
RcppArmadillo-unitTests vignette.  We run unit tests before sending the package to CRAN on as many systems as
possible, including Mac OSX (Snow Leopard), Debian, Ubuntu, Fedora 12
(64bit), Win 32 and Win64. Unit tests can also be run from the installed package by executing where an output directory can be provided as an optional first argument. Questions about RcppArmadillo should be directed to the Rcpp-devel mailing
list at
  https://lists.r-forge.r-project.org/cgi-bin/mailman/listinfo/rcpp-devel
 Questions about Armadillo itself should be directed to its forum
http://sourceforge.net/apps/phpbb/arma/ "	 0 Comments
Random [uniform?] sudokus	https://www.r-bloggers.com/2010/05/random-uniform-sudokus/	May 19, 2010	xi'an	"A longer run of the R code of yesterday with a million sudokus produced the following qqplot. It does look ok but no perfect. Actually, it looks very much like the graph of yesterday, although based on a 100-fold increase in the number of simulations. Now, if I test the adequation with a basic chi-square test (!), the result is highly negative: > chisq.test(obs,p=pdiag/sum(pdiag)) #numerical error in pdiag
Chi-squared test for given probabilities
data:  obs
X-squared = 6978.503, df = 6, p-value < 2.2e-16 (there are seven entries for both obs and pdiag, hence the six degrees of freedom). So this casts a doubt upon the uniformity of the random generator suggested in the paper by Newton and DeSalvo or rather on my programming abilities, see next post! "	 1 Comment
LSPM Joint Probability Tables	https://www.r-bloggers.com/2010/05/lspm-joint-probability-tables/	May 18, 2010	Joshua Ulrich	"
 "	 0 Comments
MLB Baseball Pitching Matchups ~ downloading pitch f/x data using the XML package in R [updatedx6]	https://www.r-bloggers.com/2010/05/mlb-baseball-pitching-matchups-downloading-pitch-fx-data-using-the-xml-package-in-r%c2%a0updatedx6/	May 18, 2010	apeescape	"Update x6 (Jul 27): so I guess people want pitch counts. The data @ MLB seems to only give the pitch count of the end result and the strikes/balls/outs of the particular pitch. Of course you can combine them to get the pitch count. Stupid WordPress comments strip out necessary HTML to properly display code, therefore, I post below! pitcher.Lince is the data frame downloaded from MLB, just for Tim Lincecum data. Should work in general. Of course if you have a lot of data, it will take time. Update x5 (Jun 13): More bug fixes. Now the default value for end.date is start.date. Update x4 (Jun 05): More bug fixes (especially for 2008). I’ve also realized that it is advisable to run the script each time for a new day instead of a range of dates, which really bloats memory usage. Update x3 (Jun 01): New code (version 0.4) fixed some bugs, grabs team info and checks game.type to choose type of game (regular season, world series, etc.). Update x2 (May 27): New code (version 0.3) has replaced old buggy one (version 0.2). Update: Please see comments below for some problems with code. I will make update shortly(?). MLB Gameday collects massive amounts of data from each at bat of each Major League Baseball game. Gameday also doubles as a web application to see discrete events in action for those who don’t have a TV, too lazy to buy MLB.tv and are mad at MLB’s blackout policies. Using this detailed data, one can do a myriad of interesting analyses. Gameday added pitch f/x, which includes data on every pitch and its characteristics (speed, location, release point, break, etc., etc.). There are existing tools to use this data: I’ve decided to construct my own R code that is more comprehensive than Erik’s. The current version simply downloads the data from the website containing the data, given a range of dates. Ultimately, I would like the user to be able to put it in a database, to query data for different pitchers, batters, teams, matchups, etc. The only package that is necessary is XML. I used three functions of importance: The first function takes an XML file URL to put it into an XML class. The second function takes the XML object, and then “parses” through to extract certain elements, using the XPath language. The below example finds the game tag (), finds the gameday_link element (), and extracts its value(s) () into a vector. The third function doesn’t take a downloadable link, but the HTML file associated with the URL. This is used as a check to see certain files exist (the data is imperfect, where it has game files when no information is filled in, crashing the code). You can also use xpathSApply to the HTML object. The code grabs the information picked by the user (namely pitch and atbat type) iteratively for the selected dates, and writes it directly to a .csv file.  I haven’t really tested my code for a range of dates so it’s not fully robust (though I fixed a couple defects). The code also takes a long time due to the volume of the data (~20 seconds for a single day on my 2.5 Ghz machine). Code: 
Filed under: Baseball, R, XML        

 "	 0 Comments
robot (SPX) DNA Management Techniques	https://www.r-bloggers.com/2010/05/robot-spx-dna-management-techniques/	May 18, 2010	Milk Trader		 0 Comments
Confusing slice sampler	https://www.r-bloggers.com/2010/05/confusing-slice%c2%a0sampler/	May 18, 2010	xi'an	"Most embarrassingly, Liaosa Xu from Virginia Tech sent the following email almost a month ago and I forgot to reply: I have a question regarding your example 7.11 in your book Introducing Monte Carlo Methods with R.  To further decompose the uniform simulation by sampling a and b step by step, how you determine the upper bound for sampling of a? I don’t know why, for all y(i)=0, we need a+bx(i)>- log(u(i)/(1-u(i))).  It seems that for y(i)=0, we get 0>log(u(i)/(1-u(i))).  Thanks a lot for your clarification. There is nothing wrong with our resolution of the logit simulation problem but I acknowledge the way we wrote it is most confusing! Especially when switching from  to  in the middle of the example…. Starting with the likelihood/posterior  we use slice sampling to replace each logistic expression with an indicator involving a uniform auxiliary variable  [which is the first formula at the top of page 220.] Now, when considering the joint distribution of , we only get a product of indicators. Either indicators that  or of , depending on whether yi=1 or yi=0. The first case produces the equivalent condition  This is how we derive both uniform distributions in  and $beta$. What is both a typo and potentially confusing is the second formula in page 220, where we mention the uniform over the set.  instead of the . It should be 

Related
ShareTweet




To leave a comment for the author, please follow the link and comment on their blog:  Xi'an's Og » R.

R-bloggers.com offers daily e-mail updates about R news and tutorials about learning R and many other topics. Click here if you're looking to post or find an R/data-science job.

Want to share your content on R-bloggers? click here if you have a blog, or  here if you don't.
 "	 1 Comment
R: Dueling normals	https://www.r-bloggers.com/2010/05/r-dueling-normals/	May 18, 2010	Matt Asher	 More playing around with R. To create the graph above, I sampled 100 times from two different normal distributions, then plotted the ratio of times that the first distribution beat the second one on the y-axis. The second distribution always had a mean of 0, the mean of first distribution went from 0 to 4, this is plotted on the x-axis. Here is my code: 	 0 Comments
Parallel Computing with R for Life Sciences	https://www.r-bloggers.com/2010/05/parallel-computing-with-r-for-life-sciences/	May 18, 2010	David Smith	I hadn’t heard of the CloudAsia 2010 conference before, but from the programme the workshop Master Class on HPC Application For Life Sciences looked like it was interesting. One workshop session in particular caught my eye: Practical Parallel Computing in R by Xie Chao and Tan Tin Wee (from the National University of Singapore). The workshop notes (PDF) provide a practical and concise overview of the parallel programming options in R, including several examples using our own foreach package under the heading “One Ring to Rule Them All”: The foreach and iterators packages created by REvolution Computing [now Revolution Analytics — ed.] provide us a convenient framework for parallel computing in R. With these two packages, you only need to write one version of your code for all parallel backends. The document gives examples of using several parallel backends with foreach, including doMC, doSNOW and doMPI.  By the way, If you’re planning to try out foreach yourself, don’t forget that a new parallel backend for Windows is now available: doSMP, included with all editions of Revolution R. doSMP is an open-source R package, we’re haven’t yet submitted it to CRAN. But if you want to get in early and try it with R 2.11, Tal Galili has compiled it and made a Windows binary compatible with R 2.11 at his R Statistics blog (thanks, Tal!). CloudAsia 2010: Master Class on HPC Applications For Life Sciences 	 1 Comment
Prototype: Web-Friendly Visualizations in R	https://www.r-bloggers.com/2010/05/prototype-web-friendly-visualizations-in-r/	May 18, 2010	Wojciech Gryc	"Developing web-friendly data visualizations is not very difficult, though as far as I know, a package that allows one to do this directly in R does not exist (e-mail me if you know of one). As someone who has been developing lots of data-oriented software tools, it’s always nice to post visualizations online. To facilitate this task, I’ve been fooling around with creating a data visualization prototype in R. While the package is very limited in what it does, I hope it’ll generate a discussion on the types of visualization tools that could help R users post their work on the web. At this stage, the package has three functions to illustrate scatter plots, line graphs, and social networks. Each function creates a new directory with all the necessary JavaScript and HTML files. The HTML file could then be embedded using an inline frame (as done below) or used as a standalone website. You can download the prototype here, and below are some examples of visualizations. Scatter Plot x = rnorm(25)

y = rnorm(25)

wv.scatterplot(x, y, ""/wv-scatterplot"", height=300, width=300, marginsize=0.1)  Line Graph x = -100:100/10

y = sin(x)

wv.lineplot(x, y, ""/wv-lineplot"", height=300, width=300, marginsize=0.1)  Social Network 

library(igraph)

g <- erdos.renyi.game(15, 0.175)

wv.sna(g, ""/wv-sna"", rnorm(15, 2, 0.75), width=400, height=400)  Next Steps I apologize in advance, as some of the code above may be buggy and it certainly isn't very customizable. The next step -- assuming there's interest -- is to abstract the graph drawing to individual functions so one can then produce multiple graphs in one canvas or frame. Making more options for interactivity, labels, and so on is also a must. Again, comments and suggestions are very welcome. "	 1 Comment
JAGS 2.1.0 and rjags 2.1.0 are released	https://www.r-bloggers.com/2010/05/jags-2-1-0-and-rjags-2-1-0-are-released/	May 17, 2010	Martyn	JAGS 2.1.0 is now available from Sourceforge.  You will find the source as well as binary packages for Windows and Mac OS X. Binary packages for Debian are available through the usual Debian channels, and packages for RPM-based Linux distributions can be found here (currently RHEL 5  and OpenSUSE 11.x)   Thanks to the team who produce the binary packages: Bill Northcott (Mac OS X), Lars Vilhuber (Fedora, RHEL, SLES, OpenSUSE), and Dirk Eddelbuettel (Debian). The  rjags 2.1.0 package for R has been uploaded to CRAN and should be available from a CRAN mirror near your. 	 0 Comments
House Mountain Hike	https://www.r-bloggers.com/2010/05/house-mountain-hike/	May 17, 2010	Matt Shotwell	"My wife Mary and my Dad Wesley and I took a hike this weekend (5/14/10) to the House Mountain state recreation area in Knox county, Tennessee. The hike was about 3.8 miles with a total elevation gain of around 1000 feet (940.23ft by GPS). The plot below gives the elevation profile over the course of the hike. Here is the Google Maps Version I’ve also uploaded some pictures.  
 The code and data for the plot above are here HouseMtn.R and HouseMtn.csv "	 0 Comments
Random sudokus [test]	https://www.r-bloggers.com/2010/05/random%c2%a0sudokus%c2%a0test/	May 17, 2010	xi'an	"Robin Ryder pointed out to me that 3 is indeed the absolute minimum one could observe because of the block constraint (bon sang, mais c’est bien sûr !). The distribution of the series of 3 digits being independent over blocks, the theoretical distribution under uniformity can easily be simulated: #uniform distribution on the block diagonal
sheik=rep(0,9)
for (t in 1:10^6){
group=length(unique(c(sample(1:9,3),sample(1:9,3),sample(1:9,3))))
sheik[group]=sheik[group]+1
} and it produces a result that is close enough to the one observed with the random sudoku generator. Actually, the exact distribution is available as (corrected on May 19!) pdiag=c(1, #k=3
(3*6+3*6*4), #k=4
(3*choose(6,2)+3*6*5*choose(4,2)+3*choose(5,3)*choose(6,2)), #k=5
(choose(6,3)+3*6*choose(5,2)*4+3*choose(6,2)*choose(5,2)*4+
choose(6,3)*choose(6,3)),#k=6
(choose(3,2)*6*choose(5,3)+3*choose(6,2)*choose(4,2)*5+
choose(6,3)*choose(6,2)*3), #k=7
(3*choose(6,2)*4+choose(6,3)*6*choose(3,2)), #k=8
choose(6,3))/choose(9,3)^2 #k=9
choose(9,6))/choose(9,3)^2 #k=9
 hence a better qq-plot:  "	 0 Comments
Rcpp 0.8.0	https://www.r-bloggers.com/2010/05/rcpp-0-8-0-2/	May 17, 2010	Thinking inside the box	"

This release brings a number of changes that are detailed below.  Of
particular interest may be the much more robust treatment of exceptions, the
new classes for data frames and formulae, and the availability of the new
helper function cppfunction for use with inline. Also of note is the new
support for the ‘LinkingTo’ directive with which packages using Rcpp will get
automatic access to the header files.

 
An announcement email went to the r-packages list 
(ETH Zuerich,
Gmane); Romain also
blogged about the release.

 

The full NEWS entry for this release follows below:
 
As always, even fuller details are in 
Rcpp Changelog page and the 
Rcpp page which also
leads to the downloads, the 
browseable
doxygen docs and zip files of doxygen output for the standard formats.
A local directory  has
source and documentation too. 
Questions, comments etc should go to the
rcpp-devel mailing list
off the R-Forge page

 "	 0 Comments
Lambda Distribution	https://www.r-bloggers.com/2010/05/lambda-distribution/	May 17, 2010	rmetrics		 0 Comments
Winning the first game in a baseball series: a harbinger, or not?	https://www.r-bloggers.com/2010/05/winning-the-first-game-in-a-baseball-series-a-harbinger-or-not/	May 17, 2010	David Smith	"For those not familiar with the major-league baseball in the US (and despite living here for more than 10 years, I still include myself in that category), the games usually played in series: team A visits the home of team B, and the two teams play two or more games against each other on successive days. It’s common wisdom that if team A wins on the first day, they’re more likely to be the victors on the second day, too. (Folks “knowledgeable in baseball and the mathematics of forecasting” give the probability of a second-day win given winning the first game at 65% to 70%.) But is that assertion borne out by the data? Decision Science News has analyzed data from all major league baseball games played between 1970 and 2009, and used R to analyze the likelihood of winning the second game in a consecutive pair of games, given the result of the first. (All of the R code and data are provided, if you want to try and replicate the analysis yourself.) You can see various analyses in DSN’s post, but the most revealing one for me was this chart on the frequency of successive wins with respect to the margin of victory (excess runs) in the first game: 
 So moderate and even significant wins by several runs don’t in the first game don’t appear to confer much benefit in the second. As DSN points out: The equation of the robust regression line is: Probability(Win_Second_Game) = .498 + .004*First_Game_Margin which suggests that even if you win the first game by an obscene 20 points, your chance of winning the second game is only 57.8% In fact, any second-game advantage from the first win that may exist is far overshadowed by the home-team advantage: “When it comes to winning the second game, it’s better to be the home team who just lost than the visitor who just won.” See the full article at Decision Science News for the details of the analysis. Decision Science News: You won, but how much was luck and how much was skill?      "	 0 Comments
Example 7.37: calculation of Hotelling’s T^2	https://www.r-bloggers.com/2010/05/example-7-37-calculation-of-hotellings-t2/	May 17, 2010	Nick Horton		 0 Comments
Index of the R-Sessions	https://www.r-bloggers.com/2010/05/index-of-the-r-sessions/	May 17, 2010	Rense Nieuwenhuis	The R-Sessions are a series of blog entries on using R. A large part consists of an R-manual I once wrote. Other posts include some tricks I found out, as well as entries detailing functions and packages I wrote for R. The series already entails over forty posts, so I decided to create an index. It is found below. On a fixed page on this website (www.rensenieuwenhuis.nl/r-project/r-sessions-index/) I will continue to update this index with new editions of the R-Sessions. A .PDF manual containing many of the R-Sessions material is available here.  	 0 Comments
Hitting the Big Data Ceiling in R	https://www.r-bloggers.com/2010/05/hitting-the-big-data-ceiling-in-r/	May 16, 2010	Ryan Rosario	"As a true R fan, I like to believe that R can do anything, no matter how big, how small or how complicated: there is some way to do it in R. I decided to approach my large, sparse matrix problem with this attitude. But here I sit a broken man. There is no “native” big data support built into R, even if using the 64bit build of R. Before venturing on this endeavor, I consulted with my advisor who reassured me that R uses the state of the art for sparse matrices. That was enough for me. My Problem For part of my Masters thesis, I wrote code to extract all of the friends and followers out to network degree 2 to construct a “small-world” snapshot of a user via their relationships. In a graph, nodes and edges grow exponentially as the degree increases. The number of nodes was on the order of 300,000. The number of edges I predict will be around 900,000. The code is still running. This means that a dense matrix would have size . Some of you already know how this story is going to end… The matrix is very sparse. Very sparse. The raw data graph.log consists of an edgelist with Twitter usernames separated by a comma. Another file names.row contains the node order from NetworkX. R has some packages for working with large datasets. Let’s take a look. Bigmemory Bigmemory is a high-level interface for creating and manipulating very large C++ matrices in shared memory or on disk. Storing matrices in shared memory allows several instances of R to work on the same dataset, particularly useful in a multicore system or an analytics server that needs to have several instances of R operating on the same data. Even better, Bigmemory supports filebacked matrices. A filebacked matrix resides on disk and also comes with a descriptor file that essentially tells R where the data is. Once a big.matrix is constructed, R simply retains a pointer to the matrix on disk. Wonderful! So how did I try to use it? The good news is that it worked. It would be nice if the constructor though had some type of progress indicator because the filesystem does not report the filesize of the matrix while it is being written on disk. As a workaround, I used separated=TRUE so that each column was written to its own file and I could see which files have been created. This worked, but it caused the filesystem to choke due to the number of files created. Trying to do anything yielded the following nightmare: Argument list too long. So when I had to delete this mess of files, I had to use some shell magic. find . -type f -name 'incidence_matrix.bin_column_*' -exec rm {} \; Now that I knew how long it took to write all 300,000 columns/files to disk (about 25 mins), I knew it would take about the same amount of time with separated=FALSE. The bad news? The file was 117GB! Of course, this is because Bigmemory apparently stores matrices as dense matrices. There does not seem to be a sparse option with Bigmemory. ff Then there’s ff. What does it stand for? Fun files? Fancy files? Flat files? ff and its sister package bit have very fine-grained controls for matrices on disk, but unfortunately, R was not up to my challenge. yielded

Error in if (length < 0 || length > .Machine$integer.max) stop(""length must be between 1 and .Machine$integer.max"") :
  missing value where TRUE/FALSE needed
In addition: Warning message:
In ff(vmode = ""logical"", dim = c(3e+05, 3e+05)) :
  NAs introduced by coercion
 So what did I do wrong? Well, nothing. What did the package do wrong? There must be something wrong to yield such a type error, right? Again, nothing is wrong. This is an R problem. I have a machine with 64bit CPUs, a 64bit capable OS (Snow Leopard) running in 64bit mode (hold keys 6 4 at bootup), running 64bit R, 64bit ff and all prerequisite libraries built for 64bit. So, what gives? The missing piece of the puzzle is that although R is 64bit capable, the ramifications mostly involve memory and the amount of memory that can be addressed. The kicker is that R does not have an int64, or long long type. R will not recognize integers that are larger than .Machine$max.integer for anything. Thanks to Hadley Wickham for pointing this out. sparseMatrix() in Matrix The Matrix package contains a handy function sparseMatrix that takes in a vector of row coordinates, a vector of column coordinates and a vector of values for location (row, col), and some other arguments. Constructing the matrix was fast, but there was a major problem: there is no way to modify the sparsity structure of the matrix. The best I could do for the full matrix (much bigger than the one used for bigmemory and ff) was to create sparse matrices based on one million elements at a time, and accumulate by adding the individual matrices together. This worked fine for a while, but the system started to drastically slow down around 65 million entries. Thanks to David Smith for suggesting sparseMatrix as it looks powerful for smaller problems. So, What to Do? The common response is, ""just read in what you need."" That's fine, but not all matrix factorizations can be treated this way, and all of these algorithms would need to be implemented by the user as nothing convenient seems to exist. If you know of any piecemeal filewise packages, comment! If what I just mentioned is not an option, I suggest using another language for the time being. Python's NumPy and SciPy modules are awesome with huge, sparse matrices, but still some issues exist when using a matrix of my size requirement. Clojure has become another good option due to its pure functional credentials. Right now, I just sound like a Clojure fanboy because I really have no explanation as to why Clojure may be better than R and Python in this regard, but it seems to be better for parallel operations necessary for large data. The conclusion? R is great, but really falls flat with large and sparse matrices. If SciPy can do it, R will do it...eventually.
 "	 0 Comments
Graphing using R	https://www.r-bloggers.com/2010/05/graphing-using-r/	May 16, 2010	Stubborn Mule	 Long-time readers of the Stubborn Mule will know that charts are a regular feature here. Almost all of these charts were produced using the R statistical software package which, in my view, produces far superior results to the most commonly used graphing tool: Excel. As a community service to help rid the world of horrible Excel charts, here is a quick tutorial on charting using R. Since R is a powerful and versatile tool, there is a lot more to it than covered here, so there may be more tutorials to come. The first step is to get R installed on your computer. R is open source and can be downloaded for free from the Comprehensive R Archive Network (CRAN). It comes in many flavours: Mac, Windows and Linux. Once you have installed R and have fired it up, you are presented with something that looks very different to Excel. This is the first indication that R is an interactive programming environment not a spreadsheet. You will see various messages, including copyright information, some instructions on how to display licence information, how to run a demo, get help, and finally you are presented with a command prompt: “>”. R is now waiting for you to type commands. As an example, try entering the following command: This will display the current “working directory” (hence “wd”), which is the default folder that R will use for reading and writing data. You can easily change the working directory, either by using the drop-down menus (which menu option varies depending on whether you are using Windows, Mac or Linux) or by using the setwd command: Unless you have a “Mule Docs” folder in a “Documents” folder, you will need to substitute the name of one of your own folders, otherwise you will get an error message. Note that you need to use forward slashes (“/”) rather than backslashes (“\”) even on Windows. You can see detailed explanations of any R command by prefixing the name of the command with a question mark: This is short for help(setwd). Of course, this assumes you know the name of the command already. To search the documentation for a keyword, use a double question-mark. For example will show a list of all the commands which feature the word “median” in their documentation. This is short for help.search(“median”). Note the use of double quotes (“) here, not required in the ?? syntax. To get started, here is a simple data file in CSV fomat (“comma separated values”). Download it and save it in your working directory (or save it somewhere else and then change R’s working directory to where you just saved the file). You can then load the data into R with the following command: While the read.csv part is self-explanatory, the “x“.  To see the contents of x, you can simply type x at the command line and press return, which will display a table with all the data read from the demo.csv file. When dealing with larger “data frames” (to use the R lingo for this type of object), having that much data flash by may not be very useful. Some other useful commands for quickly inspecting your data are: Now you are ready for your first graph. Try this command: You should see a simple, clean scatter-plot. If you would prefer a line graph, this is easily done too. The plot function has many options, which you can explore in the documentation (just enter ?plot). There are also various commands for further annotations for your chart. Try the following commands: These will add gridlines, put axis labels on the right-hand sides (R numbers chart sides from 1 to 4 starting from the bottom and working clockwise) and finally displays text on the chart.  Using R interactively like this is useful for familiarising yourself with the system and for performing quick calculations, but if you find yourself wanting to make small changes here and there, it will quickly become annoying re-typing long commands. This is when you should move to using program files. All that this involves is saving a series of R commands to a file using a text editor (you can just use a simple text editor like Notepad or TextEdit, but many fancier applications can help out by automatically highlighting R commands in different colours, a trick known as “syntax highlighting”). Here is one I prepared earlier: demo.R (by convention, R files are given the .R extension). You can download this and save it into the same folder as the demo.csv file. To execute a program file once you have saved it, you use the source command: This example will also produce a chart of the demo data, but this time it saves the result to an image file (using the Portable  Network Graphics image format). This is done using the png command: The main parameters for this command are the filename of the image you want to produce and the size of the image. After you execute all of your desired charting commands, you must close off the graphics “device” and save the results, which is done using the following command: To find out more about graphics “devices” in R, including saving to other file formats (such as PDF or JPEG), have a look at ?Devices. So that’s it. You are up and running producing charts with R. To go further from here, while you wait for further tutorials, you can explore some of the R files I have used to produce charts for the blog. I store quite a few of them here on github. 	 0 Comments
A 34 Minute Video on Using R to Analyse Winter Olympic Medal Data	https://www.r-bloggers.com/2010/05/a-34-minute-video-on-using-r-to-analyse-winter-olympic-medal-data/	May 16, 2010	Jeromy Anglim		 0 Comments
Emulating Internet Traffic in Load Tests	https://www.r-bloggers.com/2010/05/emulating-internet-traffic-in-load-tests/	May 15, 2010	Neil Gunther		 0 Comments
Typo in Bayesian Core [again]	https://www.r-bloggers.com/2010/05/typo-in-bayesian%c2%a0core%c2%a0again/	May 15, 2010	xi'an	Reza Seirafi from Virginia Tech sent me the following email about Bayesian Core, which alas is pointing out a real typo in the reversible jump acceptance probability for the mixture model: With respect to the expression provided on page 178 for the acceptance probability of the split move, I was wondering if the omission of the density of the auxiliary parameters u1, u2, and u3 — specially u2, since its density is not necessarily equal to 1 contrary to the other two — is a case of typo. This is truly a typo, the acceptance probability at the bottom of page 178 should be  since u2 is indeed distributed from a non-uniform density. The physical reason for this (unacceptable!) typo is that I cut-and-pasted the LaTeX code from Monte Carlo Statistical Methods: (page 439) where u2 is a uniform variate! (Incidentally, there is a minor typo a few lines above: when defining , it should be , another side casualty of cut-and-paste!) Obviously, this could also affect the associated R code but I checked it and found the line which I think is correcting for the normal proposal. This typo will obviously be corrected in the next printing of Bayesian Core as well as in the new edition we plan to write in July. It also illustrates the folk theorem “One can never write the reversible jump acceptance probability right” that make me avoid using reversible jump each time I am facing a small enough collection of models to consider the exhaustive list of those models. 	 0 Comments
Linear regression models with robust parameter estimation	https://www.r-bloggers.com/2010/05/linear-regression-models-with-robust-parameter-estimation/	May 15, 2010	Ralph	There are situations in regression modelling where robust methods could be considered to handle unusual observations that do not follow the general trend of the data set. There are various packages in R that provide robust statistical methods which are summarised on the CRAN Robust Task View. As an example of using robust statistical estimation in a linear regression framework consider the CPUs data that was used in previous posts on linear regression and variable selection. For this data we could fit a model with six variables using least squares and also with a fast MM-estimator from the robustbase package. First step is to make the functions and data available for analysis: The linear model using least squares is fitted as follows: The summary for this model: The linear model using an MM-estimator is fitted as follows: Note that we need to increase the default number of iterations (50) to allow the routine to converge to a solution. The summary for this model: The two models differ in the variables that are considered important and the output from the lmrob function provides a summary of the weights that have been allocated to the data. A total of fifteen of the data points have been allocated very small weights by the fitting algorithm. Related posts: 	 0 Comments
A small customization of ESS	https://www.r-bloggers.com/2010/05/a-small-customization-of-ess/	May 14, 2010	Abhijit	"JD Long (at Cerebral Mastication) posted a question on Twitter about an artifact in ESS, where typing “_” gets you “
 The last fix obviously customizes ESS permanently for your emacs setup, while the first two allow you to get to underscore using the default ESS setup. Update: Seth Falcon posted his .emacs on Twitter, which allows C-= to map the assignment operator, and leaves _ alone    (setq ess-S-assign-key (kbd ""C-=""))

(ess-toggle-S-assign-key t) ; enable above key definition

;; leave my underscore key alone!

(ess-toggle-underscore nil)  Nice, Seth!! FYI, ESS is Emacs Speaks Statistics, an emacs addon developed by Tony Rossini and others to enable intelligent editing of statistical scripts in S+, R, SAS and Stata, as well as scripts for the Gibbs Sampling programs BUGS and JAGS, and can be found here "	 0 Comments
Because it’s Friday: Optical Illusion	https://www.r-bloggers.com/2010/05/because-its-friday-optical-illusion/	May 14, 2010	David Smith	  See more of the best illusions of 2010 at the link below. Best Illusion of the Year Contest: Top finalists in the 2010 contest 	 0 Comments
New R User Group in Boston	https://www.r-bloggers.com/2010/05/new-r-user-group-in-boston/	May 14, 2010	David Smith	There’s another new R User Group, this time in Boston: the New England R User Group. Their first meeting will be on Tuesday, May 25. Get all the info by joining the Google Group at the link below. Google Groups: New England R User Group 	 0 Comments
Introducing IBrokers (and Jeff Ryan)	https://www.r-bloggers.com/2010/05/introducing-ibrokers-and-jeff-ryan/	May 13, 2010	jryan		 0 Comments
Introduction to using R in research	https://www.r-bloggers.com/2010/05/introduction-to-using-r-in-research/	May 13, 2010	Thom Baguley		 0 Comments
Using R, LaTeX, and Sweave for Reproducible Research: Handouts, Templates, & Other Resources	https://www.r-bloggers.com/2010/05/using-r-latex-and-sweave-for-reproducible-research-handouts-templates-other-resources/	May 13, 2010	Stephen Turner		 0 Comments
Is it possible to get a causal smoothed filter ?	https://www.r-bloggers.com/2010/05/is-it-possible-to-get-a-causal-smoothed-filter/	May 12, 2010	Intelligent Trading		 0 Comments
pimax(mcsm)	https://www.r-bloggers.com/2010/05/pimaxmcsm/	May 12, 2010	xi'an	"The function pimax from our package mcsm is used in to reproduce Figure 5.11 of our book Introducing Monte Carlo Methods with R. (The name comes from using the Pima Indian R benchmark as the reference dataset.) I got this email from Josué I ran the ‘pimax’ example from the mcsm manual, and it gave me the following message:
 
 but when running pimax(10^2) on my machine I did get the following picture and no error message. So I wonder if this is a matter of version of R or something else…  "	 0 Comments
Manual variable selection using the dropterm function	https://www.r-bloggers.com/2010/05/manual-variable-selection-using-the-dropterm-function/	May 12, 2010	Ralph	When fitting a multiple linear regression model to data a natural question is whether a model can be simplified by excluding variables from the model. There are automatic procedures for undertaking these tests but some people prefer to follow a more manual approach to variable selection rather than pressing a button and taking what comes out. Fast Tube by Casper When there are a large number of variables it is awkward to manually go through each one in turn to make a decision about simplification to a more parsimonious model. In R there is a function dropterm that removes some of this task by assuming that we are interested in considering the outcome of dropping each model term one at a time. To illustrate this consider the cpus data set in the MASS package which contains information about a relative performance measure and characteristics of 209 CPUs. We load the package first to make the data available: We first fit a linear model with six explanatory variables: The function dropterm requires a fitted model, which we saved in the last command, and optionally we could specify what test to use to compare the initial model and each of the possible alternative models with one less variable. We can choose to perform an F test: The output from the function call indicates that we could excude the chmin variable then re-fit the model and continue again with the same checking process. Update: The dropterm function considers each variable individually and considers what the change in residual sum of squares would be if this variable was excluded from the model. There is a link between this F test and the t test that appears as part of the model summary – this is because of the link between these two distributions. For this model we would have: Let us consider the syct variable. The t statistic in the model summary is 2.789 and if we square this value we get 7.779 which is the F statistic produced by the dropterm function. Related posts: 	 0 Comments
Revolution Analytics and R in the news	https://www.r-bloggers.com/2010/05/revolution-analytics-and-r-in-the-news/	May 12, 2010	David Smith	It was quite the media frenzy for Revolution and R last week. In conjunction with our relaunch as Revolution Analytics, we spoke to more than a dozen journalists and analysts to explain why we think R is at the center of a perfect storm for predictive analytics: with routine collection of large data sets, data analysis is now a commercial imperative; statistical training is now prevalent, and mainly done with R; and compared to expensive, legacy tools R is the best environment for implementing the analytics that modern companies need to compete. There was quite a bit of coverage of the story, and I’ve listed the highlights below. And is often the case with journalists who have to create a multi-page article based only on a short conversation, there are a couple of clarifications to make, too. Quentin Hardy wrote an extensive profile of Revolution CEO Norman Nie for Forbes magazine. It gives a history of commercial statistical software starting with SPSS, and charts the transition to open-source with several examples of R applications. I did cringe, though, at the description of R as “a freebie invented in 1993” – due credit goes to Ross Ihaka and Robert Gentleman for founding R, and also to the many other contributors for making R the most successful open-source statistical software project of all time. The article is online now, and will also be in the May 24 print issue of Forbes. Timothy Prickett Morgan for The Register focussed more on R and Revolution’s open-core strategy for commercialization of R, and the background of Revolution as a business and its employees. (I was gratified to learn that Timothy had a copy of An Introduction to R, but he overstated my involvement with it — I only write the original version of the graphics chapters. Bill Venables was the main author.) Steve Miller continues his interview with Norman Nie at the Information Management blog, sharing some of his own experiences getting started with R and asking Norman about the future of R in the commercial world with Revolution R.  Finally, Chris Kanaracus wrote in PC World about Revolution’s plans for Revolution R to be an alternative to SAS and SPSS, and analysts that took a look at Revolution’s plan and the place of R in predictive analytics included James Taylor and BeyeNetwork. 	 0 Comments
Reflections on consulting part 5 – what languages and tools to learn?	https://www.r-bloggers.com/2010/05/reflections-on-consulting-part-5-%e2%80%93-what-languages-and-tools-to-learn/	May 12, 2010	erehweb	What languages and tools should you learn as a math/stat consultant?  To jump to the answer: Excel/VBA, SQL, R, Java, and Python. Spreadsheets have many problems with verifiability and scalability, so why Excel? Excel is: If you’re consulting, you should at least know the basics of Excel.  Learning introductory VB is also very worthwhile, and there are a number of perfectly fine analytic systems than run in Excel/VBA.  Yes, there are problems with extending to a full enterprise-level system, but you can deal with that in a follow-on engagement. SQL?  Pretty much everything you’ll do is either an input that comes from a database, or an output that should be stored in one.  And it allows for very basic data analysis. R.  Unlike my fellow bloggers at Win-Vector, I’m not a big fan of R.  But you can do a lot of statistics in it, and it’s free, so no need for your clients to get an expensive licence.  It’s also a reasonably modern language, unlike many of its competitors.  Since R is rapidly becoming the statistical analysis language of choice for cash-strapped startups or clients without big legacy systems, every consultant should be able to work in it. Java?  You don’t need to be a Java expert, but you should understand enough about Java to look at programs to see what they do, and to make minor alterations – invaluable for dealing with an engineering team, if they have to implement your great ideas. Python?  Well data is always a mess, so you should have some way of cleaning it before it gums up your shiny new models.  Python’s as good as any other, and it’s more intuitive than Perl.  But this is the least important of the recommendations. I haven’t included SAS on this list.  On the one hand, it was the predominant language for statistical analysis for a long time, so you can often find consulting work in it.  It’s also a language unlike most others, so almost worth learning for that reason alone.  And it is excellent at reading in data and dealing with large amounts of it.  On the other hand, the consulting rates for SAS work are generally low, and the lack of free licences makes learning it on spec a riskier move.  So if you know SAS or can pick it up at work, that’s great.  If not, it’s probably not worth learning it. Loyal readers have already loyally read parts 1-3 (networking, networking, and networking) and part 4 (why consult).  Stay tuned for part 6 (or maybe 6-7 if I cheat again), when we’ll delve more into the wonderful world of consulting. 	 0 Comments
What Social Network Analysis software do you use?	https://www.r-bloggers.com/2010/05/what-social-network-analysis-software-do-you-use/	May 12, 2010	Michal	See a the poll here by Gabriel Rossman at Code and Culture. I voted for R and ‘igraph’. If you use R you are getting access to all the other wonderful things that come with R. Using specialized package, like Pajek, UCINET etc requires constant going back and forth between network software and some other general analytical software (sas, spss, what have you). I also use both R packages. Mainly ‘igraph’ (and that is what I voted for) because I find the network data manipulation much easier than in ‘network’. I believe it is also much more efficient computationally than ‘network’ (Gabor Csardi showed couple of comparisons on UseR 2009). On the other hand with ‘network’ you get access to ERGM and all the other statnet packages written by people at Stats Dept at University of Washington. So igraph/network is not an easy choice. I’m actually working on reliable routines that will convert igraph objects to network and vice versa. Perhaps an ideal situation would be to have a an umbrella interface to igraph, network, graph etc. very similar to DBI as an umbrella over RMySQL, RSQLite etc….. 	 0 Comments
Collect and Parse GPS (NMEA0183) Data in R	https://www.r-bloggers.com/2010/05/collect-and-parse-gps-nmea0183-data-in-r/	May 11, 2010	Matt Shotwell	I recently wrote a serial connection for R-2.11.0 so that I can communicate with serial devices, for example an old Garmin eTrex Legend. This GPS device is able to output NMEA0183 sentences to a standard serial port (4800,8,1,N). I hooked up the device and used the serial connection to collect some data using some R commands similar to the following The particular NMEA0183 sentence I want begins with $GPRMC, the recommended minimum navigation information. This sentence includes UTC time and date, latitude, longitude, heading, ground speed and others. I’ve attached a copy of an unofficial description of the NMEA0183 protocol. I wrote the following two R functions to parse the $GPRMC sentences. The functions and test data may be found here NMEA0183Parse.R. The regular expression used to identify the $GPRMC sentence is somewhat rudimentary, I welcome any insights in this regard. Also, these functions may be easily modified to parse other NMEA0183 sentences. The following may be reproduced in R verbatim without any additional setup. According to Google Maps, this location is (long/lat) is at the Charleston Battery Park, Charleston Battery Park. 	 0 Comments
Sweave for Reproducible Research and Beatiful Statistical Reports	https://www.r-bloggers.com/2010/05/sweave-for-reproducible-research-and-beatiful-statistical-reports/	May 11, 2010	Stephen Turner		 0 Comments
Number Formatting	https://www.r-bloggers.com/2010/05/number-formatting/	May 11, 2010	C		 0 Comments
R Package ‘rms’ for Regression Modeling	https://www.r-bloggers.com/2010/05/r-package-rms-for-regression-modeling/	May 11, 2010	Stephen Turner		 0 Comments
Webinar May 20: Introduction to Revolution R	https://www.r-bloggers.com/2010/05/webinar-may-20-introduction-to-revolution-r/	May 11, 2010	David Smith	I’ll be giving a live webinar on Thursday next week (May 20) titled Introduction to Revolution R. If you’re new to the R world and wondering what you can do with R, this webinar is for you. I’ll also be introducing some of the functionality unique to Revolution R included in our Revolution R Community (free to everyone) and Revolution R Enterprise (free to academics) products. Sign up at the link below.  Revolution Analytics Webinars: Introduction to Revolution R 	 0 Comments
R function names, explained	https://www.r-bloggers.com/2010/05/r-function-names-explained/	May 11, 2010	David Smith	Why is the function to print out text in R named “cat”? Why is the function to delete objects called “rm”? Unless you have a background in Unix (or Linux) programming, some of R’s command names can seem, well, a bit arcane. Jeromy Anglim explains the provenance of many of R’s command names in this post: the details are not only interesting in their own right, but the etymology may also serve as a memory guide next time you’re searching for the function to find matching text in character vectors (“grep”). Jeromy Anglim: Abbreviations of R Commands Explained: 150+ R Abbreviations 	 0 Comments
Beware of rogue header files (Bioconductor installation)	https://www.r-bloggers.com/2010/05/beware-of-rogue-header-files-bioconductor-installation/	May 11, 2010	nsaunders	Just a short note concerning a “gotcha”. As I have many times before, I opened an R console on my newly-upgraded (to lucid 10.04) Ubuntu machine, typed source(“http://bioconductor.org/biocLite.R”) and began a Bioconductor install with biocLite().  Only this time, I saw this: A quick email to the Bioconductor mailing list put me in touch with the very helpful Martin Morgan, who suggested that I check my zlib libraries.  Sure enough, the rogue “egzread” was found in /usr/local/include/zlibemboss.h, along with a second zlib.h file, in addition to /usr/include/zlib.h. I moved the rogue zlib.h out of /usr/local/include and order was restored. So in summary, watch out when installing EMBOSS on Ubuntu – it seems to mess with things that it should not. 	 0 Comments
Putting Text in a Margin	https://www.r-bloggers.com/2010/05/putting-text-in-a-margin/	May 10, 2010	C		 0 Comments
String Concatenation in R	https://www.r-bloggers.com/2010/05/string-concatenation-in-r/	May 10, 2010	C		 0 Comments
A ridiculous email	https://www.r-bloggers.com/2010/05/a-ridiculous-email/	May 10, 2010	xi'an	"Wolfram Research presumably has a robot that sends automated email following postings on arXiv: Your article, “Evidence and Evolution: A review”, caught the attention of one of my colleagues, who thought that it could be developed into an interesting Demonstration to add to the Wolfram Demonstrations Project. The Demonstrations Project, launched alongside Mathematica 6 in May 2007, is a collection of over 5,000 interactive Demonstrations that cover myriad subjects, interests, and skill levels. The Demonstrations are free to download and manipulate thanks to Mathematica Player, which is also free. Building a Demonstration is a simple and straightforward process. If you have little or no experience with Mathematica, you may want to attend one of our free online seminars. In our S14 seminar, “Creating Demonstrations,” members of the Demonstrations team guide you step-by-step through the authoring process.
 Your published Demonstrations will appear on the Wolfram Demonstrations Project website, which averages over 50,000 hits a week. We welcome any questions you might have, and look forward to seeing a Demonstration submission from you soon. but they definitely got it all wrong there! They picked my book review of a philosophy of science book, Evidence and Evolution, where I complain of the lack of a true experiment..! "	 0 Comments
Example 7.36: Propensity score stratification	https://www.r-bloggers.com/2010/05/example-7-36-propensity-score-stratification/	May 10, 2010	Ken Kleinman		 0 Comments
An economist explains: Why I use R	https://www.r-bloggers.com/2010/05/an-economist-explains-why-i-use-r/	May 10, 2010	David Smith	Economist and R blogger JD Long gave a talk last week (as part of the vconf.org project) about why he uses R to do statistical forecasts of agricultural yield for the reinsurance company he works for. I couldn’t make the live session, but a replay is now available. The audio’s a bit choppy, but if you’ve every struggled with using Excel for doing anything more than the simplest analysis of data and (in JD’s words) you’re suffering from “Excel Exceedance Syndrome”, it’s worth it for the first five minutes alone. With a regression example (the code and data are available) shows how he not only gets great analysis out of R, it’s also reproducible and faster than using other methods. JD also does a great job of turning a technical problem (lack of the right files on the demo system) into a serendipitous demonstration of the flexibility of R, by downloading the extra data and packages on the fy. There’s also some good discussion on making use of Amazon EC2 when you need extra power from R, too. vconf.org: R the Language 	 0 Comments
ggplot2: Waterfall Charts	https://www.r-bloggers.com/2010/05/ggplot2-waterfall-charts/	May 10, 2010	learnr	Waterfall charts are often used for analytical purposes in the business setting to show the effect of sequentially introduced negative and/or positive values. Sometimes waterfall charts are also referred to as cascade charts. In the next few paragraphs I will show how to plot a waterfall chart using ggplot2.  A very small fictional dataset depicting the changes to a company cash position, found in a blogpost showing how to prepare a waterfall chart in Tableau. In order to preserve the order of the lines in a dataframe I convert the desc variable to a factor; id and type variable are also added: Next the data will be slightly reworked to specify the coordinates for drawing the waterfall bars. Now everything is set to plot the first waterfall chart. geom_rect is used to draw the rectangles using the coordinates calculated in the previous step. The fill mapping could use some tweaking (my preference is to have outflows in red, inflows in green, and net position in blue), for that I change the order of the underlying factor levels. Almost ready, one more tweak to the x-axis labels: the helper function below replaces spaces with new lines, making the labels more readable. Finally, the bar labels are also added (the conditional positioning of them is quite a lengthy process, as you can see). 	 0 Comments
Abbreviations of R Commands Explained: 250+ R Abbreviations	https://www.r-bloggers.com/2010/05/abbreviations-of-r-commands-explained-250-r-abbreviations/	May 10, 2010	Jeromy Anglim		 0 Comments
Book Review – Modern Applied Statistics with S by W. N. Venables and B. D. Ripley (Springer 2003)	https://www.r-bloggers.com/2010/05/book-review-%e2%80%93-modern-applied-statistics-with-s-by-w-n-venables-and-b-d-ripley-springer-2003/	May 9, 2010	Ralph	   Order this book from Amazon Modern Applied Statistics with S (Fourth Edition) is one of the oldest and most popular books on Applied Statistics using R and S-plus. A large number of topics in Applied Statistics are covered in this book and it is certainly not for the faint hearted. A sound knowledge of the Statistical Methods covered in each Chapter is important and there are the book includes many examples of using a wide range of techniques. The book opens with an overview of the S programming language and has an introductory analysis session to get the reader into using the system and to provide an idea of the way analysis is undertaken and some of the methods that are available to the analyst. The second Chapter introduces objects, which are an important part of the S programming language and the chapter covers a number of common data manipulation tasks and also the data frame which is probably the most useful object for the user. A brief coverage of data import and export follows and could possibly benefit from providing some more examples rather than focussing on describing the function arguments. There is a nice little section of working with subsets of data frames which is an important topic for analysis. The chapter ends well on creating tables and cross-tabulation of data. The third Chapter is an overview of the S programming language and provides some good examples and sensible advice about the need to make use of the vectorised calculation features of the language rather than using loops. This is a neat feature of the S language that is very important for users to get to grips with as it will simplify and speed up code. There will of course be situations where loops are required but overall it is best to avoid then where possible. There is only a small section on classes and methods and this is probably due to the authors having a separate book that describes S programming. Chapter four covers the base graphics system as well as Trellis graphics. This is a very good chapter that provides a large range of examples of common types of displays and highlights investigating multivariate data using the Trellis graphics paradigm. The many code examples can be adapted by the reader to create displays that are suitable for a specific application. There could have been more information about editing various components of the graphs but that would probably have been outside the scope of the book. The fifth chapter provides coverage of a range of univariate statistical methods starting with probability distributions and generating pseudo random numbers from the range of distributions available in R. There follows a good section of histograms and the issue of bin widths which are important for getting a good idea of the shape of a set of data. Classical statistical tests are given very short coverage compared to other books but there is enough for people to get started with other tests. There is a good little section of robust statistical methods which is a topic that is infrequently covered in other texts. Density estimation is also covered and the chapter ends with examples of using the bootstrap for statistical inference. Linear models are covered in chapter six starting with a reasonably simple example going through fitting models, checking the goodness of fit with residual diagnostics and making predictions from a linear model. There is a good little section on robust regression showing the ease of moving between models of different types. This is followed by an illustration of applying the bootstrap to parameter estimation in a linear model. Fitting analysis of variance models is covered with an example from a designed experiment which then leads to variable selection. The chapter ends with a short discussion of multiple comparison tests and post-hoc testing. Generalized linear models (GLM) are covered in brief in chapter seven of the book starting with a couple of examples of logistic regression for binary data and then Poisson regression for data that is based on counts. The chapter provides some other useful examples but is rather short given the other potential distributions for different data sources. In chapter eight non-linear models which often arise from theoretical considerations are investigated with details of how to fit them to data and to analyse the model outputs to determine the suitability of the model. The various issues associated with non-linear models due to the requirement for an iterative method to converge to a solution and discussed in detail along with methods for investigating the suitability of the assumptions. The second half of the chapter a wide range of extensions/alternatives to multiple linear regression covering smoothers, additive models, MARS, projection pursuit regression and neural networks. This provides a taste rather than a comprehensive coverage of these topics which is a general theme of the book. Tree based methods are introduced and discussed in chapter nine and the technical details are covered in more detail than some of the other methods which may be beyond the interest of some readers. The authors do however then move swiftly on to practical applied examples showing how to fit tree models to data and suggestions on how to simplify tree models to a manageable size. The tenth chapter of the book is dedicated to mixed effects models which is a framework that allows a standard linear or non-linear model to include a mixture of fixed and random effects. The focus is mainly on the nlme package which is in frequent use by analysts using R or S-plus for their work. The authors make a good effort at explaining the use and interpretation of these models which is a good starting point for the Pinheiro and Bates book on Mixed Effects Models that covers the topic in greater detail. Generalized Linear Mixed Effects Models are covered briefly at the end of the chapter. The broad area of exploratory multivariate analysis is addressed in chapter eleven starting with the some projection techniques – from the popular principal component analysis to projection pursuit and multidimensional scaling. Next up are partitioning methods including cluster analysis used for searching for structure in data set where there is no prior information about grouping. The chapter is rounded off with a discussion of techniques that are suitable for discrete data, often in the form of a contingency table, such as mosaic plots to investigate association between variables measured in a study. The topic of classification using statistical methods is covered in chapter twelve of the book. The chapter starts with discriminant analysis, which is one of the initial techniques used for classification, and touches on robust estimation of means and variances (location and scale) which is used to reduce the impact of unusual data points. There is then coverage of other methods used in classification such as K-means, neural networks and support vector machines. The performance of the competing methods is compared with an example on foernsice glass at the end of the chapter. Chapter thirteen is devoted to survival analysis and covers different approaches to handling survivor curves such as the Cox Proportional Hazards Model with a couple of extensive examples to illustrate the different models. As with most chapters in the book an assumption is made with regards to knowledge of the statistical methods discussed in the text. Time series analylsis is the topic of interest in chapter fourteen starting with the important topics of autocorrelation functions and partial autocorrelation functions. The frequently used ARIMA models are discussed next with a short discussion of model selection for these models and forecasting future values. Seasonality is also covered as would be expected. The chapter ends with short sections on regression with autocorrelated errors and analysis of financial time series. Chapter fifteen is a short chapter on spatial statistics with three areas covered – spatial interpolation and smoothing, kriging and point process analysis. There are worked examples showing how to undertake the analysis using S and is more of a useful reference for people who understand the theory but need instructions on how to apply the methods. The final chapter is on functions for performing general optimisation tasks and is slightly out of place compared to many of the other chapters. It is a useful topic but it is not clear how easy the reader will find it to make use of the methods based on the coverage in this chapter. Overall comment: this is a very good book (and highly recommended book) but probably not ideal for the beginner as it covers a very wide range of applied statistics methods and is probably best as a reference book to be dipped into as and when necessary. It does however provide a nice overview of the range of statistical applications and modern methods. 	 0 Comments
Computational Statistics	https://www.r-bloggers.com/2010/05/computational-statistics/	May 9, 2010	xi'an	Do not resort to Monte Carlo methods unnecessarily. When I received this 2009 Springer-Verlag book, Computational Statistics, by James Gentle a while ago, I briefly took a look at the table of contents and decided to have a better look later… Now that I have gone through the whole book, I can write a short review on its scope and contents (to be submitted). Despite its title, the book aims at covering both computational statistics and statistical computing. (With 752 pages at his disposal, Gentle can afford to do both indeed!) The book Computational Statistics is separated into four parts: Computational inference, together with exact inference and asymptotic inference, is an important component of statistical methods. The first part of Computational Statistics is indeed a preliminary containing essentials of math and probability and statistics. A reader unfamiliar with too many topics within this chapter should first consider improving his or her background in the corresponding area! This is a rather large chapter, with 82 pages, and it should not be extremely useful to readers, except to signal deficiencies in their background, as noted above. Given this purpose, I am not certain the selected exercises of this chapter are necessary (especially when considering that some involve tools introduced much later in the book). The form of a mathematical expression and the way the expression should be evaluated in actual practice may be quite different . The second part of Computational Statistics is truly about computing, meaning the theory of computation, i.e. of using computers for numerical approximation, with discussions about the representation of numbers in computers, approximation errors, and of course random number generators. While I judge George Fishman’s Monte Carlo to provide a deeper and more complete coverage of those topics, I appreciate the need for reminding our students of those hardware subtleties as they often seem unaware of them, despite their advanced computer skills. This second part is thus a crash course of 250 pages on numerical methods (like function approximations by basis functions and …) and on random generators, i.e. cover the same ground as Gentle’s earlier books, Random Number Generation and Monte Carlo Methods and Numerical Linear Algebra for Applications in Statistics, while the more recent Elements of Computational Statistics looks very much like a shorter entry on the same topics as those of Parts III IV of Computational Statistics. This part could certainly sustain a whole semester undergraduate course while only advanced graduate students could be expected to gain from a self-study of those topics. It is nonetheless the most coherent and attractive part of the book. It constitutes a must-read for all students and researchers engaging into any kind of serious programming. Obviously, some notions are introduced a bit too superficially, given the scope of this section (as for instance Monte Carlo methods, in particular MCMC techniques that are introduced in less than six pages), but I came to realise this is the point of the book, which provides an entry into “all” necessary topics, along with links to the relevant literature (if missing Monte Carlo Statistical Methods!). I however deplore that the important issue of Monte Carlo experiments, whose construction is often a hardship for students, is postponed till the 100 page long appendix. (I suspect that students do not read appendices is another of those folk theorems!) Monte Carlo methods differ from other methods of numerical analysis in yielding an estimate rather than an approximation. The third and fourth parts of the book cover methods of computational statistics, including Monte Carlo methods, randomization and cross validation, the bootstrap, probability density estimation, and statistical learning. Unfortunately, I find the level of Part III to be quite uneven, where all chapters are short and rather superficial because they try to be all-encompassing. (For instance, Chapter 8 includes two pages on the RGB colour coding.) Part IV does a better job of presenting machine learning techniques, if not with the thoroughness of Hastie et al.’s The Elements of Statistical Learning: Data Mining, Inference, and Prediction… It seems to me that the relevant sections of Part III would have fitted better where they belong in Part IV. For instance, Chapter 10 on estimation of functions only covers the evaluation of estimators of functions, postponing the construction of those estimators till Chapter 15. Jackknife is introduced on its own in Chapter 12 (not that I find this introduction ultimately necessary) without the bootstrap covered in eight pages in Chapter 13 (bootstrap for non-iid data is dismissed rather quickly, given the current research in the area). The first chapter of Part IV covers some (non-Bayesian) estimation approaches for parametric families, but I find this chapter somehow superfluous as it does not belong to the computational statistic domain (except as an approximation method, as stressed in Section 14.4). While Chapter 16 is a valuable entry on clustering and data-analysis tools like PCA, the final section on high dimensions feels out of context (and mentioning the curse of dimensionality only that close to the end of the book does not seem appropriate). Chapter 17 on dependent data is missing the rich literature on graphical models and their use in the determination of dependence structures. Programming is the best way to learn programming (read that again) . In conclusion, Computational Statistics is a very diverse book that can be used at several levels as textbook, as well as a reference for researchers (even if as an entry towards further and deeper references). The book is well-written, in a lively and personal style. (I however object to the reduction of the notion of Markov chains to discrete state-spaces!) There is no requirement for a specific programming language, although R is introduced in a somewhat dismissive way (R most serious flaw is usually lack of robustness since some [packages] are not of high-quality) and some exercises start with Design and write either a C or a Fortran subroutine. BUGS is not mentioned at all. The appendices of Computational Statistics also contain the solutions to some exercises, even though the level of detail is highly variable, from one word (“1″) to one page (see, e.g., Exercise 11.4). The 20 page list of references is preceded by a few pages on available journals and webpages, which could get obsolete rather quickly. Despite the reservations raised above about some parts of Computational Statistics that would benefit from a deeper coverage, I think this book is a reference book that should appear in the shortlist of any computational statistics/statistical computing graduate course as well as on the shelves of any researcher supporting his or her statistical practice with a significant dose of computing backup. 	 0 Comments
Sweave with Emacs and ESS, problem solved!	https://www.r-bloggers.com/2010/05/sweave-with-emacs-and-ess-problem-solved/	May 9, 2010	Shige		 0 Comments
Using the update function during variable selection	https://www.r-bloggers.com/2010/05/using-the-update-function-during-variable-selection/	May 9, 2010	Ralph	When fitting statistical models to data where there are multiple variables we are often interested in adding or removing terms from our model and in cases where there are a large number of terms it can be quicker to use the update function to start with a formula from a model that we have already fitted and to specify the terms that we want to add or remove as opposed to a copy and paste and manually editing the formula to our needs. Consider the oil-bearing rocks data set that is available with the R software which is used extensively as an example by many authors. One model that can be used as a starting point is a linear model with additive terms for the three variables: Given this model, saved as an object rock.mod1, we might be interested in considering adding an interaction term between the area and perimeter measurements. The update function has various options and the simplest case is to specfiy a model object and a new formula. The new formula can use the period as short hand for keep everything on either the left or right hand side of the formula and the plus or minus sign used to add or remove terms to the model. In the case of adding an interaction term our call would be: The first function argument is the name of the model we fitted previously and the periods indicate that we want to use the same response variable and to start with the whole formula but add an interaction term between area and perimeter – the colon is used to specify an interaction term by itself. This fitted model is now: The update function can also be used to change other aspects of the linear model or in fact many other types of model are set up to repsond sensibly to using this function. Related posts: 	 1 Comment
Forsythe’s algorithm	https://www.r-bloggers.com/2010/05/forsythe%e2%80%99s-algorithm/	May 8, 2010	xi'an	In connection with the Bernoulli factory post of last week, Richard Brent arXived a short historical note recalling George Forsythe’s algorithm for simulating variables with density  when  (the extension to any upper bound is straightforward). The idea is to avoid computing the exponential function by simulating uniforms  until  since the probability of this event is  its expectation is  and the probability that n is even is . This turns into a generation method if the support of G is bounded. In relation with the Bernoulli factory problem, I think this has potential applications in that, when the function G(x) is replaced with an unbiased estimator the subsequent steps remain valid. This approach would indeed involve computing one single value of G(x), but this is also the case with Latuszyński et al.’s and our solutions… So I am uncertain as to whether or not this has practical implications. (Brent mentions normal simulation but this is more history than methodology.) 	 0 Comments
Basket Option Pricing: Step by Step	https://www.r-bloggers.com/2010/05/basket-option-pricing-step-by-step/	May 8, 2010	Lee	"I find options fascinating because they deal with the abstract ideas of volatility and correlation, both of which are unobservable and can often seem like wild animal spirits (take the current stock market as an example).  Understanding these subtle concepts is never easy, but it is essential in pricing some of the more exotic options which involve multiple underlying stocks.  To set the scene, let’s pretend that your neighbor wants to make a bet with you where he will pay you $100 if Google (GOOG) and Apple (APPL) are above 500 and 240 respectively after 1 year, but you have to pay him $25 today.  How would we determine if $25 is a good deal or not?  The first things we need to do is to define the dynamics for all the tradable assets.  We have at our disposal a money market account, as well as the stocks of Google and Apple (it is not a coincidence that I picked companies that don’t pay dividends). Below describes how we will model the dynamics of these assets.



In this model,  is the price of a share in the money market account, and  and  are the stock prices.  Notice that we have correlated the Brownian Motions  such that the stock prices will tend to move up and down together just like in the real world.  To make this model easier to work with, we will first convert the correlated Brownian Motions into independent Brownian Motions  by defining the following.



By doing this, we can rewrite the ‘real world’ dynamics as


 Writing down a bunch of stochastic differential equations was pretty mechanical, but now comes the artsy part of modeling.  The first thing we need is some stock price data, which can easily be downloaded from Yahoo Finance. This data gives us the stock price over discreet time intervals (I picked daily close prices).  To use this data, lets define the discreet return  as the solution to the following , in which case we can write .  By assuming the drift and volatility terms are constants, we can use our model of the stock price dynamics to write the discreet returns as the following.



Where  is the discreet time interval.  By doing this, we can show that the discreet returns are distributed iid normal.



We can use the iid normal distribution to calibrate all the parameters.   are simply the sample standard deviations of the discreet returns divided by ,  is the sample correlation of the discreet returns, and  are the sample means of the discreet returns divided by  (although I should note that we don’t actually need to know  to price this option). Now that we have the math worked out for calibrating the model, let’s take a look at some real data and see if the data agrees with our assumptions.  First we need to read the data into R.  The data I used can be found here. (I used R’s scan function since I’m lazy). Let’s take a look at the time series of stock prices.  Now calculate and plot the daily returns.  To check the iid normal assumption, we should do an autocorrelation plot and a QQ normal plot.  Lastly, lets also take a look at the joint distribution of the returns.  The ACF plot suggests that the daily returns are iid, but they certainly have heavier tails than what would be produced by a normal distribution. In order to keep the model simple, we’ll assume normality of the returns holds, and give ourselves a ‘buffer’ when we calculate the fair price.  Finally, time for the calibration! 

GOOG vol: 0.2262006

AAPL vol: 0.2756421

Rho: 0.5413732

 We are now at the point where we can price this option.  As usual, the price of the option is the expected discounted payoff under the risk neutral measure.  This particular option can be priced as  where the risk neutral dynamics of the stock prices are the following.



Where  are independent Brownian Motions under the risk neutral measure. We will estimate the expectation through Monte Carlo simulation under a Euler discretization scheme. Rather than assume a constant interest rate, lets make things a tad more interesting by modeling the interest rate using the Vasicek Model (I’ll assume a calibration, but we could have calibrated it to zero coupon bond prices).



The following R code simulates the risk neutral dynamics of this model and estimates the expectation. By doing so, we find that the fair price of this option is $0.31 per $1 of notional.  Thus, buying it from our neighbor for $25 seems like a deal if we think the $6 difference is a sufficient buffer to cover the simplifying assumptions we made. "	 0 Comments
Initial Post	https://www.r-bloggers.com/2010/05/initial-post/	May 8, 2010	C		 0 Comments
Connecting R and Python	https://www.r-bloggers.com/2010/05/connecting-r-and-python/	May 7, 2010	Matt Asher	There are a few ways to do this, but the only one that worked for me was to use Rserve and rconnect. In R, do this: Then you can connect in Python very easily. Here is a test in Python: 	 0 Comments
Revolution’s 2010 roadmap	https://www.r-bloggers.com/2010/05/revolutions-2010-roadmap/	May 7, 2010	David Smith	As part of all the news from yesterday, we also announced our vision and roadmap for the Revolution R product line for 2010. You can see a short summary of our vision in this two-minute video, or see more details in the roadmap whitepaper available for download. But here’s a quick overview of our plans: First, we intend to make it possible to use the R language to perform statistical analyses on very large datasets, without being constrained by the amount of RAM available, and by being able to draw on the power of a number of machines in a cluster or in the cloud to tackle these large problems. We’ve got quite a bit of experience in this area: our ParallelR libraries have been available for more than 2 years now. But now we’re taking it to the next level, so that as an R programmer you don’t have to parallelize the R computations yourself: instead, we’re writing algorithms for data manipulation and statistical models that automatically run in parallel and take advantage of the CPUs and machines available. (I talked about some of the details of this project at the R/Finance 2010 conference.)  Second, we’re working on a Web Services layer for R, to make it easier for application developers to build applications — especially Web-based applications — that take advantage of computations done in R. At the back-end of this layer sits one or more R servers (again, in a cluster or in the cloud) to support the demands of high-performance applications. Third, we’re building a graphical user interface for R. It’s being built as a thin-client application on top of the web-services layer described above. It’s being designed so that even a casual user who needs to analyze data can open a web browser and point and click to do “standard” statistical analyses … but you always have access to the R code both as a learning tool for the language, and so you can extend the GUI for new applications. Finally, we plan to make it easier to migrate to Revolution R if you’re currently using other statistical tools, to help you translate legacy data and code into the R environment. It’s a big plan, but it’s one believe in: we strongly feel that R is the best environment out there for doing statistical analysis, and we’re doing everything we can to translate R’s success in the academic world to the commercial environment. Read more in the document linked below. Revolution Analytics: Executive Roadmap: The R Revolution 	 0 Comments
Knowing whether a time-series has been differenced appropriately in order to make it stationary	https://www.r-bloggers.com/2010/05/knowing-whether-a-time-series-has-been-differenced-appropriately-in-order-to-make-it-stationary/	May 7, 2010	Pradeep Mavuluri		 0 Comments
A new site for the R community: inside-R.org	https://www.r-bloggers.com/2010/05/a-new-site-for-the-r-community-inside-r-org/	May 6, 2010	David Smith	We’ve just launched a beta/preview of a new website for the R community, inside-R.org. The site is sponsored by Revolution Analytics (who funded its development and maintenance), but it’s designed for anyone who uses or has in an interest in the R Project generally. So, you might ask, why another community site for R, when there’s already several R resources around the web? There’s the r-help mailing list, the R Project website, the R tag on Stackoverflow, the #rstats tag on Twitter, crantastic.org, r-bloggers.com and various other sites including this humble blog. But that’s actually kinda the point — information about R today is scattered all about the Web, and we wanted to create a single portal where you can find pointers to all the best, new information about R from around the Web. The idea is that the home-page of inside-R.org will be editorially managed to review all of the best R sites on a daily basis, and float the best content to the top. That’s what I’ve been trying to do for the last couple of years with the Revolutions blog (and that’s what you see on the inside-R.org homepage today), but as we roll out more features this will be more of a crowdsourced affair at inside-R.org. Soon, you’ll be able to create an account on inside-R.org and recommend to other R community members information you find valuable, and even create your own content, too. Look for some exciting new features over the coming months. There’s already one new, and I think pretty cool feature on the site: “Get R“, a one-click process for downloading R for your computer. R is distributed through the CRAN network of mirrors, but especially for someone who’s new to R is can be a tricky process to find the right link on r-project.org, select a CRAN mirror, and choose the right platform. “Get R” automatically detects what kind of computer you’re browsing with and from where, so it can choose the right platform and closest mirror for you. (Of course, you can change the defaults if you wish). For most users, this makes downloading R a 1-click process instead of a 7-click process. You can also download Revolution R Community from the same page, if you prefer. And to further promote R around the Web, you can even add a badge, such as this one: to your own blog or website, so that others can have easy access to downloading R.  So, keep an eye out for updates to the site over the next few months. You can keep up-to-date with news about inside-R.org by following @inside_R on Twitter (and of course I’ll report major updates here, too). inside-R.org: A Community Site for R – Sponsored by Revolution Analytics 	 0 Comments
Bayes vs. SAS	https://www.r-bloggers.com/2010/05/bayes-vs-sas/	May 6, 2010	xi'an	"Glancing perchance at the back of my Amstat News, I was intrigued by the SAS advertisement Bayesian Methods and so decided to take a look at those items on the SAS website. (Some entries date back to 2006 so I am not claiming novelty in this post, just my reading through the manual!) Even though I have not looked at a SAS program since the time in 1984 I was learning principal component and discriminant analysis by programming SAS procedures on punched cards, it seems the MCMC part is rather manageable (if you can manage SAS at all!), looking very much like a second BUGS to my bystander eyes, even to the point of including ARS algorithms! The models are defined in a BUGS manner, with priors on the side (and this includes improper priors, despite a confusing first example that mixes very large variances with vague priors for the linear model!). The basic scheme is a random walk proposal with adaptive scale or covariance matrix. (The adaptivity on the covariance matrix is slightly confusing in that the way it is described it does not seem to implement the requirements of Roberts and Rosenthal for sure convergence.) Gibbs sampling is not directly covered, although some examples are in essence using Gibbs samplers. Convergence is assessed via ca. 1995 methods à la Cowles and Carlin, including the rather unreliable Raftery and Lewis indicator, but so does Introducing Monte Carlo Methods with R, which takes advantage of the R coda package. I have not tested (!) any of the features in the MCMC procedure but judging from a quick skim through the 283 page manual everything looks reasonable enough. I wonder if anyone has ever tested a SAS program against its BUGS counterpart for efficiency comparison. The Bayesian aspects are rather traditional as well, except for the testing issue. Indeed, from what I have read, SAS does not engage into testing and remains within estimation bounds, offering only HPD regions for variable selection without producing a genuine Bayesian model choice tool. I understand the issues with handling improper priors versus computing Bayes factors, as well as some delicate computational requirements, but this is a truly important chunk missing from the package. (Of course, the package contains a DIC (Deviance information criterion) capability, which may be seen as a substitute, but I have reservations about the relevance of DIC outside generalised linear models. Same difficulty with the posterior predictive.) As usual with SAS, the documentation is huge (I still remember the shelves after shelves of documentation volumes in my 1984 card-punching room!) and full of options and examples. Nothing to complain about. Except maybe the list of disadvantages in using Bayesian analysis: which does not say much… Since the MCMC procedure allows for any degree of hierarchical modelling, it is always possible to check the impact of a given prior by letting its parameters go random. I found that most practitioners are happy with the formalisation of their prior beliefs into mathematical densities, rather than adamant about a specific prior. As for computation, this is not a major issue. 
Filed under: Books, R, Statistics Tagged: adaptive MCMC methods, Bayes factor, Bayesian inference, BUGS, coda, convergence diagnostics, DIC, GENMOD, Gibbs sampling, hypothesis testing, improper prior, Introducing Monte Carlo Methods with R, LIFEREG, MCMC, Metropolis-Hastings, model choice, PHREG, SAS, variable selection      

 "	 0 Comments
R: choose file dialog box	https://www.r-bloggers.com/2010/05/r-choose-file-dialog-box/	May 6, 2010	Matt Asher	Needed this one recently, it pops up a window to pick a file to be used by r, then reads the contents into myData:  	 0 Comments
How I came to R	https://www.r-bloggers.com/2010/05/how-i-came-to-r/	May 6, 2010	VCASMO - drewconway		 0 Comments
Revolution R Enterprise now free to academics	https://www.r-bloggers.com/2010/05/revolution-r-enterprise-now-free-to-academics/	May 6, 2010	David Smith	Unlike Revolution R Community which is 100% free to everyone, our commercial-grade Revolution R Enterprise distribution bundles R with proprietary components from our development team, which are normally available only to paying subscribers. (Those subscriptions are the way we get income to keep the company going.) Those components include our full ParallelR libraries for parallel programming, enhanced for 64-bit Windows, and our R Productivity Environment with full code editing and visual debugging on Windows … and there’s more to come soon in our roadmap.  Today, we’re making the full Revolution R Enterprise distribution available to anyone in the academic community, free of charge. You can read all the details in this page about the free academic download program. All you need to do is fill out a form attesting that you’re a professor or a student at an accredited degree-granting institution, and you can download the Windows 32-bit, Windows 64-bit or RHEL 5 version, free of charge. The only restrictions are that it’s for use on a single-user workstation, and you won’t have access to live technical support. (For universities/schools that full support on a server or cluster, subscriptions are still available.) Since there’s so much overlap between the R community and the academic community, this is just one more way for us to give back to the community. We want to make sure everyone in academia gets access to both R and all the extensions we’ve made to it, and will make in the future. Revolution Analytics: Free Single-User Subscription to Revolution R Enterprise for the Academic Community 	 0 Comments
Exporting R output to MS-Word with R2wd (an example session)	https://www.r-bloggers.com/2010/05/exporting-r-output-to-ms-word-with-r2wd-an-example-session/	May 6, 2010	Tal Galili	"Creating reports is one of the basic tasks in data analysis.  R provides numerous functions and packages to export it’s (beautiful) output and help compile it into a report. In this post I will present one such (basic) solution for Windows OS users for exporting R output into Microsoft Word using the R2wd (package).  There are more ways and strategies for doing this, and if encouraged by comments, I will gladly write more on the subject.
*  *  * The package R2wd (available through CRAN) relies on rcom.  It is a wrapper that uses the statconnDCOM server to communicate with MS-Word via the COM interface. R2wd can perform the basic tasks you would expect to need when creating a report from R. It allows you to: The current R2wd can still be seen as being in BETA stages.  Some features are not yet available, such as: But from a (pleasant) correspondence with the package developer, I was assured the next release will supply us with more options and features. R2wd package developer, Christan Ritter, invites feedback from users.  So if you have features you are missing in this packages, I believe he would like to know about it (you can e-mail Christan at:     christian.ritter  ridaco  be  ) The current version of R2wd is 1.1 and Christan Ritter (the package developer), says it is a “first idea” and that a more elaborate version will soon (e.g: around July) be available on CRAN.   In the meantime, Christan was so kind as to send me a more recent version of the package, which you (until it gets uploaded to CRAN), you are welcome to download from here:
R2wd 1.3 download link Being young doesn’t prevent from R2wd to do some nice things. Here is the text from the library(help=R2wd) : If Word is not already running, wdGet() opens a new Word document, otherwise, it establishes a COM handle to the instance which is already running. The functions wdTitle, wdHeader, wdBody, and wdParagraph can be used to inject text elements into Word. Moreover, bookmarks can be added via wdInsertBookmarks and wdGoToBookmark allows to navigate among the bookmarks which also exist. There is another set of convenience functions, wdSection, wdSubsection, and wdSubsubsection which insert headers of level 1, 2, or 3, start new ’Sections’ in Word, and add bookmarks.
Graphs and dataframes can be inserted intoWord, by the wdPlot, wdTable commands. The wdTable command takes a dataframe or an array as arguments, creates a Word table of the appropriate dimensions and injects the content of the dataframe or array into it. It then formats the table in Word using elementary formating elements.
The functions wdApplyTheme and wdApplyTemplate allow to work with themes and templates. Here is an example sessions to demonstrate some of what is said: Update:
Upon reading my post, Chris suggested that I’ll also add a note here about SWORD, a tool written by Thomas Baier (the creator of the StatconnDCOM server) which allows to include R-code in a Sweave-like fashion in Word documents. Here is a link to the project: http://rcom.univie.ac.at "	 0 Comments
REvolution Computing is now Revolution Analytics	https://www.r-bloggers.com/2010/05/revolution-computing-is-now-revolution-analytics/	May 6, 2010	David Smith	So, as you may have noticed from the new banner here at the blog, we’ve changed our name to Revolution Analytics. It’s still the same company and the same people, and still focused around the R Project, but now with a fresh new look and lots of exciting news. I’ll be blogging about our new academic program, our new community site, and our newly-announced roadmap throughout the day, but for now (hey, it’s a big day!) check our our press release for a summary of the news. Press Release: Revolution Analytics Defines the Future of Predictive Analytics with R  	 0 Comments
R is like Ruby	https://www.r-bloggers.com/2010/05/r-is-like-ruby/	May 6, 2010	prasoonsharma		 0 Comments
Mixed linear model approach adapted for genome-wide association studies	https://www.r-bloggers.com/2010/05/mixed-linear-model-approach-adapted-for-genome-wide-association-studies/	May 6, 2010	Stephen Turner		 0 Comments
Rearranging definitions in R	https://www.r-bloggers.com/2010/05/rearranging-definitions-in%c2%a0r/	May 6, 2010	ellbur	I came up with a handy little trick for programming in R. I like to define a lot of variables all at once without worrying about what order they’re in. The goal would be something like this: But of course that doesn’t work because K can’t refer to R2 until R2 has been defined. Now a nice feature in R (or a horrific one, depending on your tastes) is that default arguments to functions are lazily evaluated and may refer to each other (they can even refer to local variables defined within the function!). So the above may be rewritten And the variables can now be accessed as Ckt$K, Ckt$Wc, etc. This is often exactly what I want — it packages up these variables into Ckt so they won’t pollute the global environment. But sometimes I do want them to pollute the global environment, and not just by calling attach(). So how would you do that? The solution I came up with is the rarely used formals() function. It gets or sets the formal parameters of a function. And I used it to build this friendly little contraption… Now the parallel definitions can be done as But it seems such a shame to lose the lazy-evaluation with the line “Parent[[Name]] = Results[[Name]]”. (I mean it doesn’t actually matter, but it was lazy up until that point!). Problem is what I wrote next didn’t work: Can you figure out why? It’s very subtle. Basically for loops (and loops in generally, including, irritatingly, lapply) in R do not create a local environment (aka closure). That means that the loop variable (‘Name’ here) gets overwritten each time. Since “Results[[Name]]” is lazily evaluated, it doesn’t have the value either, so they all take on the value of the last one. The solution is to create a closure. This gives the final product. 	 0 Comments
Candy branching process	https://www.r-bloggers.com/2010/05/candy-branching%c2%a0process/	May 5, 2010	xi'an	The mathematical puzzle in the latest weekend edition of Le Monde is as follows: Two kids are given three boxes of chocolates with a total of 32 pieces. Rather than sharing evenly, they play the following game: Each in turn, they pick one of the three boxes, empty its contents in a jar and pick some chocolates from one of the remaining boxes so that no box stays empty. The game ends with the current player’s loss when this is no longer possible. What is the optimal strategy? This led me to consider a simple branching process starting from a multinomial  to define . and then following the above splitting process, namely the selection of the dead and of the split components,  and  with the updated value being  This process is obviously not optimal but on the opposite completely random. Running a short R program like leads to a histogram of the game duration which is as follows. (Note that the R command sample((1:3)[prc>1]) does not produce what it should when only one term of prc is different from 1, hence the condition.) Obviously, this is not a very interesting branching process in that the sequence always ends up in a few steps… Of course, this does not tell much about the initial puzzle. However, discussing the problem with Antoine Dreyer and Robin Ryder led to Antoine obtaining all winning and loosing configurations up to  by a recursive R algorithm and to Robin establishing a complete resolution (I do not want to unveil it before he does!) that involves the funny facts [a] any starting configuration with only odd numbers is loosing and [b] any  that is a power of 2, like 32, always produces winning configurations. 	 0 Comments
Game of Life in R	https://www.r-bloggers.com/2010/05/game-of-life-in-r/	May 5, 2010	Matt Asher	 Before I decided to learn R in a serious way, I thought about learning Flash/Actionscript instead. Most of my work involves evolutionary models that take place over time. I need visual representations of change. It’s certainly possible to represent change and tell an evolving story with a single plot (see for example Tufte’s favorite infographic), but there are a lot more options when you can use animations. Flash is also object oriented, well documented with hundreds of books and websites, and has a powerful (albeit challenging to learn) IDE which helps for large coding projects. The drawbacks to Flash are that it is way behind R in terms of statistical tools, is a closed, expensive language to work with, and dispute widespread use it might be so weak that a single mobile computing company might kill it. So I picked R, with the idea that when I needed animations, I would find a way to build them. The code below is my first test of using R to generate animations. It’s a variant of Conway’s Game of Life (not to be confused with the Milton Bradley version), where single celled lifeforms live or die based on how many living neighbors they have. In my version, the rules for each cell are determined randomly, in advance of the game. The board size is fixed (see the configuration options at the beginning), whereas Conway’s version was played on a theoretically infinite grid. Green cells are “alive”, black ones are “dead”. I tried for nearly an hour to match the Black=living, White=dead scheme of Conway but couldn’t get that to work, maybe you can figure out how to do it. I re-sized the resulting animated GIF with an external program, that’s another thing I still need to figure out in R.  	 0 Comments
13 videos for learning R	https://www.r-bloggers.com/2010/05/13-videos-for-learning-r/	May 5, 2010	David Smith	Jeromy Anglim has just posted a nice round-up of instructional videos for learning R. The videos are categorized into four levels:  Check out all the links at Jeromy’s post, linked below. Jeromy Anglim’s blog: Videos on Data Analysis with R: Introductory, Intermediate, and Advanced Resources 	 0 Comments
Fun with R: Clustering and MDS	https://www.r-bloggers.com/2010/05/fun-with-r-clustering-and-mds/	May 5, 2010	Millsy		 0 Comments
Videos on Data Analysis with R: Introductory, Intermediate, and Advanced Resources	https://www.r-bloggers.com/2010/05/videos-on-data-analysis-with-r-introductory-intermediate-and-advanced-resources/	May 4, 2010	Jeromy Anglim		 0 Comments
You won, but how much was luck and how much was skill?	https://www.r-bloggers.com/2010/05/you-won-but-how-much-was-luck-and-how-much-was-skill/	May 4, 2010	dan	"THE ABILITY OF WINNERS TO WIN AGAIN  Even people who aren’t avid baseball fans (your DSN editor included) can get something out of this one. When two baseball teams play each other on two consecutive days, what is the probability that the winner of the first game will be the winner of the second game?  [If you like fun, write down your prediction.] DSN’s father-in-law told him that recently the Mets beat the Phillies 9 to 1, but the very next day, the Phillies beat the Mets 10 to 0. How could this be? If the Mets were so good as to win by 8 points, how could the exact same players be so bad as to lose by 10 points to the same opponents 24 hours later? Let’s call this situation (in which team A beats team B one one day, but team B beats team A the very next day) a “reversal”, and we’ll say the size of the reversal is the smaller of the two margins of victory. In the above example, the size of the reversal was 8. Using R (code provided below), DSN obtained statistics on all major league baseball games played between 1970 and 2009 and calculated how often each type of reversal occurs per 100,000 pairs of consecutive games. The result is in the the graph above. Big reversals are rare. A reversal of size 8 occurs in only 174 of 100,000 games; a size 12 reversal happens but 10 times per 100k.  A size 13 reversal never happened in those 40 years. One might think this is because it would be uncommon for a team that is so good to suddenly become so bad and vice versa, but note that big margins of victory are rare: only 4% of games have margins of victory of 8 points or larger. Back to our question:  If a team wins on one day, what’s the probability they’ll win against the same opponent when they play the very next day? We asked two colleagues knowledgeable in baseball and the mathematics of forecasting. The answers came in between 65% and 70%. The true answer: 51.3%, a little better than a coin toss. That’s right. When you win in baseball, there’s only a 51% chance you’ll win again in more or less identical circumstances. The careful reader might notice that the answer is visible in the already mentioned chart. The reversals of size 0, (meaning no reversal, meaning the same team won twice) occur 51,296 times per 100,000 pairs of consecutive games. [At this point, DSN must admit that it is entirely possible that it has made a computational error. It welcomes others to reproduce the analysis with the code or pre-processed data at the end of this post.] What of the adage “the best predictor of future performance is past performance”? It seems less true than Sting’s observation “History will teach us nothing“. Let’s continue the investigation. Here were plot the probability of winning the second game based on obtaining various margins of victory in the first game. We simply calculated the average win rate for each margin of victory up to 11 games, which makes up 98% of the data, and bin together the remaining 2%, comprising margins of victory from 12 to 27 points. (Rest assured, the binning makes the graph look prettier, but does not affect the outcome.)  The equation of the robust regression line is: Probability(Win_Second_Game) = .498 + .004*First_Game_Margin  which suggests that even if you win the first game by an obscene 20 points, your chance of winning the second game is only 57.8% Still in disbelief? Here we do no binning and plot the margin of victory (or loss) of the first game winner as a function of its margin of victory in the first game. The clear heteroskedasticity is dealt with by iterative reweighted least squares in R’s rlm command. Similar results are obtained by fitting a loess line. This model is Expected_Second_Game_Margin = -.012 + .030*First_Game_Margin   One final note. The 51.3% chance you’ll win the second game given you’ve won the first is smaller than the so called “home team advantage”, which we found to be a win probability of 54.2% on first games and 53.8% on second games.  When the home team wins the first game, it wins the second game 54.7% of the time.
When the home team loses the first game, it wins the second game 52.8% of the time.
When the visitor wins the first game, it wins the second game 47.2% of the time.
When the visitor loses the first game, it wins the second game 45.3% of the time. Surprisingly, when it comes to winning the second game, it’s better to be the home team who just lost than the visitor who just won. So much for drawing conclusions from winning. Decision Science News has always wondered why teams are so eager to fire their coaches after they lose a few big games. Don’t they realize that their desired state of having won those same few big games would have been mostly due to luck? There you have it. Either we have made an egregious error in calculation or recent victories are surprisingly uninformative. Do your own analysis alternative 1: The pre-processed data
If you wish, you can cheat and get the pre-processed data at http://www.dangoldstein.com/flash/bball/reversals.zip  This may be of interest for people who don’t use R  or for impatient types who just want to cut to the chase. No guarantee that our pre-processing is correct. It should be all pairs of consecutive games between the same two teams. Do your own analysis alternative 2: The code I’ll provide the column names file for your convenience at http://www.dangoldstein.com/flash/bball/cnames.txt. I left out a bunch of columns names I didn’t care about. The complete list is at: http://www.dangoldstein.com/flash/bball/glfields.txt R CODE
(Don’t know R yet? Learn by watching: R Video Tutorial 1, R Video Tutorial 2)


#Data obtained from http://www.retrosheet.org/

#Go for the files http://www.retrosheet.org/gamelogs/gl1970_79.zip through

#http://www.retrosheet.org/gamelogs/gl2000_09.zip and unzip each to directories

#named ""gl1970_79"", ""gl1980_89"", etc, reachable from your working directory.
library(MASS) #For robust regression, can omit if you don't want to fit lines
#Column headers, Can get from www.dangoldstein.com/flash/bball/cnames.txt

#If you want all the headers, create from www.dangoldstein.com/flash/bball/glfields.txt

LabelsForScript=read.csv(""cnames.txt"", header=TRUE)
#Loop to get together all data

dat=NULL

for (baseyear in seq(1970,2000,by=10))

{

endyear=baseyear+9

#string manupulate pathnames

#reading in datafiles to one big dat goes here

for (i in baseyear:endyear)

 {

 mypath=paste(""gl"",baseyear,""_"",substr(as.character(endyear),start=3,stop=4),""/GL"",i,"".TXT"",sep="""")

 cat(mypath,""n"")

 dat=rbind(dat,read.csv(mypath, col.names=LabelsForScript$Name))

 }

}
rel=dat[,c(""Date"", ""Home"",""Visitor"",""HomeGameNum"",""VisitorGameNum"",""HomeScore"",""VisitorScore"")] #relevant set
rel$PrevVisitorGameNum=rel$VisitorGameNum-1

rel$PrevHomeGameNum=rel$HomeGameNum-1

rel$year=substr(rel$Date,start=1,stop=4)
rm(dat)
head(rel,20); summary(rel)
relmerge=merge(rel,rel,

  by.x=c(""Home"",""Visitor"",""year"",""HomeGameNum"",""VisitorGameNum""),

  by.y=c(""Home"",""Visitor"",""year"",""PrevHomeGameNum"",""PrevVisitorGameNum"")

  )
relmerge=relmerge[,c(

	""Home"", ""Visitor"", ""Date.x"", ""HomeScore.x"", ""VisitorScore.x"",

	""Date.y"", ""HomeScore.y"", ""VisitorScore.y""

	)]
relmerge$dx=relmerge$HomeScore.x-relmerge$VisitorScore.x

relmerge$dy=relmerge$HomeScore.y-relmerge$VisitorScore.y
#Eliminate ties

relmerge=with(relmerge,relmerge[(dx!=0) & (dy!=0),])
relmerge$reversal=-.5*(sign(relmerge$dx)*sign(relmerge$dy))+.5

relmerge$revsize=relmerge$reversal*pmin(abs(relmerge$dx),abs(relmerge$dy))

relmerge$winnerMarginVicG1=with(relmerge,sign(dx)*dx)

relmerge$winnerMarginVicG2=with(relmerge,sign(dx)*dy)
write.csv(relmerge,""reversals.csv"")
mat=NULL

mat= data.frame(cbind(

	ReversalSize=0:12,

	Count=table(relmerge$revsize),

	Prob=table(relmerge$revsize)/length(relmerge$revsize),

	Per100k=table(relmerge$revsize)/length(relmerge$revsize)*100000

	))

mat

cat(""Probability previous winner wins again: "", mat[1,3],""n"")
##Graph Size of Reversal Frequency

png(""SizeOfReversal.png"",width=450)

plot(mat$ReversalSize,mat$Per100k,xlab=""Size of Reversal"",ylab=""Frequency in 100,000 games"",type=""lines"")

dev.off()
##Graph Chance of Winning Given Previous Win of Various Margins

png(""WinGivenMargin.png"",width=450)

brks=cut(relmerge$winnerMarginVicG1,breaks=c(0,1,2,3,4,5,6,7,8,9,10,11,27))

winsVsMargin=tapply(relmerge$winnerMarginVicG2>0,brks,mean)

names(winsVsMargin)=1:12

plot(winsVsMargin,ylim=c(0,1),axes=FALSE,xlab=""Margin of Victory in First Game"",ylab=""Chance of Winning Second Game"")

axis(1,1:12,labels=c(""1"",""2"",""3"",""4"",""5"",""6"",""7"",""8"",""9"",""10"",""11"",""12+""))

axis(2,seq(0,1,.1))

winModel=rlm(winsVsMargin~ as.numeric(names(winsVsMargin)))

abline(winModel)

dev.off()
##Graph Expected Margin of Victory Given Past Margin of Victory

png(""MarVic.png"",width=450)

mm2=rlm(relmerge$winnerMarginVicG2 ~ relmerge$winnerMarginVicG1)

plot(jitter(relmerge$winnerMarginVicG1),

   jitter(relmerge$winnerMarginVicG2),xlab=""Margin of Victory in Game 1"",

   ylab=""Margin of Victory of Game 1 Winner in Game 2"")

abline(mm2)

dev.off()
#Probability of team winning game two if they won game 1 by n points

winModel$coefficients[1]+winModel$coefficients[2]*20
#Expected margin of victory in game two given win in game 1

mm2$coefficients[1]+mm2$coefficients[2]*33
#Home Team Advantage: First game, second game

with(relmerge,{cat(mean(dx > 0), mean(dy > 0))})
#Home team advantage second game given home won first game

# Equals 1- Visitor p win second game given visitor lost the first game

with(relmerge[relmerge$dx > 0,],mean(dy > 0))
#Home team advantage second game given home lost first game

#Equals 1 - Visitor p win second game given visitor won first game

with(relmerge[relmerge$dx < 0,],mean(dy > 0))


 "	 0 Comments
Modifying basic plots in R	https://www.r-bloggers.com/2010/05/modifying-basic-plots-in-r/	May 4, 2010	Luke Miller		 0 Comments
Difficulty with mcsm?	https://www.r-bloggers.com/2010/05/difficulty-with%c2%a0mcsm/	May 4, 2010	xi'an	"An email from Keith I got this morning: Professor Robert,
I have loaded the mcsm package to windows.
The following messages appear in the R console: But when I use the demo command as on page 37 from Introducing Monte Carlo Methods with R, I get: How do I fix this? I think the fix is in loading the package by library(mcsm) [each time one needs it] after installing it [only once]… "	 0 Comments
Compcache on Ubuntu on Amazon EC2	https://www.r-bloggers.com/2010/05/compcache-on-ubuntu-on-amazon-ec2/	May 4, 2010	heuristicandrew	The following fully-automatic Bash script downloads, compiles, and initializes compcache version 0.6.2 on Ubuntu Karmic Koala (9.10).  This script creates two swaps with a maximum of 4GB uncompressed size each. Two swaps are used to take advantage of 2 CPUs (or CPU cores in a multicore CPU). Compcache is a fascinating memory compression system.   The old alternative to compcache is to use a swap file backed by a hard disk (without using compression), but this kind of swapping is extremely slowed to physical memory or memory compressed by compcache. Compcache creates a swap device backed by compressed memory or by another swap file.  Basically, you get more RAM—almost for free—though compcache adds a small overhead for compression.  Compression should actually increase performance when backed by a hard drive because it reduces expensive disk I/O.  When memory pages are empty (which happens more often than you’d think), compcache stores nothing.  I run data analysis using party in R, and it needs huge amounts of memory (well over 10GB).  Amazon EC2 gives me a large starting memory capacity, and compcache extends the physical RAM.   The script doesn’t install or make permanent changes, so if you don’t like it, reboot to start over.  It was tested on Amazon EC2 with Canonical 64-bit server (ami-55739e3c), and it should work fine with other Ubuntu versions such as the new Lucid Lynx LTS (10.04). For other ways to use Compcache, see compcache: CompilingAndUsingNew    . 	 0 Comments
R: directing output to file on the fly, output flushing	https://www.r-bloggers.com/2010/05/r-directing-output-to-file-on-the-fly-output-flushing/	May 4, 2010	Matt Asher	To start sending all output to a file, do this:  Related to this I recently had to use:  This forces your console to print out any buffered content. Doing this will cost time, but if you are running a very long script and wonder if it is still alive, you might do something like this:  	 0 Comments
Where do you want an R User Group?	https://www.r-bloggers.com/2010/05/where-do-you-want-an-r-user-group/	May 4, 2010	David Smith	Wishing there was a local R User’s group in your area, but can’t find one? New user groups are springing up all the time, but you can kickstart the process by registering interest for a group in your area at meetup.com. As you can see from the map below, there’s interest in many places — your vote could prompt an R user in your locality to set up a new group (or you could always start one yourself!). meetup.com: R Users Group Meetup Groups    	 0 Comments
Developing a user-friendly regular expression function (easyGregexpr)	https://www.r-bloggers.com/2010/05/developing-a-user-friendly-regular-expression-function-easygregexpr/	May 4, 2010	Michael		 0 Comments
Virtual Conference: R the Language	https://www.r-bloggers.com/2010/05/virtual-conference-r-the-language/	May 3, 2010	JD Long	On Tuesday May 4th at 9:30 PM central, 10:30 eastern, I’ll be giving a live online presentation as part of the Vconf.org open conference series. I’ll be speaking about R and why I started using R a couple years ago. This is NOT going to be a technical presentation but rather an illustration of how an R convert was created and why R became part of my daily tool set. If your not familiar with the vconf.org project, you should read a little about it. It’s just getting started but I love the idea that it’s not for profit and all presentations are Creative Commons license. You know that cool new technology you’ve been playing with? Yeah that one. You really should give a vconf about it. I know I’d like to hear about it! 	 1 Comment
Building Scoring and Ranking Systems in R	https://www.r-bloggers.com/2010/05/building-scoring-and-ranking-systems-in-r/	May 3, 2010	bryan	"This guest article was written by author and consultant Tristan Yates (see his bio below). It emphasizes R’s data object manipulation and scoring capabilities via a detailed financial analysis example.

Scoring and ranking systems are extremely valuable management tools.  They can be used to predict the future, make decisions, and improve behavior – sometimes all of the above.  Think about how the simple grade point average is used to motivate students and make admissions decisions.

R is a great tool for building scoring and ranking systems.  It’s a programming language designed for analytical applications with statistical capabilities.  The capability to store and manipulate data in list and table form is built right into the core language.
  read more "	 0 Comments
Introduction to R, live virtual talk from JD Long tomorrow	https://www.r-bloggers.com/2010/05/introduction-to-r-live-virtual-talk-from-jd-long-tomorrow/	May 3, 2010	David Smith	JD Long (of Cerebral Mastication fame) will be giving a live “virtual talk” introducing R: “What it’s best at and how you can use it to help you massage data”. The talk will be at 7:30PM Pacific time tomorrow, Tuesday May 4. You can register at the link below. vconf.org: Presentation: R The Language 	 0 Comments
First annual R plot replication prize	https://www.r-bloggers.com/2010/05/first-annual-r-plot-replication-prize/	May 3, 2010	Matt Asher	 $100 to the first person who can figure out how I created this plot and replicate it. Some hints: This is based on a random sampling of unstated size, so I don’t expect that your graph will be an absolute, exact match. I’ll add $1 to the prize for every day that goes by without a winner until the end of the year. After that I’ll consider it an unsolved mystery and reveal the code I used. Post your guesses for the code as a comment to this post. First correct answer wins. Good luck to all! 	 0 Comments
Example 7.35: Propensity score matching	https://www.r-bloggers.com/2010/05/example-7-35-propensity-score-matching/	May 3, 2010	Ken Kleinman		 0 Comments
Displaying data using level plots	https://www.r-bloggers.com/2010/05/displaying-data-using-level-plots/	May 3, 2010	Ralph	A level plot is a type of graph that is used to display a surface in two rather than three dimensions – the surface is viewed from above as if we were looking straight down and is an alternative to a contour plot – geographic data is an example of where this type of graph would be used. A contour plot uses lines to identify regions of different heights and the level plot uses coloured regions to produce a similar effect. To illustrate this type of graph we will consider some surface elevation data that is available in the geoR package. The data set in this package is called elevation and stores the elevation height in feet (as multiples of ten feet) for a grid region of x and y coordinates (recorded as multiples of 50 feet). To access this data we load the geoR pacakage and then use the data function: For some packages we need the call to the data function to make a set of data available for our use. The elevation object is not a data frame so our first step is to create our own data frame to be used to create the level plots using the different graphics packages. We extract the x and y grid coordinates and the height values, multiplying them by 50 and 10 respectively to convert to feet for the graphs. Rather than trying to plot the individual values we need to create a surface to cover the whole grid region as the points themselves are too sparse. We make use of the loess function to fit a local polynomial trend surface (using weighted least squares) to approximate the elevation across the whole region. The function call for a local quadratic surface is shown below: The next stage is to extract heights from this fitted surface at regular intervals across the whole grid region of interest – which runs from 10 to 300 feet in both the x and y directions. The expand.grid function creates an array of all combinations of the x and y values that we specify in a list. We choose a range every foot from 10 to 300 feet to create a fine grid: The predict function is then used to estimate the surface height at all of these combinations of x and y coordinates covering our grid region. This is saved as an object z which will be used by the base graphics function: The lattice and ggplot2 expect the data in a different format so we make use of the as.numeric function to convert from a table of heights to a single column and append to the object we create based on all combinations of x and y coordinates: The data is now in a format that can be used to create the level plots in the various packages. Base Graphics The function image in the base graphics package is the function we use to create a level plot. This function requires a list of x and y values that cover the grid of vertical values that will be used to create the surface. These heights are specified as a table of values, which in our case was saved as the object z during the calculations on the local trend surface. The text on the axis labels are specified by the xlab and ylab function arguments and the main argument determines the overall title for the graph. The function call below creates the level plot: After the image function is used we call the box function mainly for aesthetic purposes to ensure there is a line surrounding the level plot. The graph that is created is shown below: Base Graphics Level Plot The default colour scheme used by the base graphics produces an attractive level plot graph where we can easily see the variation in height across the grid region. It is basically a fancy version of a contour plot where the regions between the contour lines are coloured with different shades indicating the height in those regions. Lattice Graphics The lattice graphics package provides a function levelplot for this type of graphical dispaly. We use the data stored in the object elevation.fit to create the graph with lattice graphics. The formula is used to specify which variable to use for the three axes and a data frame where the values are stored – as there are three dimensions it is the z axis that is specified on the left hand side of the formula. The axes labels and title are specified in the same way as the base graphics. The range of colours used in the lattice level plot can be specified as a vector of colours to the col.regions argument of the function. We make use of the terrian.colors function to create this vector which a range of 100 colours which are less striking than those used above with the base graphics. The level plot that we can is shown here: Lattice Graphics Level Plot This is in general similar to the base graphics display but the actual plot region is a different shape that makes things look slightly different. ggplot2 The ggplot2 package also provides facilities for creating a level plot making use of the tile geom to create the desired graph. The function ggplot forms the basis of the graph and various other options are used to customise the graph: This large number of options that are added to the graph change various settings. The choice of colours for the heights used on graph is selected by the scale_fill_gradient function with colours ranging from black to white. The scale_x_continuous and scale_y_continuous options are used to stretch the tiles to cover the whole grid region covering up the default gray background – this makes the graph more visually appealing. The graph that is produced is shown here: ggplot2 Level Plot The graph from ggplot2 is visually as impressive as the other graphs – there is more smoothing between the colours which blurs some of the lines on the other graphs because of the type of colour gradient that was selected. This blog post is summarised in a pdf leaflet on the Supplementary Material page. 	 0 Comments
The new GUI for ggplot2 (using Deducer) – the designer wants your opinion	https://www.r-bloggers.com/2010/05/the-new-gui-for-ggplot2-using-deducer-%e2%80%93-the-designer-wants-your-opinion/	May 1, 2010	Tal Galili	After discovering that R is expected (this summer) to have a GUI for ggplot2 (through deducer), I later found Ian’s gsoc proposal for this GUI.  Since the system is in it’s early stages of development, Ian has invited people to give comments, input and critique on his plans for the project. For your convenience (and with Ian’s permission), I am reposting his proposal here.  You are welcome to send him feedback by e-mailing him (at: [email protected]), or by leaving a comment here (and I will direct him to your comment).  Download (PDF, 2.9MB) 	 0 Comments
CRAN Search	https://www.r-bloggers.com/2010/06/cran-search/	June 30, 2010	C		 0 Comments
My Experience at Hadoop Summit 2010 #hadoopsummit	https://www.r-bloggers.com/2010/06/my-experience-at-hadoop-summit-2010-hadoopsummit/	June 30, 2010	Ryan	This week I had the opportunity the trek up north to Silicon Valley to attend Yahoo’s Hadoop Summit 2010. I love Silicon Valley. The few times I’ve been there the weather was perfect (often warmer than LA), little to no traffic, no road rage and people overall seem friendly and happy. Not to mention there are so many trees it looks like a forest! The venue was the Hyatt Regency Great America which seemed like a very posh business hotel. Walking into the lobby and seeing a huge crowd of enthusiasts and an usher every two steps was overwhelming!  After being welcomed by the sound of the vuvuzela, we heard about some statistics about the growth of Hadoop over the years. Apparently in 2008 there were 600 attendees to Hadoop Summit and in 2010 attendance grew to 1000. The conference started with three hours of keynotes and sessions that everyone attended in one room. The tone was somewhat corporate in nature, but there were also several gems in there about how Yahoo uses Hadoop: Yahoo also discussed Hadoop Security, feature to integrate Hadoop with Kerberos to provide secure access and processing of data that is sensitive to business. Hadoop Security is currently in beta in the Yahoo distribution of Hadoop. Perhaps the biggest impact of Hadoop Security is that businesses can now colocate business sensitive data without worrying about unauthorized prying eyes. Yahoo’s distribution of Hadoop with Security can be downloaded from Github. Yahoo also introduced Oozie, an open-source workflow system for managing Hadoop jobs including HDFS, Pig and MapReduce. Oozie seems similar to Chris Wensel’s (@cwensel) Cascading system. Oozie is open-source and can be downloaded from Github. Yahoo’s transparency and contributions to the Hadoop and open-source community are very empowering. This quote sums it up well: “Hadoop is the technology behind every click on Yahoo!” In addition to Hadoop, Yahoo has open-sourced a project from the Inktomi days called Traffic Server and the Subversion repository is here. Yahoo also suggested that they will be moving on to a new venture: public cloud computing and storage, which would be a competitor to Amazon Web Services as well as Google Storage and a Google cloud computing solution that may or may not have already been announced (there were plans or rumors of a Google cloud computing service, but I cannot find any references to it). Surprisingly, most of the focus on cloud computing, particularly Amazon Web Services, was on Elastic MapReduce rather than EC2. Amazon pre-announced several new features. Cloudera had announcements of its own. Cloudera Desktop, a unified user interface for users and operators of Hadoop clusters as described by the developers, has been open-sourced and renamed HUE. You can download HUE from Cloudera’s GitHub repository. Cloudera will also offer monitoring, configuration support for its enterprise users via Cloudera Enterprise.  Over lunch I met up with Jakob Homan (@blueboxtraveler) from Yahoo and then with Charlie Glommen (@radarcg) from TouchCommerce. It is always great to meet fellow tweeters in person. I must say the lunch was awesome – the best risotto I have ever tasted! Anyway, we then broke off into three tracks: developer, applications, and research. I spent most of my time in the research track. The first three sessions were about working with large graphs, and I wished I had seen them BEFORE struggling so much with a large graph in my Master’s thesis.  First up was Jimmy Lin (@lintool) from University of Maryland. He introduced some large graph design patterns (slides) that can decrease running time by 70%. His method involves using message passing where computations are performed at each vertex and partial results are passed along the edges as messages. The first design patter was im-mapper combining to make local aggregation more efficient. The second design pattern, smarter partitioning creates more opportunities to create local aggregation by using range partitioning instead of hash partitioning. The final design pattern was called “Schimmy” which was named after the developers and is essentially a merge join between messages and graph structure. More information can be found in Schatz and Lin’s paper. Lin also commented on that fact that current Algorithms courses focus on serial algorithms and that there are many things that one must take into account when developing parallel algorithms. Lin has written a book to introduce students and other users to the MapReduce way of thinking. It is available online for free. Next up was Christos from the famous Faloutsos family of computer science. His talk was more theoretical (slides) and had less emphasis on Hadoop, but was very interesting. He introduced a design pattern method called EigenSpokes for detecting triangles and connected structure of large graphs. He also discussed some findings from his PEGASUS system. Finally, Sergei Vassilvitskii from Yahoo presented the appropriately titled “XXL Graph Algorithms” talk (slides). He suggested the following approach for working with large graphs: I also attended a talk on set similarity joins by Chen Li (UC Irvine) where he suggested that minhash does not find all similar items because it is probabilistic (I somewhat disagree, but that’s just the facts). The materials for his research, including source code, is on here. I also attended “Exact Inference in Bayesian Networks using MapReduce” (slides) by Alex Kozlov of Cloudera. At this point, my brain was becoming more inefficient at absorbing knowledge! During this session I finally got to meet Flip Kromer (@mrflip) from Infochimps, Anand Kishore from Yahoo (@semanticvoid) and Pete Skomoroch (@peteskomoroch) from LinkedIn, which lead to me also meeting Florian Leibert (@floleibert) from Twitter. I then decided to switch tracks and I attended a presentation (slides) of Cascalog by developer Nathan Marz (@nathanmarz). Cascalog is a very expressive querying language for Hadoop. It is based on Clojure, a Lisp dialect written on top of the Java Virtual Machine, and its syntax is very similar to Datalog (or Prolog). In some ways, its simplicity to import a file and run a job on it seems to parallel R. I can’t wait to dive into this some more! I also attended Jerome Boulon’s (Netflix) talk on Honu, which is a large scale streaming data collection and processing pipeline (slides). After a long day, I really enjoyed the free reception: pizza, cornbread, and really good quesadilla slices. I finished the day off eating dinner at Redbrick Pizza while watching the pathetic UCLA vs. U. South Carolina game and wishing I could stay in Silicon Valley longer  . Alas, here I am on my way back to Southern California. The slides for Hadoop Summit 2010 tracks are now available. Videos should be available in a week. 	 0 Comments
Drawing pedigree examples using the kinship R package	https://www.r-bloggers.com/2010/06/drawing-pedigree-examples-using-the-kinship-r-package/	June 30, 2010	Gregor Gorjanc		 0 Comments
Contest: Road Traffic Prediction for Intelligent GPS Navigation	https://www.r-bloggers.com/2010/06/contest-road-traffic-prediction-for-intelligent-gps-navigation/	June 30, 2010	Tal Galili	 Competition with prizes are an amazing thing.  If you are not sure of that, I urge you to listened to Peter Diamandis talk about his experience with the X prize (start listening at minute 11:40):  At short – prizes can give up to 1 to 50 ratio of return on investment of the people giving funding to the prize.  The money is spent only when results are achieved.  And there is a lot of value in terms of public opinion and publicity.  And the best of all (for the promoter of the competition) – prizes encourage people to take risks (at their own expense) in order to get results done. All of that said, I look at prize baring competition as something worth spreading, especially in cases where the results of the winning team will be shared with the public. The IEEE ICDM Contest (“Road Traffic Prediction for Intelligent GPS Navigation”), seems to be one of those cases.  Due to a polite request, I am republishing here the details of this new competition, in the hope that some of my R colleagues will bring the community some pride   Data mining competition affiliated with IEEE International Conference on Data Mining 2010 (ICDM), Sydney, Australia, Dec 14-17. The task is to predict city traffic based on simulated historical measurements or real-time stream of notifications sent by individual drivers from their GPS navigators. Prizes worth $5,000 will be awarded to the winners.  Over the last century, number of cars engaged in vehicular traffic in cities has increased rapidly, causing many difficulties for all citizens: traffic jams, large and unpredictable communication delays, pollution etc. Excessive traffic became a civilization problem that affects everyone who lives in a city of 50,000 or larger, anywhere in the world. Complexity of processes that stand behind traffic flow is so large, that only data mining algorithms – from the domains of structure mining, graph mining, data streams, large-scale and temporal data mining – may bring efficient solutions for these problems. With the proposed competition, we want to ask researchers to devise the best possible algorithms that tackle problems of traffic flow prediction, for the purpose of intelligent driver navigation and improved city planning. There are Tyree independent tasks: Everyone is welcome to participate. Competition starts now and will last till September 6th, 2010. More details on: http://tunedit.org/challenge/IEEE-ICDM-2010 	 0 Comments
June 2010 edition of R Journal available	https://www.r-bloggers.com/2010/06/june-2010-edition-of-r-journal-available/	June 30, 2010	David Smith	The latest edition of the R Journal is now available. This issue includes in-depth articles on the packages IsoGene, glmmBUGS, cshapes, tmvtnorm, neuralnet, glmperm and exactci/exact2x2, plus an example of reproducible research in R. There’s also a review of the book A Beginner’s Guide to R. The complete edition is available for download in PDF format at the link below. The R Journal: Volume 2/1, June 2010 	 0 Comments
Rmetrics slides	https://www.r-bloggers.com/2010/06/rmetrics-slides/	June 30, 2010	romain francois	I presented Rcpp at the Rmetrics conference earlier today, this was a really good opportunity to look back at all the work Dirk and I have been commiting into Rcpp.  I’ve uploaded my slides here (pdf) and on slideshare : and some pictures on flickr:  	 0 Comments
R Journal 2/1	https://www.r-bloggers.com/2010/06/r-journal-21/	June 30, 2010	Paolo Sonego		 0 Comments
Setting graph margins in R using the par() function and lots of cow milk	https://www.r-bloggers.com/2010/06/setting-graph-margins-in-r-using-the-par-function-and-lots-of-cow-milk/	June 30, 2010	Hrishi Mittal	"It is fairly straightforward to set the margins of a graph in R by calling the par() function with the mar (for margin!) argument. For example, par(mar=c(5.1,4.1,4.1,2.1)
 sets the bottom, left, top and right margins respectively of the plot region in number of lines of text.  Another way is by specifying the margins in inches using the mai argument: par(mai=c(1.02,0.82,0.82,0.42))
 The numbers used above are the default margin settings in R. You can verify this by firing up the R prompt and typing par(“mar”) or par(“mai”). You should get back a vector with the above values. The bottom, left and top margins are the largest because that’s where annotations and titles are most likely to be placed. Since we can specify margins both in terms of lines of text and inches, let’s find out how high one line of text is by default: par(“mai”)/par(“mar”)
[1] 0.2 0.2 0.2 0.2
 0.2 inches! There are ways to change this line height but that’s a useful number to keep in mind. The default size of the figure region is approximately 7 inches wide by 7 inches high. You can verify this by typing par(“fin”) at the R prompt. So, by default the figure is 35 lines high and wide. One way to verify this is by trying to run the following code: par(mar=c(35,35,0,0))
plot(1:10) What happens? We get an error saying “figure margins too large”. That was bound to happen because we used up all of the figure region in margins and left no space for the plot to be drawn! You are probably never going to set such large margins, but in my experience errors like that occur when I’m working with multiple plot layouts (using the mfrow argument – I might write a post about that some time). Margin lines are numbered starting from 0. We already know the number of margin lines from par(“mar”) but let’s make a graph to illustrate this point and see how the margin lines are numbered: plot(1:10,ann=FALSE,type=”n”,xaxt=”n”,yaxt=”n”)
for(j in 1:4) for(i in 0:10) mtext(as.character(i),side=j,line=i)
  In the above example, we used the mtext() function (which as the name suggests places text in the margins) to label the margin lines. The mgp argument of the par() function is a vector of 3 values which specify the margin line for the axis title, axis labels and axis line. The default value of mgp is c(3,1,0), which means that the axis title is drawn in the fourth line of the margin starting from the plot region, the axis labels are drawn in the second line and the axis line itself is the first line. Sometimes the axis labels may be very long and overlap with the axis title (for example, large numbers in scientific notation on the y axis). To overcome this we can use par() to first increase the left margin and then use mgp to set the axis title line. Note that using mgp applies the same set of margin values to axes on all four sides. Alternatively, we can suppress the drawing of the default axis label and use mtext() function specifying the line argument to a value higher than default. Let’s look at an interesting example to try this out.  Recently, there was a blog post showing some interesting data about the milk production powers of Wisconsin’s super-efficient cows. So, let’s pick up that data and plot the total milk production for the last few decades. 
cows
#This bit of code is to remove the commas in numerical fields in the original dataset. I don’t know of any automatic ways to do this in R.
for (i in 1:ncol(cows)){
if(length(grep(“,”,cows[[i]]))>0)
  cows[[i]] 
}
plot(cows$Total.milk.production~cows$Year,las=1,xlab=”Year”, ylab=”Total Milk Production (in pounds?)”)
 #This bit of code is to remove the commas in numerical fields in the original dataset. I don’t know of any automatic ways to do this in R.
for (i in 1:ncol(cows)){
if(length(grep(“,”,cows[[i]]))>0)
  cows[[i]] 
} plot(cows$Total.milk.production~cows$Year,las=1,xlab=”Year”, ylab=”Total Milk Production (in pounds?)”)
  Now, you may argue that showing cow milk production in scientific (E) notation is a bit too nerdy, but I think it’s a good enough example here. The Y axis title overlaps the axis labels, making the graph hard to read and a bit ugly. So, let’s fix it by increasing the left margin using mar and placing the axis title in a higher margin line using mgp: 
par(mar=c(5,6,4,2)+0.1,mgp=c(5,1,0))
plot(cows$Total.milk.production~cows$Year,las=1,xlab=”Year”, ylab=”Total Milk Production (in pounds?)”)
  That looks better, but did you notice we knocked out the X axis title? That happened because as I wrote earlier, mgp applies to both the axes. So we asked par to place the axes titles in line number 5. Since line numbering starts at 0, that’s the sixth line in the margin. But we only left 5 lines worth of margin space on the bottom X axis. So the X axis title did not fit within the figure. To get around this problem, there are at least three solutions. Let’s first look at the hardest one.  
par(mar=c(5,6,4,2)+0.1)
plot(cows$Total.milk.production~cows$Year,las=1,xlab=”Year”, ylab=””)
mtext(“Total Milk Production (in pounds?)”,side=2,line=5)
  Voila! So we dropped the mgp argument, set the left margin wide, suppressed the default Y axis label and then used mtext to place the title in line 5. Thus, we used the defaults for the X axis title and used a custom function call for the Y axis title. What are the easy solutions then? Just use lattice or ggplot2 – they will take care of the margins automatically almost in all cases without you having to worry about it. If you are wondering why I am wrestling with these base graphics settings, there is a good reason. I’m building a web-based graphing application using R, so I need to automatically and quickly create good looking graphs for a variety of use cases. In my experience, the base graphics functions are faster than using lattice and ggplot2 simply because loading the packages takes a few too many seconds. In building the code for Pretty Graph to handle all sorts of user input data and help people visualise them as different types of graphs, I am having to hack around the base R code to make it produce good graphs consistently. I think that one can make very good graphs using the basic functions if one spends some time learning the different parameters. This is the first in a series of blog posts where I talk about my experience in building R graphs and some interesting quirks of R graphics functions. I hope it will be a good learning experience for me. "	 0 Comments
Rcpp 0.8.3	https://www.r-bloggers.com/2010/06/rcpp-0-8-3/	June 29, 2010	Thinking inside the box	"
It comes about three weeks after the
0.8.2
release. And even though we 
promised to
concentrate on documentation, it contains a raft of new features:

 
The main thing here is Rcpp sugar for which we also have a new
(seventh !!) vignette
Rcpp-sugar.
As a quick example, consider this simple C++ function that takes two vectors
from R and creates a new one conditional on the relative values:

 
The full NEWS entry for this release follows below:
 
As always, even fuller details are in 
Rcpp Changelog page and the 
Rcpp page which also
leads to the downloads, the 
browseable
doxygen docs and zip files of doxygen output for the standard formats.
A local directory  has
source and documentation too. 
Questions, comments etc should go to the
rcpp-devel mailing list
off the R-Forge page

 "	 0 Comments
Analyze Gold Demand and Investments using R	https://www.r-bloggers.com/2010/06/analyze-gold-demand-and-investments-using-r/	June 29, 2010	C		 0 Comments
Entropy augmentation the modulo way	https://www.r-bloggers.com/2010/06/entropy-augmentation-the-modulo-way/	June 29, 2010	Matt Asher	 Long before I had heard about the connection between entropy and probability theory, I knew about it from the physical sciences. This is most likely how you met it, too. You heard that entropy in the universe is always increasing, and, if you’re like me, that made very little sense. Then you may have heard that entropy is a measure of disorder: over times things fell apart. This makes a little more sense, especially to those teenagers tasked with cleaning their own rooms. Later on, perhaps you got a more precise, mathematical definition of entropy that still didn’t fully mesh with the world as we observe it. Here on earth, we see structures getting built up over time: plants convert raw energy to sunflowers, bees build honeycombs, humans build roads. Things do sometimes fall apart. More precisely, levels of complexity tend to grow incrementally over long periods of time, then collapse very quickly. This particular asymmetry seems to be an ironclad rule for our word, which I assume everyone understands, at least implicitly, though I can’t remember anywhere this rule is written down as such. In the world of probability, entropy is a measure of unpredictability. Claude Shannon, who created the field of Information Theory, gave us an equation to measure how much, or little, is known about an incoming message a prori. If we know for sure exactly what the message will be, our entropy is 0. There is no uncertainty. If we don’t know anything about the outcome except that it will be one of a finite number of possibilities, we should assume uniform probability for any one of the outcomes. Uncertainty, and entropy, is maximized. The more you look into the intersection of entropy and statistics, the more you find surprising, yet somehow obvious in retrospect, connections. For example, among continuous distributions with fixed mean and standard deviation, the Normal distribution has maximal entropy. Surprised? Think about how quickly a sum of uniformly distributed random variables converges to the Normal distribution. Better yet, check it out for yourself: Try increasing and decreasing “n” and see how quickly the bell curve begins to appear. Lately I’ve been thinking about how to take any general distribution and increase the entropy. The method I like best involves chopping off the tails and “wrapping” these extreme values back around to the middle. Here’s the function I created: Now is a perfect time to use a version of our “perfect sample” function: The image at the top of this post shows the Chi Square distribution on 2 degrees of freedom, with Modulo 3 Entropy Enhancement (see how nice that sounds?). Here’s the code to replicate the image: Here’s another plot, using the Normal distribution and Modulo 1.5:  One nice property of this method of increasing entropy is that you get a smooth transition with logical extremes: As your choice of Mod goes to infinity, the distribution remains unchanged. As your Mod number converges to 0, entropy (for that given width) is maximized. Here are three views of the Laplace, with Mods 5, 1.5, and 0.25, respectively. See how nicely it flattens out? (Note you will need the library “VGAM” to sample from the Laplace).  It’s not clear to me yet how entropy enhancement could be of practical use. But everyone loves enhancements, right? And who among us doesn’t long for a  little extra entropy for time to time, no? 	 0 Comments
Tips for managing memory in R	https://www.r-bloggers.com/2010/06/tips-for-managing-memory-in-r/	June 29, 2010	David Smith	R is an in-memory application, so every new object you create takes up RAM. (Yes, there are ways around that, but that’s a topic for another article.) If you’re working on a small machine (say, a 32-bit Windows system with 1Gb of RAM or less) you might need to be careful with the object you create. This StackOverflow question offers some useful tips for managing objects in RAM, including code for the function lsos to list objects sorted by size: StackOverflow: Tricks to manage the available memory in an R session? 	 0 Comments
Sweave.sh in Eclipse-StatET	https://www.r-bloggers.com/2010/06/sweave-sh-in-eclipse-statet/	June 29, 2010	Gregor Gorjanc		 0 Comments
Analyze Twitter Data Using R	https://www.r-bloggers.com/2010/06/analyze-twitter-data-using-r/	June 28, 2010	C		 0 Comments
Second year of entries!	https://www.r-bloggers.com/2010/06/second-year-of-entries/	June 28, 2010	Nick Horton		 1 Comment
Bootstrapping the latest R into Amazon Elastic Map Reduce	https://www.r-bloggers.com/2010/06/bootstrapping-the-latest-r-into-amazon-elastic-map-reduce/	June 28, 2010	JD Long	I’ve been continuing to muck around with using R inside of Amazon Elastic Map reduce jobs. I’ve been working on abstracting the lapply() logic so that R will farm the pieces out to Amazon EMR. This is coming along really well, thanks in no small part to the Stack Overflow [r] community. I have no idea how crappy coders like me got anything at all done before the Interwebs. One of the immediate hurdles faced when trying to use AMZN EMR in anger is that the default version of R on EMR is 2.7.1. Yes, that is indeed the version that Moses taught the Israelites to use while they wandered in the desert. I’m impressed by your religious knowledge. At any rate, all kinds of things go to hell when you try to run code and load packages in 2.7.1. When I first started fighting with EMR the only solution was to backport my code and alter any packages so they would run in 2.7.1. Yes, that is, as Moses would say, a Nudnik. Nudnik also happens to be the pet name my neighbors have given me. They love me. Where was I? Oh yeah, Methusla’s R version. Recently Amazon released a neat feature called “Bootstrapping” for EMR. Before you start thinking about sampling and resampling and all that  crap, let me clarify. This is NOT statistical bootstrapping. It’s called bootstrapping because it’s code that runs after each node boots up, but before the mapper procedure runs. So to get a more modern version of R loaded on to each node I set up a little script that updates the sources.list file and then installs the latest version of R. And since I’m a caring, sharing guy, here’s my script: And if that doesn’t show up for some reason, you can find all 5 lines of its bash glory here over at github. If you’re not conveniently located in Chicago, IL you may want to change your R mirror location. The bootstrap action can be set up from the EMR web GUI or if you’re firing the jobs off using the elastic-mapreduce command line tools you just add the following option: “–bootstrap-action s3://myBucket/bootstrap.sh” assuming myBucket is the bucket with your script in it and bootstrap.sh contains your bootstrap shell script. And then, as my buddies in Dublin say, “Bob’s your mother’s brother.” And before you ask, yes, this slows crap down. I’ll probably hack together a script that will take the R binaries and other needed upgrades out of Amazon S3 and load them in a bootstrap action which will greatly speed things up. The above example has one clear advantage over loading binaries from S3: It works right now. And remember folks, code that works right now kicks code that “might work someday” right in the balls. And then mocks it while it cries. 	 1 Comment
How to peg 7 cores with doSMP	https://www.r-bloggers.com/2010/06/how-to-peg-7-cores-with-dosmp/	June 28, 2010	David Smith	"Statistics PhD student Nathan VanHoudnos has an 8-core laptop, and by his own admission, takes “an almost unhealthy pleasure in pushing [his] computer to its limits”. It seems like he’s found an outlet for this passion with the new doSMP library included with Revolution R, that allows him to use all his processors for some gnarly simulations in R: 
  Actually, he’s written his code to use 7 processors instead of 8: it’s good practice to leave one CPU unpegged if you want to continue to use your computer interactively during the computations. See Nathan’s R code at the link below. Nathan VanHoudnos: Pegging your multicore CPU in Revolution R, Good and Bad "	 0 Comments
Plot Multiple Time Series using the flow / inkblot / river / ribbon / volcano / hourglass / area / whatchamacallit plots ~ blue whale catch per country w/ ggplot2	https://www.r-bloggers.com/2010/06/plot-multiple-time-series-using-the-flow-inkblot-river-ribbon-volcano-hourglass-area-whatchamacallit-plots-blue-whale-catch-per-country-w-ggplot2/	June 27, 2010	apeescape	"Ever since I first looked at this NYT visualization by Amanda Cox, I’ve always wanted to reproduce this in R. This is a plot that stacks multiple time series onto one another, with the width of the river/ribbon/hourglass representing the strength at each time. The NYT article used box office revenue as the width of the river. It’s also an interactive web app. thanks to some help from graphic designers. AFAIK, ggplot2 can stack area plots using geom_area or create flow plots for one set of data using geom_ribbon, but not both. So I created a function that creates the necessary transformed data to use in geom_polygon. I used blue whale catch data from Masaaki Ishida to illustrate my function. The location of the river along the y-axis is centered around the mean at each time. The data is also smoothed over so it looks nicer.  Some links that may be helpful: (messy) R Code: 
Filed under: ggplot2, R, Whaling        

 "	 0 Comments
Another harmonic mean approximation	https://www.r-bloggers.com/2010/06/another-harmonic-mean-approximation/	June 26, 2010	xi'an	Martin Weinberg posted on arXiv a revision of his paper, Computing the Bayesian Factor from a Markov chain Monte Carlo Simulation  of the Posterior Distribution, that is submitted to Bayesian Analysis. I have already mentioned this paper in a previous post, but I remain unconvinced of the appeal of the paper method, given that it recovers the harmonic mean approximation to the marginal likelihood… The method is very close to John Skilling’s nested sampling, except that the simulation is run from the posterior rather than from the prior, hence the averaging on the inverse likelihoods and hence the harmonic mean connection. The difficulty with the original (Michael Newton and Adrian Raftery’s) harmonic mean estimator is attributed to “a few outlying terms with abnormally small values of” the likelihood, while, as clearly spelled out by Radford Neal,  the poor behaviour of the harmonic mean estimator has nothing abnormal and is on the opposite easily explainable. I must admit I found the paper difficult to read, partly because of the use of poor and ever-changing notations and partly because of the lack of mathematical rigour (see, e.g., eqn (11)). (And maybe also because of the current heat wave.) In addition to the switch from prior to posterior in the representation of the evidence, a novel perspective set in the paper seems to be an extension of the standard harmonic mean identity that relates to the general expression of Gelfand and Dey (1994, Journal of the Royal Statistical Society B) when using an indicator function as an instrumental function. There is therefore a connection with our proposal (made with Jean-Michel Marin) of considering an HPD region for excluding the tails of the likelihood, even though the set of integration is defined as “eliminating the divergent samples with “. This is essentially the numerical Lebesque algorithm advanced as one of two innovative algorithms by Martin Weinberg. I wonder how closely related the second (volume tesselation) algorithm is to Huber and Schott’s TPA algorithm, in the sense that TPA also requires a “smaller” integral…. 	 1 Comment
Weekend art in R (Part 2)	https://www.r-bloggers.com/2010/06/weekend-art-in-r-part-2/	June 26, 2010	Matt Asher	 I put together four of the best looking images generated by the code shown here: Weekend art Part 1 is here. 	 1 Comment
Stock Analysis using R	https://www.r-bloggers.com/2010/06/stock-analysis-using-r/	June 26, 2010	C		 1 Comment
Read Compressed Zip Files in R	https://www.r-bloggers.com/2010/06/read-compressed-zip-files-in-r/	June 25, 2010	--	"One of the great things that I am learning about R is that it is really powerful as a data management tool.  I just found how to unzip files.  I could use Python for this in SPSS, but it just feels like it is more natural to do in R.  Of course, you have to change the paths and the file names, but you should get the hint. Note:  Right now, this works on .zip files.  I wonder if other formats will work as well? Also:  I use Eclipse and StatET to develop my R code.  Here is some help at Jeromy’s blog to get you up and running here. Lastly:  Thanks to Hadley Wickham for the help on how to search an entire directory using regex @hadleywickham ## Look at your current directory

## and set it to what you want it to be

## need forward slash on windows!!

getwd()

setwd(""C:/Users/Brock/Documents/IPEDS/ENROLLMENT - Migration/"")

getwd() ## get help on zipfiles

?unzip
## create a vector file names we want to extract from

files.temp <- c(""EF2008C.zip"",""EF2006C.zip"",""EF2004C.zip"",""EF2002C.zip"",""EF2000C.zip"",""EF98_C.zip"",""EF1988_C.zip"")
## create a loop to extract the files to the directory set above

## will build the file name of the extracted file

unzip(""EF2008c.zip"")

for (i in files.temp)

unzip(i)
## a quicker way that doesnt require that you know which files - just does all

## \ allows you to use the . in .zip, the . is a special character

## $ is tells the pattern to search is the end?  not sure about this one

for (i in dir(pattern=""\.zip$""))

	unzip(i)
 ## clean up the memory and close

rm(list=ls())

q()

n

 "	 0 Comments
Because it’s Friday: Insect sex	https://www.r-bloggers.com/2010/06/because-its-friday-insect-sex/	June 25, 2010	David Smith	Birds do it, bees do it. But the bees and their insect brethren definitely do it in a more interesting way. Don’t believe me? Check out Isabella Rosselini’s description of bee sex and the other videos in her educational “Green Porno” series. It’s fascinating stuff. For some light summer reading, I also recommend Olivia Judson’s “Dr. Tatiana’s Sex Advice To All Creation”, a book of “Dear Abby”-style questions and answers about sex problems from the weirder corners of the animal kingdom. The folks at McSweeny’s have taken this one step further from Dear Abby to the raunchier style of Savage Love with Dan Savage’s Wild Kingdom. One question begins: “I’m a 2-hour-old asexual amoeba, and I have kind of a weird problem… like most single-cell folks, I went through a phase in my teens where I was binary-fissionizing myself so often I thought my psuedopods would fall off.” Follow the link below for the answer. McSweeny’s: Dan Savage’s Wild Kingdom 	 0 Comments
Pollution from the BP oil spill	https://www.r-bloggers.com/2010/06/pollution-from-the-bp-oil-spill/	June 25, 2010	David Smith	"There’s been a lot of talk about the slicks and plumes of oil from the Deepwater Horizon disaster, but how does the presence of that oil translate into measurable pollution in the air, water, and sediment? The EPA is now releasing pollutants and making the data available for analysis. Because the data are online, it’s a simple process to read it into R and do visualization and analysis. The blog R-Chart shows us how to map the sampling locations around the Louisiana coastline, for example: 
 This is a simple analysis, but demonstrates the principle of working with this public data in R. It would be interesting to see some more in-depth analyses of the data. R-Chart: Plotting BP Oil Spill Testing Data using R     "	 1 Comment
ASCII Scatterplots in R	https://www.r-bloggers.com/2010/06/ascii-scatterplots-in-r/	June 25, 2010	Matt Shotwell	I really like R‘s stem function, it creates a stem-and-leaf plot right in the R console, no fancy graphics devices required! In a recent R-help post, Ralf Bierig presented a very nice ASCII scatterplot representing two densities. Unfortunately, I don’t know of any R function that will generate this type of plot, but I will argue that they are very useful to quickly visualize data, and to present data in ASCII-only media, such as an R mailing list message. I wrote a little prototype R function I am calling scat ( to compliment stem, and because the output may be somewhat ‘crappy’ ) to generate a simple ASCII scatterplot. I put the code on a wiki. Unfortunately, the spambots have forced me to disallow editing of the wiki  . You can load the latest version of scat by downloading the file from the wiki, or the original from here scat.R, or within R with Here are some examples (note: this doesn’t work well unless you view the ASCII in a monospace font): 	 0 Comments
R Commander – two-way analysis of variance	https://www.r-bloggers.com/2010/06/r-commander-%e2%80%93-two-way-analysis-of-variance/	June 25, 2010	Ralph	Two way analysis of variance models can be fitted to data using the R Commander GUI. The general approach is similar to fitting the other types of model in R Commander described in previous posts. Fast Tube by Casper The “Statistics” menu provides access to some analysis of variance models via the “Means” sub-menu: The “Models” menu provides access to various diagnostics for analysis of variance models via the “Graphs” sub-menu including: Other useful resources are provided on the Supplementary Material page. 	 0 Comments
R Commander – one-way analysis of variance	https://www.r-bloggers.com/2010/06/r-commander-%e2%80%93-one-way-analysis-of-variance/	June 25, 2010	Ralph	One way analysis of variance models can be fitted to data using the R Commander GUI. The general approach is similar to fitting the other types of model in R Commander described in previous posts. Fast Tube by Casper The “Statistics” menu provides access to some analysis of variance models via the “Means” sub-menu: The “Models” menu provides access to various diagnostics for analysis of variance models via the “Graphs” sub-menu including: Other useful resources are provided on the Supplementary Material page. 	 0 Comments
Surf	https://www.r-bloggers.com/2010/06/surf/	June 25, 2010	Stubborn Mule	 I hope this will not come as too much of a disappointment to anyone, but despite the title, this post has nothing to do with the ocean. Here “Surf” refers to the newly established Sydney R user group. While the acronym may be a little forced (it actually stands for “Sydney Users of R Forum”), as a long-time user of the R programming language for statistics and a resident of Sydney, I have signed up and will be doing my best to make it to the first meeting. Any other Sydney-siders who read the post on graphing in R and would like to learn more about R may be interested in coming along too as the group is aimed as much at beginners as old-timers like the Mule. I might even see you there. If I do make it along to the meeting, I will report back here on what it was like. UPDATE: I did make it along and will in fact be presenting at the next forum meeting. 	 0 Comments
Graphing Twitter friends/followers with R (updated)	https://www.r-bloggers.com/2010/06/graphing-twitter-friendsfollowers-with-r-updated/	June 24, 2010	cornelius	"Edit: And here is an update of the update, this one contributed by Kai Heinrich. Here’s an updated version of my script from last month, something I’ve been meaning to do for a while. I thank Anatol Stefanowitsch and Gábor Csárdi for improving my quite sloppy code. 

# Load twitteR and igraph packages.

library(twitteR)

library(igraph)




# Start a Twitter session.

sess <- initSession('USERNAME', 'PASSWORD')




# Retrieve a maximum of 20 friends/followers for yourself or someone else Note that

# at the moment, the limit parameter does not [yet] seem to be working.

friends.object <- userFriends('USERNAME', n=20, sess)

followers.object <- userFollowers('USERNAME', n=20, sess)




# Retrieve the names of your friends and followers from the friend

# and follower objects.

friends <- sapply(friends.object,name)

followers <- sapply(followers.object,name)




# Create a data frame that relates friends and followers to you for expression in the graph

relations <- merge(data.frame(User='YOUR_NAME', Follower=friends), data.frame(User=followers, Follower='YOUR_NAME'), all=T)




# Create graph from relations.

g <- graph.data.frame(relations, directed = T)




# Assign labels to the graph (=people's names)

V(g)$label <- V(g)$name




# Plot the graph using plot() or tkplot().

tkplot(g)

 "	 0 Comments
Why Learn R? It’s the language of Statistics	https://www.r-bloggers.com/2010/06/why-learn-r-its-the-language-of-statistics/	June 24, 2010	Joseph Rickert	In the Introduction to his book “R for SAS and SPSS Users” (Springer 2009) Robert Muenchen offers ten reasons for learning R if you already know SAS or SPSS. All ten reasons say something important about R. However, his fourth reason: “R’s language is more powerful than SAS or SPSS. R developers write most of their analytic methods using the R language; SAS and SPSS developers do not use their own languages to develop their procedures” is fundamental. To me, this expresses something about R that should speak to anyone who does statistical modeling no matter what tools she or he may be using. What is so compelling about R’s powerful language? I think that there is a direct analogy here with natural language. Every language enables thoughts peculiar to the culture in which it developed. If you speak more than one language, how many times have you labored to say something in another language that just comes so naturally in your mother tongue? Whether by design, or historical accident, some languages are just better than others for saying certain things. I propose that in the same way that English is the language of business , and that French may still be the language of diplomacy, R is the language of Statistics. I don’t just mean that R “is spoken” by many or even most statisticians. R’s superiority for statistics is deeper than that. R is a language with syntax and structure that have been explicitly designed to formulate expressions about statistical objects. At this time, it may be le premier langue for statistical thinking that enables the formulation of ideas, and notions about statistical models and data that are difficult to express succinctly in other languages including mathematical notation. For example, suppose you want to discuss multiple regression. A mathematical exposition might begin with the equation: (1)    Y = X(beta) + epsilon    A statistician will naturally interpret this as an expression of the regression model, but (1) is primarily a statement about the relationship of random variables, abstract mathematical entities not statistical models. In contrast, the R expression (2)    model   is a statement about the linear model that relates the data structures x and y. For a person who “speaks” some R, (2) “means” the model object coefficients, residuals etc. that result from fitting a linear model to the data structures x and y. Moreover, (2) actually makes “model” an object, packed with information that describes the regression and can be thought about as a whole and  “talked about” with other R expressions. There is certainly some overlap, but expressions (1) and (2) are really about different concepts. As another example of the expressive power of the R language, consider how difficult it is to formulate multi-level hierarchical models in standard mathematical notation. With the aid of the notation “j[i]” which is used to encode group membership (j[10] = 2 means the tenth element in the data indexed by i belongs to group 2) Gelman and Hill (Cambridge 2007: a must read for anyone new at multi-level modeling) write the simple varying intercept model with one additional predictor as yi =( alpha)j[i] + (beta)xi +(epsilon)i. This nonstandard notation gets messy quickly as complexity increases, and as Gelman and Hill point out it doesn’t enable the unique specification of the model. (They discuss five ways to write the same model.) By way of contrast, their R code using the lmer function (now lme4):  model  is succinct, preserves the syntax for linear models, and generalizes reasonably well with complexity. Just like there are some things that come naturally in your mother tongue but are awkward to express in another language, R enables concise statements about statistical models that are difficult to express otherwise. R’s syntax is not the only feature that contributes to its expressive power. The interplay of R’s objects (nouns) with methods (verbs) facilitates formulating expressions that make sense under many different circumstances. For example, one can use the same R function “summary” to  form statements that talk about data: summary(x),  as well as statements that talk about models: summary(model). This ability of functions to have methods for different kinds of objects is a hallmark of R that greatly eases the process of learning the language.  It is easy to say statistical things in R, and just a little bit of language skill goes a long way in turning statistical thoughts into action packed, working statistical sentences. 	 0 Comments
World Bank API R package available!	https://www.r-bloggers.com/2010/06/world-bank-api-r-package-available/	June 23, 2010	C		 0 Comments
R Commander – logistic regression	https://www.r-bloggers.com/2010/06/r-commander-%e2%80%93-logistic-regression/	June 23, 2010	Ralph	We can use the R Commander GUI to fit logistic regression models with one or more explanatory variables. There are also facilities to plot data and consider model diagnostics. The same series of menus as for linear models are used to fit a logistic regression model. Fast Tube by Casper The “Statistics” menu provides access to various statistical models via the “Fit models” sub-menu including: The “Models” menu provides access to various diagnostics for statistical models via the “Graphs” sub-menu including: The “Hypothesis Tests” sub-menu can be used to generate comparisons between pairs of models. Other useful resources are provided on the Supplementary Material page. 	 0 Comments
How to: Debug in R	https://www.r-bloggers.com/2010/06/how-to-debug-in-r/	June 23, 2010	David Smith	Revolution Analytics is proud to sponsor the New York R User Group. The last meeting was on the theme of debugging in R, and some videos of the talks are now available at the Video Rchive. Jay Emerson have a talk on Basic debugging in R and Harlan Harris dived deeper on advanced debugging techniques. Also presenting were Peter Flom (on demystifying error messages) and Revolution’s Joseph Rickert, who demonstrated the interactive debugging features of the Revolution R Enterprise. Due to a Flip-cam battery failure, neither of those talks were recorded. But Joseph did recreate a 3-minute version of his demo of debugging in R for the Revolution website. New York R User Group: Demystifying error messages and debugging in R    	 0 Comments
Scoping Bugs	https://www.r-bloggers.com/2010/06/scoping-bugs/	June 22, 2010	Mark Fredrickson	I ran a across a strange bug in R recently. Like all the best programming languages, R treats functions as first class objects. That is to say that functions can be passed as arguments and return values from functions, named as variables, and, while not part of the strict definition of first class functions, maintain copies of the creating environment. This last point is known as lexical (or static) scope. Lexical scoping was a major innovation to making programs simpler to understand. With lexical scoping, variable names are defined “locally” — that is, if a function is working a variable foo, that variable cannot be written over by a caller having its own variable foo. Here is an example that illustrates the property, using R. What does the last line return? If you answered 42, you’d be correct. The two uses of the name my.variable would cause a dynamically scoped program to return 100. Under lexical scoping, these are distinct variables, defined by the different scopes of the f and g functions. Under dynamic scoping, applying g to 100 would lead f to look up my.variable and find the value of 100. While this example is contrived, in programs of any size, lexical scoping (at least as the default) prevents different outer scopes from changing the behavior of inner functions. (See Clojure’s binding form for an example of useful dynamic scope on demand.) So lets take this a step further, and create some functions that save their lexical environment. To show the correct behavior, here is a small Scheme program that creates 5 functions, each of which returns its index when called (lambda means “create a new function”): Now here is the same thing in R: Whoa! What is going on? Clearly, something is amiss with R’s scoping rules. To be honest, I’m not entirely sure what (though I will unveil a work around). I had originally written this code in an imperative for loop, and my immediate thought was that R was bitten by a classic JavaScript bug. JavaScript has a strange quirk where by loop indices are not considered local to the scope, and are rewritten during each iteration. A simple workaround is to nest the loop code in a function and immediately call it: R does not exactly suffer from this issue, as the JavaScript work-around does not, well, work-around the bug: After poking and prodding, I found a (bizarre) solution in the same vein: The new version is significantly more verbose. The critical aspects are defining a maker function (you can’t just in-line that code) and applying the function to some dummy argument. Apparently, these are the necessary genuflections to R to make the calling environment sticky. There are several reasons why both R and JavaScript could be getting these scoping rules wrong. First, while both allow first class functions, they are not as frequently used as in some other languages. I may very well be the first user to test R on its ability to properly scope functions created in loops. A second possibility may be more fundamental: R and JavaScript are imperative, C-style block languages. One writes a programs as a series of declarative statements: first do this, next do this, now do this. Languages that treat programs as transformations of data (and here I’m referring to Lisps specifically, as I have the most exposure to this family) do a very good job with scoping rules. In fact, writing your own Lisp is a fairly simple process, covered in Chapter 4 of SICP, and getting environments right does not seem an especially difficult task. The apparent difficulties in getting scope correct are even greater impetus for doing work in and on Incanter. Combining R’s wealth of statistical tools with Clojure’s proper scoping rules would be an ideal combination. Perhaps the best work around for R bugs is to write the program in Clojure? The R version used in this post was 2.11.0 (2010-04-22). Update: I found another, more elegant, workaround. 	 0 Comments
Linear Modeling in R and the Hubble Bubble	https://www.r-bloggers.com/2010/06/linear-modeling-in-r-and-the-hubble-bubble/	June 22, 2010	Neil Gunther		 0 Comments
Reaching escape velocity	https://www.r-bloggers.com/2010/06/reaching-escape-velocity/	June 22, 2010	Matt Asher	 Sample once from the Uniform(0,1) distribution. Call the resulting value . Multiply this result by some constant . Repeat the process, this time sampling from Uniform(0, ). What happens when the multiplier is 2? How big does the multiplier have to be to force divergence. Try it and see: 	 0 Comments
Analyzing competitive nordic skiing with R	https://www.r-bloggers.com/2010/06/analyzing-competitive-nordic-skiing-with-r/	June 22, 2010	David Smith	"Here’s another great example of R being used to analyze sports data. Statistician and skier Joran Elias has started a project to analyze and visualize international cross country ski racing results, and he publishes his analysis at the blog Statistical Skier. All of the analyses are done using R (and for data, SQLite via the RSQLite package). As much as I love to ski, I’m not so familiar with the intricacies of ski racing as a sport, but it looks like there’s a rich source of data ripe for analysis, and Joran is pulling out some interesting results. For example, here’s the recent history of one recently-retired five-time Olympian Sabina Valbusa: 
  I can only assume a low “FIS Points” is better: Joran notes “best season overall might have been 2002-2003”. A great use of ggplot2 and smoothing to visualize these data. Check out the blog for some other analyses — especially if you’re a ski junkie. Statistical Skier: Data based analysis and commentary on nordic skiing "	 1 Comment
Employee productivity as function of number of workers revisited	https://www.r-bloggers.com/2010/06/employee-productivity-as-function-of-number-of-workers-revisited/	June 22, 2010	Allan Engelhardt	"We have a mild obsession with employee productivity and how that declines as companies get bigger.  We have previously found that when you treble the number of workers, you halve their individual productivity which is mildly scary.
 Let’s try the FTSE-100 index of leading UK companies to see if they are significantly different from the S&P 500 leading American companies that we analyzed four years ago.
 We will of course use the R statistical computing and analysis platform for our analysis, and once again we are grateful to Yahoo Finance for providing the data.
 The analysis script is available as ftse100.R and is really simple: Leave it to run and this is what you get: The power law still broadly holds.  In a large company, the productivity of the individual employee is only ¼ of the productivity in a company with one-tenth of the number of workers. The analysis for the FTSE All-Share index is easy (ftse-all.R) and gives a slope of -0.7605541 for the 301 companies with the required information, which is much worse.  More convincingly, fitting the companies with more than 1,000 employees (to avoid some bias of smaller companies needing to have large profits per employee in order to be big enough to afford a stock market listing) gives a slope of -0.2838. Jump to comments. 

Area Plots with Intensity Coloring
 I am not sure apeescape’s ggplot2 area plot with intensity colouring is really the best way of presenting the information, but it had me intrigued enough to replicate it using base R graphics. The key technique is to draw a gradient line which R does not support natively so we have to roll our own code for that. Unfortunately, lines(..., type=l) does not recycle the colour col= argument, so we end up with rather more loops than I thought would be necessary. We also get a nice opportunity to use the under-appreciated read.fwf function. 

Benchmarking feature selection with Boruta and caret
 Feature selection is the data mining process of selecting the variables from our data set that may have an impact on the outcome we are considering. For commercial data mining, which is often characterised by having too many variables for model building, this is an important step in the analysis process. And since we often work on very large data sets the performance of our process is very important to us. Having looked at feature selection using the Boruta package and feature selection using the caret package separately, we now consider the performance of the two approaches. Neither approach is suitable out of the box for the sizes of data sets that we normally work with. 

Big data for R
 Revolutions Analytics recently announced their big data solution for R. This is great news and a lovely piece of work by the team at Revolutions. However, if you want to replicate their analysis in standard R , then you can absolutely do so and we show you how. 

R code for Chapter 2 of Non-Life Insurance Pricing with GLM
 We continue working our way through the examples, case studies, and exercises of what is affectionately known here as “the two bears book” (Swedish björn = bear) and more formally as Non-Life Insurance Pricing with Generalized Linear Models by Esbjörn Ohlsson and Börn Johansson (Amazon UK | US ). At this stage, our purpose is to reproduce the analysis from the book using the R statistical computing and analysis platform, and to answer the data analysis elements of the exercises and case studies. Any critique of the approach and of pricing and modeling in the Insurance industry in general will wait for a later article. 

R: Eliminating observed values with zero variance
 I needed a fast way of eliminating observed values with zero variance from large data sets using the R statistical computing and analysis platform . In other words, I want to find the columns in a data frame that has zero variance. And as fast as possible, because my data sets are large, many, and changing fast. The final result surprised me a little. "	 0 Comments
The most violent municipalities in Mexico (2008)	https://www.r-bloggers.com/2010/06/the-most-violent-municipalities-in-mexico-2008/	June 21, 2010	Diego Valle-Jones		 0 Comments
R Layout command.	https://www.r-bloggers.com/2010/06/r-layout-command/	June 21, 2010	C		 0 Comments
MMDS 2010	https://www.r-bloggers.com/2010/06/mmds-2010/	June 21, 2010	Joseph Rickert	The 2010 Workshop on Algorithms for Modern Massive Data Sets (MMDS 2010) finished up this past Friday (June 18th) at Stanford. This was an exceptionally well organized conference: four days of mind-stretching talks on algorithm development and the challenges of working with massive data sets approached from almost every conceivable angle. The approximately 100 attendees were a diverse group of computer scientists, developers and quantitative marketing people from industry and academia. The mix of academics and industry types appeared to be pretty well balanced with a number of speakers listing themselves as having both university and company affiliations. This mashup of people, backgrounds and ideas reflected what I think was the overall theme and rationale for the workshop: moving deep theory ideas to practical solutions.  The level of the talks ranged from Peter Norvig’s (head of Google research) opening tutorial on the practical problems of running Google’s data center that is so massive that shark attacks, blasphemy outbreaks and other 6+ sigma events are real worries — to Konstantin Mischaikow’s (Rutgers university) presentation on a “Combinatorial Framework for Nonlinear Dynamics” for which a good background in algebraic topology would have been helpful. Most of talks were fairly technical, but many of the speakers came across as being very skilled at communicating to non-specialists. (For a short time, I even had the illusion that I understood some algebraic topology!)  Maybe it was the perfect weather, or the beautiful Stanford campus, or the fact that the coffee was always hot, but it seemed to me that people were just in the mood for talking openly about their work.  Too much and too varied a body of material was presented for me, an outsider to this field, to summarize here, but the following is my take on some of the major themes — the recurring melodies if you will — that played as background music through a good many of the presentations: 	 0 Comments
New blog from Rmetrics Foundation	https://www.r-bloggers.com/2010/06/new-blog-from-rmetrics-foundation/	June 21, 2010	David Smith	The Rmetrics Foundation (the sharp minds behind the Rmetrics suite of packages for financial analysis in R) have just launched a new blog where you can keep up with the latest Rmetrics news. Amongst the recent news: a ne eBook about data management of Indian financial market data, and a new interface between Rmetrics and AMPL. You can also follow Rmetrics on Twitter for updates. Rmetrics Foundattion: Blog  	 0 Comments
Example 7.42: Testing the proportionality assumption	https://www.r-bloggers.com/2010/06/example-7-42-testing-the-proportionality-assumption/	June 21, 2010	Ken Kleinman		 0 Comments
A quetion	https://www.r-bloggers.com/2010/06/a-quetion/	June 21, 2010	Shige		 0 Comments
The police records for 2009 are out.	https://www.r-bloggers.com/2010/06/the-police-records-for-2009-are-out/	June 20, 2010	Diego Valle-Jones		 0 Comments
Here’s the distribution of the first million digits of the…	https://www.r-bloggers.com/2010/06/here%e2%80%99s-the-distribution-of-the-first-million-digits-of-the/	June 20, 2010	Human Mathematics	"Here’s the distribution of the first million digits of the square root of two’s decimal expansion. Number of digits | is:
   0's |  99 818
  1's |  98 926
  2's | 100 442
  3's | 100 191
  4's | 100 031
  5's | 100 059
  6's |  99 885
  7's | 100 012
  8's | 100 347
  9's | 100 126  If each digit had a Bernoulli chance of coming up (like a 10-sided  die), you’d expect to see 10 000 ± 30 times.  And going on with that  same assumption, the chance of the least-frequent digit coming up less  than 99 000 times would be something like one percent. What does it mean?  I will meditate on this and expand √2  in different bases besides 10. "	 0 Comments
QSPR modeling with signatures	https://www.r-bloggers.com/2010/06/qspr-modeling-with-signatures/	June 20, 2010	Egon Willighagen		 0 Comments
R-INLA package	https://www.r-bloggers.com/2010/06/r-inla-package/	June 19, 2010	Shige		 0 Comments
Estimating Probability of Drawdown	https://www.r-bloggers.com/2010/06/estimating-probability-of-drawdown/	June 19, 2010	Joshua Ulrich	"
 "	 1 Comment
More powerful iconv in R	https://www.r-bloggers.com/2010/06/more-powerful-iconv-in-r/	June 19, 2010	Matt Shotwell	"The R function iconv converts between character string encodings, for example, from the locale dependent encoding to UTF-8: However, R has long-running trouble with embedded null characters ('\0') in strings. Hence, if we try to convert to an encoding that permits embedded null characters, iconv will fail: The ‘embedded nul’ error is thrown by mkCharLenCE, after the real conversion is complete. The converted string exists in memory, though not in a form that R can currently represent as a STRSXP. Hence the error when passed to mkCharLenCE. The issue of embedded null characters has been discussed previously on the R mailing lists (see this thread), but I don’t think this is the issue here. The point here is that the C implementation of iconv operates on binary data, not necessarily null terminated C strings. Hence, in order to fully utilize the iconv mechanism, the R-level iconv ought to accept and return objects that can handle arbitrary binary data, i.e.of type RAWSXP, in addition to character vectors. To this end, I’ve written a small patch (13 lines w/o documentation) against the current R-devel sources (r52328) that allows the R-level iconv to accept an argument of type RAWSXP, in addition to character vectors. Now, when a raw object is passed to iconv, no character substitution is performed, the arguments sub and mark are ignored, and a raw object is returned. However, rather than returning NA (NA does not exist for RAWSXPs) when conversions are invalid or incomplete, a partially converted object is returned. The following patch doesn’t touch any of the code associated with STRSXPs, nor affect the behavior of iconv when a character vector is passed. Once compiled into R the new iconv will operate on raw vectors. Continuing with our example: The patch code is listed below, and also available here R-devel-iconv-0.0.patch. P.S. Thanks to Tal Galili for recommending the GeSHi plugin for wordpress, it worked out nicely for the R and patch (lang=""diff"") code in this post, though I prefer the more subtle coloring in the patch code. "	 2 Comments
What I need to know…	https://www.r-bloggers.com/2010/06/what-i-need-to-know/	June 19, 2010	Manos Parzakonis	is maps and geographical data representation in R. In case you’re curious too this is a good study material from R-Bloggers : maps ; geographical ; spatial Ok. This could be a tweet rather than a post… 	 0 Comments
ggplot2 GUI progress	https://www.r-bloggers.com/2010/06/ggplot2-gui-progress/	June 19, 2010	Tal Galili	(Written by Ian Fellows) Below is a link to the first of a weekly (or bi-weekly) screen-cast vlog of my progress building a GUI for the ggplot2 package. http://neolab.stat.ucla.edu/cranstats/gsoc_vlog1.mov comments and suggestions are more than welcome, and can e-mailed to me at: [email protected] 	 0 Comments
The perfect fake	https://www.r-bloggers.com/2010/06/the-perfect-fake/	June 19, 2010	Matt Asher	 Usually when you are doing Monte Carlo testing, you want fake data that’s good, but not too good. You may want a sample taken from the Uniform distribution, but you don’t want your values to be uniformly distributed. In other words, if you were to order your sample values from lowest to highest, you don’t want them to all be equidistant. That might lead to problems if your underlying data or model has periods or cycles, and in any case it may fail to provide correct information about what would happen with real data samples. However, there are times when you want the sample to be “perfect”. For example, in systematic sampling you may wish to select every 10th object from a population that is already ordered from smallest to biggest. This method of sampling can reduce the variance of your estimate without introducing bias. Generating the numbers for this perfect sample is quite easy in the case of the Uniform distribution. For example, R gives you a couple easy ways to do it: When it comes to other distributions, grabbing a perfect sample is much harder. Even people who do a lot of MC testing and modeling may not need perfect samples every day, but it comes up often enough that R should really have the ability to do it baked right into to the language. However, I wasn’t able to find such a function in R or in any of the packages, based on my searches at Google and RSeek. So what else could I do but roll my own? This function should work with any distribution that follows the naming convention of using “dname” for the density of the distribution and has as its first parameter the number of values to sample. The histogram at the top of this post shows the density of the Lapalce, aka Double Exponential distribution. Here is the code I used to create it: As you can see, my function plays nice with distributions specified in other packages. Here are a couple more examples using standard R distributions: Besides plotting the results with a histogram, there are specific tests you can run to see if values are consistent with sampling from a known distribution. Here are tests for uniformity and normality. You should get a p-value of 1 for both of these: If you notice any bugs with the “perfect.sample” function please let me know. Also let me know if you find yourself using the function on a regular basis.  	 0 Comments
Why R doesn’t suck	https://www.r-bloggers.com/2010/06/why-r-doesnt-suck/	June 19, 2010	Paul Butler	"I first encountered the R programming language a few years ago when I needed to make some plots. Although I’ve used it occasionally since, I always considered it a sort of “Perl for statisticians” — a useful swiss-army knife with ugly syntax and inconsistent semantics. My workflow generally involved manipulating the data in Python and using R to make a simple plot, minimizing the amount of R code I wrote as much as possible. When I recently decided to sit down and properly learn the language, I was pleasantly surprised that underneath the line noise was an interesting and unique language. R is a descendant of LISP and, deep down, maintains some of the beauty its ancestor. It also borrows some unique and interesting features from other functional and dynamic languages. R is true to its LISP roots in that you can create, modify, and evaluate parse trees from the code itself. One way to do so is with the quote() special-function, which returns its argument, unevaluated, as an expression object that can be traversed, modified and evaluated. A fun (though not especially useful) consequence of this is that you can write an expression which returns itself as a quote:


> (function(x) substitute((x)(x)))(function(x) substitute((x)(x)))

(function(x) substitute((x)(x)))(function(x) substitute((x)(x)))

> expression <- (function(x) substitute((x)(x)))(function(x) substitute((x)(x)))

> expression == eval(expression)

[1] TRUE

 By default, R uses eager evaluation, so expressions are evaluated as soon as they are assigned. However, R takes after functional languages like Haskell and O’Caml in that it allows lazy evaluation, where expressions are only evaluated at the time they are first used. For example, consider the Haskell code:


m = sum [1..]

 Where sum returns the sum of a list and [1..] is the (infinite) list of all natural numbers. In most languages, the assignment would cause the program to loop forever trying to sum all the natural numbers so it can assign that value to m. In Haskell, the assignment does complete; it simply assigns the expression sum [1..] to m so that it can be evaluated when the value of m is first used. In R we can accomplish something similar with the delayedAssign() function:


delayedAssign(""m"", sum(1:Inf))

 Note that in R, unlike O’Caml, the variables may be explicitly made lazy with delayedAssign, but are evaluated automatically when they are used. Unfortunately, R evaluates lazy variables when they are pointed to by a data structure, even if their value is not needed at the time. This means that infinite data structures, one common application of laziness in Haskell, are not possible in R. When using higher-order functions, it’s sometimes useful to be able to treat operators as functions. Python accomplishes this in a clunky way: there is an operator module which redefines the built-in operators as functions. R takes a more functional approach. As in Haskell and O’Caml, operators are just syntactic sugar for ordinary functions. Enclosing any operator in backticks lets you use it as if it were an ordinary function. For example, calling `+`(2, 3) returns 5. In fact, both the infix and prefix form are indistinguishable once they are parsed.


> quote(3 + 4) == quote(`+`(3, 4))

[1] TRUE

 One surprising fact in R is that the assignment operators (, < and =) are functions like any other. As a result, they can be overwritten or passed around as desired, though neither strikes me as a particularly good idea. Continuations in R are a way of “breaking out” of a computation and jumping down the call stack to return early. The R function callCC() (call with current continuation) takes one argument, a function. It then evaluates that function, passing in a special function as an argument. callCC() then returns the first value that the special function is called with, or the return value of evaluating its argument if the special function is not called before the function returns. To give you a better idea of what that looks like, consider this example:


> callCC(function(m) {return(4)})

[1] 4

> callCC(function(m) {m(2); return(4)})

[1] 2

 Calling the function m(2) essentially cuts the computation short, drops down in the call stack to callCC, and returns 2. If you’ve used continuations in another language, note that in R the exit function can only be called before callCC() returns. This makes R’s continuation semantics less powerful than those of languages like Scheme, Smalltalk, and Ruby. R is not without its flaws and legacy baggage (you can trace its roots back to the S programming language 35 years ago), but once you learn to use it right, it’s a very powerful and indispensable language. "	 0 Comments
Those dice aren’t loaded, they’re just strange	https://www.r-bloggers.com/2010/06/those-dice-aren%e2%80%99t-loaded-they%e2%80%99re-just-strange/	June 18, 2010	Matt Asher	" I must confess to feeling an almost obsessive fascination with intransitive games, dice, and other artifacts. The most famous intransitive game is rock, scissors, paper. Rock beats scissors.  Scissors beats paper. Paper beats rock. Everyone older than 7 seems to know this, but very few people are aware that dice can exhibit this same behavior, at least in terms of expectation. Die A can beat die B more than half the time, die B can beat die C more than half the time, and die C can beat die A more than half the time. How is this possible? Consider the following three dice, each with three sides (For the sake of most of this post and in my source code I pretend to have a 3-sided die. If you prefer the regular 6-sided ones, just double up every number. It makes no difference to the probabilities or outcomes.): Die A: 1, 5, 9
Die B: 3, 4, 8
Die C: 2, 6, 7 Die A beats B  of the time which beats C  of the time which beats A  of the time. Note that the ratios don’t all have to be the same. Here’s another intransitive trio: Die A: 2, 4 ,9
Die B: 1, 8, 7
Die C: 3, 5, 6 Take a moment to calculate the relative winning percentages, or just trust me that they are not all the same…. Did you trust me? Will you trust me now in the future?  In order to find these particular dice I wrote some code in R to automate the search. The following functions calculate the winning percentage for one die over another and check for intransitivity: I then checked every possible combination. How many unique configurations are there? Every die has three numbers on it, and you have three die for a total of nine numbers. To make things simpler and avoid ties, no number can be used more than once. If each sides of a die was ordered and each of the die was ordered, you’d have  different combinations, which is to say a whole mess of them. But our basic unit of interest here isn’t the digits, it’s the dice. So let’s think about it like this: For die A you can choose 6 of the 9 numbers, for die B you can pick 3 of the remaining 6, and for die C you’re stuck with whatever 3 are left. Multiply this all together: and you get 1680 possibilities. But wait? What’s that you say? You don’t care which die is A, which is B, and which is C? Fantastic. That reduces the number of “unique” configurations by , which is to say 6, at least if my back-of-the-envelope calculations are correct. Final tally? 280. Not bad. Unfortunately, there no obvious way to ennumerate each of these 280 combinations (at least not to me there isn’t). So I ended up using a lot of scratch work and messing around in the R console until I had what I believed to be the right batch. Sorry, I no longer have the code to show you for that. After testing those 280 configurations, I found a total of 5 intransitive ones, including the 2 dice shown previously and the following 3 sets: Die A: 2, 9, 3
Die B: 1, 6, 8
Die C: 4, 7, 5 Die A: 7, 1, 8
Die B: 5, 6, 4
Die C: 9, 3, 2 Die A: 7, 3, 5
Die B: 2, 9, 4
Die C: 8, 6, 1 Did I make a mistake? According to my calculations,  of the combinations are intransitive. That represents 1.786% of the total. How might I very this? That’s right, it’s Monte Carlo time. Using the following code, I created all  permutations of dice and sides, then sampled from those 362,880 sets of dice many, many times: Final percentage: 1.807%. That’s pretty close to , and much closer than it is to either  or , so I’m going to conclude that I got them all and got it right.  What happens if your dice have fewer, or more, sides? Turns out you need at least 3 sides to achieve intransitivity. Can you have it with 4 sides? What about 5, 6, or 7? To estimate the fraction of dice configurations which are intransitive for different numbers of sides I wrote the following code. Note that this could take a while to run, depending on the number of “tires” you use: If you wait through all that you might notice some interesting patters emerge, which probably have explanations rooted in theory but it’s getting on nap time, so I’ll wrap this post up.    I think what fascinates me the most about intransitive dice, and games like rock, scissors, paper, is that they represent breakdowns in what math folks like to call a “total order”. Most of our calculations are done in this nice land of numbers where you can count on transitivity.  is positively correlated with  and  is positively correlated with , doesn’t mean that  and  are positively correlated. Strange, no? But you can prove it with an example covariance matrix. Have you got one to show me? "	 0 Comments
Revolution Analytics: Startup to watch	https://www.r-bloggers.com/2010/06/revolution-analytics-startup-to-watch/	June 18, 2010	David Smith	Jack Germain of LinuxInsider interviewed Revolution CEO Norman Nie for his “Startup to Watch” column. Amongst the topics covered: the R language (Norman: “There are no statistical expressions that can not be written in R”), Revolution’s recent name-change and announcement of our development roadmap, and the challenges of competing with SAS and Norman’s former company, SPSS. Read the full interview at the link below. LinuxInsider: Analytics ‘R’ Us   	 0 Comments
The impact of the drug war in Mexico	https://www.r-bloggers.com/2010/06/the-impact-of-the-drug-war-in-mexico/	June 18, 2010	David Smith	" For the last couple of years, Mexico has been in the midst of an escalating drug war, with violent crime on the upswing in many areas. But tracking the impact quantitatively is difficult: in Mexico, about 85% of crimes go unreported, and corruption leads to inaccurate reporting in some districts. Diego Valle has taken on the task of visualizing and analyzing the available data with R, and has come up with some startling results. For example, by comparing national with local statistics, Diego has identified a massive underreporting of 1153 murders in the state of Chihuahua: 
  Diego even employs Benford’s Law to uncover evidence of data falsification. There’s lots more fascinating analysis in Diego’s report, including this choropleth of homicide rate across Mexico: 
 On a personal note, we’d planned to return to a favorite vacation spot near Zihuatanejo this year. After hearing from locals that the once-tranquil village had been overrun, we cancelled our planned trip. Zihua is near the centre of that hotspot on the south-central coast. Read Diego’s full post for lots more great analysis. He’s made all of his R code available, too. Diego Valle’s Blog: Statistical Analysis and Visualization of the Drug War in Mexico   "	 0 Comments
R: Command Line Calculator using Rscript	https://www.r-bloggers.com/2010/06/r-command-line-calculator-using-rscript/	June 18, 2010	Stewart MacArthur		 0 Comments
R Commander – linear regression	https://www.r-bloggers.com/2010/06/r-commander-%e2%80%93-linear-regression/	June 18, 2010	Ralph	We can fit various linear regression models using the R Commander GUI which also provides various ways to consider the model diagnostics to determine whether we need to consider a different model. Fast Tube by Casper The “Statistics” menu provides access to various statistical models via the “Fit models” sub-menu including: The “Models” menu provides access to various diagnostics for statistical models via the “Graphs” sub-menu including: Other useful resources are provided on the Supplementary Material page. 	 0 Comments
Occupational Wage Comparison Plotted in R	https://www.r-bloggers.com/2010/06/occupational-wage-comparison-plotted-in-r/	June 17, 2010	C		 0 Comments
Do Not Log-Transform Count Data, Bitches!	https://www.r-bloggers.com/2010/06/do-not-log-transform-count-data-bitches/	June 17, 2010	jebyrnes	 OK, so, the title of this article is actually Do not log-transform count data, but, as @ascidacea mentioned, you just can’t resist adding the “bitches” to the end. Onwards. If you’re like me, when you learned experimental stats, you were taught to worship at the throne of the Normal Distribution.  Always check your data and make sure it is normally distributed!  Or, make sure that whatever lines you fit to it have normally distributed error around them!  Normal!  Normal normal normal!   And if you violate normality – say, you have count data with no negative values, and a normal linear regression would create situations where negative values are possible (e.g., what does it mean if you predict negative kelp!  ah, the old dreaded nega-kelp), then no worries.  Just log transform your data.  Or square root.  Or log(x+1). Or SOMETHING to linearize it before fitting a line and ensure the sacrament of normality is preserved. This has led to decades of thoughtless transformation of count data without any real thought as to the consequences by in-the-field ecologists. But statistics has had a better answer for decades – generalized linear models (glm for R nerds, gzlm for SAS goombas who use proc genmod.  What? I’m biased!) whereby one specifies a nonlinear function with a corresponding non-normal error distribution.  The canonical book on this was first published ’round 1983.  Sure, one has to think more about the particular model and error distribution they specify, but, if you’re not thinking about these things in the first place, why are you doing science? “But, hey!” you might say,  “Glms and transformed count data should produce the same results, no?” From first principles, Jensen’s inequality says no – consider the consequences for error of the transformation  approach of log(y) = ax+b+error versus the glm approach y=e^(ax+b)+error.   More importantly, the error distributions from generalized linear models may often be far far faaar more appropriate to the data you have at hand.  For example, count data is discrete, and hence, a normal distribution will never be quite right.  Better to use a poisson or a negative binomial. But, “Sheesh!”, one might say, “Come on!  How different can these models be?  I mean, I’m going to get roughly the same answer, right?” O’Hara and Kotze’s paper takes this question and runs with it.  They simulate count data from negative binomial distributions and look at the results from generalized linear models with negative binomial or quasi-poisson error terms (see here for the difference) versus a slew of transformations.   Intriguingly, they find that glms (with either distribution) always perform well, while each transformation performs poorly at some or all values.   Estimated root mean-squared error from six different models.  Curves from the quasi-poisson model are the same as the negative binomial.  Note that the glm lines (black solid) all hang out around 0 as opposed to the transformed fits. More intriguingly to me are the results regarding bias.  Bias is the deviation between a fit parameter and its true value.  Bascially, it’s a measure of how wrong your answer is.  Again, here they find almost no bias in the glms, but bias all over the charts for transformed fits. Estimated mean biases from six different models, applied to data simulated from a negative binomial distribution. A low bias means that the method will, on average, return the ‘true’ value.  Note that the bias for transformed fits is all over the place.  But with a glm, bias is always minimal. They sum it up nicely For count data, our results suggest that transformations perform poorly. An additional problem with regression of transformed variables is that it can lead to impossible predictions, such as negative numbers of individuals. Instead statistical procedures designed to deal with counts should be used, i.e. methods for fitting Poisson or negative binomial models to data. The development of statistical and computational methods over the last 40 years has made it easier to fit these sorts of models, and the procedures for doing this are available in any serious statistics package. Or, more succinctly, “Do not log-transform count data, bitches!” “But how?!” I’m sure some of you are saying.  Well, after checking into some of the relevant literature, it’s quite straightforward. Given the ease of implementing glms in languages like R (one uses the glm function, checks diagnostics of residuals to ensure compliance with model assumptions, then can use Likliehood ratio testing akin to anova with, well, the Anova function) this is something easily within the grasp of the everyday ecologist.  Heck, you can even do posthocs with multcomp, although if you want to correct your p-values (and there are reasons to believe you shouldn’t), you need to carefully consider the correction type. For example, consider this data from survivorship on the Titanic (what, it’s in the multcomp documentation!) – although, granted, it’s looking at proportion survivorship, but, still, you’ll see how the code works: There are then a variety of ways to plot or otherwise view glht output. So, that’s the nerdy details.  In sum, though, the next time you see someone doing analyses with count data using simple linear regression or ANOVA with a log, sqrt, arcsine sqrt, or any other transformation, jump on them like a live grenade.  Then, once the confusion has worn off, give them a copy of this paper.  They’ll thank you, once they’re finished swearing. O’Hara, R., & Kotze, D. (2010). Do not log-transform count data Methods in Ecology and Evolution, 1 (2), 118-122 DOI: 10.1111/j.2041-210X.2010.00021.x 	 0 Comments
Stack exchange for statistical analysis needs you!	https://www.r-bloggers.com/2010/06/stack-exchange-for-statistical-analysis-needs%c2%a0you/	June 17, 2010	Rob J Hyndman	The proposal to create a StackExchange site for statistical analysis is steadily moving forward. We have now completed the scoping stage which involved finding enough people willing to express an interest in the idea, and voting on some example questions to define what is allowed and what is not allowed on the site. The on-topic questions that have been selected are these: Examples of questions considered off-topic are: The next phase is to get people to commit to contributing to the site. Many readers of this blog have already registered as “followers” — now you have to make a commitment to be a contributor as well. The site won’t launch until there are enough people committed to being part of it. Just go to the site and indicate that you are willing to be an active participant once it launches. If you’re wondering what this is all about, and why this is a much better approach than the various usenet and email help groups, there’s a nice summary on Tal Galili’s blog.   	 0 Comments
Chart the U.S. Gross National Product with the Federal Reserve API	https://www.r-bloggers.com/2010/06/chart-the-u-s-gross-national-product-with-the-federal-reserve-api/	June 17, 2010	C		 0 Comments
Installing Ruby on Linux as a User other than root	https://www.r-bloggers.com/2010/06/installing-ruby-on-linux-as-a-user-other-than-root/	June 17, 2010	C	Ruby is best known as the language behind the rails web application framework.  However, it is a very flexible general purpose language that can be used for tasks of direct interest to R Developers (parsing files, interacting with databases, processing XML or JSON, math functions, statistics, machine learning, etc).  If you do not have root access on a Linux server, you may still be able to install the ruby language and rubgems. Start by checking the version currently installed (if any):  whereis ruby which ruby ruby –version  Create or navigate to a temporary directory.  mkdir ~/tmp cd ~/tmp    Get a source archive of ruby and ruby gems from RubyForge.  wget http://rubyforge.org/frs/download.php/25689/ruby-1.8.6-p110.tar.gz    Uncompress and extract the downloaded source into such like $HOME/tmp.  tar -xzvf ruby-1.8.6-p110.tar.gz    Change directory to the location of extracted sources.  cd ruby-1.8.6-p110    Run configure script with –prefix option set to $HOME (avoid permissions issues).  This will result in an installation of ruby in your home directory.  ./configure –prefix=$HOME make make install    Add $HOME/bin to your path (in ~/.bash_profile or other startup script).  export PATH=/home/username/bin/:.:$PATH    Verify the correct version was installed:  ruby –version    Keep in mind that various subdirectories will be created by this process (bin, lib, share).  Download Gems wget http://rubyforge.org/frs/download.php/69366/rubygems-1.3.6.zip   Set GEM_HOME to a gems directory that you have write access to. Replace /home/username with your own home directory location. Use “pwd” to get correct syntax.   export GEM_HOME=/home/username/gems    Run the ruby gems setup program.  ruby setup.rb    Depending upon your environment, you may want to set up aliases, configure your PATH or other environmental variables.  alias ruby=’~/bin/ruby’ export GEM_HOME=/home/csaternos/gems export RUBYOPT=rubygems 	 0 Comments
Playing with Primes in R (Part II)	https://www.r-bloggers.com/2010/06/playing-with-primes-in-r-part-ii/	June 17, 2010	Neil Gunther		 0 Comments
Messing with R packages	https://www.r-bloggers.com/2010/06/messing-with-r-packages/	June 17, 2010	nmv	"This was really frustrating. I’m trying to modify a package from Matt Johnson and although I could get the package he sent me to install flawlessly, I couldn’t un-tar it, make a change, re-tar it, and then R CMD INSTALL it. I was about to pull out my hair. The error I got was:
ERROR: cannot extract package from ‘hrm-rev9.tar.gz’ The secret: you have to have the name correct.
R CMD INSTALL hrm-rev9.tar.gz
barfs. But
R CMD INSTALL hrm_0.1-9.tar.gz
works fine. I’m sure it’s somewhere in the docs. I just couldn’t find it.  As always, I made a script to do it for me: (Updated 6/17/2010 15:41)


#!/bin/bash

# Quick script to tar & gzip the package, remove the old one, and install the new one

# I'll add options automatically tag and release it later. 
#Set the library that I'm using

LIB=""/home/vanhoudn/R/i486-pc-linux-gnu-library/2.10/""
#Commit

svn commit -m ""Build commit"" 
#get the revision number from svn

REV=`svn info -R | grep Revision | cut -d: -f 2 | sort -g -r | head -n 1 | sed 's/ //g'`
#Build the filename

FILENAME=""hrm_0.1-$REV.tar.gz""
# I need to tar up the pkg so I can install it.

# Jump to the parent directory and work from there.

cd ..

# Exclude any hidden files under the directories (svn has a bunch)

# and add the named files

tar czf $FILENAME --exclude '.*' hrm/DESCRIPTION hrm/NAMESPACE hrm/src hrm/R 
# Remove the old version of the package

R CMD REMOVE -l $LIB hrm
# Install the new package

R CMD INSTALL $FILENAME
# Clean up

rm $FILENAME
# Go back to our previous directory

cd hrm

 "	 0 Comments
Shrinking R’s PDF output	https://www.r-bloggers.com/2010/06/shrinking-rs-pdf-output/	June 17, 2010	Adam M. Wilson		 0 Comments
A new Q&A website for Data-Analysis (based on StackOverFlow engine) – is waiting for you	https://www.r-bloggers.com/2010/06/a-new-qa-website-for-data-analysis-based-on-stackoverflow-engine-%e2%80%93-is-waiting-for-you/	June 17, 2010	Tal Galili	"StackOverFlow.com (“SO” for short) is a programming Q & A site that’s free. Free to ask questions, free to answer questions, free to read. Free, And fast. For the R community, SO offers a growing database of R related questions and answer (click the link to check them out). You might be asking yourself what’s so special about SO over other available resources such as R mailing lists, R blogs, R wiki and so on?
That is a great question.

The answer is that SO succeeds in doing a great job synthesizing aspects of Wikis, Blogs, Forums, and Digg/Reddit to offer a very powerful Q&A website.   In SO, the new questions are like forum/blog posts (A main text with comments/answers).  After someone answers a question, other users can give a thumb-up or a thumb-down to the answer (like digg/reddit).  And all content can be edited, like a wiki page, by the users (provided the user has enough “karma points”).
You also get badges (“awards”) for a bunch of actions (like coming to the website every day for a month.  Giving an answer that got X amount of thumb-ups and so on).  The awards allows someone who is asking a question to see how much the person who had answered him has good reputation (in terms of acceptance/appreciation of his answers by other SO members).
It also offers a small (but effective) ego-boost for the person who gives answers. Well, StackOverFlow has one limitation.  It deals ONLY with programming questions.  Other questions like:	 Will not be answered, and the threads will get closed as being “off topic”.  Why? because such questions are dealing with: statistics, data analysis, data mining, data visualization – But in no means in programming. So there is no StackOverFlow-like Q&A website for data analysis… Until now! In the past few weeks, Rob Hyndman and other users, have made much effort to push the creation of a new website, based on the StackOverFlow engine, to allow for statistically related Q&A.
His proposal for a new website is almost complete.  All it need is for you (yes you), to go to the following link:
Stack Exchange Q&A site proposal: Statistical Analysis 
And commit yourself to the website (that is, declare that you will have interest in reading, asking and answering questions on it) Once a few more tens of people will commit – the website will go online. Hope to see you there. "	 0 Comments
Comparing standard R with Revoutions for performance	https://www.r-bloggers.com/2010/06/comparing-standard-r-with-revoutions-for-performance/	June 17, 2010	Allan Engelhardt	"Following on from my previous post about improving performance of R by linking with optimized linear algebra libraries, I thought it would be useful to try out the five benchmarks Revolutions Analytics have on their Revolutionary Performance pages. For convenience I collected their tests into a single script revolution_benchmark.R that I can simply run with Rscript --vanilla revolution_benchmark.R. The results, compared with the speed-up factors Revolution claims for their version: In all instances Revolution’s claimed speed-up is greater, though probably not significantly so for the Matrix Multiply test and hardly so for the Principal Components Analysis.  (Of course, I do not have a copy of Revolution Analytics’ product, so I can’t verify their claims or make a comparable test.) Whether saving 48 seconds on a linear discriminant analysis is enough to justify buying the product is a decision I leave to you: you know what analysis you do.  For me, there are (many) orders of magnitudes to be gained by better algorithms and better variable selections so I am not too worried about factors of 2 or even 10.  For extra raw power, I run R on a cloud service like AWS which scales well for many problems and is easy to do with stock R while I guess there are some sort of license implications if you wanted to do the same with Revolution’s product.  (But I like Revolution and am still trying to find an excuse to use their product.) Your mileage may vary. Jump to comments. 

Faster R through better BLAS
 Can we make our analysis using the R statistical computing and analysis platform run faster? Usually the answer is yes, and the best way is to improve your algorithm and variable selection. But recently David Smith was suggesting that a big benefit of their (commercial) version of R was that it was linked to a to a better linear algebra library. So I decided to investigate. The quick summary is that it only really makes a difference for fairly artificial benchmark tests. For “normal” work you are unlikely to see a difference most of the time. 

R tips: Eliminating the “save workspace image” prompt on exit
 When using R , the statistical analysis and computing platform, I find it really annoying that it always prompts to save the workspace when I exit. This is how I turn it off. 

R tips: Keep your packages up-to-date
 "	 2 Comments
Shrinking R’s PDF output	https://www.r-bloggers.com/2010/06/shrinking-rs-pdf-output-3/	June 17, 2010	AdamWilson	R is great for graphics, but I've found that the PDF's R produces when drawing large plots can be extremely large. This is especially common when using spplot() to plot a large raster. I've made a 15 page PDF full of rasters that was hundreds of MB in size.  Obviously I don't need all the detail (every pixel of the raster) represented in the pdf and would rather have it reduced in size somehow.  So I wrote an R function to automate the following:  Here's the function:   	 0 Comments
Calling Ruby, Perl or Python from R	https://www.r-bloggers.com/2010/06/calling-ruby-perl-or-python-from-r/	June 16, 2010	C		 0 Comments
Conferenza a Padova	https://www.r-bloggers.com/2010/06/conferenza-a-padova/	June 16, 2010	xi'an	 Today and tomorrow, I am attending the annual Italian statistical society meeting. While I appreciate very much the invitation, as well as the opportunity to walk through  Padova and Venezia for a short (and alas rainy!) hour on the way there (leaving home at 8am, walking in Venezia at noon!), I am rather skeptical of the impact of my talk on Bayes factor approximations there, given that the audience will mostly be made of people who had already heard me talk on that topic or seen my poster in Benidorm. Here are the slides anyway.  	 0 Comments
Mary, Chloe, and Miriam at breakfast	https://www.r-bloggers.com/2010/06/mary-chloe-and-miriam-at-breakfast/	June 16, 2010	Matt Shotwell	 	 0 Comments
R-help follow-up: truncated exponential	https://www.r-bloggers.com/2010/06/r-help-follow-up-truncated-exponential/	June 16, 2010	Matt Shotwell	I recently posted the message below with regard to sampling from the truncated exponential distribution. I left out the derivation of the CDF (mostly because text math is ugly), so I’ve included it here. There is also a short JSS article about truncated distributions in R. This problem in particular may likely be found in an introductory text on survival analysis or probability theory. Where  is the level of truncation and  is the rate, the normalization constant  is given by  The truncated exponential CDF is then   Solving for the inverse CDF yields the itexp function below. From the R-help list: Since there is a simple closed form for the truncated exponential CDF, you can use inverse transform sampling. I believe this is quite common in survival analysis methods. The first step is to compute and write an R function to compute the inverse CDF for the truncated exponential, say where u is the quantile, m is the rate, and t is the level of truncation. Next, we draw from the truncated exponential with something like 	 0 Comments
R Sapply Problem	https://www.r-bloggers.com/2010/06/r-sapply-problem/	June 16, 2010	Quantitative Finance Collector		 0 Comments
R Commander – hypothesis testing	https://www.r-bloggers.com/2010/06/r-commander-%e2%80%93-hypothesis-testing/	June 16, 2010	Ralph	The R Commander GUI can be used to perform classical hypothesis testing. There are menu options to undertake the variants on the t-test as well as tests on proportions or equality of variances for two samples of data. Fast Tube by Casper The “Statistics” menu provides access to various hypothesis tests via the “Means” sub-menu including: Other useful resources are provided on the Supplementary Material page. 	 0 Comments
The distribution of online data usage	https://www.r-bloggers.com/2010/06/the-distribution-of-online-data-usage/	June 16, 2010	David Smith	"AT&T has recently announced it will no longer offer unlimited data plans for new iPhone users in the US, and now some carriers in the UK have followed suit. In each case, the providers claim that only a very small number of users actually use enough data to warrant an unlimited plan, and most users use relatively little and will benefit from the cheaper, capped plans. But what does “most users” mean? Journalist Charles Arthur of the Guardian, in an article about the impact of the data caps, illustrated the distribution of data usage amongst mobile users with this chart, created in R: 
 Reader Barry Rowlingson thought this chart looked odd (is that really a 97% percentile?), but given that the chart lacks axis labels, it’s hard to make sense of anyway. So he set to recreate the chart: given the mean (200Mb) and one quantile (the 97% quantile is 500Mb), he figured out the unstated standard deviation of the Normal distribution (159Mb) and recreated the graph with axis labels: ss=seq(0,700,len=100)plot(ss,dnorm(ss,mean=200,sd=159),col=”red”,type=”l”)abline(v=500,col=”black”) 
  I added the black vertical line to indicate the 97% percentile at 500Mb. Anyway, it’s nice that R is being used to illustrate statistical concepts like this, just a shame that the chart wasn’t quite right. Thanks to Barry for providing this alternative. The Guardian: Why file-sharing has killed ‘unlimited’ mobile data contracts  "	 0 Comments
Date and Time in R	https://www.r-bloggers.com/2010/06/date-and-time-in-r/	June 15, 2010	C		 0 Comments
Welcome guest blogger, Joseph Rickert	https://www.r-bloggers.com/2010/06/welcome-guest-blogger-joseph-rickert/	June 15, 2010	David Smith	I’m about to head out for a two-week holiday, so I’ll be off the grid for a little while. But I have queued up some (hopefully!) interesting stories to auto-post while I’m away, so there’ll still be plenty to read every weekday as usual here on the blog.  Also joining us for the next couple of weeks is guest blogger Joseph Rickert. Joseph is an R engineer here at Revolution Analytics. He’s been using R and other statistics software for several years as a statistician and research analysis in fields as diverse as economics, clinical trial analysis, and manufacturing. Joseph has also recently given talks at the NYC R User Group and the Data Mining Camp. Look for Joseph’s thoughts on using R over the next couple of weeks. 	 0 Comments
Updated SoilWeb for the iPhone + Alpha Android Version	https://www.r-bloggers.com/2010/06/updated-soilweb-for-the-iphone-alpha-android-version/	June 15, 2010	dylan	Major updates to the SoilWeb iPhone Application. read more 	 0 Comments
Statistical Analysis and Visualization of the Drug War in Mexico	https://www.r-bloggers.com/2010/06/statistical-analysis-and-visualization-of-the-drug-war-in-mexico/	June 15, 2010	Diego Valle-Jones		 0 Comments
RcppArmadillo 0.2.3	https://www.r-bloggers.com/2010/06/rcpparmadillo-0-2-3/	June 15, 2010	Thinking inside the box	"
It adds a tiny bit of configuration to permit Sun Studio / suncc to
successfully build the package. There is no code change, and no configuration
change for the other platforms.  Thanks for Brian Ripley for additional
testing, and of course for running those build instances (and everything else he does) for the
R project, and to Conrad Sanderson as
upstream author of the Armadillo C++ library for linear algebra.

 
As usual, more information is on the 
RcppArmadillo page.
Questions, comments etc should go to the
rcpp-devel mailing list
off the R-Forge page.


 "	 0 Comments
New R User Group in Kassel, Germany	https://www.r-bloggers.com/2010/06/new-r-user-group-in-kassel-germany/	June 15, 2010	David Smith	There’s yet another local R user group launching this week, this time in Kassell, in central Germany. Their first meeting is on June 24. If my rusty German is holding up, looks like there will be some good discussion on editors and GUIs, how to find packages and functions, and R community resources. There’s been lots of activity in new R user groups lately, which is a great sign. There are now 40 active groups listed in the Local R User Group Directory, with more than come: looks like there may be new groups starting up in Austria and Tokyo soon, too. Meetup.com: Kasseler useR Group 	 0 Comments
R for Matlab Users	https://www.r-bloggers.com/2010/06/r-for-matlab-users/	June 15, 2010	Quantitative Finance Collector		 0 Comments
Faster R through better BLAS	https://www.r-bloggers.com/2010/06/faster-r-through-better-blas/	June 15, 2010	Allan Engelhardt	"Can we make our analysis using the R statistical computing and analysis platform run faster?  Usually the answer is yes, and the best way is to improve your algorithm and variable selection. But recently David Smith was suggesting that a big benefit of their (commercial) version of R was that it was linked to a to a better linear algebra library.  So I decided to investigate. The quick summary is that it only really makes a difference for fairly artificial benchmark tests.  For “normal” work you are unlikely to see a difference most of the time. I use R on a 64-bit Fedora 12 Linux system.  Fortunately, it is very easy to rebuild R using different libraries on this platform.  For the following, I will assume that you have a working rpmbuild environment.  The test system has a quad core Intel Xeon E5420 CPU with each core running at 2.50 GHz. Benchmarking R is complex.  Very complex.  But for this simple test we use two tests from the R Benchmarks page: MASS-ex.R and R-benchmark-25.R.  The first is a simple benchmark using the examples from the MASS package, and has the advantage that it reflects real-world problems and real-world analysis, albeit small problems and short analysis.  The second is a much more artificial example and primarily test matrix operations. We run the MASS benchmark as: While the R-benchmark-25 is simply: For the MASS benchmark we simply capture the real elapsed time while the R benchmark 2.5 provides more detailed output for the three classes of tests (matrix calculation, -functions, and program execution) as well as overall summaries.  They are all shown in the table below. For the experiments that follow the first thing to do is to grab copies of the source RPMs for R and for ATLAS: At the time I did this, I got R-2.11.0-1.fc12.src.rpm and atlas-3.8.3-12.fc12.src.rpm.  I crank up the level of optimization that I do when building from source so the first thing is to edit ~/.rpmrc to include the line optflags: x86_64 -O3 -march=native -m64 -g.  With that in place we can simply do: We now have a compiler-optimized version of R and we can re-run our tests.  It doesn’t make much difference, but that is also good to know. Now let’s try linking to the ATLAS BLAS libraries instead.  I assume you have them installed (yum install atlas if not) so you can just grab a copy of R-atlas.diff to change the spec file like this: You now have a version of R that uses the ATLAS BLAS libraries, so you can re-run the tests.  The results are in the table below in the “Optimized R + Standard ATLAS” row. As expected, the matrix operations from the R-benchmark-25.R runs a lot faster: they complete in about 30-40% of the time, much of which comes from the multi-threading so all four CPU cores are used. However, for the analysis-heavy code is MASS-ex.R there is little difference.  If anything, we see a tiny increase in running time. 
Multi-threaded BLAS libraries make no significant difference to real-world analysis problems using R.
 For good measure we also try an optimized version of ATLAS, but it does not make much difference on the x86_64 architecture: And (only) for completeness, we also try the standard Netlib BLAS and LAPACK libraries (yum install blas lapack) by the same method as the ATLAS library above but with a slightly different change to the SPEC file: R-blas.diff.  It performs a little better than vanilla R. For more information about rebuilding R with different BLAS libraries, see the linear algebra section in the R Installation and Administration manual. Jump to comments. 

Comparing standard R with Revoutions for performance
 Following on from my previous post about improving performance of R by linking with optimized linear algebra libraries , I thought it would be useful to try out the five benchmarks Revolutions Analytics have on their Revolutionary Performance pages. 

R versus SAS/SPSS in corporations
 A recent question on one of the LinkedIn groups about the advantages of using R over commercial tools like SAS or IBM SPSS Modeller drew lots of comments for R. We like R a lot and we use it extensively, but I also wanted to balance the discussion. R is great, but looking at commercial organizations near the end of 2011 it is not necessarily the right choice to make. 

R tips: Keep your packages up-to-date
 In this entry in a small series of tips for the use of the R statistical analysis and computing tool, we look at how to keep your addon packages up-to-date. 

R tips: Installing Rmpi on Fedora Linux
 Somebody on the R-help mailing list asked how to get Rmpi working on his Fedora Linux machine so he could do high-performance computing on a cluster of machines (or a single multicore machine) using the R statistical computing and analysis platform . Sinc… 

R tips: Eliminating the “save workspace image” prompt on exit
 "	 0 Comments
Clustergram: visualization and diagnostics for cluster analysis (R code)	https://www.r-bloggers.com/2010/06/clustergram-visualization-and-diagnostics-for-cluster-analysis-r-code/	June 15, 2010	Tal Galili	"In 2002, Matthias Schonlau published in “The Stata Journal” an article named “The Clustergram: A graph for visualizing hierarchical and .  As explained in the abstract: In hierarchical cluster analysis dendrogram graphs are used to visualize how clusters are formed. I propose an alternative graph named “clustergram” to examine how cluster members are assigned to clusters as the number of clusters increases.
This graph is useful in exploratory analysis for non-hierarchical clustering algorithms like k-means and for hierarchical cluster algorithms when the number of observations is large enough to make dendrograms impractical. A similar article was later written and was (maybe) published in “computational statistics”. Both articles gives some nice background to known methods like k-means and methods for hierarchical clustering, and then goes on to present examples of using these methods (with the Clustergarm) to analyse some datasets. Personally, I understand the clustergram to be a type of parallel coordinates plot where each observation is given a vector.  The vector contains the observation’s location according to how many clusters the dataset was split into.  The scale of the vector is the scale of the first principal component of the data.  After finding out about this method of visualization, I was hunted by the curiosity to play with it a bit.  Therefore, and since I didn’t find any implementation of the graph in R, I went about writing the code to implement it. The code only works for kmeans, but it shows how such a plot can be produced, and could be later modified so to offer methods that will connect with different clustering algorithms. The function I present here gets a data.frame/matrix with a row for each observation, and the variable dimensions present in the columns.
The function assumes the data is scaled.
The function then goes about calculating the cluster centers for our data, for varying number of clusters.
For each cluster iteration, the cluster centers are multiplied by the first loading of the principal components of the original data.  Thus offering a weighted mean of the each cluster center dimensions that might give a decent representation of that cluster (this method has the known limitations of using the first component of a PCA for dimensionality reduction, but I won’t go into that in this post).
Finally all of our data points are ordered according to their respective cluster first component, and plotted against the number of clusters (thus creating the clustergram). My thank goes to Hadley Wickham for offering some good tips on how to prepare the graph. Here is the code (example follows) The iris data set is a favorite example of many R bloggers  when writing about R accessors , Data Exporting, Data importing, and for different visualization techniques.
So it seemed only natural to experiment on it here. Here is the output:
 Looking at the image we can notice a few interesting things.  We notice that one of the clusters formed (the lower one) stays as is no matter how many clusters we are allowing (except for one observation that goes way and then beck).
We can also see that the second split is a solid one (in the sense that it splits the first cluster into two clusters which are not “close” to each other, and that about half the observations goes to each of the new clusters).
And then notice how moving to 5 clusters makes almost no difference.
Lastly, notice how when going for 8 clusters, we are practically left with 4 clusters (remember – this is according the mean of cluster centers by the loading of the first component of the PCA on the data) If I where to take something from this graph, I would say I have a strong tendency to use 3-4 clusters on this data. But wait, did our clustering algorithm do a stable job?
Let’s try running the algorithm 6 more times (each run will have a different starting point for the clusters) Resulting with:  (press the image to enlarge it)

Repeating the analysis offers even more insights.
First, it would appear that until 3 clusters, the algorithm gives rather stable results.
From 4 onwards we get various outcomes at each iteration.
At some of the cases, we got 3 clusters when we asked for 4 or even 5 clusters. Reviewing the new plots, I would prefer to go with the 3 clusters option.  Noting how the two “upper” clusters might have similar properties while the lower cluster is quite distinct from the other two. By the way, the Iris data set is composed of three types of flowers.  I imagine the kmeans  had done a decent job in distinguishing the three. It is worth noting that the current way the algorithm is built has a fundamental limitation:  The plot is good for detecting a situation where there are several clusters but each of them is clearly “bigger” then the one before it (on the first principal component of the data). For example, let’s create a dataset with 3 clusters, each one is taken from a normal distribution with a higher mean: The resulting plot for this is the following:

The image shows a clear distinction between three ranks of clusters.  There is no doubt (for me) from looking at this image, that three clusters would be the correct number of clusters. But what if the clusters where different but didn’t have an ordering to them?
For example, look at the following 4 dimensional data:  In this situation, it is not clear from the location of the clusters on the Y axis that we are dealing with 4 clusters.
But what is interesting, is that through the growing number of clusters, we can notice that there are 4 “strands” of data points moving more or less together (until we reached 4 clusters, at which point the clusters started breaking up).
Another hope for handling this might be using the color of the lines in some way, but I haven’t yet figured out how. Hadley Wickham has kindly played with recreating the clustergram using the ggplot2 engine.  You can see the result here:
http://gist.github.com/439761
And this is what he wrote about it in the comments: I’ve broken it down into three components:
* run the clustering algorithm and get predictions (many_kmeans and all_hclust)
* produce the data for the clustergram (clustergram)
* plot it (plot.clustergram)
I don’t think I have the logic behind the y-position adjustment quite right though. Here is an example of how it looks:
 In a first look, it would appear that the clustergram can be of use.  I can imagine using this graph to quickly run various clustering algorithms and then compare them to each other and review their stability (In the way I just demonstrated in the example above). The three rules of thumb I have noticed by now are: Yet there is more work to be done and questions to seek answers to: I am looking forward to reading your input/ideas in the comments (or in reply posts). "	 0 Comments
plagiarism exposed!	https://www.r-bloggers.com/2010/06/plagiarism-exposed/	June 14, 2010	xi'an	"Last morn, I had the surprise of receiving the following email: This is to inform you that the following abstract has been submitted to the 3rd International Conference of the ERCIM WG on COMPUTING & STATISTICS (ERCIM’10) Ab#: 114
Title: Goodness of Fit Via Mixtures of Beta distributions
Keywords: nonparametric estimation, posterior conditional predictive p-value.
Abstract: We consider a Bayesian approach to goodness of fit, that is, to the problem of testing whether or not a given parametric model is compatible with the data at hand . Since we are concerned with a goodness of fit problem, it is more of interest to consider a functional distance to the tested model d(F;F) as the basis of our test, rather than the corresponding Bayes factor, since the later puts more emphasis on the parameters. It is both of high interest and of strong difficulty to come up with a satisfactory notion of a Bayesian test for goodness ofit to a distribution or to a family of distributions. The abstract is a plagiarism of your work. I am informing you of about this in case the author has tried to plagiarize the whole paper. The same author has submitted a second abstract plagiarizing another paper.  The author uses bogus affiliations and I cannot trace his institution in case he has one. It is somehow comforting to see that such a gross example of plagiarism can get detected, despite the fact that our paper never got published. Although I am sure there must be conferences that do not apply any filter on the submission… This paper with Judith Rousseau was once submitted to Series B, but I could not come to complete the requested revision for programming motives, the task of modifying the several thousand lines of C code driving the beta mixture estimation filling me with paralysing dread! This is actually the time when I stopped programming in C (the fact that I ever really programmed in C is actually debatable!). This is unfortunate as the spirit of the paper was quite nice, using an idea borrowed from Verdinelli and Wasserman to build a genuine Bayesian goodness of fit test… I do not think there is much to salvage at this later stage, given the explosion of Bayesian non-parametrics. "	 0 Comments
StatEt in Ubuntu 10.04	https://www.r-bloggers.com/2010/06/statet-in-ubuntu-10-04/	June 14, 2010	nmv	"I wanted a “lightweight” version of Eclipse to run R from Ubuntu. (I installed eclipse-pde using apt-get. It worked fine.) Once it was running, I installed StatEt via the “Install new software” feature from http://download.walware.de/eclipse-3.5. While it was downloading, I opened up an R console and ran install.packages(""rJava""). When the installation of both StatEt and rJava finished I restarted Eclipse. This is when things stopped working and I couldn’t really find any step-by step directions on how to proceed. Here is what I did:         Where  is your username. (You are providing the path to rJava, for some reason, even though Eclipse will detect it during the setup in the “R_Config” step, it doesn’t seem to share that information with the JRE.)  "	 0 Comments
New R User Groups in Sydney, South Asia	https://www.r-bloggers.com/2010/06/new-r-user-groups-in-sydney-south-asia/	June 14, 2010	David Smith	For R users in the Southern Hemisphere on the other side of the Prime Meridian, there are two new entries in the Local R User Group Directory: Sydney and South Asia. The Sydney group wins the prize for the best group acronym ever: SURF. The Sydney Users of R Forum will have their welcome meeting on July 6 July 20, with a talk from Eugene Dubossarsky and then dinner at a nearby pub. (Sounds lovely — I really miss Australia sometimes!) And for R users in the South Asia region there’s a new mailing list R South Asia to join and connect with other R users in India and environs. No physical meetings planned as yet, but maybe soon? Revolutions: Local R User Group Directory 	 1 Comment
Repulsive dots pattern, the difference of distance	https://www.r-bloggers.com/2010/06/repulsive-dots-pattern-the-difference-of-distance/	June 14, 2010	Matt Asher	What if you wanted to randomly place objects into a field, and the more objects you had, the more they rejected newcomers placed nearby? To find out, I setup a simulation. The code, shown at the end, isn’t all that interesting, and the plots shown below aren’t all that special. I think there is one interesting part of this, and that’s how the clustering changes depending on how distance is measured. One of the plots uses the traditional “L2″ distance, the other uses L1″ (Manhattan taxi cab) measure . Each plot shown below has almost exactly the same number of dots (277 vs 279). Can you tell which uses L1 and which uses L2 just by looking? Plot A:  Plot B:  Here’s the code. Run it and see for yourself. Make sure to change adjust the values which have comments next to them. Uncommenting “print(force)” can help you pack a maxRepulse value. 	 0 Comments
Example 7.41: hazard function plotting	https://www.r-bloggers.com/2010/06/example-7-41-hazard-function-plotting/	June 14, 2010	Nick Horton		 0 Comments
June 20, online Registration deadline for useR! 2010	https://www.r-bloggers.com/2010/06/june-20-online-registration-deadline-for-user-2010/	June 13, 2010	Tal Galili	"useR!2010 is coming. I am going to give two talks there (I will write more of that soon), but in the meantime, please note that the online registration deadline is coming to an end. This was published on the R-help mailing list today: ————- The final registration deadline for the R User Conference is June 20,
2010, one week away.  Later registration will not be possible on site! Conference webpage:  http://www.R-project.org/useR-2010
Conference program: http://www.R-project.org/useR-2010/program.html Registration:
http://www.R-project.org/useR-2010/registration/registration.html The conference is scheduled for July 21-23, 2010, and will take place at
the campus of the National Institute of Standards and Technology (NIST) in
Gaithersburg, Maryland, USA.  Following the successful useR! 2004, useR! 2006, useR! 2007, useR! 2008,
and useR! 2009, conferences, the conference is focused on: 1. R as the `lingua franca’ of data analysis and statistical computing,
2. providing a platform for R users to discuss and exchange ideas on
how R can be used to do statistical computations, data analysis,
visualization and exciting applications in various fields,
3. giving an overview of the new features of the rapidly evolving R
project. As for the predecessor conferences, the program will consist of two parts:
invited lectures and user-contributed sessions.  Prior to the conference,
there will be tutorials on R, descriptions of which are available at
http://www.R-project.org/useR-2010/tutorials INVITED LECTURES Invited speakers will include Mark Handcock, Frank Harrell Jr, Friedrich Leisch, Michael Meyer,
Richard Stallman, Luke Tierney, Diethelm Wuertz. USER-CONTRIBUTED SESSIONS The sessions will be a platform to bring together R users, contributors,
package maintainers and developers in the S spirit that `users are
developers’. People from different fields will show us how they solve
problems with R in fascinating applications.  The sessions are organized
by members of the program committee, including Dirk Eddelbuettel, John Fox, Virgilio Gomez-Rubio,
Richard Heiberger, Torsten Hothorn, Aaron King, Jan de Leeuw,
Nicholas Lewin-Koh, Andy Liaw, Uwe Ligges, Martin Maechler,
Katharine Mullen, Heather Turner, Ravi Varadhan, H. D. Vinod,
John Verzani, Alan Zaslavsky, Achim Zeileis. The program will cover topics such as * Applied Statistics & Biostatistics
* Bayesian Statistics
* Bioinformatics
* Chemometrics and Computational Physics
* Data Mining
* Econometrics & Finance
* Environmetrics & Ecological Modeling
* High Performance Computing
* Machine Learning
* Marketing & Business Analytics
* Psychometrics
* Robust Statistics
* Social network analysis
* Spatial Statistics
* Statistics in the Social and Political Sciences
* Teaching
* Visualization & Graphics
* and many more. IMPORTANT DATES *********************************************************************
**   2010-06-20 registration deadline
**                (later registration NOT possible on site)
*********************************************************************
2010-07-20   tutorials
2010-07-21   conference start
2010-07-23   conference end We hope to meet you in Gaithersburg! "	 0 Comments
R Commander – data manipulation and summaries	https://www.r-bloggers.com/2010/06/r-commander-%e2%80%93-data-manipulation-and-summaries/	June 13, 2010	Ralph	Previously we considered the R Commander interface as a simple GUI for the R statistical software system. Here we will look at how to undertake data manipulation and creating basic statistical summaries of data sets. Fast Tube by Casper The R Commander GUI has two menus “Data” and “Statistics” that are used for manipulating data sets and calculating descriptive statistics and various commonly used statistical techniques. In the “Data” menu there is a sub-menu “Manage variables in active data set” that has some useful features. These include: The “Statistics” menu provides access to various descriptive and summary statistics via the “Summaries” sub-menu including: There are other data manipulation options and summary functions available from these two menus. Other useful resources are provided on the Supplementary Material page. 	 0 Comments
bug in schoolmath	https://www.r-bloggers.com/2010/06/bug-in-schoolmath/	June 13, 2010	xi'an	"Neil Gunther has pointed out on his blog that the prime number decomposition R package schoolmath contains mistakes in the function primes, listing 1 as a prime number but also including decomposable numbers like 133 in its list of prime numbers: > primes(100,140)
[1] 101 107 111 113 123 129 131 137
> primes(50,140)
[1]  51  53  59  61  67  71  73  79  83  89  97 101 103 107 109 113 127 131 133
[20] 137 139
> is.prim(primes(133)
[1] FALSE
> is.prim(primes(200,300))
[1] FALSE TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE TRUE
[13]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE
> sum(1-is.prim(primes(1,1000)))
[1] 10
>  data(primlist)
> sum(1-is.prim(primlist[1:25000]))
[1] 3309 This is rather annoying and I hope it gets quickly fixed! "	 0 Comments
Beancounter minor bug fix release 0.8.10	https://www.r-bloggers.com/2010/06/beancounter-minor-bug-fix-release-0-8-10/	June 13, 2010	Thinking inside the box	"

The new version is now in
Debian, at 
CPAN and on my
beancounter
page here. Enjoy!

 "	 0 Comments
Primes in R (Part III): Schoolmath is Broken!	https://www.r-bloggers.com/2010/06/primes-in-r-part-iii-schoolmath-is-broken/	June 13, 2010	Neil Gunther		 1 Comment
Dynamic Modeling 3: When the first-order difference model doesn’t cut it	https://www.r-bloggers.com/2010/06/dynamic-modeling-3-when-the-first-order-difference-model-doesn%e2%80%99t-cut-it/	June 12, 2010	Tony	Data must be selected carefully.  The predictive usefulness of the model is grossly diminished if outliers taint the available data.  Figure 1, for instance, shows the Defense spending (as a fraction of the national budget) between 1948 and 1968. Note how the trend curve (as defined by our linear difference model from the last post: see appendix for a fuller description) is a very poor predictor.  Whatever is going on here isn’t a first-order process.  So, let’s neglect the model entirely for a moment.  The huge variations in spending between 1950 and 1952 indicate there were years within the selected time span for which the defense spending dramatically increased because of some exogenous shock, and then spending trended downwards. One way we can judge the usefulness a little more scientifically is to run a regression on the differences.  In other words, Plot Y(t+1) on top of Y(t), and insert the regression line.  Check it out: In general, the regression is a pretty good fit for the clustered points in the middle.  However, we also have some nasty outliers.  Something about this data isn’t a first-order process.  Wikipedia to the Rescue.  It is history that inconveniently interrupts our model, causing these outliers.  President Harry Truman cut back military spending in the wake of World War II.  However, any hope Truman may have had for shifting the national focus away from foreign military affairs was ruined by the onset of the Korean War late in 1950.  Thus we see the large spike in spending evident in Figure 1.  A predictive first-order model which spans this event with just these data will be limited in its effectiveness.  The grossly underfitted line of Figure 1 is little better than useless for indicating the spending during any given year.  A much more effective model would start by following the onset of the Korean War (a completely unpredictable event if prediction were solely based on these data), and trace the evolution of the expenditure as it decreased from its start-of-conflict high in 1952, as we see in Figure 3. It ain’t perfect, but it’s definitely much better.  So, dynamic modeling with first-order linear difference equations has an enormous array of applications.  However, it is easy to be seduced by the numbers without carefully considering the data and the substantive implications of these findings.  Any inattentiveness in this respect may easily lead to meaningless, incorrect, or downright silly conclusions. Next Time: Saving the code, changing the method. References: Code adapted from http://www.courtneybrown.com/classes/ModelingSocialPhenomena/Assignments/Assignment2NationalDefenseOutlaysCourtneyBrownMathModeling.htm Dataset taken from http://www.courtneybrown.com/classes/ModelingSocialPhenomena/Assignments/Assignment2CourtneyBrownMathModeling.htm Appendix: This is the R code to generate these Graphs: Please forgive my poor style (reusing variable names and whatnot).  It works, and that’s enough for me at the moment. 	 0 Comments
R Tools for Dynamical Systems ~ bifurcation plot in R for system of ODEs	https://www.r-bloggers.com/2010/06/r-tools-for-dynamical-systems-bifurcation-plot-in-r%c2%a0for%c2%a0system%c2%a0of%c2%a0odes/	June 12, 2010	apeescape	"As per request, here is the code that I wrote to draw bifurcation plots in R. Bifurcation diagrams for discrete maps can be done using this code by James Jones. It is a little easier since approximation is not needed. In the following code, I used the deSolve library to draw bifurcation diagrams for a system of ODEs (continuous). You basically need to choose the parameter you’d like to perturb (param.name), for a given range (param.seq). When plotting, it plots the chosen parameter range on the x-axis and the variable of choice (plot.variable) on the y-axis. This is an example for the Lotka-Volterra.  
Filed under: deSolve, Food Web, R        

 "	 0 Comments
A different way to view probability densities	https://www.r-bloggers.com/2010/06/a-different-way-to-view-probability-densities/	June 12, 2010	Matt Asher	The standard, textbook way to represent a density function looks like this:  Perhaps you have seen this before? (Plot created in R, all source code from this post is included at the end). Not only will you find this plot in statistics books, you’ll also see it in medical texts, sociology, and even economics books. It gives you a clear view of how likely an observation is to fall in a particular range of . So what’s the problem? The problem is that what usually concerns us isn’t probability in isolation. What matters is the impact that observations have on some other metric of importance, like the total or average. The key thing we want to know about a distribution is: What range of observations will contribute the most to our expected value, and in what way? We want a measure of influence. Here’s the plot of the Cauchy density:  From this view, it doesn’t look all that different from the Normal. Sure it’s a little more narrow, with “fatter tails”, but no radical difference, right? Of course, the Cauchy is radically different from the normal distribution. Those slightly fatter tails give very little visual indication that the Cauchy is so extreme-valued that it has no expected value. Integrating to find the exception gives you infinity in both directions. If your distribution is like this, you’ve got problems and your plot should tell you that right away. Here’s another way to visualize these two probability distributions:  Go ahead and click on the image above to see the full view. I’ll wait for you… See? By plotting the density multiplied by the observation value on the y-axis, you get a very clear view of how the different ranges of the function effect the expectation. Looking at these, it should be obvious that the Cauchy is an entirely different beast. In the normal distribution, extreme values are so rare as to be irrelevant. This is why researchers like to find ways to treat their sample as normally distributed: a small sample gives enough information to tell the whole story. But if your life (or livelihood) depends on a sum or total amount, you’re probably best off plotting your (empirical) density in the way shown above. Another bit of insight from this view is that the greatest contribution  to the expectation comes at 1 and -1, which in the case of the Normal isn’t the mean, but rather the second central moment (plus or minus). That’s not a coincidence, but it’s also not always the case, as we shall see. But first, what do things look like when a distribution gets completely out of hand? The Student’s t distribution, on 1 Degree of Freedom , is identical to  the Cauchy. But why stop at a single DF? You can go all the way down to the smallest (positive) fraction.  The closer you get to zero, the flatter the curve gets. Can we ever flatten it out completely? Not for a continuous distribution with support over an infinite range. Why not? Because in order for the slope of  to continue to flatline it indefinitely, the density function would have to be some multiple of , and of course the area under this function diverges as we go to infinity, and densities are supposed to integrate to 1, not infinity, right? What would the plot look like for a continuous function that extends to infinity in just one direction? Here’s the regular Exponential(1) density function plot:  Now look at the plot showing contribution to expectation:  Were you guessing it would peak at 1?  Again, the expectation plot provides insight into which ranges of the distribution will have the greatest impact on our aggregate values. Before I go look at an a discrete distribution, try to picture what the expectation curve would look like for the standard  distribution. Did you picture a diagonal line? Can we flatten things out completely with an infinitely-supported discrete distribution? Perhaps you’ve heard of the St. Petersburg Paradox. It’s a gambling game that works like this: you flip a coin until tails comes up. If you see one head before a tails, you get $1. For 2 heads you get $2, for 3 heads $4, and so on. The payoff doubles each time, and the chances of reaching the next payoff are halved. The paradox is that even though the vast majority of your winnings will be quite modest, your expectation is infinite.  The regular view of the probability mass function for provides almost no insight:  But take a look at the expectation plot:  Flat as a Nebraska wheat field. You can tell right away that something unusual is happening here. I could go on with more examples, but hopefully you are beginning to see the value in this type of plot. Here is the code, feel free to experiment with other distributions as well. 	 0 Comments
Prototype: More Web-Friendly Visualizations in R	https://www.r-bloggers.com/2010/06/prototype-more-web-friendly-visualizations-in-r/	June 12, 2010	Wojciech Gryc	"I’ve spent some more time thinking about how best to put together the package for creating web-friendly, interactive data visualizations in R. I have a pretty substantial JavaScript package that does a lot of basic visualizations now, and it’s really exciting to see where this is going. With this in mind, I’m releasing a new version of the R package prototype I keep discussing in this blog. A number of functions are included here, including wv.plot(), wv.lineplot(), wv.snaplot(), wv.bargraph. The documentation still needs a lot of work, and there are no interactive abilities yet (though they exist in the JavaScript code). What is most exciting about this package is that a lot of the steps one takes to make a complete graph have been split into individual functions. Thus, while one can make a scatterplot with wv.plot(), one can also use wv.axis() and wv.points() to do so as well. Each data visualization gets its own ID, or can be assigned one, so one can later start passing visualization (e.g. the points in the scatterplot itself) as arguments to other functions, thus allowing one to begin adding functions for interactivity. A few examples of the visualizations are shown below, along with the necessary R code to get them to display. Note that these are embedded into the blog, I did so through the use of an inline frame. Basic Scatterplot The code below will generate a basic scatterplot.
x = rnorm(30)

y = rnorm(30)

wv.plot(x, y, ""~/Desktop/scatterplot"", height=300, width=300, xlim=c(-2.5,2.5), ylim=c(-2.5,2.5), xbreaks=c(0), ybreaks=c(0))  Plot with Multiple Data Types Supposing you want to have a scatterplot with multiple point types and a line. You can build this manually with the following code. x = rnorm(30); y = rnorm(30); z = runif(30);

wv.open(""~/Desktop/plot3/"", height=300, width=300);

wv.axis(c(-3.5, 3.5), c(-3.5, 3.5), xbreaks=-2:2, ybreaks=-2:2);

wv.points(x, y, xlim=c(-3.5, 3.5), ylim=c(-3.5, 3.5));

wv.lines(sort(x), z, col=""red"", xlim=c(-3.5, 3.5), ylim=c(-3.5, 3.5));

wv.close();  Bar Graph This is a new graph format. x = c(2.5, 7, 11);

wv.bargraph(x, cats, ""~/Desktop/barplot"", ylim=c(0, 15), ybreaks=(1:5)*3);  As always, comments are welcome. "	 0 Comments
New R User Group in Atlanta	https://www.r-bloggers.com/2010/06/new-r-user-group-in-atlanta/	June 11, 2010	David Smith	There’s a new R User Group in Atlanta, GA. Well, it’s not exactly new — they’ve already had their first meeting — but due to a cock-up on my part I failed to announce it at the time. (Memo to self: don’t forget to star emails in Gmail that you have to follow up on!) There’s a Google Group where you can find info about the Atlanta R User Group, including two nice slide decks from the first meeting on Introduction to Computing with R and Graphics in R. Google Groups: Altanta R User Group  	 0 Comments
The R Revolution on TV	https://www.r-bloggers.com/2010/06/the-r-revolution-on-tv/	June 11, 2010	martin	"I never thought I would ever embed videos from FOX on my blog, but this one needs to be covered: 
Watch the latest business video at video.foxbusiness.com
 Watch SPSS co-founder Norman Nie talking about the “… unbelievably powerful open source language called R …” and “… I am not sure that SPSS is our biggest competitor there is an even larger competitor out there called SAS …” Good luck on this mission! "	 0 Comments
Performance benefits of linking R to multithreaded math libraries	https://www.r-bloggers.com/2010/06/performance-benefits-of-linking-r-to-multithreaded-math-libraries/	June 11, 2010	David Smith	"R wasn’t originally designed as a multithreaded application — multiprocessor systems were still rare when the R Project was first conceived in the mid-90’s — and so, by default, R will only use one processor of your dual-core laptop or quad-core desktop machine when doing calculations. For calculations that take a long time, like big simulations or modeling of large data sets, it would be nice to put those other processors to use to speed up the computations. There are several parallel processing libraries for R available that allow you to explicitly run loops in R simultaneously (ideally, each on a different processor), but using them does require you to rewrite your code accordingly.  But there is a way to make use of all your processing power for many computations in R, without changing a line of code. That’s because R is a statistical computing system, and at the heart of many of the algorithms you use on a daily basis — data restructuring, regressions, classifications, even some graphics functions — is linear algebra. The data are transformed into vector and matrix objects, and the internals of R have been cleverly designed to link to a standard “BLAS” API to perform calculations on vectors and matrices. The binaries provided by the R Core Group on CRAN (with one exception, see below) are linked to an “internal BLAS which is well-tested and will be adequate for most uses of R”, but is not multi-threaded and so only uses one core. But the beauty of linking to the BLAS API is that you can re-compile R to link to a different, multi-threaded BLAS library and, voilà, suddenly many computations are using all cores and therefore run much faster. The MacOS port of R on CRAN is linked to ATLAS, a “tuned” BLAS that uses multiple cores for computations. As a result, R on a multi-core Mac (as all new Macs are these days) really zooms. But the Windows binaries on CRAN are not linked to an optimized BLAS. It’s possible to compile and link R yourself, but it can be tricky. That’s what we do at Revolution for our Windows and Linux distributions of Revolution R. When we compile R, we link it to the Intel Math Kernel Libraries, which includes a high-performance BLAS implementation tuned to multi-core Intel chips. “Tuning” here means using efficient algorithms, optimized assembly code that exploits features of the chipset, and multi-threaded algorithms that use all cores simultaneously. As a result, you get some serious speed boosts for many operations in R, especially on a multi-core system. Here are some examples: 
       As you can see, using the Intel MKL libraries on a 4-core machine gives some dramatic speedups (about a quarter of the 1-core time, as you might expect). Perhaps more surprisingly, using the Intel MKL libraries on a 1-core machine is also faster than using R’s standard BLAS library: this is a result of the optimized algorithms, not additional computing power. You even get improvements on non-Intel chipsets (like AMD). [A side note: These calculations were actually all run on an 8-core machine, specifically, an Intel Xeon 8-core CPU with 18 GB system RAM running Windows Server 2008 operating system. The complete benchmark code is available on this page. The results for Revolution R 1-core and 4-core were calculated by restricting the Intel MKL library to use 1 thread and 4 threads, using the Revolution-R-specific commands setMKLthreads(1) and setMKLthreads(4) respectively. This has the effect of using only the power of the specific number of cores, even when more cores are available. Note: if you’re using Revolution R and are doing explicit parallel programming with doSMP, it’s a good idea to call setMKLthreads(1) first. Otherwise, your parallel loops and the multi-threaded linear algebra computations will compete for the same processor and actually degrade performance.] These results are dramatic, but multi-threaded BLAS libraries aren’t a panacea. Not all R commands ultimately link to BLAS code, even ones you might expect. (For example, lm for regression uses a non-BLAS QR decomposition by default. Edit: as pointed out by Doug Bates, lm and glm end up calling the older Linpack routines using level-1 (vector-vector) BLAS instead of the newer, level-3-based (matrix-matrix) Lapack routines because of the need to handle certain rank-deficient cases cleanly.) And if your R code ultimately does not involve linear algebra, you can’t expect any improvement at all. (For example, the “Program Control” R benchmarks by Simon Urbanek show only marginal performance gains in Revolution R.) This is when explicit parallel programming is the route to improved performance. We’re also working on dedicated statistical routines for Revolution R Enterprise that are explicitly multi-threaded for single-machines and also distributable to multiple machines in a cluster or in the cloud, but that’s a topic for another post. Revolution Analytics: Performance Benchmarks  "	 0 Comments
The Deepwater Horizon, in context	https://www.r-bloggers.com/2010/06/the-deepwater-horizon-in-context/	June 11, 2010	David Smith	"The Earth is pretty big. Give or take, it’s about 36,000 feet (11km) to the height of a 747 soaring above Everest, and (in pleasing symmetry) about the same distance down the bottom of the Mariana Trench. (Nonetheless, if you shrunk the Earth down to scale, it would still be smoother than a billiard ball.) So I was surprised when I saw this infographic of the Earth from the tallest mountain to the deepest trench, and saw how much of that distance the Deepwater Horizon project spanned. Pre-spill, I mean: from the floating rig down the riser to the floor of the Gulf of Mexico, and then down the well to the oil reservoir itself. Here’s a detail of the chart: 
  There’s about 3,000 feet of vertical in that segment; the full chart is drawn on a linear scale (an excellent design choice) and spans 24 screens that size. The Deepwater Horizon project spans 6 of those screens, or about a quarter of the entire vertical span of the Earth. The numbers don’t do it justice though — I suggest taking a look at the full chart and scrolling from top to bottom. You may be surprised at the sense of scale you get from it.  Our Amazing Planet: Tallest Mountain to Deepest Ocean Trench    "	 0 Comments
Quantitative Candlestick Pattern Recognition (HMM, Baum Welch, and all that)	https://www.r-bloggers.com/2010/06/quantitative-candlestick-pattern-recognition-hmm-baum-welch-and-all-that/	June 10, 2010	Intelligent Trading		 0 Comments
R on TV	https://www.r-bloggers.com/2010/06/r-on-tv/	June 10, 2010	David Smith	Had a fun day today in a TV studio for Fox Business News, where Revolution CEO Norman Nie was giving an interview to Liz Claman at Fox Business Network. As you might expect, the interview focused a lot more on the business side than on technical capabilities of Revolution R, but there was some good discussion of the impacts of statistical analysis. The producers did mix up Norman’s background a bit though: as much as I’d like to say that I work with a billionaire, it wasn’t Norman that sold SPSS to IBM (that happened about a year after he left). Also, the interviewer inferred that Norman was bringing technology from SPSS to Revolution, when (as Norman pointed out in the interview) Revolution R is built on the open-source R project instead. Nonetheless, it’s great to see some national exposure for R. Fox Business Network: Changing the Face of Analytics 	 0 Comments
Plotting BP Oil Spill Testing Data using R	https://www.r-bloggers.com/2010/06/plotting-bp-oil-spill-testing-data-using-r/	June 10, 2010	C		 0 Comments
oro.nifti 0.1.5	https://www.r-bloggers.com/2010/06/oro-nifti-0-1-5/	June 10, 2010	Brandon Whitcher		 0 Comments
oro.dicom 0.2.6	https://www.r-bloggers.com/2010/06/oro-dicom-0-2-6/	June 10, 2010	Brandon Whitcher		 1 Comment
Rcpp 0.8.2	https://www.r-bloggers.com/2010/06/rcpp-0-8-2/	June 10, 2010	Thinking inside the box	"
The full NEWS entry for this release follows below:
 
As always, even fuller details are in 
Rcpp Changelog page and the 
Rcpp page which also
leads to the downloads, the 
browseable
doxygen docs and zip files of doxygen output for the standard formats.
A local directory  has
source and documentation too. 
Questions, comments etc should go to the
rcpp-devel mailing list
off the R-Forge page

 "	 0 Comments
RcppArmadillo 0.2.2	https://www.r-bloggers.com/2010/06/rcpparmadillo-0-2-2/	June 9, 2010	Thinking inside the box	"
This release works well with the most recent
inline
release 0.3.5. One can now employ inlined R code as we generalized
how/which headers are included and how library / linking information is added
thanks a plugin mechanism. This is the first RcppArmadillo version to provide such
a plugin,  We also updated the included Armadillo  headers to its most recent release 0.9.10, added
some more operators and provide a utility function RcppArmadillo:::CxxFlags()
to provide include directory information on the fly.

 
An example of the direct inline approach for the fastLm function:

 
This creates a compiled function fun which, by using
 Armadillo, regresses a vector ys
on a matrix Xs (just how the fastLmPure() function in the
package does) --- yet is constructed on the fly using cxxfunction
from 
inline.

 
More information is on the 
RcppArmadillo page.
Questions, comments etc should go to the
rcpp-devel mailing list
off the R-Forge page.


 "	 0 Comments
Data Mining with WEKA example implemented in R	https://www.r-bloggers.com/2010/06/data-mining-with-weka-example-implemented-in-r/	June 9, 2010	C		 0 Comments
3 lines of R code to Process a Web Service	https://www.r-bloggers.com/2010/06/3-lines-of-r-code-to-process-a-web-service/	June 9, 2010	C		 1 Comment
Thoughts on Making Data Work	https://www.r-bloggers.com/2010/06/thoughts-on-making-data-work/	June 9, 2010	David Smith	"I really enjoyed all four talks at today’s online conference, Making Data Work. (Disclosure: Revolution sponsored this conference.) I thought the four speakers together gave a great overview of issues related to the processing, analysis, and visualization of big data. Mike Driscoll started off with a useful categorization for data size. “Small Data” (<10Gb) fits in the memory of one machine. ""Medium Data"" (10GB-1TB) fits on the disk of one machine. ""Large Data"" (1TB and above) requires distributed storage on multiple machines. Naturally, the tools and processes you use in each case varies, and Mike summarized his recommendations as follows: 1. Use the right tool for the job2. Compress everything3. Split up your data4. Work with samples. Mike offers a handy idiom in Perl to take a 1% sample of a text file:   perl -ne “print if (rand()<0.01)"" data.csv > sample.csv5. Use Statistics (or, as Mike put it, “the grammar of data science”). Naturally, the R Project was featured heavily here.6. Copy from others, i.e. use open-source tools like GitHub, R, and R packages from CRAN7. Avoid “chart typology”: instead of just dumping data into standard charts like pie charts and bar charts, think of charts as compositions, not containers. Mike gave a great example of a bespoke chart created using the R package ggplot2.8. Use color wisely9. Tell a story. Mike rounded out his talk with a real-life example from the mobile phone business: using a Greenplum database of billions of phone calls from millions of callers, a social-networking model in R revealed that callers in a local “call network” (of subscribers who regularly call each other) were 700% more likely to switch providers (“churn”) if their friends in the network do so. Mike tells this story dramatically by visualizing the social network and showing how the “infection” of churn spreads through the network over time. See Mike’s slides for the details. Next up was Joe Adler, author of the excellent R in a Nutshell reference manual. Joe began with a fascinating historical factoid: Herman Hollerith, a statistician working on the 1880 census, not only invented the punch card and founded the Tabulating Machine Company which would become one of the companies to launch IBM, but also was arguably the first implementor of map-reduce techniques (for which Hadoop is now famous). He could justifiably be called the first Data Scientist. Joe mentioned some of the challenges of with with big data at LinkedIn. For example, the People You May Know feature is a recommendation engine for 70 million users – an O(n^2) problem, and therefore computationally intensive. Joe’s suggestions aligned with Mikes: compressing data, eliminating data not used for analysis, etc.  Joe did make one interesting observation regarding P-values and sampling: when you’re dealing with very large data sets, don’t forget that P-values aren’t necessarily useful: statistical significance isn’t the same thing as practical significance. He gave a great example of testing (using a Pearson Chi-Squared test in R) of whether the ratio of boys to girls born in the United States in 2006 varies by day of week. Using a 10% sample of the births, the P-value (about 0.3) is not significant. But when using all of the data, you get a very significant P-value (<0.001). Does this mean that boys are significantly more likely to be born on a Monday than on a Wednesday? Probably not ... and even if it does, the difference isn't meaningful. Joe also included a great discussion on privacy issues related to predicting from combined data sets of both on-line and off-line behavior, recalling the outrage directed at DoubleClick in 2000.   Next up was Hilary Mason from Bit.ly, who have a great presentation about practical approaches to machine learning. She included lots of useful tips, like using Amazon Mechanical Turk to bootstrap hand-categorizing a sample of data, and then using supervised learning techniques to infer the remaining labels. She also mentioned Yahoo Boss, which looks like a great way of defining your own search to extract semi-structured data from the unstructured Web. Hilary introduced a handy new (to me, at least) term: “streamlining”, the process of data mining where the algorithm only sees each data point once. (Thanks to the folks on Twitter who let me know that this is widely referred to as on-line learning or streaming analytics in the machine-learning community.)  Finally, Ben Fry, one of the creators of the Processing programming language, demonstrated some beautiful interactive visualizations of data. I’ve been impressed with Processing visualizations featured on FlowingData and elsewhere, but not as impressed when I learned that Processing graphics have featured both on the cover of Nature and in the movie The Hulk. (When Nick Nolte is using your software, that’s when you know you’ve made it to the big leagues.) Kudos to O’Reilly for putting on such a great raft of speakers. As an on-line conference, it worked well — if I’d had to travel I probably wouldn’t have made it to a conference like this, and the price was very reasonable. On the other hand, you don’t get to mingle with the other attendees (which is one of the biggest benefits of live conferences, IMO), but maybe they can fix that problem for the next conference. (You can sometimes get a similar experience from following a Twitter stream, but sadly Twitter was in permanent fail-whale mode today.) I’m looking forward to the followup. O’Reilly Conferences: Making Data Work "	 0 Comments
Pegging your multicore CPU in Revolution R, Good and Bad	https://www.r-bloggers.com/2010/06/pegging-your-multicore-cpu-in-revolution-r-good-and-bad/	June 9, 2010	nmv	" I take an almost unhealthy pleasure in pushing my computer to its limits. This has become easier with Revolution R and its free license for academic use. One of its best features is debugger that allows you to step through R code interactively like you can with python on PyDev. The other useful thing it packages is a simple way to run embarrassingly parallel jobs on a multicore box with the doSMP package.  

library(doSMP)
# This declares how many processors to use.

# Since I still wanted to use my laptop, during the simulation I chose cores-1.

workers <- startWorkers(7)

registerDoSMP(workers)
# Make Revolution R not try to go multi-core since we're already explicitly running in parallel

# Tip from: http://blog.revolutionanalytics.com/2010/06/performance-benefits-of-multithreaded-r.html

 setMKLthreads(1)
chunkSize <- ceiling(runs / getDoParWorkers())

smpopts <- list(chunkSize=chunkSize)
#This just let's me see how long the simulation ran

beginTime <- Sys.time()
#This is the crucial piece. It parallelizes a for loop among the workers and aggregates their results

#with cbind. Since my function returns c(result1, result2, result3), r becomes a matrix with 3 rows and

# ""runs"" columns.

r <- foreach(icount(runs), .combine=cbind, .options.smp=smpopts) %dopar% {

       # repeatExperiment is just a wrapper function that returns a c(result1, result2, result3)

	tmp <- repeatExperiment(N,ratingsPerQuestion, minRatings, trials, cutoff, studentScores)

}
runTime <- Sys.time() - beginTime
#So now I can do something like this:

boxplot(r[1,], r[2,], r[3,],

		main=paste(""Distribution of Percent of rmse below "", cutoff,

			""n Runs="", runs, "" Trials="",trials, "" Time="",round(runTime,2),"" minsn"",

			""scale: "",scaleLow,""-"",scaleHigh,

			sep=""""),

		names=c(""Ave3"",""Ave5"",""Ave7""))

 If you are intersested in finding out more of about this, their docs are pretty good.  Revolution Analytics responded to my support request after I mentioned it on twitter. Apparently, they had done something to the forums which corrupted my account. Creating a new account fixed the problem, so now I can report the bugs that I
find and get some help.  It turns out that you get a small speed improvement by setting setMKLthreads(1). Apparently, the libraries Revolution R links against attempt to use multiple cores by default. If you are explicitly parrallel programing, this means that your code is competing with itself for resources. Thanks for the tip! "	 0 Comments
Efficient Mixed-Model Association eXpedited (EMMAX) to Simutaneously Account for Relatedness and Stratification in Genome-Wide Association Studies	https://www.r-bloggers.com/2010/06/efficient-mixed-model-association-expedited-emmax-to-simutaneously-account-for-relatedness-and-stratification-in-genome-wide-association-studies/	June 9, 2010	Stephen Turner		 0 Comments
Stratified sampling	https://www.r-bloggers.com/2010/06/stratified%c2%a0sampling/	June 9, 2010	xi'an	The recently arXived paper of Goldstein, Rinott and Scarsini studies the impact of refining a partition on the precision of a stratified maximising/integration Monte Carlo approach. Quite naturally, if the partition gets improved, simulating points in each set of the partition can only improve the quality of the approximation, whether the problem is in maximising or in integrating. However, the authors include an interesting (formal) counterexample where the stratification leads to a higher L1 (if not L2) error. (And they include extensions of more classical results to cases when the function is observed with errors or contains missing data.) The difficulty I have with stratification in practice is that it is really difficult to come up with a partition which is relevant for the problem at hand and whose partition weights are known exactly… Reading this nice mathematical paper also led me to ponder the possibility of doing unbiased maximisation, while wondering if stratification was eventually compelling for maximisation, since only one set in the partition is of interest. Eliminating sets from the simulation would thus be leading to higher efficiency, if this was feasible. 	 0 Comments
Go Guerrill-R on Your Data in August	https://www.r-bloggers.com/2010/06/go-guerrill-r-on-your-data-in-august/	June 8, 2010	Neil Gunther		 1 Comment
Plotting World Bank Data with R	https://www.r-bloggers.com/2010/06/plotting-world-bank-data-with-r/	June 8, 2010	C		 0 Comments
Rcpp 0.8.1	https://www.r-bloggers.com/2010/06/rcpp-0-8-1-2/	June 8, 2010	Thinking inside the box	"
There are a few fairly visible new things in this release. As we want to
focus the next few minor releases on completing the documentation, we started
by adding a total of four (!!) new vignettes:
 
The most interesting new feature is what we call Rcpp modules and is
modeled after Boost::Python. This makes it pretty easy to expose C++
functions and classes to R — without having to write glue code.  This is
pretty new and may change a tad over the coming releases, but it is also
quite exciting.

 
Other changes concern more improvements for use of inline which should now
allow packages like our
RcppArmadillo
to be used with it, and some bug fixes. The full NEWS entry for this release follows below:
 
As always, even fuller details are in 
Rcpp Changelog page and the 
Rcpp page which also
leads to the downloads, the 
browseable
doxygen docs and zip files of doxygen output for the standard formats.
A local directory  has
source and documentation too. 
Questions, comments etc should go to the
rcpp-devel mailing list
off the R-Forge page

 "	 0 Comments
PostGIS in Action Book Review	https://www.r-bloggers.com/2010/06/postgis-in-action-book-review/	June 8, 2010	dylan	 I was recently asked to review a soon to be published book on PostGIS, a spatial extension to the very popular Postgresql relational database. I was very excited about receiving an early copy of this book, as the authors have provided countless tips, fixes, and clever query examples on the PostGIS mailing list over the years. After spending a couple weeks looking through the book, I have to say that I am very impressed with the quality and completeness. Indeed, this is the book that I wish would have been available when I was starting out with PostGIS. The authors do an excellent job of promoting the idea that a relational database and SQL are well suited for spatial data modeling and analysis. read more 	 1 Comment
R is eve R ywhe R e	https://www.r-bloggers.com/2010/06/r-is-eve-r-ywhe-r-e/	June 8, 2010	martin	"R did definitely not start to be THE statistical computing tool. The “two Rs” in far down-under just needed some tool which was not too expensive and structured enough to support the elementary statistics classes filled with hundreds of students. Another constraint was the computing lab which was large enough, but “only” filled with Mac IIs. As the “two Rs” were computer savvy and knew the S-language, they started with a simple copy of the basic functionality of the S-language. Everything else which happened since the early 90s is history by now. There are quite a few tools and languages which managed to set fundamental standards like Postscript, pdf, LaTeX, Java, etc. but, e.g., C++ didn’t offer inline Java or the formula editor in Word did not get a LaTeX compatible mode.
Not so for R. Sample session which uses the R-interface to the Oracle Data Mining functions   The list of applications which connect to R and support the utilization of R functions in some way is quite long already, and seems to get new prominent members every now and then: Certainly, with Rserve it is more or less a piece of cake to talk to R for anyone who knows the basics of programming, but companies like SAS and Oracle are quite big players, who usually care a xxxx about what other projects and/or standards do. In some sense it looks like the Goliaths start to surrender to the David, although he never really attacked … "	 0 Comments
Norman Nie talks R and statistics with CNET’s Dave Rosenberg	https://www.r-bloggers.com/2010/06/norman-nie-talks-r-and-statistics-with-cnets-dave-rosenberg/	June 8, 2010	David Smith	Revolution’s CEO Norman Nie recently gave an interview to CNET’s Dave Rosenberg, who blogs on disrupting the software market. Among the topics covered: What is R (Nie: “R is the most powerful statistical computing language on the planet”), what is predictive analytics (“statistical modeling by another name”) and plans for Revolution R (“we intend to bring predictive analytics to the masses”). Check out the full interview at the link below. CNET News / Software, Interrupted: ‘R’ language bringing statistical analytics to the masses (Q&A)  	 0 Comments
Could we run a statistical analysis on iPhone/iPad using R?	https://www.r-bloggers.com/2010/06/could-we-run-a-statistical-analysis-on-iphoneipad-using-r/	June 8, 2010	Tal Galili	" I now came across David smith’s post on the REvolution blog, pointing to instruction on the R wiki for how to install R on the iPhone!
I didn’t try it myself since it both requires jailbreaking the iPhone, and I don’t have an iPhone.  But it is still interesting to know of. I don’t use Mac! Not that there is anything wrong with that, but I don’t use Mac… Yet at the same time, wonderful people like my wife, my brother, my thesis advisor and even my mother-in-law – all use mac.  So one can’t help but wonder if I might be missing out on something. Still, for a Windows user like me it is a bit difficult to understand the hype around the iPhone 4 release:

Such releases tend to look to me more like this spoof video about the release of the apple “i”. So while not using apples product, I have a deep respect for the impact it has made in peoples lives.  Which begs the question: Could you use R on an iPhone (or an iPad) ?? This question (and the motivation for this post) was raised in an R help mailing list thread a week ago. After receiving permission from the threads author, I am republishing the content that was presented there in the hopes it might be of interest to other R community members. And here is what “Marc Schwartz” wrote:
 Hi all, There have been posts in the past about R being made available for the iPhone and perhaps more logically now, on the iPad. My recollection is that the hurdle discussed in the past was primarily a lack of access to a CLI on the iPhone’s variant of OSX, compelling the development of a GUI interface for R specifically for these devices. R itself, can be successfully compiled with the iPhone development tools. Well, now there is another, clearly more profound reason. The FSF has recently communicated with Apple on the presence of a GPL application (GNU Go) in the iTunes store because the iTunes TOS infringes upon the GPL. Apple, given a choice, elected to remove the application, rather than amending their TOS. The FSF also informed the developers of the iPhone port of GNU Go that their distribution is in violation of the GPL. R Core and any others considering an iPhone/iPad port of R, if you are not already aware, take note… More information is here: http://www.fsf.org/news/2010-05-app-store-compliance/ with an update here: http://www.fsf.org/news/blogs/licensing/more-about-the-app-store-gpl-enforcement So, until Apple amends their TOS agreement, it looks like there will be no GPL apps available for the iPhone/iPad, since the only way to make applications available for these platforms is via the iTunes store (unless you unlock the device). Hence, no R for these devices in the foreseeable future. Hi all, Thanks to an offlist e-mail from Thomas (Lumley), I have spent the past few days getting a bit more edumacated on additional restrictions on the opportunity for R to appear on the iPhone/iPad. These in fact go well beyond the GPL issue that I raised in my initial post, which in and of itself, is not trivial by any means. I now know, or at least think I know, more about these issues than I probably wanted, but I also want to present a better picture of the situation. Note that I am not a lawyer and am not intending to represent my findings from a legal perspective. I am reporting them here using a common sense approach from my own reading of the relevant Apple iPhone SDK language, as well as being based upon specific examples and discussions that I located on the web. So, in summary, here are the key issues. I will follow each with some additional details below, but note that all such restrictions would need to be removed or otherwise overcome, before R could in fact appear on these two platforms, at least through Apple approved means in the App Store. 1. Distribution of GPL covered applications is not permissible via the App Store due to the Apple Terms of Service language, which infringes upon rights granted under the GPL. ‘Nuff said. 2. The use of FORTRAN is precluded (explicitly from SDK version 4.x forward) Version 3.x of the iPhone SDK has the following language: 3.3.1   Applications may only use Documented APIs in the manner prescribed by Apple and must not use or call any private APIs. Apple of course does not offer a FORTRAN compiler, as those who build R from source on OSX, as I do, are keenly aware. Thanks to Simon, we have one to use for OSX, but one is not officially available for the iPhone/iPad in the SDK. Note that there is an important distinction here. The ability to build R versus the ability to have the resultant application pass Apple’s review to be able to appear in the App Store. The above language has also been interpreted to apply to Java/Flash, precluding those environments from appearing on the iPhone/iPad. However, the beta release of the 4.x version of the iPhone SDK has the following language in the same section: 3.3.1 — Applications may only use Documented APIs in the manner prescribed by Apple and must not use or call any private APIs. Applications must be originally written in Objective-C, C, C++, or JavaScript as executed by the iPhone OS WebKit engine, and only code written in C, C++, and Objective-C may compile and directly link against the Documented APIs (e.g., Applications that link to Documented APIs through an intermediary translation or compatibility layer or tool are prohibited). Thus, in fact, one can only use Objective-C, C, C++ or JavaScript to develop applications for the iPhone/iPad. No FORTRAN, upon which of course, R is dependent. The above language has also been interpreted to further reinforce restrictions specifically on Java/Flash. 3. The implementation of programming language interpreters, of which R is one, is precluded. The following language appears in version 3.x of the iPhone SDK: 3.3.2   An Application may not itself install or launch other executable code by any means, including without limitation through the use of a plug-in architecture, calling other frameworks, other APIs or otherwise. No interpreted code may be downloaded or used in an Application except for code that is interpreted and run by Apple’s Documented APIs and built-in interpreter(s). The above language has been (pardon the pun) interpreted to restrict the implementation of programming language interpreters on these devices. Some sites I found have narrowly focused on the “No interpreted code may be downloaded” part of the language as an “out” of sorts. However, upon review, they seem to ignore the “or used” wording, which would seem to be independent of the action of downloading. If the language was “and used”, then one could envision the use of locally stored or entered code, but this is not the case. In either case, of course, R would not be one of Apple’s “built-in” interpreters. I found two interesting examples of how Apple has either approved or rejected applications that can be considered interpreters. It is not clear where Apple drew the line between these two, such that it might enable one to better differentiate the reasoning and therefore design an app that would pass muster with them. Thus, one complication may be Apple’s lack of internal consistency in making decisions on allowing or disallowing apps in the App Store. The first is an application which was intended to be a BASIC interpreter on the iPhone and which was apparently rejected by Apple under the name BasicMatrix: http://smartcalc.coollittlethings.com/?p=3 After the author substantially restricted the functionality of the application to being an “enhanced calculator” under the name “SmartCalc”, Apple approved the application. However, a contrarian example is “Frotz” which is a game type of an application currently available at no cost in the App Store for both the iPhone and the iPad. It is in fact an interpreter of so-called “Z code” (http://en.wikipedia.org/wiki/Z-machine) to create interactive fiction adventures. The web page for the app is: http://code.google.com/p/iphonefrotz/wiki/FrotzMain The app was initially accepted by Apple. A later version however was rejected, because the updated version could download new game files (Z code files) via the internet. Once the author removed the download functionality, Apple accepted the updated version of the app. Frotz is also a GPL’d application, so its longevity in the App Store is logically in question and I sent an e-mail to the author to be sure that he was aware of the FSF’s recent actions. The additional implications for R here, vis-a-vis Frotz, would be the ability to download, install and use CRAN packages. I will touch on this issue again below. 4. The implementation of anything resembling the CRAN network, to facilitate add-on packages for R, would be highly problematic for multiple reasons. The following language appears in version 3.x of the iPhone SDK 3.3.3   Without Apple’s prior written approval, an Application may not provide, unlock or enable additional features or functionality through distribution mechanisms other than the App Store. For those who have an iPhone/iPad and are familiar with so-called “In App” purchases, this refers to the mechanism approved by Apple to provide add-on functionality to already installed applications. The notion of In App purchases is somewhat misleading, as in fact the activity may involve no actual direct monetary cost to download the additional component(s). Thus, arguably between the SDK language, which would preclude a CRAN type network unless approved by Apple, combined with the relevant language I reference from 3.3.2 above, there would be substantive restrictions on the ability of a “default” R installation from being able to download and utilize add-on R packages. Further, since one cannot compile programs on the iPhone/iPad, there would have to be some means to pre-compile any add-on packages for R on these two platforms, similar to what is done for Windows and OSX presently on CRAN to create package binaries. Further, packages that use FORTRAN, tcl/tk, Java, Perl or other such libraries would of course, also be problematic, both from a functional and where appropriate, a GPL perspective. Even if one got by some of those issues and pre-compiled add-on packages were to be made available via some distribution process, the entire process of R package installation, updating and management would have to be re-written specifically for these platforms. Thus, there would be a meaningful amount of development work that some group of folks would have to undertake. That all being said, it is clear that a remote client/server implementation of R would be possible, as long as the client-side R GUI application on the iPhone/iPad meets Apple’s requirements. Anyone using the WolframAlpha app ($1.99 U.S.) on the iPhone or iPad (I do on the former) will understand this model. See http://products.wolframalpha.com/iphone/ and http://products.wolframalpha.com/ipad/index.html for more information. Somebody just has to be willing to fund the backend server farm to provide the service.    Needless to say, a fully browser based client/server implementation would also work and be cross-platform, provided that the input/output web pages are appropriately scaled for the mobile displays. WA also provides a free alternative to their dedicated apps via a mobile site at http://m.wolframalpha.com/ as an example. Similar considerations here, as above, for the backend server platform would be applicable. The key advantage of the dedicated app is a more efficient keyboard layout providing easier access to special character sets, rather than scrolling through them using the default keyboard. I hope that the above is helpful to folks. Needless to say, I do not present the above as being the definitive reference, but it seems to be at least a logical interpretation of the current situation. Ken, See comments inline. On Jun 1, 2010, at 2:25 PM, Ken Williams wrote: > Hi Marc,
>
> I want to debate a couple points from your post:
>
>> 1. Distribution of GPL covered applications is not permissible via the App
>
>> Store due to the Apple Terms of Service language, which infringes upon
> rights
>> granted under the GPL.
>>
>> ‘Nuff said.
>
> I’m not sure I agree with this, but there’s so much wiggle room in
> interpreting what the GPL means that there would probably be no way to
> decide without a courtroom & judge, so I’ll leave this part alone. =)  I
> also haven’t yet read your other post where you discuss this. Please do, including the links therein. It’s not my interpretation, it is the FSF’s action and Apple’s response to that action, which sets at least an operational precedence, if not one that could also affect any future litigation pertaining to GPL’d apps in the App Store. That all just took place within the past week or so, which is what prompted my initial post on the matter, since it would be relevant to any R offering via that channel. >> 3.3.1    Applications may only use Documented APIs in the manner
> prescribed by
>> Apple and must not use or call any private APIs.
>
> I believe that language only refers to *Apple’s* APIs.  In other words, they
> don’t want you to call hidden functions that aren’t supposed to be exposed
> to developers.  If it meant no use of any APIs private to the developer, it
> would rule out pretty much every application in existence, considering that
> a call from one function to any other is an API call. I am not in disagreement on that point. The key issue to date has been the lack of a compatible FORTRAN compiler, at least off the shelf, based upon what I can tell. Arguably, that is at least a notable deterrent to use FORTRAN on the iPhone for now. There are other programming language tools for current iPhone development, but nothing for FORTRAN that I can find. I can find no references to anyone building iPhone apps using FORTRAN (even in part) and the few queries that I can find that even mention an interest in doing so, reference the same issues that I have. Some have referenced f2c, but it is not clear to me that such an approach would work for R, not to mention the development overhead and the extensive testing of any conversion of critical functionality. That all changes with the new language in 4.x.
– Show quoted text –
The key wording change relevant to R (and of course for other iPhone developers and tool providers) in 4.x is: “Applications must be originally written in Objective-C, C, C++, or JavaScript as executed by the iPhone OS WebKit engine” Ignore the other wording pertaining to API’s and other layers for the time being. The app must be written natively in one of those four languages. There appears to be no interpretation that I can find that differentiates a scenario where a library of low level functions, written in a language such as FORTRAN, may be called from a higher level language such as those listed above. Unless there is some subtlety in differentiating the abstraction layers within which the application is executed on the iPhone, I see no recourse here. Note that the entities that provide iPhone cross-compilation/framework tools (eg. MonoTouch, Titanium, unity3D, Rhodes, etc.) which convert other code directly to native iPhone apps are also trying to figure out where they stand. Similarly, folks who develop natively in other languages are also having headaches over the new SDK wording. There is even a question relevant to cross-compilation tools that take another language and convert it to, for example, Obj-C, as an intermediate step, before subsequent compilation to an iPhone native binary. So the message that everyone is coming away with is, if you want to develop for the iPhone, write your code using one of these four languages, period. No doubt, some folks will test the boundaries and we will get more definitive answers in time. Is it possible that the SDK language will change before 4.x is released as a stable OS/SDK? Sure, but that does not seem likely. >> 3.3.2    An Application may not itself install or launch other executable
> code
>> by any means, including without limitation through the use of a plug-in
>> architecture, calling other frameworks, other APIs or otherwise. No
>> interpreted code may be downloaded or used in an Application except for
> code
>> that is interpreted and run by Apple’s Documented APIs and built-in
>> interpreter(s).
>
> I think this indeed pretty effectively rules out installation of packages
> from CRAN, which is a bummer – unless those modules are downloaded &
> installed through the app store.  Not sure if that would even work though,
> since they’re not apps. As I note, between 3.3.2 and 3.3.3, any add-on functionality, such as CRAN packages, would be problematic any way you read it. > As for the “interpreted code” stuff, there’s so much murkiness about what
> constitutes interpreted code that I don’t know if this is a deal-breaker or
> not.  At one extreme, it could prohibit pressing buttons in an app and then
> “interpreting” those presses as commands for the app to “do something.”  At
> the other extreme,  Somewhere in the middle, it would seem to cover language
> translation apps.  The notion of “interpreted” is just not very
> well-defined.  For instance, most people think of Perl as an interpreted
> language, but it compiles to bytecode before executing just like Java (it
> just doesn’t typically save it to a bytecode file). I would say that, beyond the SDK language parsing issues relevant to interpreters, given that Apple rejected BasicMatrix and that there are no other programming language interpreters in the App Store, these are pretty goods sign that R would not pass Apple’s review under these parameters. I take a fairly pragmatic approach there. > Finally, I do agree with the general tone implied in your post – it is a
> major major hassle that Apple’s overlords control the distribution channel
> for software on non-jailbroken iDevices.  I don’t like it at all, for the
> exact reason that people like you & me & the rest of the world now have to
> sit around speculating whether our helpful apps will pass muster with the
> cabal. As I noted in my closing comments in my second post, if one has a desire to make R’s functionality available on smartphones (iPhone, Android, etc.) or iPad-class devices, then a client/server approach may be the most efficient means to do so. That approach also avails you of more powerful computing platforms than the client side mobile devices have, at least at present, which will also limit aspects of portable functionality. Regards, Marc Indeed, the client/server approach is what is used in MatLab Mobile,
which is now on sale in the app store.
See http://blogs.mathworks.com/desktop/2010/05/24/introducing-matlab-mobile-%E2%80%93-an-iphone-app-to-connect-remotely-to-your-matlab/ If matlab can do it, then surely the R community can as well. Regards,
Gustaf ———————– I hope the above will be interesting/useful to some of you in the future.
Best,
Tal "	 0 Comments
What is R?	https://www.r-bloggers.com/2010/06/what-is-r-3/	June 7, 2010	David Smith	Probably the question I get asked the most about R is, “What is R”? It’s can be a hard question to answer, because R is so many things. It’s data analysis software. It’s an environment for data analysis. It’s a language. It’s an open-source project. It’s a community. The Wikipedia page for R does a great job of describing the details of the features and history of R, but I was looking for something more concise that I could point people to when asked? I wrote up my take on the What is R? at inside-R.org. Inside-R.org: What is R? 	 1 Comment
Ruby Script to parse ISBNs listed in  R-Project to populate an Amazon	https://www.r-bloggers.com/2010/06/ruby-script-to-parse-isbns-listed-in-r-project-to-populate-an-amazon/	June 7, 2010	C		 0 Comments
Example 7.40: Nelson-Aalen plotting	https://www.r-bloggers.com/2010/06/example-7-40-nelson-aalen-plotting/	June 7, 2010	Nick Horton		 0 Comments
An Interest Rate R Package Plan	https://www.r-bloggers.com/2010/06/an-interest-rate-r-package-plan/	June 7, 2010	Quantitative Finance Collector		 0 Comments
Using R for Introductory Statistics 3.2	https://www.r-bloggers.com/2010/06/using-r-for-introductory-statistics-3-2/	June 6, 2010	Christopher Bare	…continuing my sloth-like progress through John Verzani’s Using R for Introductory Statistics. Previous installments: Chapters 1 and 2 and 3.1. Boxplots provide a visual comparison between two or more distributions. For problem 3.8, we’re asked to compare the reaction times of cell phone users verses a control group, to test the theory that using a cell phone while driving is a bad idea. Comparing the centers and spreads can be done with the following boxplot.  The tilde operator, ~, is used to define a model formula, which is something I aspire to understand someday but currently am clueless about. Looking at the same data as a density plot might give a better picture of each distribution.  Still, boxplots are nice because they give you a sense of the center, range, dispersion, and skew of a sample in a compact and comparable form. Plus, you can plot several boxplots side-by-side.  Problem 3.11 uses data from the 1887 Michelson-Morley experiments attempting to find variations in the speed of light due to earth’s motion through the aether, believed at the time to be the medium through which light waves traveled. The correct value for the speed of light is shown in red. And finally, whadya know, this stuff came in handy for some (probably not very rigorous) performance analysis. 	 0 Comments
biomaRt and GenomeGraphs: a worked example	https://www.r-bloggers.com/2010/06/biomart-and-genomegraphs-a-worked-example/	June 6, 2010	nsaunders	"As promised a few posts ago, another demonstration of the excellent biomaRt package, this time in conjunction with GenomeGraphs. Here’s what we’re going to do: If you want to follow along on your own machine, it will need to be quite powerful.  We’ll be processing exon arrays, which requires a 64-bit machine with at least 4 GB (and preferably, at least 8-12 GB) of RAM.  As usual, I’m assuming some variant of Linux and that you’re comfortable at the command line.

1.  The data
First, grab your raw data.  In this example, we’ll use GEO series GSE12236, titled “Whole Genome Exon Arrays Identify Differential Expression of Alternatively Spliced, Cancer-related Genes in Lung Cancer.”  It consists of n = 40 samples divided into 20 x 2 pairs.  Each sample in a pair is either normal or adenocarcinoma lung tissue from the same individual.  Create a directory for your project and: You should now see 40 files, ending with .CEL.gz, in your working directory.  No need to unzip them. Still a little more preparation before the main event.  In the same directory as the CEL files, open a text editor and create a file named covdesc.  A glance at the first few lines will tell you what this is all about: Two columns:  the first, with no header, a list of the CEL file names.  You can quickly create this using “ls -1 *.gz > covdesc”.  The second, headed “disease.state” (you can call it what you like) describes each file according to whether the sampled tissue is normal or adenocarcinoma.  You get this information by looking at the samples (GSM) at the link to the GEO series. Penultimate preparation step:  fetch the file exon.pmcdf_1.1.tar.gz from the XMap website and install it using “R CMD INSTALL exon.pmcdf_1.1.tar.gz”.  This is a customised CDF (chip descriptor file), used to analyse exon arrays. And finally, we need a file that maps the exon probesets to human chromosomes.  You can get these data from Affymetrix, after creating an account at their site and logging in.  Scroll down to “Current NetAffx Annotation Files”, locate the latest probeset annotation CSV file, download it and then do the following: That will create a much smaller file (~ 62 MB or so) named HuEx.small.csv, with only the 7 columns that you need.  This useful tip brought to you by Mark Robinson, as part of this aroma-project tutorial. 2.  RMA normalisation
We’ll use the Bioconductor simpleaffy package to read and normalise the CEL data.  Open R in your working directory and start with: Too easy.  Next. 3.  Probeset analysis
Next step – identify the most differentially-expressed probesets.  There are multiple approaches to this problem.  The simplest is to perform a pairwise-comparison (normal versus carcinoma) and calculate the fold-change and a simple significance statistic, such as a p-value.  This simple approach is frowned upon by experienced microarray statisticians – at the very least, you should perform a multiple testing correction, since there are many more variables (probesets) than samples.  However, we’ll stick with this simple, oft-used (and abused) approach for the purposes of this example.
Everything you need is in the simpleaffy package: The pairwise.comparison method bears a little explanation.  Our samples are paired and the pairings are reflected in the covdesc file:  adenocarcinomas (1,3,5…) = normals (2,4,6…).  This means that the lists normals(1:20) and adenocarcinomas(1:20) are in the correct order and we can tell pairwise.comparison to use paired replicates in the t-test.
The pairwise.filter method can apply various criteria to the results of the pairwise comparison.  Here, we specify that we want probesets present on all microarrays, with fold change greater than 2 (remember, log2(2) = 1) and t-test p-value less than 0.001.
Let’s look at the 10 most up- and down-regulated probesets: 4.  Fetching the genes
Finally, we get to biomaRt.  Let’s stay with the “top ten” probesets, which were up-regulated: That should be reasonably self-explanatory, from the previous post on how to use biomaRt. 5.  Visualisation
Finally, we move to the GenomeGraphs package for visualisation.  What we want is to plot the RMA values in genomic context.  As an example, we’ll take the gene MUC13, from the top of the top ten list.  Let’s break down the process into a series of tasks: Here’s the R to do that.  Normal (grey) and adenocarcinoma (red) MUC13 exon expression  Line 23 generates the object to be plotted.  You simply decide which tracks you want in the plot and the order in which they should appear, create the appropriate objects, then add them to a list.  If you use the syntax “gene” = muc13.gene, the word “gene” appears as a label to the left of the gene track.  The last steps (lines 24-26) are to create a PNG file and write to it using gdPlot() which takes the object to be plotted and the minimum/maximum chromosome coordinates to display as arguments.  If all goes to plan, you should finish with a plot like the one shown above-left; click it to see a larger version. So what does the plot tell us?  As with all microarrays, the data are quite noisy but it’s clear that in general, expression of most exons is higher in adenocarcinomal tissue.  There is an open-access publication associated with this dataset:  “Whole genome exon arrays identify differential expression of alternatively spliced, cancer-related genes in lung cancer”.  Interestingly, the authors do not identify MUC13 as a significantly up-regulated gene, in either the main text or the supplementary data.  This may be due to differences in statistical procedure:  remember, the approach taken in this post was the simplest (and worst) way to go.  However, they do highlight a related gene, MUC4.  MUC13 has been identified in several other cancer studies including gastric, colon and ovarian cancers, so it seems that over-expression of mucins is a quite widespread cancer phenotype. That’s the basics of biomaRt and GenomeGraphs.  Once you get the hang of it, it’s relatively easy to write more useful code which, for example, can batch-process many genes of interest and create plots with the appropriate titles and file names for each gene. "	 0 Comments
General steps to compile JAGS in a Linux based OS system and install rjags in R	https://www.r-bloggers.com/2010/06/general-steps-to-compile-jags-in-a-linux-based-os-system-and-install-rjags-in-r/	June 6, 2010	Yu-Sung Su		 0 Comments
JAGS, rjags, and coda packages for Fedora	https://www.r-bloggers.com/2010/06/jags-rjags-and-coda-packages-for-fedora/	June 6, 2010	Yu-Sung Su		 0 Comments
The 1000 most-visited sites analyzed using R	https://www.r-bloggers.com/2010/06/the-1000-most-visited-sites-analyzed-using-r/	June 5, 2010	C		 0 Comments
static symbols too?	https://www.r-bloggers.com/2010/06/static-symbols-too/	June 5, 2010	Matt Shotwell	Continuing in the context of this previous post… 1. Select a static symbol from the R sources, like Connections 2. Get its address 3. Use the symbol This effect could probably be prevented by selecting additional compiler and/or and linker options. P.S. Nice job Oleg Sklyar, Duncan Murdoch, Mike Smith, Dirk Eddelbuettel, Romain Francois with inline-0.3.5 	 1 Comment
Free Online Statistics Books	https://www.r-bloggers.com/2010/06/free-online-statistics-books/	June 5, 2010	C		 0 Comments
“Programming with Data – a Guide to the S Language” by John Chambers	https://www.r-bloggers.com/2010/06/programming-with-data-a-guide-to-the-s-language-by-john-chambers/	June 5, 2010	C		 0 Comments
GUI chart formatting with playwith	https://www.r-bloggers.com/2010/06/gui-chart-formatting-with-playwith/	June 4, 2010	C		 3 Comments
On particle learning	https://www.r-bloggers.com/2010/06/on%c2%a0particle%c2%a0learning/	June 4, 2010	xi'an	In connection with the Valencia 9 meeting that started yesterday, and with Hedie‘s talk there, we have posted on arXiv a set of comments on particle learning. The arXiv paper contains several discussions but they mostly focus on the inevitable degeneracy that accompanies particle systems. When Lopes et al. state that  is not of interest as the filtered, low dimensional  is sufficient for inference at time t, they seem to implicitly imply that the restriction of the simulation focus to a low dimensional vector is a way to avoid the degeneracy inherent to all particle filters. The particle learning algorithm therefore relies on an approximation of  and the fact that this approximation quickly degenerates as t increases means that this approximation impacts the approximation of . We show that, unless the size of the particle population exponentially increases with t, the sample of ‘s will not be distributed as an iid sample from .  The graph above is an illustration of the degeneracy in the setup of a Poisson mixture with five components and 10,000 observations. The boxplots represent the variation of the evidence approximations based on a particle learning sample and Lopes et al. approximation, on a particle learning sample and Chib’s (1995) approximation, and on an MCMC sample and Chib’s (1995) approximation, for 250 replications. The differences are therefore quite severe when considering this number of observations. (I put the R code on my website for anyone who wants to check if I programmed things wrong.) There is no clear solution to the degeneracy problem, in my opinion, because the increase in the particle size overcoming degeneracy must be particularly high… We will be discussing that this morning. 	 0 Comments
Hack-at-it 2010	https://www.r-bloggers.com/2010/06/hack-at-it-2010/	June 4, 2010	Di Cook		 1 Comment
Because it’s Friday: Stochastic degradation	https://www.r-bloggers.com/2010/06/because-its-friday-stochastic-degradation/	June 4, 2010	David Smith	When you upload a video to YouTube, they don’t store a perfect digital copy of your media file. The video is actually re-encoded into the MPEG-4 video format. This saves space on YouTube’s servers, but also introduces some random degradation to the video (as a result of the lossy compression process). So what happens if you then save that converted video, and upload it to YouTube again? The video (and audio) is degraded a little bit more. And what if you repeat that process 750 times? Here’s the result:   Althouse:  I was thinking that the problem of making a copy of a copy of a copy had become a thing of the past because of digital files. 	 0 Comments
Connecting Revolution R to MySQL on Windows	https://www.r-bloggers.com/2010/06/connecting-revolution-r-to-mysql-on-windows/	June 4, 2010	David Smith	My colleague Saar Golde was having some troubles connecting Revolution R to MySQL on Windows (64-bit). Turned out the problem was the lack of an environment variable. He documented the instructions for fixing the problem on Windows 7, below. Thanks, Saar! The Problem: A client is about to send me a couple of large MySQL tables, so I needed to install a MySQL server and connect it to R so I can do some analysis on it. The Process: 1. Install a MySQL server. For some reason I could not get MySQL 5.1 to talk to R using the RMySQL package. Not sure what was wrong – maybe it’s the lack of an available MySQL 5.1 server for 64-bit windows. So I opted to use MySQL 5.0 (the exact version is 5.0.91-community-nt MySQL Community Edition). 2. Install the RMySQL package Downloaded from Revolution’s repository, but CRAN should also work fine. 3. Load RMySQL This is the tricky step: > require(MySQL) does not work right off the bat. You get an error like this one: Error in utils::readRegistry(“SOFTWARE\\MySQL AB”, hive = “HLM”, maxdepth = 2) :Registry key ‘SOFTWARE\MySQL AB’ not found Error : .onLoad failed in ‘loadNamespace’ for ‘RMySQL’ 4. Manually add MYSQL_HOME variable to the list of system variables This is for Windows 7 –  it should not be very different for other versions. Right click on ‘my computer’, choose ‘properties’, click on ‘advanced system settings’, under the ‘advanced’ tab click on ‘Environment Variables…’. Create a new system variable (not a user variable!) named ‘MYSQL_HOME’ and enter the MySQL directory address there. In my case it is ‘C:/Program Files/MySQL/MySQL Server 5.0’ (notice the slash instead of the standard Microsoft backslash). 5. Restart R If there is an alternative way for R to recognize the change in the system variable, that may be preferable. In any case, restarting works. require(RMySQL) works now. 6. Go have a beer. Repeat if necessary. 	 0 Comments
Debugging basics in R	https://www.r-bloggers.com/2010/06/debugging-basics-in-r/	June 4, 2010	VCASMO - drewconway		 0 Comments
How to Create an R Package in Windows	https://www.r-bloggers.com/2010/06/how-to-create-an-r-package-in-windows/	June 4, 2010	Quantitative Finance Collector		 0 Comments
inline 0.3.5	https://www.r-bloggers.com/2010/06/inline-0-3-5-2/	June 3, 2010	Thinking inside the box	"

This is some ways a continuation of the
0.3.4 release 
I had made in December. That release had opened the door for the wide use of inline in our
Rcpp package.
And just how Rcpp has grown, we now have needs beyond the initial change. See
the 
post on Romain’s blog for details, but in a nutshell we are now gaining
 
Last but not least, our thanks to Oleg Sklyar for letting us extend his amazing
inline package for use by
Rcpp.

 "	 1 Comment
Introductory Statistics with R	https://www.r-bloggers.com/2010/06/introductory-statistics-with-r-2/	June 3, 2010	C		 0 Comments
Making Data Work online conference	https://www.r-bloggers.com/2010/06/making-data-work-online-conference/	June 3, 2010	David Smith	O’Reilly is hosting a conference on June 9 on the topic of the analysis of large data sets. The title of the conference is Making Data Work: Ever since Hal Varian proclaimed that data analysis is the sexy career for the coming decade, people have been talking about data. And big data. And even bigger data. This online conference, Making Data Work, brings together four experts on data and data analysis to show you what data is all about. How is data used in successful enterprises? What kinds of tools do you use to work wth the data? How do you use use visual techniques to give you insight to what the data is saying, and present those conclusions to others? Along the way, we’ll talk about topics such as statistics, machine learning, really large databases, and baseball, and we’ll look at industries as diverse as banking and biology. Amongst the speakers are Mike Driscoll of Dataspora, who we’ve featured several times on this blog as an leading innovator of large-scale data analysis using the R language. Mike was also recently quoted in an article about Revolution and R in Forbes Magazine, and he recently published a most thought-provoking article about new tools for big-data analysis, and the processes needed to make the analysis itself help instead of hinder the decision-making process. Also speaking is Joe Adler from LinkedIn, author of the excellent R reference manual, R in a Nutshell. I’m not familiar with the work of the other two speakers, Hilary Mason (Bit.ly) and Ben Fry (Ben Fry LLC), but the abstracts for their talks on (respectively) machine learning and visualization look very interesting.    Best of all, it’s an on-line conference, so you don’t even need to leave your desk to take part! Registration is just US$149, but as a sponsor Revolution Analytics has passes to give away — all you need to do is follow RevolutionR on Twitter for a chance to win one of three passes on Monday. Update: Winners will be selected at random from RevolutionR’s followers at 6PM PDT on Monday and notified via DM. O’Reilly: Making Data Work Online Conference 	 0 Comments
MODIS processing with R, GDAL, and the NCO tools	https://www.r-bloggers.com/2010/06/modis-processing-with-r-gdal-and-the-nco-tools/	June 3, 2010	Adam M. Wilson		 0 Comments
Political Science with R	https://www.r-bloggers.com/2010/06/political-science-with-r/	June 3, 2010	David Smith	"As a discipline, Political Science — the analysis of the theory and practice and politics — has been around for quite a while. (Our own CEO here at Revolution Analytics, Norman Nie, has been a leading academic and author in the field for over 40 years.)  But it’s only in recent years that a deluge of data about politics has erupted: detailed demographic information about constituents; tracking polls taken on a daily basis from dozens of polling firms; campaign donations; information from just about every walk of life that can give insight into a voter’s intentions or reactions to policy. Around election time in particular, the new data is captured and published on a minute-by-minute basis. As a result, advanced statistical techniques that lend themselves to drawing nuances from disparate data streams are increasingly being used to forecast the results of elections. Take one recent example: the British parliamentary elections. Professor Simon Hix and Nick Vivyan of the London School of Economics and Political Science used R to analyse polling data. Their Hix-Vivyan Prediction method pools data from numerous national polls to infer the elections of MPs in each constituency, and thereby predict the outcome of the election. R is an ideal system for this kind of analysis: not only does it provide the advanced statistical techniques to do the analysis and make the predictions, but because it’s a scripted language they were able to re-run the analysis on a day-by-day basis as new polling data was released and present the results as beautiful graphics like these: 
 On the day before the election the Hix-Vivyan model predicted the Conservatives would win 293 seats, shy of the 326 required to avoid a hung parliament. (A hung parliament was indeed the result, with Conservatives at 306 seats, eventually forming a coalition with the Liberal Democrats.)  This is just one example of political scientists using advanced statistical techniques to predict election outcomes. Nate Silver at fivethirtyeight.com also tracked the UK election closely, and his in-depth analyses of the US House, Senate and Presidential elections are must-reads for any junkie of the US election system. (Incidentally, Nate has also recently branched out into ranking the World Cup Soccer teams using statistical techniques.) And Andrew Gelman regularly posts about political analysis (always with a Bayesian perspective, and often using R), for example on the recent primary elections in the US. And Boris Shor (from the University of Chicago) often publishes in-depth analysis of individual races in US elections at his blog (click here to download a case study on how he uses Revolution R Enterprise for the analysis). In fact, there’s so much going in statistical analysis of US elections that I think I’ll we’ll to come back to the topic in a follow-up post. [Update: Corrected spelling of both Hix and Vivyan. Apologies to both.] British politics and policy at LSE: One day to go: Hix-Vivyan Prediction up to 3 May   "	 0 Comments
graph gallery collage	https://www.r-bloggers.com/2010/06/graph-gallery-collage/	June 3, 2010	romain francois	"It does not quite respect the one color only requirements of the mango’s t-shirt contest, but I played with Shape Collage and the graphics from the graph gallery to make this



 "	 0 Comments
Plans for a Real tty Connection	https://www.r-bloggers.com/2010/06/plans-for-a-real-tty-connection/	June 2, 2010	Matt Shotwell	I reverted the name of the tty connection that I described previously, to “serial” connection and updated the patch to R 2.11.1. This name is more appropriate to its function and also makes way for another patch I am working on which implements the full POSIX terminal interface (essentially all of termios.h). The connection provided in the new patch will be more appropriately named a tty connection. I think this type of connection could serve a variety of purposes. Most of the items below are motivated from recent R-help and R-devel posts. 	 0 Comments
Baseball, basketball, and (not) getting better as time marches on	https://www.r-bloggers.com/2010/06/baseball-basketball-and-not-getting-better-as-time-marches-on/	June 2, 2010	dan	PROS ARE NOT GETTING BETTER AT FREE THROWS  Rick Larrick recently told Decision Science News that baseball players have been getting better over the years in a couple ways. First, home runs and strikeouts have increased. The careless or clueless reader might note that this is curious, for from the batter’s perspective home runs are a good thing and strikeouts are a bad thing. What’s going on? Batters may be swinging harder, increasing the chance of both. The purported improvement is a result of the benefit of a home run being greater than the cost of a strikeout. After all, a home run results in at least one run, often more, and runs are a big deal since the typical team earns only about 5 of them per game. DSN wondered how the players learned to swing harder from one decade to the next. Was it based on feedback from coaches? Or from fans / media attention? According to Larrick, the number of attempted stolen bases has decreased over the years. Apparently, it is only worth it to steal if one can pull a very high percentage of the time, higher than had been believed in previous years (anyone know the stat?). So while crowds (presumably) like the action of stolen bases, players do not respond by doing it more. Winning seems more important than pleasing the crowd, which is a strike against the fan-feedback hypothesis. After our post on winning back-to-back baseball games, some folks like our friend Russ Smith made comparisons to the hot hand effect. There is something to it. However, in the baseball example one starts with a prior of .5 (since one doesn’t even know which two teams are playing), while in basketball the chance a pro will make a free throw is about .75 (since one can condition on the player being a pro). What is surprising is that in both cases, the past success tells you next to nothing. This conversation lead your Editor to find this NY Times article which shows that, surprisingly, pro basketball players are not getting better at free throws over the years. So, the question to the readers is: Why do some athletic abilities improve as history marches on (e.g., running speeds, batting, base-stealing) and others do not (e.g., free throws)? P.S. For the record, Decision Science News is not becoming a sports blog. It is just a phase the Web site is going through. That said, there has been interest in seeing this kind of result in other sports, so that analysis will be coming in future posts, in glorious, glorious R and ggplot2. (Don’t know R yet? Learn by watching: R Video Tutorial 1, R Video Tutorial 2) Photo credit: http://www.flickr.com/photos/cakecrumb/4398699952/. A cupcake was chosen because Jeff gave us empirical evidence that people like cupcakes much more than a control food. 	 0 Comments
RODM: An R package for Oracle Data Mining	https://www.r-bloggers.com/2010/06/rodm-an-r-package-for-oracle-data-mining/	June 2, 2010	C		 0 Comments
Color Palettes in R	https://www.r-bloggers.com/2010/06/color-palettes-in-r/	June 2, 2010	C		 0 Comments
Supercharging business analytics with R	https://www.r-bloggers.com/2010/06/supercharging-business-analytics-with-r/	June 2, 2010	David Smith	Really pleased with how our webinar today, Supercharge BI and Dashboards with Predictive Analytics, turned out. Steve Miller from OpenBI gave a great introduction to the motivations and need for predictive analytics. Andrew Lampitt from Jaspersoft was next up, with an overview of the Jaspersoft system for BI reports and dashboards. My section focused mainly on R and its capabilities for data visualization and predictive analytics (i.e. statistical modeling), plus some of the enhanced capabilities of Revolution R. Steve wrapped everything together with a very cool demonstration of using a simple interface in Jaspersoft to segment the Current Population Survey data set and compare various models of the subsets (GAMs, MARS, and quantile regressions, to name just a couple) using Trellis graphics. It was a record-breaking webinar in terms of attendance too — it seems like there’s real interest in integrating R-based analytics into BI systems. If you missed it, don’t worry: the slides and a replay (in WMV format) are now available for download from the link below. In particular, check out the links in Steve’s slides, where he provides some great resources on BI books and predictive methods in R. Revolution Analytics webinars: Supercharge BI and Dashboards with Predictive Analytics 	 1 Comment
Data preparation for Social Network Analysis using R and Gephi	https://www.r-bloggers.com/2010/06/data-preparation-for-social-network-analysis-using-r-and-gephi/	June 2, 2010	prasoonsharma		 0 Comments
Picture of me with Chengwu Yang, PhD	https://www.r-bloggers.com/2010/06/picture-of-me-with-chengwu-yang-phd/	June 2, 2010	Matt Shotwell	 	 0 Comments
Marking Your Graphics	https://www.r-bloggers.com/2010/06/marking-your-graphics/	June 2, 2010	Millsy		 0 Comments
More Free Online Instructional Videos on R	https://www.r-bloggers.com/2010/06/more-free-online-instructional-videos-on-r/	June 2, 2010	Jeromy Anglim		 0 Comments
Singapore Presentations	https://www.r-bloggers.com/2010/06/singapore-presentations/	June 1, 2010	ellis		 0 Comments
Indian Financial Market Data	https://www.r-bloggers.com/2010/06/indian-financial-market-data/	June 1, 2010	ellis		 0 Comments
Trading for Speed in H2H Fantasy Leagues	https://www.r-bloggers.com/2010/06/trading-for-speed-in-h2h-fantasy-leagues/	June 1, 2010	Millsy		 0 Comments
R Commander – a good introductory GUI for R	https://www.r-bloggers.com/2010/06/r-commander-%e2%80%93-a-good-introductory-gui-for-r/	June 1, 2010	Ralph	The R software is very powerful and flexible but one of the complaints of new users is that the learning curve is steep and can be daunting. There have been various projects to create GUIs for R with varying levels of sophistication, one of which is R Commander by John Fox. Fast Tube by Casper This interface is worth considering for beginners as it provides access to commonly used functions while also producing the R code required for the analysis which will hopefully smooth out the learning curve for new starters. Other bloggers have mentioned this system see here and here. The GUI is reasonably straightforward as the intention does not appear to be to provide a point and click interface to the wide range of functions available in R. The above video covers the basics and how to import data from text files, such as tab delimited or comma separated variable file formats. It is also possible to import data from Excel spreadsheets assuming that a bit of pre-processing has been undertaken by the user, which is never a bad thing prior to loading data for analysis. One interesting element of the GUI is a menu dedicated to continuous and discrete distributions that allow calculation of quantiles, probabilites or random numbers in addition to plotting a graph of the distribution for a given set of parameters. This could be a useful tool for understanding more about distributions. Other useful resources are provided on the Supplementary Material page. 	 0 Comments
Reminder: Supercharging BI with R webinar tomorrow	https://www.r-bloggers.com/2010/06/reminder-supercharging-bi-with-r-webinar-tomorrow/	June 1, 2010	David Smith	"If you haven’t done so already, it’s not too late to register for tomorrow’s 1-hour webinar, Supercharge BI and Dashboards with Predictive Analytics. I’ll be one of the presenters, talking about R and its applications for business intelligence. The topics and speakers will be: 
Introduction: Analytics and BI (Steve Miller, OpenBI)
How BI Complements Predictive Analytics (Andrew Lampitt, Jaspersoft)
The Power of R for Business Intelligence (David Smith, Revolution Analytics)
Demonstration (OpenBI)
Q & A

   The live webinar will start at 9AM Pacific (12 noon New York time) tomorrow, Wednesday June 2. Registrations are open at the link below. Revolution Analytics webinars: Supercharge BI and Dashboards with Predictive Analytics. "	 0 Comments
Program for useR! 2010 announced	https://www.r-bloggers.com/2010/06/program-for-user-2010-announced/	June 1, 2010	David Smith	The program of invited lectures, contributed talks and posters for this year’s annual R user conference useR! 2010 is now available, and it looks to be another excellent event. (Revolution Analytics is a sponsor of useR! 2010.) Of note this year is an increasing focus on commercial applications of R, with no less than three multi-talk sessions on applications and uses of R in business. There will also be sessions on uses of R in biostatistics, grid computing, visualization, social network analysis and much, much more. The conference will close with an invited lecture from the guru of open-source Richard Stallman on “Free Software in Ethics and in Practice”. The conference takes place July 20-23 near Washington, DC and registrations are still open. useR! 2010: Conference Program   	 0 Comments
MLB Baseball Pitching Matchups ~ grabbing pitcher and/or batter codes by specify game date using R XML	https://www.r-bloggers.com/2010/06/mlb-baseball-pitching-matchups-grabbing-pitcher-andor-batter-codes-by-specify-game-date-using-r%c2%a0xml/	June 1, 2010	apeescape	"MLB Gameday stores its game data in XML format, with the players denoted in ID numbers. To find out who is who, the codes are stored in pitchers.xml or batters.xml of each game. My DownloadPitchFX.R script can download the ID numbers, but it doesn’t look to see who the ID is because of the extra processing time. But to use the data (say in RMySQL), it helps to have another script that figures out the ID number for any player. The following script (GetPitcherBatterCodes.R) requires the last and/or first name of the player, the team that he plays on and the specific date the player is assumed to play. It outputs a data frame with the matched name (however many) and their ID numbers. You can also let just.player = FALSE to download all of the players listed in that game (although it does that anyways). The input for the team name is fairly general. You can use the codes that are specified in Gameday (“SF”, “sfn”), or the actual city of the team (“San Francisco”), or its team name (“Giants”). Some output: 
Filed under: Baseball, R, XML        

 "	 1 Comment
Recent picture of my niece Lily	https://www.r-bloggers.com/2010/06/recent-picture-of-my-niece-lily/	June 1, 2010	Matt Shotwell	 	 0 Comments
Access attribute_hidden Functions in R Packages	https://www.r-bloggers.com/2010/06/access-attribute_hidden-functions-in-r-packages/	June 1, 2010	Matt Shotwell	"Maybe the title should have been prepended with “Don’t…” The source code of R is littered with “attribute_hidden” declarations. These declarations attempt to ensure that the variable or function may only be accessed by code in the core R distribution, and not by R extension packages. Generally there is a good reason for this. For example, there is no good reason why R extension packages should call do_scan, the C level wrapper for the R function scan. If package code needs to use scan, it should call it with R code. Package developers should consult with the R development community when they want to access attribute_hidden functions, there may be alternatives. Also, It would be much more elegant and useful to convince an R core developer to simply modify the attribute_hidden declaration or open up access to an API. In the meantime, we may want to use other methods to access attribute_hidden functions at the C level. For instance, not every attribute_hidden function or variable has an R level counterpart. Another reason to access attribute_hidden symbols is to partially expose the R connections API, which is currently not accessible to R extension packages. However, comments in the R source code hint that the connections API may be opened to R packages at some point in the future ( hopefully   ).  Ideally, the R source code or R CMD check mechanism would be modified to prevent the sort of trickery presented below. I hope this post will contribute to the tireless efforts of the R core development team in improving R. The rest of this post is dedicated to describing how attribute_hidden symbols may be accessed on the Linux/BSD/Mac OS X? platforms in a fairly portable manner. It may also be possible to extend this approach to Windows. Those who just want to see a package that demonstrates the method, see example_1.0.tar.gz DISCLAIMER: It is possible to use this method with packages that pass R CMD check. However, accessing attribute_hidden symbols circumvents an R safety mechanism and should not be used in production packages. The attribute_hidden declaration is defined in multiple places in the R source code, including src/include/Defn.h: This declaration is a compiler extension that affects how the symbol may be accessed. From the gcc documentation, the “hidden” attribute has the following meaning: Hidden visibility indicates that the symbol will not be
placed into the dynamic symbol table, so no other “module”
(executable or shared library) can reference it directly.
….
Note that hidden symbols, while they cannot be referenced
directly by other modules, can be referenced indirectly via
function pointers Clearly, attribute_hidden functions may be accessed using function pointers, which requires knowing the address where the function is loaded in memory when R is executed. In general, this information is not accessible to R packages. However, this information is contained in the R executable file. The trick is to extract the address of the function we want to use, and then construct a function pointer to use it. Suppose we want to use the attribute_hidden function getConnections_no_err, defined in the file src/main/connections.c: The first step is to find out the address of this function in memory when R is executed. As I mentioned this information is contained in the R executable file, usually located at $(R_HOME)/bin/exec/R. The objdump program in the binutils package may be used to extract such information: In this example, objdump outputs several pieces of information associated with the function getConnection_no_err, the first of which is the (hex) address where the function will be loaded in memory when R is executed. We can isolate this bit of information with an additional command: The next step is to construct a function pointer that that we can assign this address, and thereby call the function. In our package source code, we would use the following declarations and then we could call the function with a statement like However, in this particular case, we wouldn’t be able to do much with the Rconnection pointer, other than pass it to another function. In order to get any useful information about the Rconnection, we would first need to copy the struct Rconn declaration from the file src/include/Rconnections.h file to our package source code. Of course, this is not considered “good” programming practice. However, Rconnections.h is not a public header. Until this header is made public, there would be little alternative. This is the gist of how attribute_hidden symbols may be accessed in package code. There are various tricks that may be utilized to automate collecting the symbol address from the R executable. For those interested, I have prepared a small R extension package example_1.0.tar.gz containing a single function get_mode that is passed a connection description and returns the mode (e.g.""rw""), by accessing the internal Rconnection pointer. For example:  Pay special attention to the configure.ac file. This is where most of the work of finding the address for getConnection_no_err occurs. Also, note that this package passes R CMD check under R-2.11.0. The trick of accessing attribute_hidden functions is not a “robust” method, as we might say in statistics. That is, it is easily broken. If objdump, grep, awk are not installed, the installation will fail. Also, if an incorrect address is found, installation may fail, or get_mode may result in a segmentation fault, or may produce other odd behavior. Please leave feedback if you try it out. "	 0 Comments
How to install R packages from source on Mac OS X	https://www.r-bloggers.com/2010/07/how-to-install-r-packages-from-source-on-mac-os-x/	July 31, 2010	Jon	"In my last post about installing the rgdal R package on Mac OS X, I had apparently glossed over the last step of compiling the R package from source.  I hadn’t realized that Mac OS X actually doesn’t come default with the necessary software to do this.  Most times, prepackaged binaries work fine but occasionally, packages may not have a binary available or may need to be customized.  For example, in rgdal, where the default search paths are incorrect for the GDAL frameworks on Mac OS X. In order to compile a package from the source code, you’ll need the package source (typically a .tar.gz file on Mac OS X) and a compiler (e.g. gcc).  In my opinion, the easiest way to set up the necessary tools is to install Xcode from the Apple Developer site. You’ll need to register (free) for an Apple Developer Connection account to access the download.
(Update, May 20, 2011: Since version 4, Xcode is no longer free for non-developers but can be purchased for $4.99 from the Mac App Store.  Xcode 3 remains free from the Apple Developer site and is sufficient if you just need the compilers installed). Once Xcode has been installed, you’ll then have all the required tools to compile R packages from source.  To do so, simply navigate in Terminal to the folder you saved the source to (the .tar.gz file), and run: R CMD INSTALL packagename.tar.gz

 or some variety of the above with any custom configurations. The R command will automatically run the appropriate commands to compile the package. Happy compiling! "	 0 Comments
StatProb [wiki]	https://www.r-bloggers.com/2010/07/statprob-wiki/	July 31, 2010	xi'an	Via the [financial and technical] support of Springer, probability and statistics societies are launching a specialised wiki called StatProb. It operates as a wiki in that authors can submit short articles on any topic, with further co-authors joining in later to improve those articles, but with the contents guaranteed via the filter of an editorial board. The members of the board and subsequent associate editors are nominated by the statistical societies involved in the project. (For instance, I was nominated by the Royal Statistical Society., Susie Bayarri by ISBA, George Casella by the ASA, etc.) As a starting basis, StatProb will reproduce a few hundred entries from the incoming International Encyclopedia of Statistical Sciences edited by Miodrag Lovric (to which I contributed). Obviously, the wiki will only work if enough contributors submit their piece and make StatProb a reference for statistics. I joined the project because, as opposed to costly encyclopedias, wikis are living things that evolve with the field (if enough activity is maintained by its members) and that can be accessed freely by all. Another good thing about StatProb is that entries are submitted in LaTeX, making the output looking fairly reasonnable. (To start the ball rolling, we submitted this short piece on random number generation with George Casella, exctacted from an older piece that had been sitting around for a while. It does not mean to be the only piece on random number generation, nor on MCMC or Monte Carlo methods. And it can be updated and augmented as in other wikis.) Unless I am confused, I think the site will be officially launched at JSM 2010 in Vancouver this weekend. 	 0 Comments
Read a new book…	https://www.r-bloggers.com/2010/07/read-a-new-book/	July 31, 2010	Manos Parzakonis	"From the book website : IPSUR stands for Introduction to Probability and Statistics Using R,
ISBN: 978-0-557-24979-4, which is a textbook written for an
undergraduate course in probability and statistics. The approximate
prerequisites are two or three semesters of calculus and some linear
algebra in a few places. Attendees of the class include mathematics,
engineering, and computer science majors.  Now, there is a new way to read R books (anyway, new to me!) 

 "	 0 Comments
highlight 0.2-2	https://www.r-bloggers.com/2010/07/highlight-0-2-2/	July 31, 2010	romain francois	I’ve released highlight 0.2-2 to CRAN. This release adds the possibility to control the font size of the latex code generated by sweave (when using the driver that is provided by highlight) For example, this C++ code (using Rcpp) will be highlighted in tiny size.  This is something we had to do manually when preparing the slides for useR! 2010 	 0 Comments
Save R plot as a BLOB	https://www.r-bloggers.com/2010/07/save-r-plot-as-a-blob/	July 30, 2010	Lee	I recently posed a question on stackoverflow on whether anyone knew an efficient way to save an R plot to a MySQL database as a BLOB.  My plan was to use my personal desktop to perform R routines and save them to a web server, where they could then be accessed and displayed on a web page using a little PHP magic.  After getting numerous responses on what a terrible idea this was, I was able to piece my own solution together.  The steps are fairly simple.  First, save the plot as a temp file, Second, read it back into R as a binary string.  Third, insert the binary text into the database using the RODBC library.  The code snippet is below.  I understand the ‘other way’ to do this (and most would argue better) would be to ftp the file to the server and save only the file path to the database.  I guess I’m just being a bit lazy by letting the database do all the storage details, but so far so good. 	 0 Comments
Thinking about Graphs	https://www.r-bloggers.com/2010/07/thinking-about-graphs/	July 30, 2010	C		 0 Comments
Free R Chart iPhone App	https://www.r-bloggers.com/2010/07/free-r-chart-iphone-app/	July 30, 2010	C		 0 Comments
Viewing a bipartite network	https://www.r-bloggers.com/2010/07/viewing-a-bipartite-network/	July 30, 2010	Timothée	I published two functions to visualize trophic networks with three or n levels, although most of my work consists in dealing with two-mode (bipartite) networks. The R library bipartite provides two functions to visualize such webs (plotweb and visweb), and I am generally happy with the results of the latter. However, I sometimes find it difficult to use at it seems to require link intensity to be given as integers, and I never have this information. Even if it is easy to do something like it is not really satisfactory. There are also some other minor things with the bipartite package (namely, it requires the matrix to be entered « the wrong way », i.e. with the columns where one should expect to find the rows) and the graphical output of this particular function that make me uncomfortable using it. I wrote a very short piece of R code that uses lines from the original function (to be exhaustive : the calculation of the margin size, and the sorting of the matrix to give it an ecological meaning), and obtained the following output quite easily:  I quite like the look of this, and I think that it is really easier to follow who interacts with who by just « sliding » along the grey lines. You can get the R code here. 	 0 Comments
Because it’s Friday: Double Rainbow	https://www.r-bloggers.com/2010/07/because-its-friday-double-rainbow/	July 30, 2010	David Smith	It’s sunny up here in Seattle (I’m attending BioConductor 2010 — Revolution’s a sponsor) now that the morning fog has lifted. So in honor of sunny Friday afternoons, I bring you the Double Rainbow song:   (If you’ve been in a wi-fi-less cave for the past couple of weeks, check out the original.) 	 0 Comments
An analysis of the Wikileaks data with R	https://www.r-bloggers.com/2010/07/an-analysis-of-the-wikileaks-data-with-r/	July 30, 2010	David Smith	"At his Zero Intelligence Agents blog, Drew Conway has taken on the task of performing a quantitative analysis (using R, of course) of the controversial Afghanistan document dump  from Wikileaks. He’s started with an analysis of the overall flow of information in the five Afghanistan regions, categorized by type of activity (enemy, neutral, etc.).  
 (Click to enlarge.) It’s a 10,000-foot view of the data to be sure, but even show it does show some interesting trends: the relative quiet of some regions, surges and ebbs in the war, and the interchanges of activity between the various agents. Drew offers more analysis: Given the nature of the reports, we would expect a noticeable degree of seasonality (peaks and valleys) given the natural ebb and flow of war. Any drastic deviations from this expectation could indicate a strong degree of selection on the part of Wikileaks. As you can see, however, the data generally do fit this expectation. Note the dramatic upward trending seasonality present in the heavy reporting areas of RC EAST and RC SOUTH. Perhaps more interestingly, though, is the sudden increase in the number of NEUTRAL reports present in the data for RC EAST and RC CAPITAL for the period roughly between mid-2006 and mid-2008. Be sure to follow ZIA as Drew dives deeper into the analysis of this fascinating data set. Zero Intelligence Agents: Wikileaks Afghanistan Data "	 0 Comments
New Fantasy Ball Junkie Article	https://www.r-bloggers.com/2010/07/new-fantasy-ball-junkie-article/	July 30, 2010	Millsy		 0 Comments
Visualizing 3d data – plotting quartiles separately	https://www.r-bloggers.com/2010/07/visualizing-3d-data-plotting-quartiles-separately/	July 30, 2010	respiratoryclub	"In this previous post, we’ve looked at displaying three dimensional data.  One major problem is when there is a high density of data, it can be difficult to see what’s going on in a 3 dimensional plot. One way of looking at the data in more detail is to break it up.  Take a look at this graph:
 This is a plot of data of air quality in Nottingham, UK, taken hourly in 2009 (the code to create it in base R is on the bottom of the page).  On the left is a scatterplot of NO2 against ozone (plot A).   The different colours indicate grouping the data by the level of ozone into quartiles.  On the right are plots of the NO vs NO2 for the same data, but a  separate plot for each quartile of the ozone data.  The points are all colour co-ordinated, so the red points indicating the upper quartile of the ozone data in plot A are matched by red points in plot B. So you can see by comparing plot E and D, that at the lowest quartile of ozone levels, there is a greater spread of both NO2 and NO. How this is done is pretty simple (most of the code is to make things vaguely pretty).  Essentially, the values for x,y and z are put into a matrix xyz.  The rows of the matrix are ordered according to the z variable.  The rows which deliniate each quartile are calculated, and then the plots for B to E of x vs y are drawn, using only the rows for that quartile.  The axes are plotted so that they are the same scale for each of the plots.  There’s not much room for the axis labels – so these are added afterwards with the legend command.   Then on the left the plot for y (on the horizontal axis) and z (on the vertical axis) is drawn, with some added lines to show where the boundaries of each quartile lie.  The colours are stored in the xyz matrix in the col column.  Like most of my code, the graph is portable, you just need to input different values for x, y and z and re-label the names for each variable.  The original dataset is the same one which I have used for my previous posts.  It is from the UK airquality database.  If you copy this file into your working directory and run the code below, you’ll repeat the plot. Any suggestions for improvements / comments would be most appreciated! "	 0 Comments
Using Optmatch and RItools for Observational Studies	https://www.r-bloggers.com/2010/07/using-optmatch-and-ritools-for-observational-studies/	July 29, 2010	Mark Fredrickson	I am a contributor to the optmatch and the RItools packages for R. These two packages are separate, but complimentary. Both packages provide tools for adjusting observational data to exhibit “balance” on observed covariates. In a randomized control trial, treatment and control groups should have identical distributions over all covariates, observed and unobserved. Matching provides a method to create smaller groups in an observational study that are similarly balanced. Balance can be quantified so that alternative matches can be compared. When an acceptable match has been found, analysis can then proceed as if nature provided a blocked, randomized study. Both optmatch and RItools use a canonical dataset consisting of nuclear plants. From help(nuclearplants): The data relate to the construction of 32 light water reactor (LWR) plants constructed in the U.S.A in the late 1960’s and early 1970’s. The data was collected with the aim of predicting the cost of construction of further LWR plants. 6 of the power plants had partial turnkey guarantees and it is possible that, for these plants, some manufacturers’ subsidies may be hidden in the quoted capital costs. With these data, we may wish to know if certain variables lead to higher or lower construction costs. One particular variable is pr, an indicator if a previous lightwater reactor at the same location was present. Such an installation might significantly increase or decrease costs. The rest of this document uses matching and balance testing to provide an answer to just that question. I start by loading the packages and the data: Before getting into the matching process, lets take a quick look at the balance of the data on all variables, except cost and pr, the LWR indicator. xBalance, among other tests, provides an omnibus balance test across any number of variables. This test compares the null hypothesis of “the data are balanced” against the alternative hypothesis of a lack of balance, where balance is what we would expect in a randomized trial with the same sample size. The test follows a chi-squared distribution, which xBalance will happily report: With a reported p-value of 0.19, the balance of this sample is not terrible (by conventional levels of hypothesis testing), but we might prefer something closer to 1. While there is no a priori p-value we should prefer, experience indicates that p-values in the neighborhood of .5 are achievable and mimic true randomized designs (though optimal balance levels are a subject of ongoing research). A full discussion of matching procedures is beyond the scope of this document (see Rosenbaum (2010) for a more comprehensive discussion). In brief, matching attempts to group units with similar covariates, as if they had been blocked in a randomized experiment. The optimal match would be two units identical on every variable, observed and unobserved. In most datasets, no two units will be identical on all observed covariates. Instead, we can use a measure that summarizes all covariates and match based on the summary. The propensity score, the probability of receiving treatment given the observed covariates, has been a popular summary measure (for more on the theory, see Rosenbaum and Rubin (1983)). I’ll use a logistic regression to estimate the propensity scores of my observations, using a subset of the available variables: With a propensity model, optmatch provides several functions for computing matched sets of observations. The fullmatch function takes a treatment by control matrix containing distances between observations and returns a factor indicating the set membership, if any, of all observations. Computing the distance matrix is simple using the mdist function. This function takes a linear model, a function, or a formula to produce distances based on propensity models, aribtrary user functions, or Mahalanobis distances between observations. We’ll use the propensity model. See the help page for mdist for the other alternatives.  We can compare the first match with a second, in which a caliper is placed on the date variable. This will constrain the matching algorithm, disallowing matches on observations with widely differing date values, even if the over all propensity scores are similar. Calipers can lead to poorer matches on observed variables but provide a method by which researchers can include subject matter information in the matching process. For example, if the cost of construction decreased over time due to increased efficiency in construction practices. With two possible matches, do either produce adequate balance? As noted previously, the RItools package provides a method of quantifying balance in a matched set. The method (discussed in detail in Hansen and Bowers (2008)) compares treatment and control units within each block on a difference of means for each variable. Combining the these differences follows a chi-squared distribution. We can compare all the matches at the same time, along with the raw data (see the strata argument). Both matches provide good balance. With a value of 0.821 we might be tempted to prefer the unconstrained match; however, with a p-value of 0.345, the match with a caliper also provides reasonable assurances of balance. As either provides plausible balance, researchers might choose to concentrate on substantively important covariates. When xBalance reports “std.diffs” (as above), we can plot the result to get a visual picture of balance on each covariate.  Since we now have data that approximates a randomized experiment, we can use the same techniques to analyze this data as any blocked randomized experiment. For example, one-way ANOVA using pr as the treatment factor and m1 as the blocking factor. Under conventional levels, we do not observe either the treatment or the blocking factor reach statistical significance. So we can conclude that existing lightwater reactors do not have an effect on construction costs that we can differentiate from chance. In the analysis, I chose one of two plausible matches. It so happened that I selected the match with the larger p-value. Does this indicate that we should select the match with the highest p-value, as it most closely approximates a randomly allocated treatment? I would caution against that conclusion. Within the set of matches that are plausibly balanced, it is difficult to argue that one match is truly better than another. While in expectation, randomized treatments are perfectly balanced, in pratice, small deviations should be expected (with fewer deviations in larger experimental populations). In short, don’t sweat the small stuff. Find a reasonable match and go with it. In fact, you may find that matches with lower p-values provide interesting substantive results. Here is an analysis of the second match, which included a caliper on the date of construction: This matching indicates a significant blocking effect, which suggests that limiting matches by date may have something to do with the resulting costs. If we had blindly pursued higher p-value matches, we might not have observed this interesting result. 	 0 Comments
The Mule goes SURFing	https://www.r-bloggers.com/2010/07/the-mule-goes-surfing/	July 29, 2010	Stubborn Mule	 A month ago I posted about “SURF”, the newly-established Sydney R user forum (R being an excellent open-source statistics tool). Shortly after publishing that post, I attended the inaugural forum meeting. While we waited for attendees to arrive, a few people introduced themselves, explaining why they were interested in R and how much experience they had with the system. I was surprised at the diversity of backgrounds represented: there was someone from the department of immigration, a few from various areas within the health-care industry, a group from the Australian Copyright Council (I think I’ve got that right—it was certainly something to do with copyright), a few from finance, some academics and even someone from the office of state revenue. Of the 30 or so people who came to the meeting, many classed themselves as beginners when it came to R (although most had experience with other systems, such as SAS). So if there’s anyone out there who was toying with the idea of signing up but hesitated out of concern that they know nothing about R, do not fear. You will not be alone. The forum organizer, Eugene Dubossarsky, proceeded to give an overview of the recent growth in R’s popularity and also gave a live demo of how quickly and easily you can get R installed and running. Since there were so many beginners, Eugene suggested that a few of the more experienced users could act as mentors to those interested in learning more about R. As someone who has used R for over 10 years, I volunteered my services. So feel free to ask me any and all of your R questions! As well as being a volunteer mentor, I will have the pleasure of being the presenter at the next forum meeting on the 18th of August. Regular readers of the Stubborn Mule will not be surprised to learn that the topic I have chosen is The Power of Graphics in R. Here’s the overview of what I will be talking about: In addition to its statistical computing prowess, R is one of the  most sophisticated and flexible tools around for visualizing  quantitative data. It can produce a wide variety of chart types,  including scatter plots, box plots, dot plots, mosaic plots, 3D charts  and more. Tweaking chart settings and adding customized annotations is a  breeze and the charts can readily be output to a range of formats  including images (jpeg or png), PDF and metafile formats. Topics covered in this talk include: Anyone  who ever has a need to visualize their data, whether simply for  exploration or for producing slick graphics for reports and  presentations can benefit from learning to use R’s graphics features.  The material presented here will get you well on your way. If you have  ever been frustrated when trying to get charts in Excel to behave  themselves, you will never look back once you switch to R. For those of you in Sydney who are interested in a glimpse of how I use R to produce the charts you see here on the blog, feel free to come along. I hope to see you there! 	 0 Comments
RcppArmadillo 0.2.4	https://www.r-bloggers.com/2010/07/rcpparmadillo-0-2-4/	July 29, 2010	Thinking inside the box	"

This release upgrades the included Armadillo version to 0.9.52
(see here for Conrad’s high-level changes).
We had to make two minor tweaks.  In the fastLm()
help page example we switched from inv() to pinv()
The short NEWS file extract follows:

 
More information is on the 
RcppArmadillo page.
Questions, comments etc should go to the
rcpp-devel mailing list
off the R-Forge page.

 "	 0 Comments
Red-R	https://www.r-bloggers.com/2010/07/red-r/	July 29, 2010	Shige		 0 Comments
IMIS & AMIS	https://www.r-bloggers.com/2010/07/imis%c2%a0%c2%a0amis/	July 29, 2010	xi'an	A most interesting paper by Adrian Raftery and Le Bao appeared in the Early  View section of Biometrics.  It aims at better predictions for HIV prevalence—in the original UNAIDS  implementation, a naïve SIR procedure was used, based on the  prior as importance function, which sometimes resulted in  terrible degeneracy—, but its methodological input is about incremental mixture importance sampling (IMIS), thus relates to the general topic of adaptive Monte Carlo methods I am interested in. (And to some extent to our recent AMIS paper.) Actually, a less elaborate (and less related) version of the IMIS algorithm first appeared in a 2006 paper by Steele, Raftery and Edmond in JCGS in the setting of finite mixture likelihoods and I somehow managed to miss it… Raftery and Bao propose to replace SIR with an iterative importance sampling technique developed in 2003 by Steele et al. that has some similarities with population Monte Carlo (PMC). (A negligible misrepresentation of PMC in the current paper is that our method does not use “the prior as importance function’”.) In its current format, the IMIS algorithm starts from a first guess (e.g., the prior distribution) and builds a sequence of Gaussian (or Gaussian mixture) approximations whose parameters are estimated from the current population, while all simulation are merged together at each step, using a mixture stabilising weight  where the weights  depend on the number of simulations at step r. This pattern also appears in our adaptive multiple importance sampling (AMIS) algorithm developed in this arXiv paper with Jean-Marie Cornuet, Jean-Michel Marin and Antonietta Mira, and in the original paper by Owen and Zhou (2000, JASA) that inspired us. Raftery and Bo extend the methodology to an IMIS with optimisation at the initial stage, while AMIS incorporates the natural population Monte Carlo stepwise optimisation developed in Douc et al. (2008, Annals of Statistics) that brings the proposal kernel closer to the target after each iteration. The application of the simulations to conduct model choice found in the current paper and in Steele   et al. can also be paralleled with the one using population Monte Carlo we conducted for cosmological data in MNRAS. Interestingly, Raftery and Bo (and also Steele  et al.) refer to the defensive mixture paper of Hesterberg (1995, Technometrics), which has been very influential in my research on importance sampling, and (less directly) to Owen and Zhou (2000, JASA), who did propose the deterministic mixture scheme that inspired AMIS. Besides the foundational papers of Oh and Berger (1991, JASA) and West (1993, J. Royal Statistical Society Series B), they also mention a paper by Raghavan and Cox (1998, J. Statistical Simulation & Computation) I was not aware of, which introduces as well a mixture of importance proposals as a variance stabilising technique. 	 0 Comments
Pie Charts in ggplot2	https://www.r-bloggers.com/2010/07/pie-charts-in-ggplot2/	July 29, 2010	C		 0 Comments
A free book on probability and statistics with R	https://www.r-bloggers.com/2010/07/a-free-book-on-probability-and-statistics-with-r/	July 29, 2010	David Smith	G Jay Kerns has published a 400+ page introductory text on Probability and Statistics. All of the examples and illustrations are done using R (as Jay puts it, “The people at the party are Probability and Statistics; the handshake is R”) so if you want to brush up on your probability and learn R at the same time, this might be a good resource. It would also be great for teaching: Jay wrote the book based on an undergraduate course he gave at Youngstown State University. There’s also a plug-in for R Commander to access some of the methods via dialogs. Jay’s book is free, in both senses of the word. You can download the PDF for free from Lulu, or purchase a printed copy for just over $30. Jay has also published all of the LaTeX sources if you want to build the book yourself. And if you’re already using R, you can read the book with just three commands: install.packages(“IPSUR”)library(IPSUR)read(IPSUR) I haven’t read the entire book, but glancing through it, it looks like a comprehensive overview of the basics of Statistics: distributions, hypothesis testing, estimation, linear regression, and even touches on resampling and nonparametric methods. I did want to point out one minor error on page xiii, though. It’s “We’re not in Kansas any more, Toto”, not “We’re not in Kansas any more, Dorothy”. They’d take away my Rainbow Card if I didn’t mention that.  G Jay Kearns: Introduction to Probability and Statistics using R 	 0 Comments
Bootstrap, strap-on, anal-yzing… statistics is getting weirder by the moment	https://www.r-bloggers.com/2010/07/bootstrap-strap-on-anal-yzing-statistics-is-getting-weirder-by-the-moment/	July 29, 2010	danganothererror	"I have spent the better portion of the day trying to get a bootstrap working. I have adapter a pre-written bootstrap function, but I wanted to use a generic function, mostly for reaping fame and glory. My hypothesis was that writing a hand-written, unoptimized function will consume more computer time than the generic, presumably optimized, boot() function. Was I wrong! In this example, I random sample (with replacement) a vector of n values and store them in a matrix. These values that are written to the matrix are percent of values of our initial vector values that are smaller or equal to the bin number (see the code). Once we’ve got all the values, we can calculate the median by columns for further tinkering. There’s a lot of material covering bootstrapping on the Internet, but for a quick reference, see this Wikipedia entry. At this StackOverflow thread, Aniko provided me with valuable information on what I was doing wrong. This is the code I used – first part is the hand written function, and the second part is my attempt at cracking the matter with a generic function. All this produces this graph. Click to enlarge (the picture). We can then try the power of the generic boot::boot() function that comes with the R core packages. The fun part of doing a bootstraping with the boot() function is to figure out how to write the statistic function correctly. If you look at the boot() help page (?boot) you will notice that you need to provide at least two parameters to the statistic argument: data and an index parameter. Help page says that the index parameter is a vector, which is a bit confusing from where I sit. In other words, this is actually a “an empty object” (let’s call the object i for now) that tells the boot() how to crawl over your data. If your data is in a form of a vector, you will place the index as you would use to subset a vector – object[i]. If it’s a data.frame and you want to re-sample rows, you would call  object[i, ]… Let’s see a working example, things may be clearer there. Notice the data[i] in the function. This will tell the boot() function to extract i elements of the data. If we had a data.frame, and rows were what we wanted to sample randomly, we would have written data[i, ].
And now we call the boot function to perform 10 “boots” on our data. And here is the code to plot the output of the boot object (actually, it’s the print.boot object!). If you don’t believe me, try it on your own and see if you get a similar picture as above. The plot thickens! Have I, by doing a bootstrap with a generic function, profited time-wise at all? Here are the elapse times for 10000 iterations for our hand written bootstrap function and the generic, respectively. So you can see, I profit by 0.724 seconds. Was it worth it?   I can, for the moment, only wonder if I could improve the generic function to beat the hand-written one. Does anyone skilled in optimizing chip in any tips? And here’s how a graph with 10000 iterations looks like. The green dashed lines represent a 95% confidence interval, which means that 95% of iterations fall between those two lines. Bootstrap with 10000 iterations. CI is confidence interval. "	 0 Comments
Taking R to the Limit: Parallelization	https://www.r-bloggers.com/2010/07/taking-r-to-the-limit-parallelization/	July 29, 2010	Szilard	Video, slides and code of the talk “Taking R to the Limit: Parallelization” by Ryan Rosario at the Los Angeles area R Users Group in July 2010 as follows. Slides: Download (PDF, 1.91MB) R code: here. Video:  If you have a question to the speaker, please leave a comment below. Also, this site encourages discussions between all interested in the talks (please use the comments for that). 	 0 Comments
Notes from useR! 2010	https://www.r-bloggers.com/2010/07/notes-from-user-2010/	July 29, 2010	Ellen Ko		 0 Comments
Taking R to the Limit, Part I – Parallelization in R	https://www.r-bloggers.com/2010/07/taking-r-to-the-limit-part-i-%e2%80%93-parallelization-in-r/	July 28, 2010	Ryan	" Tuesday night I had the opportunity to present on high performance computing in R, and the Los Angeles R Users’ Group. There was so much to talk about that I had to split my talk into two parts. The first part was parallelization and the second part will be big data (and a bit left over from parallelization including Hadoop). My slides are posted on SlideShare, and available for download here.    The corresponding demonstration code is here. Topics included: Video of the presentation with my commentary:
 The video was created with Vara ScreenFlow and I am very happy with how easy it is to use and how painless editing was. For Part 2, Large Datasets in R, click here. "	 0 Comments
Blogging about R – presentation and audio	https://www.r-bloggers.com/2010/07/blogging-about-r-%e2%80%93-presentation-and-audio/	July 28, 2010	Tal Galili	At the useR!2010 conference I had the honor of giving a (~15 minute) talk titled “Blogging about R”.  The following is the abstract I submited, followed by the slides of the talk and the audio file of a recording I made of the talk (I am sad it got a bit of “hall echo”, but it’s still listenable…) P.S: this post does not absolve me from writing up something (with many thanks and links to people) about the useR2010 conference, but I can see it taking a bit longer till I do that.  —————– This talk is a basic introduction to blogs: why to blog, how to blog, and the importance of the R blogosphere to the R community. Because R is an open-source project, the R community members rely (mostly) on each other’s help for statistical guidance, generating useful code, and general moral support. Current online tools available for us to help each other include the R mailing lists, the community R-wiki, and the R blogosphere.  The emerging R blogosphere is the only source, besides the R journal, that provides our community with articles about R.  While these articles are not peer reviewed, they do come in higher volume (and often are of very high quality). According to the meta-blog R-bloggers.com, the (English) R blogosphere has produced, in January 2010, about 115 “articles” about R. There are (currently) a bit over 50 bloggers (now about 100) who write about R, with about 1000 (now ~2200) subscribers who read them daily (through e-mails or RSS). These numbers allow me to believe that there is a genuine interest in our community for more people – perhaps you? – to start (and continue) blogging about R. In this talk I intend to share knowledge about blogging so that more people are able to participate (freely) in the R blogosphere – both as readers and as writers.  The talk will have three main parts: *  *  * Tal Galili founded www.R-bloggers.com and blogs on www.R-statistics.com *  *  *  Click here to download the audio file  Download (PDF, 5.09MB) 	 0 Comments
RUG Introduction: Los Angeles area R Users Group	https://www.r-bloggers.com/2010/07/rug-introduction-los-angeles-area-r-users-group/	July 28, 2010	Szilard	A nice group of people from academia and industry meeting about once a month at UCLA. Attendance is usually 30-40, but gradually increasing (also about 300 registered members). If you’d like to join, visit the group’s website: http://www.meetup.com/LAarea-R-usergroup/, you’ll find some description of past meetings, and you’ll be able to RSVP for coming ones. The materials of the talks (video, slides, code) will be uploaded right here (https://www.r-bloggers.com/RUG), but if you are in the Los Angeles area don’t miss the discussions after the talks and the networking opportunities offered by the meetings. All events are absolutely free for anyone attending, all you need is to sign up to the website and RSVP for the meetings. If you have questions, please contact the organizers (Szilard Pafka and Jan de Leeuw) via the group’s website. 	 0 Comments
IPSUR book used LyX-Sweave	https://www.r-bloggers.com/2010/07/ipsur-book-used-lyx-sweave/	July 28, 2010	Gregor Gorjanc		 0 Comments
Taking R to Eleven	https://www.r-bloggers.com/2010/07/taking-r-to-eleven/	July 28, 2010	David Smith	R, the open-source statistical software system, is certainly a hot topic these days. It’s been the subject of increasing media interest over the last year or so, and the user community is expanding rapidly: there are now about 40 R user groups around the world, and last week’s worldwide R User Conference was the most successful ever. So why the sudden attention for R? Steve Miller at Information Management posted last week an insightful analysis of why the time is right for R, and Revolution’s role in its commercial success. (Steve also had some kind words to say about this very blog — thanks, Steve! By the way, if you’d like to receive the monthly Revolution newsletter that Steve mentions in his article, you can sign up here.). He charts R’s ascendance in 10 steps:   It’s a great summary, and I agree with all of it. Let me expand on the last two points: Regarding the challenges of building upon R, Revolution has laid out its plan not just to bring scalable, high-performance analysis of large data sets to R, but also to provide a modern Web Services integration platform for R analytics, and to create an easy-to-use GUI for the more casual user. You’ll be hearing much more about those initiatives in the coming weeks. And finally, supporting the open-source R community is a critical part of Revolution’s mission. And not just because it’s the right thing to do: after all, Revolution is building a business on decades of collective work by volunteers to the R Project, starting with the foundation created Robert Gentleman and Ross Ihaka, realized by the dedication of the R Core Group, and expanded by the thousands of contributors of R packages. But also because the R community itself is a key part of the value of R: its innovation, its adaptiveness to new applications, and the resources and help from community members themselves. That’s why Revolution is supporting the R Community in a number of ways. Not just by contributing code to the R project (like the foreach and iterators packages), but also in supporting local user groups, sponsoring R conferences, funding students to do research and development for R, and evangelizing the benefits of R to the media and analyst community. And just last week, we launched inside-R.org, a portal for the open-source R community, to make it easier to find the wealth of R resources around the Web. We’ll keep working on points 9 and 10 in Steve’s list above, and we look forward to continuing the R story … to 11 and beyond. Information Management: The Revolution Analytics Blog   	 0 Comments
Extension to an R Package: brew gets a weave	https://www.r-bloggers.com/2010/07/extension-to-an-r-package-brew-gets-a-weave/	July 28, 2010	Matt Shotwell	I’ve had enough of copy and pasting output from my R session into my email editor, blog, etc. I need something like Sweave for plain text files. In particular, I want the result of parsing with Sweave, but without the latex markup. For example, the Sweave output of this code chunk looks like this But I want something like From my latest reading (today) of the Sweave manual, Sweave does not have a documented option for this. I decided not to explore the code base of Sweave just yet, due to its size (src/library/utils/R/Sweave.R is nearly 1000 lines, and half are related to the RweaveLatex driver). I think it’s probably overkill to write an Sweave driver for my purpose. The brew package, by Jeff Horner is a nice little package that does something similar to Sweave for text files. The source code for brew is still manageable in size, so I decided to start there. I had originally thought that the brew package incorporated the type of functionality I mentioned above, but it does not. However, brew allows the user to provide their own ‘template’ parsing function. Essentially, when brew encounters the delimiters ‘‘ and ‘%%>‘ in a text file, the text within the delimiters is passed to the user-supplied template parsing function, which should return a character vector to be printed in place of the delimited text. In order to duplicate a the desired portion of Sweave functionality in brew , I wrote the following template parsing function. The first block below is a file (featurefull.brew) I modified from the brew source package that demonstrates all of the brew functionality, and the extended functionality provided by brew.weave. The second block shows the default output of brew for this file. The third block shows the output using the brew.weave template function. There are (of course) some limitations. This extension applies only to output that can be captured with the capture.output function in the utils package. Also, brew does not provide a mechanism to propagate changes to the R environment made by template () code. Also, I might like to simultaneously HTML escape the output for easy pasting into hypertext documents. This could be done with additional template parsers in brew. However, you can only use one template parser at a time. Another interesting possibility would be to use the evaluate function and others in Hadley Wickham’s new package evaluate, as the intended purpose of this package seems to be in support of brew and Sweave-like functionality. I have a potential extension/solution to the issues above (and potential package if there is interest) that I’ll discuss in a later post.  	 0 Comments
Social contagion? Maybe not…	https://www.r-bloggers.com/2010/07/social-contagion-maybe-not%e2%80%a6/	July 28, 2010	Michal	Recently there was a lot of racket about results presented in papers by a pair Nicholas Christakis & James Fowler. Their papers gained a lot of attention both in the academic world as well as in media. In their papers they claim to provide evidence for social contagion (transmission through social networks) of several types of individual characteristics. They wrote articles addressing, among other, social transmission of: obesity (friends of obese people are likely to be obese too), smoking (smokers tend to be friends with smokers), happiness (friends of happy people tend to be happy too), or loneliness (people are more likely to feel lonely if their friends feel lonely too, my personal favourite…). All of these are based on data from Farmingham Heart Study about which I wrote some time ago. For the reference, the relevant papers are listed at the bottom. Authors claim to find very strong effects which they attribute to social transmission of the studied characteristic (obesity, smoking etc.) dismissing other potential explanations based on various arguments. I am sure for anybody interested these things the results were jaw-dropping. For example, in the case of obesity, the reported result was that if your friend is obese then you risk for becoming obese increases by 57% (!) as compared to the situation if your friend did not become obese. For an overly-critical person like me the results were not so much jaw-dropping but eyebrow-raising. Among other things, to me, the results were based on somewhat strange analytical techniques where, at the same time, social networks literature suggests different approaches. Christakis & Fowler were not referring to the existing methodology at all. Most notably to SIENA models for network and behavior dynamics or models for social selection and social influence developed by people at MelNet. I was not the only one not fully convinced, see for example here or here. Anyway, I’m not going to review all the results here as somebody did that pretty well recently. The paper The Spread of Evidence – Poor Medicine via Flawed Social Network Analysis was uploaded couple of days ago to the Arxiv. The author, Russel Lyons, mathematician from Indiana University, takes a closer look at the papers I mentioned. In general, he finds flaws that range from problems in the arcane details of estimation strategy to undergraduate-level mistakes in interpreting confidence intervals. All the flaws Lyons finds fall into two categories: The bottom line is: substantive claims Christakis & Fowler make are not supported by results they show. Again, I’m not going to copy-paste from Lyons’ paper. Have a look yourself here. Couple of end-thoughts: Some of the papers by Christakis & Fowler I’m referring to: Links: (2010-07-30) Seems like Lyons paper is a bit of an old news. The first version was available on his website at least in April, as featured by this piece at Slate. Also, see this article at NYT from September 2009. 	 0 Comments
Seamless R and C++ integration	https://www.r-bloggers.com/2010/07/seamless-r-and-c-integration/	July 28, 2010	Szilard	Video of the talk “Seamless R and C++ integration” by Dirk Eddelbuettel at the Los Angeles area R Users Group in March 2010 (to see the video in a larger size, click inside the video but outside the play button):  	 0 Comments
Web applications with R	https://www.r-bloggers.com/2010/07/web-applications-with-r/	July 28, 2010	Szilard	Video of the talk “Web applications with R” by Jeroen Ooms at the Los Angeles area R Users Group in January 2010:  	 0 Comments
Juggling with fire	https://www.r-bloggers.com/2010/07/juggling-with-fire/	July 27, 2010	romain francois	People were juggling with fire sur la place de la Comedie tonight in Montpellier 	 0 Comments
A summer of books	https://www.r-bloggers.com/2010/07/a-summer-of-books/	July 27, 2010	xi'an	The summer started with a research in pair session in CiRM on the R edition of Bayesian        Core, but I am also involved two other book projects. The first one was mentioned in a previous post, namely the translation of Introducing  Monte Carlo Methods with R into French. I have now recovered all translated chapters, involving not less than six translators! (My son completed his two last chapters while in CiRM, another benefit of the fortnight there!) So I need to get over those chapters to ensure some minimal homogeneity in the style and the notations. Not an immense amount of work given the near perfect productions of Robin Ryder, Julyan Arbel and Pierre Jacob, but still needs to be done (a perfect opportunity for the long flight to Vancouver!) The second book is an edited volume following the exciting meeting on mixtures last March in Edinburgh. I have so far received twelve of the fifteen chapters from the contributors and hope against all odds to pack the volume for Vancouver, in order to discuss with the Wiley representative there. (I also hope it will be possible to include a picture I took during my trip to Ben Nevis as the cover picture..!)  	 0 Comments
Hadley Wickham’s ggplot2 / Data Visualization Course Materials	https://www.r-bloggers.com/2010/07/hadley-wickhams-ggplot2-data-visualization-course-materials/	July 27, 2010	Stephen Turner		 0 Comments
Mystery solved: The discrepancy in homicide data	https://www.r-bloggers.com/2010/07/mystery-solved-the-discrepancy-in-homicide-data/	July 27, 2010	Diego Valle-Jones		 0 Comments
useR! 2010	https://www.r-bloggers.com/2010/07/user-2010/	July 27, 2010	romain francois	I was in useR! last week, it was great to catch up with friends, see what people are doing with R, tell people what I am doing with R, etc … the conference was great This year I presented with Dirk in Laurel and Hardy mode and I’ve uploaded our slides in my slideshare account I also took some time to visit Washington and take a few pictures (tagged with user2010 on flickr 	 0 Comments
rgeos – Update	https://www.r-bloggers.com/2010/07/rgeos-update/	July 27, 2010	rundel	So I have been remiss in posting updates of progress on rgeos as my summer has gotten busier but the project is progressing smoothly and everything is on schedule. We have just recently passed an important personal milestone, parsing and running of all the JTS unit tests are now working and the rgeos code passes all but a handful of tests. (Failing tests appear mostly to do with issues surrounding dimension collapse which, I do not believe, is currently handled by geos). We are nearly feature complete and will now be focusing on testing and writing up documentation / examples. Below are a couple of examples that came up as answers to questions posted to the R-sig-geo mailing list. Note that the naming convention of the geos functions has changed to use a single g (for geos) as a prefix instead of using the awkward RGEOS per Hadley’s earlier suggestion. Old function names are retained for the time being but will throw a depricated warning.   This can be done with existing functions in sp (see the overlay function) but this is how you could do it with rgeos. (I just realized that this is pretty much exactly the same as the cities example from my last post)  	 0 Comments
Statistical Analysis StackExchange site now available	https://www.r-bloggers.com/2010/07/statistical-analysis-stackexchange-site-now-available/	July 26, 2010	Rob J Hyndman	The Q&A site for statistical analysis, data mining, data visualization, and everything else to do with data analysis has finally been launched. Please head over to and start asking and answering questions. Also, spread the word to everyone else who may be interested — work colleagues, students, etc. The more people who use the site, the better it will be. There are already 170 questions, 513 answers and 387 users. Eventually the site will move to a different domain name and have its own logo, etc.  For now it is in “public beta” which means that it is fully functional, but we are still working out some of the details (such as what it will be called, who will be the moderators, etc.). R questions are allowed on this new site as well as on the original StackOverflow.com. We are still figuring out how to avoid the problem of having answers on two sites. For now, more statistical questions should be directed to stats.StackExchange.com and more programming-oriented questions should go to StackOverflow.com.   	 0 Comments
Hacker News User Base Changed?	https://www.r-bloggers.com/2010/07/hacker-news-user-base-changed/	July 26, 2010	C		 0 Comments
Rcpp 0.8.5	https://www.r-bloggers.com/2010/07/rcpp-0-8-5/	July 26, 2010	Thinking inside the box	"
This release constitutes a quick
follow-up to the last release
0.8.4
which we got out just before 
CRAN closed for summer vacations.
Some fixes were made right after last release: 
two harmless warnings from the help file parser of the
development version of R are now addressed, and we stopped using shell
expansions in the Makefile snippets. We also added to some internal speedups
we discovered while prepapring the talk about
RProtoBuf for
last
week’s useR! meeting.

 

The NEWS entry follows below:
 
As always, even fuller details are in 
Rcpp Changelog page and the 
Rcpp page which also
leads to the downloads, the 
browseable
doxygen docs and zip files of doxygen output for the standard formats.
A local directory  has
source and documentation too. 
Questions, comments etc should go to the
rcpp-devel mailing list
off the R-Forge page

 "	 0 Comments
A tty Connection for R	https://www.r-bloggers.com/2010/07/a-tty-connection-for-r/	July 26, 2010	Matt Shotwell	I completed (some time ago) an initial version of a patch for R, version 2.11.1, that adds a POSIX tty connection. The patch is confirmed to compile and works on Mac OS X (thanks to Ashwin Bhat of Georgia Tech) and Debian GNU Linux. However, it should be portable to other POSIX compliant operating systems. The tty patch allows for a variety of interesting operations in R, including “Press any key to continue” type functionality, reading passwords from the terminal without echoing the characters, and communications with any device that emulates a tty (i.e. /dev/tty*). I had originally developed the patch in order to collect data from a microcontroller via a virtual (USB) serial port. I plan to write more about the patch with examples in the future. However, for now, I will just post the code, which is licensed according to the GPL version 2. Users familiar with the POSIX tty interface should be able to use the function (tty) easily. Instructions to compile the patch are essentially identical to those that I wrote for the earlier serial connection patch, replacing serial with tty in the appropriate places. The patch code is here R-2.11.1-tty.patch. The patch is relatively small. Most of the code relates to parsing arguments to the tty function, as the POSIX tty interface has may options. Below are the results of diffstat 	 0 Comments
Snapshots from useR! 2010	https://www.r-bloggers.com/2010/07/snapshots-from-user-2010/	July 26, 2010	David Smith	"I’ve still got more to write from days 2 and 3 of useR! 2010, but in the meantime here are a few snapshots from my camera roll. (Click on the images for larger versions.) 
 Jeroen Ooms (left) chats with other useRs in the main hallway at NIST. 
 Karim Chine demonstrates his Electric-R portal for R on Amazon’s EC2 cloud. 
 Andrew Lampitt talks about Business Intelligence with Jaspersoft and Revolution R.  Jonathan Lees demonstrates analysis of earthquake data with R. 
 James “JD” Long and Joseph Rickert hat-battle at the conference dinner. Unfortunately the flash on my camera wasn’t quite up to catching any of the keynote presenters on the large NIST stage, but video of many of the invited talks should soon be available at Drew Conway’s Video Rchive. "	 0 Comments
Extracting Raster Values from Points in R and GRASS	https://www.r-bloggers.com/2010/07/extracting-raster-values-from-points-in-r-and-grass/	July 26, 2010	toddjobe	"A common task in GIS analysis is to extract the value of a remotely
sensed environmental variable at a point location.  For instance we
may wish to extract the elevation of a field plot from a digital
elevation model.  The elevation data is a raster (i.e. grid) and the
plots are a point shapefile (or a simple text file of X, Y
locations).  The command for doing this in ArcGIS is
ExtractValuesToPoints available in the Spatial Analyst package.
Situations may arise where ArcGIS is not the most efficient way of
extracting these values.  So, here, I provide a brief overview of how to
extract raster values to points in R and GRASS.
 
This is strikingly easy is R.  My work usually requires more
statistical sophistication than is available in ArcGIS.  As a
result, I have completely switched to doing the extraction in R.  I
known I am going to end in R eventually, and it is easier to automate
than writing a long python script in ArcGIS.
 
For the purpose of this exercise.  All the data must be have the
same spatial projection.
 You also need the maptools and sp packages.
 library(maptools) # autoloads sp
gr 
            coordinates     gr.asc
1497 (569292, 1224170)  6094.6080
539  (567718, 1227840)  7964.5331
1023 (564565, 1225810)  -293.6599
663  (562462, 1227260) -5351.7297
69   (563675, 1229710) -2923.9394
716  (563062, 1227180) -4241.8255
339  (567636, 1228780)  4781.6488
509  (561722, 1227870) -3958.7312
805  (560981, 1226690)   139.9091
155  (560884, 1229220) -2719.7353

 
That is it.  Fast, and easy.
 
Extracting raster values in GRASS is somewhat faster than in R, but
it takes a little bit more planning in that you have to explicitly
create the column that the raster values will go into.
 The basic flow of this is that you create an empty column in the
point dataset with the right data type (i.e. varchar(10) string
of length 10, double precision floating point numbers, int
integers).  Then, fill the column with the raster values.
 v.db.addcol map=pt columns=”grval double precision”
v.what.rast vector=pt raster=gr column=grval

 "	 0 Comments
Richard Stallman talk+Q&A at the useR! 2010 conference (audio files attached)	https://www.r-bloggers.com/2010/07/richard-stallman-talkqa-at-the-user-2010-conference-audio-files-attached-2/	July 26, 2010	Tal Galili	The audio files of the full talk by Richard Stallman are attached to the end of this post. —————– Videos of all the invited talks of the useR! 2010 conference can be viewed on the R User Group blog  —————– Last week I had the honor of attending the talk given by Richard Stallman, the last keynote speaker on the useR 2010 conference.  In this post I will give a brief context for the talk, and then give the audio files of the talk, with some description of what was said in the talk. Richard Stallman can be viewed as (one of) the fathers of free software (free as in speech, not as in beer). He is the man who led the GNU project for the creation of a free (as in speech, not as in beer) operation systems on the basis of which GNU-Linux, with its numerous distributions, was created. Richard also developed a number of pieces of widely used software, including the original Emacs,[4] the GNU Compiler Collection,[5], the GNU Debugger[6], and many tools in the GNU Coreutils Richard also initiated the free software movement and in October 1985 he also founded it’s formal foundation and co-founded the League for Programming Freedom in 1989. Stallman pioneered the concept of “copyleft” and he is the main author of several copyleft licenses including the GNU General Public License, the most widely used free software license. You can read about him in the wiki article titles “Richard Stallman” The useR 2010 conference is an annual 4 days conference of the community of people using R.  R is a free open source software for data analysis and statistical computing (Here is a bit more about what is R). The conference this year was truly a wonderful experience for me.  I  had the pleasure of giving two talks (about which I will blog later this month), listened to numerous talks on the use of R, and had a chance to meet many (many) kind and interesting people. The talk took place on July 23rd 2010 at NIST U.S.  and was the concluding talk for the useR2010 conference.  The talk consisted of a two hour lecture followed by a half-hour question and answer session. On a personal note, I was very impressed by Richards talk.  Richard is not a shy computer geek, but rather a serious leader and thinker trying to stir people to action.  His speech was a sermon on free software, the history of GNU-Linux, the various versions of GPL, and his own history involving them. I believe this talk would be of interest to anyone who cares about social solidarity, free software, programming and the hope of a better world for all of us. I am eager for your thoughts in the comments (but please keep a kind tone). Here is Richard Stallmans  (2 hours) talk:  Audio file to download – Richard Stallman talk at the useR! 2010 conference (~2 hours)  The second part of the talk consisted of Richard Stallman answering the following questions: Audio file to download – Richard Stallman talk at the useR! 2010 conference – Q&A session (~25 minutes)  Final note, more talks from the useR2010 conference are expected to be put online here, thanks to Drew Conway. 	 0 Comments
Using SAS for Data Management, Statistical Analysis, and Graphics	https://www.r-bloggers.com/2010/07/using-sas-for-data-management-statistical-analysis-and-graphics/	July 26, 2010	Nick Horton		 0 Comments
New R User Group in Melbourne	https://www.r-bloggers.com/2010/07/new-r-user-group-in-melbourne/	July 26, 2010	David Smith	Data mining analyst Yuval Marom has just started a new user group for users of R in Melbourne (the one in Australia). He says:  The group is open to people with all levels of experience with R, including complete beginners and complete experts, and people from all industries – private, government, and academic. The main purpose of the group is to provide a forum for the exchange of knowledge and experiences to help people along in their journey of learning to use R. The actual format and content of the meetings will be determined by the members.  The group’s first meeting will be in the Melbourne CBD on August 9 from 6-8PM. For more info and to RSVP or join the mailing list, fill out the form at the link below. Melbourne R User Group: RSVP  	 0 Comments
R Plotting tips	https://www.r-bloggers.com/2010/07/r-plotting-tips/	July 26, 2010	E.Crema		 0 Comments
Circle packing with R	https://www.r-bloggers.com/2010/07/circle-packing-with-r/	July 26, 2010	Michael Bedward		 0 Comments
Asher’s enigma	https://www.r-bloggers.com/2010/07/asher%e2%80%99s-enigma/	July 25, 2010	xi'an	"On his Probability and statistics blog, Matt Asher put a funny question (with my rephrasing): Take a unit square. Now  pick two spots at  random along the perimeter, uniformly. For each of these two locations, pick another random point from one of the  three other sides of the square and draw the segment. What is the probability the two segments intersect? And what is the distribution for the intersection points? The (my) intuition for the first question was 1/2, but a quick computation led to another answer. The key to the computation is to distinguish whether or not both segments share one side of the square. They do with probability  in which case they intersect with probability 1/2. They occupy the four sides with probability 1/6, in which case they intersect with probability 1/3. So the final answer is 17/36 (as posted by several readers and empirically found by Matt). The second question is much more tricky: the histogram of the distribution of the coordinates is peaked towards the boundaries, thus reminding me of an arc-sine distribution, but there is a bump in the middle as well. Computing the coordinates of the intersection depending on the respective positions of the endpoints of both segments and simulating those distributions led me to histograms that looked either like beta B(a,a) distributions, or like beta B(1,a) distributions, or like beta B(a,1) distributions… Not exactly, though. So not even a mixture of beta distributions is enough to explain the distribution of the intersection points… For instance, the intersection points corresponding to segments were both segments start from the same side and end up in the opposite side are distributed as  where all u‘s are uniform on (0,1) and under the constraint . The following graph shows how well a beta distribution fits in that case. (Not perfectly, though!)
The R code is u=matrix(runif(4*10^5),ncol=4)
u[,c(1,3)]=t(apply(u[,c(1,3)],1,sort))
u[,c(2,4)]=-t(apply(-u[,c(2,4)],1,sort))
y=(u[,1]*(u[,4]-u[,3])-u[,3]*(u[,2]-u[,1]))/(u[,1]+u[,4]-u[,2]-u[,3])
 Similarly, if the two segments start from the same side but end up on different sides, the distribution of one coordinate is given by  under the constraint . The outcome is once again almost distributed as a beta:
The corresponding R code is u=matrix(runif(4*10^5),ncol=4)
u[,c(1,3)]=-t(apply(-u[,c(1,3)],1,sort))
y=(u[,1]*(1-u[,3])-u[,3]*u[,4]*(u[,2]-u[,1]))/(1-u[,3]-u[,4]*(u[,2]-u[,1]))
 "	 0 Comments
R Chart	https://www.r-bloggers.com/2010/07/r-chart/	July 24, 2010	C		 0 Comments
useR 2010 at NIST in Gaithersburg	https://www.r-bloggers.com/2010/07/user-2010-at-nist-in-gaithersburg/	July 24, 2010	Thinking inside the box	"

As at the preceding
useR! 2008 in Dortmund and
useR! 2009 in Rennes, I presented a three-hour
tutorial on high-performance computing with R. This covers
scripting/automation, profiling, vectorisation, interfacing compiled code,
parallel computing and large-memory approaches. The slides, as well as a condensed 2-up version, are now on my
presentations
page.
 

On Wednesday, Romain and I had a chance
to talk about recent work on Rcpp,
our R and C++ integration. Thursday, we followed up with a presentation on 
RProtoBuf —
a project integrating Google’s
Protocol Buffers with R which much to our
delight already seems to be in use at Google itself! It was quite fun to do
these two talks jointly with Romain.  But my other coauthor Khanh had to be at a
conference related to his actual PhD work. So on Friday it was just me to give
a presentation about 
RQuantLib
which brings QuantLib to R.

 
Slides from all these talks have now been added to my presentations
page. I will also upload them via the conference form so that they can be part
of the conference’s collection of presentations which should be forthcoming.

 "	 0 Comments
Local R User Group Panel from useR! 2010 (Video)	https://www.r-bloggers.com/2010/07/local-r-user-group-panel-from-user-2010-video/	July 24, 2010	Drew Conway	As I mentioned last week, I will be hosting videos of several of the keynote speakers from this year’s useR! 2010 conference at the video Rchive.  As it happens, the first video I was able to upload was the panel discussion we held on starting local R user groups.  I have uploaded the video, which is also embedded below (after the jump). I was joined on the panel by an illustrious assembly of R community members, which included:  	 0 Comments
useR! 2010 – Local R User Group Panel	https://www.r-bloggers.com/2010/07/user-2010-local-r-user-group-panel/	July 24, 2010	VCASMO - drewconway		 0 Comments
How to calculate with dates and hours in R	https://www.r-bloggers.com/2010/07/how-to-calculate-with-dates-and-hours-in-r/	July 24, 2010	danganothererror	A while ago I was asked whether calculating with datums and hours is possible in R. Especially, if you added an hours to 23:45 (say Jan 1, 2010) , would R know to jump to the next day – 00:45 (jan 2, 2010)? The answer is yes. Our only job is to tell R in what format our datum and time is. After we have done so, we can use these datums and times for different calculations. Let’s say we have a vector (or any other object type) with datum and hour: We need to tell R that this is in fact a POSIX type object, whereas we need to specify which number corresponds to what. We will tell it that the first number is day, followed by a dot (.), followed by a month, followed by a dot… You get the picture. This is done through the format argument. While the inclusion of tz argument (time zone) is not applicable here, be careful if you’re calculating with datums and times from different time zones. For the following examples, I’ve added another element to the vector to spice things up. For pedagogic purposes, I will make things a wee more complicated (at least for novice). Let’s write two simple (and I mean SIMPLE) yet not mandatory functions that will convert hours and minutes to seconds. Because calculating with POSIX objects is done via seconds (see below), by using these functions we free ourselves from typing 3 * 3600 every time we want to use 3 hours or 3*60 for 3 minutes. At least in my view, this makes the code (where we calculate) a bit more readable. Here are the two functions: This will add three hours to the current date/time values in dtm This will add 3 hours and 10 minutes. If we hadn’t used our functions, our input would look like this See, I told you hrs() and mns() is more intuitive.   We can also subtract. Let’s cut off 111 seconds We are interested if datum will change if we add enough hours (or minutes). Let’s add 15 hours. No surprises there – first element changed to the next day and the seconds element fell short for 15 minutes. If we’re curious how far apart are two datums, we use the following function: All this and more can be also found in The R Book by Michael J. Crawley. This particular chapter can be accessed through the web. Data manipulations with R by Phil Specter can in part be previewed here (kudos to aL3xa for pointing it out). Help pages on datums and time classes can be found in 	 0 Comments
useR! 2010 done and dusted	https://www.r-bloggers.com/2010/07/user-2010-done-and-dusted/	July 23, 2010	Abhijit	"The useR! 2010 R users conference just finished up this afternoon with a thought-provoking, controversial, and sometimes hilarious talk by Richard Stallman of GNU fame. It started on Tuesday with great tutorials (I took ones on MICE for multiple imputation and Frank Harrell’s excellent regression modeling). In between these bookends was a wonderful conference where I got the chance to put faces to names (from their online presence), make many new friends, hopefully no enemies, and learn quite a bit. The breadth of activity in the R community is truly breathtaking, and it was encouraging to see a good fraction of the participants and presenters be from domains other than stat/math/CS, and who have invested in learning R to solve their substantive problems. Most of us stat/math types learned R or other statistical software primarily through classwork in undergrad or grad school, in other words a safe environment in which to learn. These individuals are learning out of necessity to solve the questions in their domain, and investing in R learning has allowed them to do just that, and this conference provided them a portal to give back and show us their R stories. The conference environment was excellent and collegial, and the organizers provided almost an European atmosphere, where we all felt well taken care of (the wine didn’t hurt, either). Not surprising, since this conference’s last three incarnations were in Northern Europe, and the next will be in England. We found a great environment for meeting people, learning new ideas, considering collaborations and generally realizing potential new directions in our thought processes on data analysis and statistical ideas in general. 
One of the highlights for me was an excellent blogging call-to-arms by Tal Galili of R-bloggers.com fame. It has provoked me to start sharing my ideas once again on this blog, and I’m sure you’ll find this blog much more updated and active in the coming days and months. Though I probably will not go to the Old Country across the Pond for next year’s meeting (not even to see Lady Godiva), I know I’ll be there in spirit, having had such a blast on this one. And to the organizers of this one, both on the organizing committee and anonymous volunteers, a heartfelt thank you. "	 0 Comments
Thoroughly Unusual Itches	https://www.r-bloggers.com/2010/07/thoroughly-unusual-itches/	July 23, 2010	C		 0 Comments
The counterfactual GPS!	https://www.r-bloggers.com/2010/07/the-counterfactual-gps/	July 23, 2010	dan	WHAT IF YOUR GPS TOLD YOU WHAT WOULD HAVE HAPPENED IF YOU HAD TAKEN THE OTHER ROUTE?  Not long ago, your Decision Science News editor was planning a trip to a book group meeting along with another member. The monthly book group takes place in Cove Neck Long Island, about an hour East of Manhattan. Given the starting point (see map), the two had an email exchange about the best route. Your editor preferred to take the Southern route (above), as suggested by multiple Web sites, which gave time estimates under average conditions as well as under heavy traffic. These sites suggested that under the worst possible traffic, the trip would take as long as 1 hour 30 minutes. However, the driver, citing “30 years of New York driving experience”, expressed certainty that going up the West Side Highway and taking the Kennedy (nee Triborough) bridge would be fastest. Your editor did not bring up his three years of daily commuting from the West Village to Long Island and went along for the ride, for which he was, and is, very thankful. Even if the northern route is longer, he reasoned, there will that much more of the driver’s delightful company to enjoy. As the reader might expect, the northern route took about 2 hours and 15 minutes, possibly the longest voyage from the Tribeca to the North Shore since the advent of the canoe. But that is all just background. During the trip, your editor thought, “wouldn’t it be interesting to have a GPS that would show you where you are on the path you have chosen, but also show you where you would be had you chosen another path. A counterfactual GPS!” But how would this fanciful counterfactual GPS know how long it would take you on the other route? Assuming some kind of large-scale participatory program, all GPSes could send back anonymous information about where they are and how fast they are going. In essence, the counterfactual GPS could just pick a car that is taking the other route, follow it on the other path, and display its position on your GPS, complete with nagging message (as above). It is not unlike choosing a person in another line at the grocery store to see what would have happened if you did not choose the line you did. And what if nobody else is going to the same destination? Not a problem. Once the ‘followed’ car turns off the route, the counterfactual GPS picks another car to follow. And what if you feel that you can drive faster than some random car that is traveling on the other route? Not a problem, the counterfactual GPS can sample all the cars traveling a piece of the route and pick one whose speed relative to other cars on its route is the same as your observed speed relative to other cars on your route. And what if hardly anybody is driving at all when you are traveling? Again, not a problem. As soon as you indicate the two routes, the counterfactual GPS will start collecting statistics on both of them, in order to form up-to-the-minute estimates of how fast traffic is moving on each stretch of the route. A counterfactual GPS would be more fun than educational, but it could improve the decision making of those who use it. That is, it could teach you whether it is a good idea or a bad idea to ignore the advice of the GPS. When this was brought up at one of the famous and daily Yahoo Research lunches, Sharad begged to differ, saying that such a device would cause people to persist in their false belief that they are better at route planning than GPSes. Sharad reasoned (and he may correct us if we are wrong) that if the GPS is correct 60% of the times you disagreed with it, then it may be a long time before you realize that it is right more often than you are, and that your coincidental lucky streaks of beating it on occasion would only serve to make you think that you’ve identified special instances in which you have privileged information (even though such instances may be purely due to chance). In short, the counterfactual GPS could induce one to overfit the situation and engage in “probability matching” (deciding to trust the GPS 60% of the time) instead of always trusting it (the quote rational unquote thing to do). Your editor supposes that if the counterfactual GPS kept long-term statistics, and then used onboard copies of R and ggplot2 to render and email out reports, such reports could help these people who are not good at trial-by-trial learning. Like Sharad, your editor feels that people would be much more often right than wrong by trusting GPSes or mapping software. However, still, in 2010, there is information that can be profitably exploited, and with enough feedback, people might be able to outperform the GPS. For instance, if one sees an oil tanker on its side on the suggested route, it is likely that the GPS doesn’t know about this, making it is a good idea to go another way. (Sharad says in such cases, everyone will seek a detour, so staying put may be wisest). What do you think, dear Decision Science News readers? Would a counterfactual GPS make people better decision makers because it can teach people when and when not to trust the GPS? Or would it not make people better decision makers because it would encourage folks to believe they can eventually outsmart it (just as many people believe they’ll eventually outsmart the craps table or the stock market)? 	 0 Comments
Why building R packages is good for you	https://www.r-bloggers.com/2010/07/why-building-r-packages-is-good-for-you/	July 23, 2010	Timothée	Basically every function you use in R is part of a package (often the base or stats one). Most of the advances routines, such as the differential equations solvers in simecol are brought to R in the form of Fortran or C code. It is not, however, required to learn any other language that R to contribute a package to the community. There is a great post at Dang, another error on how to create a R-only R packages (I believe they are a minority among the R packages). Here I share two reasons for which I think it is important to contribute even the simplest packages. Developing R packages is a great idea if you need to share code among several peoples, or if you need to routinely apply the same functions over and over again. In my lab, we use a Fluostar Reader spectrophotometer to measure bacterial growth. While it is a good and reliable piece of hardware, the results are exported to Excel (which is bad if you want to do an analysis more complicated than, basically, opening the .xls file). We use this spectrophotometer to produce a huge amount of data. I have myself accumulated over a thousand runs over the last months. Most of the time, I want to extract the growth rate of different bacteria, deal with the replicates, and format the results in a meaningful way. I am not the only one in the lab to do it, so I decided to give a try to this whole ‘package creating’ stuff. A couple days later, I came up with a package that allowed us to perform most of the analyses (i.e. growth rate, population size, and that’s nearly all we do anyway). Developing the package allowed us to save a considerable amount of time. First, the analyses are simple to do, with the help of a step-by-step tutorial. Second, even with a basic understanding of R, students and technical staff of the lab are « on their own » to format the data and launch preliminary analyses. Third, we all speak the same language, and anybody can work on the code of anyone else, because we all use the same functions. And finally, the results are now reproducible, because we know precisely how they were obtained. Reproducibility of the research is one of the most important thing to consider when deciding to provide a package to the community. In an (hopefully) upcoming paper on ecological measures, we developed new indicators and routines of trophic web generation, that were described in appendices. However, we also programmed a R package that we used for the analysis, and offered the referees to download it and try for themselves. I think that these aspects (better team-work and easier reproducibility) are especially important to consider when working with R code. Sure, it is easier not to have the supplementary work to write the documentation, check the examples, and other associated tasks. But having things in a package also allows you to load your daily functions in one line, which is, on the long term, really time saving. Not to mention accurate version control and other associated goodnesses. 	 0 Comments
Turning your data into a 3d chart	https://www.r-bloggers.com/2010/07/turning-your-data-into-a-3d-chart/	July 23, 2010	respiratoryclub	Some charts are to help you analyse data.  Some charts are to wow people.  3d charts are often the latter, but occasionally the former.  In this post, we’ll look at how to turn your data into a 3d chart.   Let’s use the data from this previous post.  Use the code which turns the .csv spreadsheet into 3 variables, x, y, and z. 3d charts generally need other packages.  We’ll kick off with scatterplot3d, which perhaps makes things too easy:  The difficulty with 3d plots is that by definition, you’re looking at a 3d plot on a 2d surface.  Wouldn’t you like to be able to rotate that plot around a bit?  We’ll use the package rgl.  Then type: This pulls up an interactive window which you can rotate.  Very helpful? Perhaps, but there are too many plots.  Perhaps you only want to look at the middle 33% of the plots (i.e. look at a subset of the plot)? This looks much better.  We’ve said we’d start at 33% of the way through the x,y,z co-ordinates, and end at 66% with the startplot and endplot variables.  This is helpful – remember this is one year of data, and we’ve just displayed the middle of the year.  The heatmap also helps to distinguish between plots, but in this case it doesn’t add any extra data – more of that in posts to come.  	 0 Comments
Plaudits for R	https://www.r-bloggers.com/2010/07/plaudits-for-r/	July 23, 2010	David Smith	I just discovered that R core member Paul Murrell has been maintain a list of plaudits for R: newspaper articles, book reviews, remarks on mailing lists and blogs, and even gratitudes from individual R users. He’s collected dozens of entries since 2001 — great materials here if you ever need more evidence of the awesomeness of R.   Paul Murrell: Plaudits for R    	 0 Comments
Building an R package (under Windows) without C, C++ or FORTRAN code	https://www.r-bloggers.com/2010/07/building-an-r-package-under-windows-without-c-c-or-fortran-code/	July 23, 2010	danganothererror	"Why build and R package? It basically boils down to be able to brag at your local pub that a new version of YOUR package is on CRAN as of 7 p.m. CET. But seriously, if you’ve produced some function that other people might benefit (or have ordered them) from using them, like your boss, co-workers or students, consider building a package. The chances of broken dependencies and ease of installing everything outweighs the effort of learning how to build one. If you feel your functions (that may be new in some respect) could benefit an even wider audience, consider submitting it to CRAN (I will not discuss how to do that here, but do read the Ripley reference I mention later). I have set out to build a test package to prepare myself when the time comes and will really need to build one of my own. This here is an attempt I made to document steps I took when building a dummy package (called texasranger (yes, THE Texas Ranger!)) with one single function. I have attempted to build documentation and all other ladeeda things that are mandatory for the package to check out O.K. when building it. Before you dig into the actual preparation and building itself, you will need a bunch of tools. These come in a bag with a linux distribution, but you will have to add them yourself if you’re on Windows. This is basically the only thing that is different when trying to build a package on Windows/Linux. I will not go into details regarding these tools (perl, MS html help compiler, if you have C/C++/FORTRAN code you will need GNU compiler set) , a TeX distro), – I will, however, advise you to check out Making R package under Windows (P. Rossi). There, you will find a detailed description (see page 2-6) of how to proceed to get all the correct tools and how to set them up.  When you have done so, you are invited to come back here. Feel free to follow just mentioned tutorial, as it goes a bit more in-depth with explaining various aspects. The author warns that MiKTeX will not work (see the datum of the document), but things might have changed since then and it now works, at least for me. I have followed the aforementioned Making R package under Windows (by P. Rossi), slides Making an R package made by R.M. Ripley and of course the now famous Writing R Extensions (WRE) by R dev core team (you are referred to this document everywhere). I would advise everyone to read them in this listed order – or at least read WRE last. First two can be read from cover to cover in a few minutes – the last one is a good reference document for those pesky “details”. In my experience, I started to appreciate WRE only after I have read the first two documents. Enough chit-chat, let’s get cracking! 1. These are the paths I entered (see document by Rossi what this is all about) to enable all the tools so that I can access them from the Command prompt (Command prompt can be found under Accessories, another term for it may be Terminal or Console on different OSs):  2. Use R function  to create directory structure and some files (DESCRIPTION and NAMESPACE). I used the following arguments:  See argument code_files for an alternative way of telling the function where to read your functions. I suspect this may be very handy if you have each function in a separate file. 3. Fill out DESCRIPTION and NAMESPACE (if you decide to have a name space, read more @ WRE document). Pay special attention to export, import, useDynLib… All of the above mentioned documentation will help guide you through the process with minimal effort.
A side (but important) note. You should write your functions without them calling  or  to dig up other function and packages. Read more about NAMESPACE and how to specify which functions and packages to “export” (or “import”) and how. 4. Create  documentation files. This is said to be the trickiest part. I still don’t have much experience with this so I can’t judge how tricky it can be – but I can tell you that it may be time consuming. Make sure you take time to document your functions well. If you were smart, you wrote all this down while you were writing (or preparing to write) a function and this should be more or less a session of copy-paste. Use  to create template Rd files ready for editing. They are more or less self explanatory (with plenty of instructions). It help if you know LaTeX, but not necessary. Also, I suspect the function may dump the files into the correct /man directory automatically – if it doesn’t, do give it a hand and move the files there yourself. Perhaps worth mentioning is that if you want to reference to functions outside your package, use(notice the options square brackets [])  , e.g.  or  – To refer to “internal” package function (those visible by the user), use  4a. If you have datasets you wish to include in your package (assuming those in library(help=”datasets”) are not sufficient), you will need to do two things. First, prepare your object (list, data.frame, matrix…). Save it and prepare documentation. Saved .rda file goes to data/ directory. The documentation file goes into the same directory (man/) as other .Rd files. If your dataset is not bigger than 1 MB you shouldn’t worry, otherwise consult the Manual on how to prepare a   4b. You should also build a vignette, where you can explain at greater length what your package is about and maybe give a detailed (or more detailed) workflow with the accompanying functions. You can use Sweave or knitr, and the folder to place your .Rnw file is vignettes/. 5. To check the documentation for errors, use  and/or  6. Next, you should run a check on your package before you build it. You should run it from the directory where the package directory is located. I’ve dumped my package contents to d:/workspace/texasranger/ and executed the commands from d:/workspace/  If you get any errors, you will be referred to the output file. READ and UNDERSTAND it. 7. Build the package with the command  This will create a file and will add a version (as specified in the DESCRIPTION file, i.e. package_name_1.0-1.tar.gz, see WRE for specifics on package version directives). package_name is actually the name of the directory (which should be the name of your package as well). If you use Windows, you can build a .zip file AND install the package (uses install.packages) at the same time. Use command  8. Rejoyce. "	 0 Comments
Creating a Presentation with LaTeX Beamer – Equations and tikz	https://www.r-bloggers.com/2010/07/creating-a-presentation-with-latex-beamer-%e2%80%93-equations-and-tikz/	July 23, 2010	Ralph	Many presentations created using LaTeX beamer included mathematical equations and these can be easily included in a presentation and in this post we will consider using the tikz package to add various interesting elements to equations, such as lines between text on a slide and part of an equation. Fast Tube by Casper The examples on this page have been inspired by the good examples on global nodes detailed on texample.net. The example can be adapted to apply to equations for various statistical models and we will consider the model for a general row-column experiment design. To create this example we need to make tikz available in our tex file by adding the following code to the preamble: The second and third lines refer to particular elements of tikz that will be used in the example. If we want to access nodes in different areas of our latex document we need to make use of the remember picture style and the easiest way to do this is to make a global declaration rather than on each specific picture: The example will be the model for a row-column experiment design and we will have four elements in a bullet list that are linked by arrows to different parts of the equation, which in turn are highlighted in their own box. We use the itemize environment to add the first bullet point: At the end of the item we add a tikz node so that we can draw a line from the end of this line to one of the boxes in the equation. We also provide a name s1 so that it can be identified by tikz. This node uses a style to shift the location where the line starts from, that we need to define in our document using this code: The next step is to create the equation and the nodes and background boxes for each element of the equation that will be linked to the bullet list. The code for the equation is shown here: If we look closely at this code the latex for the equation itself is very straightforward and the complication comes from adding a tikz node to four of the elements of the equation. The nodes themselves have colours (blue/red/green/yellow) and rounded corners. They each have a label d1 to d4 so that tikz can draw lines between the bullet list and the elements of the equation. We then finish off the bullet list with three more elements: Each of the items in this bullet list have their own identifier s2 to s4 that we will use to draw arrows. We create a separate picture environment for the four arrows linking the bullet list to the equation. Each element is defined as a path between two nodes with additional information about the shape of the arrow. There are many variants on this example that could be incorporated in a presentation. Other useful resources are provided on the Supplementary Material page. Also head over to texample.net for more examples of using the tikz package. 	 0 Comments
UseR! 2010 day 1	https://www.r-bloggers.com/2010/07/user-2010-day-1/	July 22, 2010	David Smith	Just a couple of quick notes about the first day of talks at useR! 2010. It’s been a jam-packed schedule — so many good talks to see and people to meet, I just wish I had more time for it all! One stand-out for me so far has been Frank Harrell’s keynote lecture Information Allergy, on the dangers of misusing statistics in Medicine, was amazing. You know a talk is thought-provoking when you’re still thinking about the consequences in free moments the day after. It’s worthy of an entire blog post on its own. I’ve also been excited to see the number of real-life applications using R presented at the conference. In one session alone, I saw how R is used to precisely locate earthquakes (by comparing actual arrival times of signals in seismograph data to their predicted arrival times); how it’s used to measure and report on water quality in Australia; and even how it’s used to measure the amount of greenhouse gases leaching out of landfills, from LIDAR measurement data. Really fascinating stuff. The launch-party for inside-R.org last night was a lot of fun too: having about 150 R users together to drink and chat was a great way to learn lots of new things and meet some great people. Thanks to everyone who came along. (If you’re at JSM in Vancouver, we’ll be hosting another social event on Tuesday, August 3.) Overall, so far it’s been a really outstanding conference: smooth organization, great people, interesting talks, and a really palpable sense of excitement about R. Anyway, I have to run now to give my talk. I’ll write more when I get a free moment. 	 0 Comments
Quick scatterplot with associated histograms	https://www.r-bloggers.com/2010/07/quick-scatterplot-with-associated-histograms/	July 22, 2010	respiratoryclub	"R can produce some beautiful graphics, and there are some excellent packages, such as lattice and ggplot2 to represent data in original ways.  But sometimes, all you want to do is explore the realtionship between pairs of variables with the minimum of fuss. In this post we’ll use the data which we imported in the previous post to make a quick graphic.  I’ll assume you already got as far as importing the data and placing the variable for NO concentration into x and ozone into y. We’re going to make a scatterplot with the histogram of x below the x axis, and the histogram of y rotated anti-clockwise through 90 degrees and alongside the y axis (all will become clear).  The first thing is to set up the graphics display: The layout command tells R to split the graphical output into a 3 by 3 array of panels.  Each panel is given a number corresponding to the order in which graphics are plotted into it.  To see this array, type: This output shows that the display is split into 4 zones.  The top right is a large area for plot one, the top left is a smaller panel for plot 2, and the bottom right is for plot 3. So then, we need something for the top right – a straight forward scatter plot of x vs y (we set the maximum for the x axis with the xlim parameter of plot and using the maxx variable, which contains the maximum value held in the vector: Then, we need to create a histogram of the y values, and plot it to the left of the histogram appropriately orientated.  To do this we first store a histogram into the variable yh, and then plot it with the barplot command.  The reason for this is that barplots can be easily rotated: The breaks variable stores the number of bins into which the histogram is divided, maxy is the maximum value for the vector y, yh is the histogram, and then barplot extracts the heights of the bars from the histogram object draws it as a bar chart, but flips it on its side.  The negative sign before yh$intensities points the bars to the left rather than the right.
We do the same for the x values, and also then reset the graphics display to defaults. We get this output:

The advantage of this over the straight scatterplot is that you can see the density of overlapping points on the histogram.  I’ve set the number of bins in the histogram to 50 – it’s worth playing around with this with your data.  There are more elegant ways of doing this, but if you have paired variables x and y, and you want to quickly look at their distributions and association, this code works fine. "	 0 Comments
Update to ‘Basic R’ eBook	https://www.r-bloggers.com/2010/07/update-to-basic-r-ebook/	July 22, 2010	Rmetrics blogs		 0 Comments
BP Oil and Gas Recovery	https://www.r-bloggers.com/2010/07/bp-oil-and-gas-recovery/	July 21, 2010	C		 0 Comments
Bugs [genuine]	https://www.r-bloggers.com/2010/07/bugs%c2%a0genuine/	July 21, 2010	xi'an	Nothing related with programming, I am afraid! Just a few bugs visiting my passion vine…      I find the green bugs quite interesting with their back “engravings”, even though they are most likely pests … 	 0 Comments
My Talk on Animations at useR! 2010 (NIST, Gaithersburg)	https://www.r-bloggers.com/2010/07/my-talk-on-animations-at-user-2010-nist-gaithersburg/	July 21, 2010	Yihui Xie	As every useR knows, the useR! 2010 conference is being held at NIST in Gaithersburg these days. I have just finished my talk on the R package animation this afternoon. Here are my slides and R code for those who are interested: Have fun, even if you are a PhD!  	 0 Comments
Inside-R.org, a new community site for R	https://www.r-bloggers.com/2010/07/inside-r-org-a-new-community-site-for-r/	July 21, 2010	David Smith	As recently as a couple of years ago, finding information about R in the Web was hard. Other than the canonical content and mailing list archives at the official R project site, www.r-project.org, there wasn’t too much else dedicated to R on the web — and what was there was hard to find on Google without the help of sites like Rseek.org.  Fast-forward to today, and all that’s changed. If you search for R on Google, you’ll actually find content related to the R project on the first page. Dozens of blogs now cover R regularly, as the RSS feed of r-bloggers.com demonstrates. StackOverflow.com has over 1500 questions related to R. Crantastic.org lists more than 2500 user-contributed packages for R. On top of that, there are thousands of personal websites with useful tips, tricks and other useful suggestions for R. Revolution Analytics created Inside-R.org for the R community to highlight these useful resources for R from around the Web, and make them accessible and searchable from a single site. From today, you’ll be able to find blog posts about R, information about R packages from crantastic.org, questions about R from StackOverflow, and the help pages for all R functions in the latest R distribution, all accessible from a comprehensive search. You can even contribute your own tips and tricks for other R users. I’ll be blogging about the new features of Inside-R.org over the next couple of days, and you can read an overview in today’s press release. Insider-R.org is for you, the R users, so check it out and let us know what you think. Inside-R.org: A Community Site for R, Sponsored by Revolution Analytics 	 0 Comments
R Cheat Sheets and more	https://www.r-bloggers.com/2010/07/r-cheat-sheets-and-more/	July 21, 2010	Paolo Sonego		 0 Comments
Reminder: Launch Party for Inside-R.org tonight	https://www.r-bloggers.com/2010/07/reminder-launch-party-for-inside-r-org-tonight/	July 21, 2010	David Smith	If you’re in the DC area (attending useR!2010, perhaps) this evening, Revolution Analytics (in conjunction with the DC Area R User Group) is hosting a party to celebrate the launch of Inside-R.org at the Crowne Plaza in Rockville at 8PM. Come along for drinks and food, mingle with other R users, hear about the new community site for R, and enter for a chance to win an Xbox 360 or Zune (with thanks to Microsoft). Details at the link below. Revolution Analytics Presents: inside-R.org Launch Party!  	 0 Comments
Matrix scatterplot of the Airquality data using lattice	https://www.r-bloggers.com/2010/07/matrix-scatterplot-of-the-airquality-data-using-lattice/	July 20, 2010	respiratoryclub	"In this post we will build on the last one, and create a matrix scatterplot.  The package lattice allows for some really excellent graphics.  In case you haven’t already seen it I recommend the R Graph Gallery for some examples of what it can do – browse the graphics by package used to create them.  We’ll use the same dataset as last time, where we made a plot of the NO levels in the atmosphere vs ozone levels for Nottingham, UK. First step is to load the lattice package. Download the dataset from here, and put the file in your working directory.  Now we’ll put the dataset into the matrix data. So that it’s easier to follow, I’ve extracted 3 vectors from the matrix: x, y, and z.   These are the columns of the data for NO, ozone and SO2.  Hopefully this will help you follow things.  When working with graphs, I usually do this (in the last post I extracted x and y).  If I make a nice graphic I can then “cut and paste” it into another program, and just change the data in x, y and z and hey presto, the same graphic is instantly used with new data. For a matrix scatterplot, we need to make a matrix of the variables to compare.  We join the vectors into a matrix and then name the columns. You can look at the first 10 lines of mat with Finally we create the matrix plot: The final result is here:
 For those unfamiliar with scatterplots – this plot is essentially 3 scatterplots of x vs y, x vs z and y vs z.  The middle left plot is the scatterplot created in this previous post.  The package lattice can do lots more than this – get help on line for it with the command "	 0 Comments
Getting going – importing data and plotting a simple graphic	https://www.r-bloggers.com/2010/07/getting-going-importing-data-and-plotting-a-simple-graphic/	July 20, 2010	respiratoryclub	The most difficult part of the learning curve in R is often getting going – many datasets are pre-installed in the packages and organised, so it is difficult to see how you to import your own data into R.  This post takes you step by step through the process of making a table from a spreadsheet and then a simple graph. The first thing is to get some data.  A .csv file is a common “spreadsheet” like file.  Currently I’m working with some air quality data downloaded from the UK air quality archive.  The data I’ve downloaded is of 2009 data from Nottingham, UK containing automated measurements of Nitric Oxide, NO2, Ozone, and Sulphur Dioxide.  The file is here.  You can cut and paste the code below into R. The first thing to do is put the data into a variable, called data.  Copy the spreadsheet file into your working directory.  We then use the read.csv for this: 	 0 Comments
New R User Group in Slovenia	https://www.r-bloggers.com/2010/07/new-r-user-group-in-slovenia/	July 20, 2010	David Smith	The new R User Group in Ljubljana, Slovenia is giving the Sydney group a run for the money when it comes to the best user group name. It’s called Anonymous Rcoholics — great name! You can join the group at the link below. Discussions are in English and Slovene. Google Groups: Anonymous Rcoholics 	 0 Comments
userR! 2010 Videos to be Hosted at Rchive	https://www.r-bloggers.com/2010/07/userr-2010-videos-to-be-hosted-at-rchive/	July 20, 2010	Drew Conway	Today, I am packing up the car and heading south to my old home, Washington, DC, for the useR! 2010 conference, which is being held at the National Institute of Standards and Technology.  Incidentally, where I was an intern in the Information Technology Lab during college. If you are not able to make the trip to Gaithersburg, MD; fear not, through the hard work of Szilard Pafka (organizer of the LA R user’s group) and Katherine Mullen, coordinator of useR!, I will be hosting many of the conference’s keynotes, lectures and several of the panel discussions at the Video Rchive.  It may be several days before all of the videos are uploaded, but be sure to check back at the Rchive next week for any updates. If you are attending useR!, do try to make it to our panel discussion on starting a local R users group in your area (Thursday, 3:25pm in the Red Room).  The panel includes several prominent charactersmembers of the R community, and should be a very entertaining and informative discussion. Hope to see you there! 	 0 Comments
Analysing the ISMB 2010 meeting using R	https://www.r-bloggers.com/2010/07/analysing-the-ismb-2010-meeting-using-r/	July 20, 2010	nsaunders	"The colossus of bioinformatics meetings, ISMB, convened in Boston this year from July 9 – 13.  As in recent years, the meeting was covered online at its website, FriendFeed and Twitter. I thought it would be fun to run a quick analysis of activity at the FriendFeed room using R.

1.  Fetch the data
We can use the FriendFeed API to fetch data in JSON format.  R provides two useful packages:  RCurl, for making the HTTP request and rjson (or RJSONIO), to parse the results into a list.  Since we don’t know in advance how many entries to expect, we set some arbitrarily large maximum number of entries, loop towards it and break when no more entries are returned. The list ismb.data currently contains 178 entries.  Each entry is itself a list of items that describe the entry.  You can get an idea of its structure using summary(): 2. Entries, comments and likes
We’d like to see the title, date, number of comments and number of likes for each entry.  One way to do that is to convert ismb.data to a data frame.  There is surely an elegant way to achieve this using, for example, the plyr package, but here’s an ugly way using sapply(): Now that we have a data frame, it’s easy to sort.  Let’s look at the 10 entries that generated the most discussion.  I’ve edited the output here, to highlight just the relevant parts with counts in the first column: So the keynotes and the PLoS session on writing a paper were popular.  We can look at the “likes” too: A slightly-different picture, with some non-keynotes creeping into the list.  This confirms my FriendFeed experience; whilst you might assume that “liking” takes less effort, people will in fact comment at length if they really care about the topic. How many posts generated no discussion? Quite a high proportion – around 69%.  It did seem as though there was less online activity this year – perhaps attendees could explain why?  I have heard rumours that the wireless connectivity was not optimal. 3. Who contributed?
Let’s name some names!
There is, surely, an apply-type function to grab names and comments from the list, ismb.data and count up the comments.  In its absence, here once again is my ugly solution which does at least employ plyr: 4. Activity over time
Finally, let’s bring in ggplot2, to display daily comment activity over the course of the conference.  This is pretty rough and ready.  I’m sure that you can do better.  ISMB2010 comments July 9-13 In 2011, ISMB returns to Vienna.  Let’s hope for more conference microblogging in the years ahead. "	 0 Comments
The wonderful world of vim	https://www.r-bloggers.com/2010/07/the-wonderful-world-of-vim/	July 20, 2010	xi'an	When searching for a way to recover control carriage return symbols ^M into genuine ones (under vim), I found this handy page of vim tips. And then saw that the enthusiastic author had even a blog about vim! Obviously, this post will be meaningless for all readers not aware of the joys of the vim editor, so let me stress that “vim is a highly configurable text editor built to enable efficient text editing. It is an improved version of the vi editor distributed with  most UNIX systems. vim is distributed free as charityware. If you find vim a  useful addition to your life please consider helping needy children in Uganda.” vim has now reached version 7.3a [beta] 	 0 Comments
R: Clash of the cannon cycles	https://www.r-bloggers.com/2010/07/r-clash-of-the-cannon-cycles/	July 19, 2010	Matt Asher	 Imagine a unit square. Every side has length 1, perfectly square. Now imagine this square was really a fence, and you picked two spots at random along the fence, with uniform probability over the length of the fence. At each of these two locations, set down a special kind of cannon. Just like the light cycles from Tron, these cannons leave trails of color. To aim each cannon, pick another random point from one of the three other sides of the fence, and aim for that point. Sometimes there will be a collision within the square, other times no. The image at top shows the results of five trials. The red dots are where the trails from a pair of cannons collided. My burning question: What is the distribution for these dots? Before reading on, try to make a guess. Where will collisions be most likely to happen? Somewhere in the world, there lives a probabilist who could come up with a formula for the exact distribution in an hour, but that person doesn’t live in my house, so I took the Monte Carlo approach, coded in R: After carefully writing and debugging much more code than I expected, I ran a trial with several thousand cannon fires and plotted just the collisions. Here is what I saw:  Looks pretty uniform, doesn’t it? If it is, I will have gone a very long way just to replicate the bi-variate uniform distribution. My own original guess was that most collisions, if they happened in the square, would be towards the middle. Clearly this wasn’t the case. Looks can be deceiving, though, so I checked a histogram of the x’s (no need to check the y’s, by symmetry they have the same distribution):  Very interesting, no? The area near the edges appears more likely to have a collision, with an overall rounded bowl shape to the curve. The great thing about Monte Carlo simulations is that if something unexpected happens, you can always run it again with more trials. Here I changed “iters” to 100,000, ran the code again, and plotted the histogram.  Now its clear that the distribution spikes way up near the edges, and appears to be essentially flat for most of the middle area. It seems like it may even go up slightly at the very middle. Just to be sure, I ran a trial with one million iterations:   Now it definitely looks like a small upward bulge in the middle, though to be sure I would have to do run some statistical tests or use an even larger Monte Carlo sample, and given how inefficient my code is, that could take the better part of a week to run. So for today I’ll leave it at that. One final statistic of note: During my run of one million iterations, 47.22% of all collisions happened inside the box. What do you think, is the true, theoretical ratio of collisions within the box a rational number?  	 0 Comments
Indian state population plot	https://www.r-bloggers.com/2010/07/indian-state-population-plot/	July 19, 2010	prasoonsharma		 0 Comments
Analyze Online R User Conference Data	https://www.r-bloggers.com/2010/07/analyze-online-r-user-conference-data/	July 19, 2010	C		 0 Comments
R Beginner’s Guide Book Update 7/19/2010	https://www.r-bloggers.com/2010/07/r-beginners-guide-book-update-7192010/	July 19, 2010	John Quick	Update: Statistical Analysis with R is now available! I am excited to announce that I have submitted the entire first draft of my R Beginner’s Guide book, which is to be published through Packt. The tenth and final chapter was submitted a full month ahead of schedule. The printed book could become available in as little as three to four months. Below is a list of the major topics covered in the R Beginner’s Guide. 	 0 Comments
Welcome to Gosset’s student	https://www.r-bloggers.com/2010/07/welcome-to-gosset%e2%80%99s-student/	July 19, 2010	respiratoryclub	Welcome to Gosset’s student, a blog about statistics, with a focus on using R.  We’re all learning R, as it’s constantly being improved.  The blog will aim for brevity, and a focussed approach to getting some stats done, rather than elegance of code.  Discussions which show how to replicate the result in a more elegant fashion, or which criticize the approach which the blog takes will be most welcome. 	 0 Comments
Creating a Presentation with LaTeX Beamer – Boxes	https://www.r-bloggers.com/2010/07/creating-a-presentation-with-latex-beamer-%e2%80%93-boxes/	July 19, 2010	Ralph	We can add coloured boxes with text or mathematics into a LaTeX beamer presentation which is particularly useful if we have definitions, theorem or computer code to highlight this information that may not be so accessible within a paragraph of text. Fast Tube by Casper The easiest way to create a box is to use the various pre-defined environments such as definition: We might want to create a box with our own title in which case the exampleblock environment can be used with the new title as an argument to this environment: Other useful resources are provided on the Supplementary Material page. 	 0 Comments
New StackOverflow site for Statistics	https://www.r-bloggers.com/2010/07/new-stackoverflow-site-for-statistics/	July 19, 2010	David Smith	Since launching StackOverflow.com as a Q&A forum for programmers, the StackExchange network is now launching about three new sites a week on topics as specialized as cooking, jobs, and many many more. There’s a well-defined, community-oriented process for setting up a new site: first it must be proposed, and then enough potential users must commit to the site and define its mission with sample questions. Finally, with enough committed users it enters a beta phase before being formally launched. A new site devoted to questions about Statistics has completed the proposal and definition phases, is about to enter the beta phase. A typical question, voted as an example of the type of question appropriate to the site, is:  Can you give an example of where I might prefer to use a z-test vs a t-test?  The more people that participate in the beta and support a thriving ecosystem of questions and answers, the more likely it is that the Statistics StackExchange will be a reality. You can’t ask or answer questions at the site just yet, but you can commit to be a participant in the site and you’ll receive an invitation to the beta test. If you’re interested, follow the link below and click the “commit” button. Update: The private beta has now started, so if you hadn’t already committed, you’ll have to wait until the public beta starts at noon Pacific time on Monday, July 26. StackExchange Area 51: Statistics  	 0 Comments
Using R for Data Management, Statistical Analysis and Graphics soon to start shipping	https://www.r-bloggers.com/2010/07/using-r-for-data-management-statistical-analysis-and-graphics-soon-to-start-shipping/	July 19, 2010	Nick Horton		 0 Comments
Flexmix taks long time…	https://www.r-bloggers.com/2010/07/flexmix-taks-long-time/	July 19, 2010	Shige		 0 Comments
Npmlreg and Flexmix	https://www.r-bloggers.com/2010/07/npmlreg-and-flexmix/	July 18, 2010	Shige		 0 Comments
Join the Statistical analysis Q&A beta	https://www.r-bloggers.com/2010/07/join-the-statistical-analysis-qa-beta/	July 18, 2010	danganothererror	Statistical Analysis Q&A is a web page dedicated to exchange questions, answers and ideas primarily on statistics. See http://stackoverflow.com/ to get the general idea what I’m talking about. You still have a chance to sign up for closed beta testing. If you miss it, no worries mate, after about a week, it will be opened for the whole world to see. See also Tal’s writing on the wall of R-statistics blog. Just recently, a MetaOptimize page has been launched to serve the statistical needs of people everywhere. The page has been building up steam but I’m still not sure which way it’s gonna go. I hope MO and SA will compliment each other and will not get caught in the stats feud. 	 0 Comments
Cherry Picking to Generalize ~ retrospective meta-power analysis using Cohen’s f^2 of NASA temp + visualization	https://www.r-bloggers.com/2010/07/cherry-picking-to-generalize-retrospective-meta-power-analysis-using-cohen%e2%80%99s-f2-of-nasa-temp-visualization/	July 17, 2010	apeescape	"Previously, I plotted a grid of NASA GISS global temps in ggplot2 to show general trends by the brute force method. Here, I will again use the brute force method to do a simple power analysis on a portion of the data (data here). The general aim is to figure out what the minimum sample size is needed to show strong confidence in the significance of the trend (hopefully in a compact but comprehensive way). There are many blog posts on the trend since 1998, where some people call an insignificant (too short of a) trend, while others say it is a sign for a new trend. Let’s put some numbers by doing a power analysis, assuming 0.8 is good power and 0.6 is OK. Power is the probability of correctly assessing significant effects relative to the null distribution (hypothesis). Correctly assessing significant effects means that the estimated parameter of interest is inside the rejection region. When the parameter is outside of the rejection region, we call it Type I Error. We usually denote  as the significance level to choose a threshold between rejection and acceptance of the null. This means that the power is directly related to the rejection threshold. The lower (and easier) the threshold, the stronger the effect size needs to be to counteract. The effect size is basically the strength of the relationship between the variables. This could be a standardized measures that are easier to calculate (getting rid of variance complications) or unstandardized where it’s easier to interpret (Thomas 1997). The effect size is related to power usually by using the F-distribution. Add to: Facebook | Digg | Del.icio.us | Stumbleupon | Reddit | Blinklist | Twitter | Technorati | Yahoo Buzz | Newsvine Aside from changing the experiment / observational study altogether, the prime way to strengthen effect size is to increase the sample size. Therefore, you see many plots that show power as a function of sample size. Power is used prospectively to assess sample sizes that are needed to get significant results from experiments. It is also used retrospectively to assess our confidence in our results. For this example, we use the post-hoc power analysis and a standardized measure of effect size (Cohen’s ). The idea is to extend (in an ad-hoc visualization) to compensate for some of its weaknesses for this relatively easy method. Time series issues are ignored for now, so hopefully people can look at this article with a charitable eye. I used the pwr package in R to calculate Cohen’s f^2 (Quick-R has a good reference). Cohen’s f^2 requires the coefficient of determination R^2 of linear regression models:  What I did was calculate a bunch of linear regressions of length 5 ~ 30, categorized by the final year of the time series. So for 2009 (“the latest year”), I calculated all the linear trends from 2004 ~ 2009 to 1979 ~ 2009. I calculated this series of series for a series from 1949 to 2009 (if that makes sense!). One of the criticisms of Thomas (1997) is that  measures (and other effect sizes) have high variability. Just calculating one  of the whole dataset is not reliable, and it becomes just a restatement of the statistical significance of the data. But in this case, we reduce this variability (or more like show patterns) by bootstrapping over the whole dataset. These bootstraps are aggregated into 10-year intervals, which assume that these categories are related to each other (dubious, but hey). The end result is a multiple power vs. sample size graphs plastered on the wall, aggregated by 10-year intervals:  This graph has 5 parts (aagh, info. overload!): The last point is there to show the direction of the significance, and its variability. We see that the recent data has higher over all, and the first three intervals have generally higher powers than the theoretical values after a sample size of 15. The variations in the smoothed curves are only useful in relation to the other smoothed curves (doesn’t really represent the real variability in power). The mean of the smoothed curves are useful to the point where linearity and autocorrelation assumptions aren’t violated. The first (most recent) category is relatively clean, while the third category breaks down after about the 25th sample size. If I were to say if a 1998 to current trend was to have sufficient power, I would say (to be a little conservative), we should have data up to 2012. Sensitivity can be assessed by using different aggregations. For comparison, this is the graph w/o the aggregations:  R Code: Refs: 
Filed under: Climate Change, Power Analysis, pwr, R        

 "	 0 Comments
Latent variable analysis package for R	https://www.r-bloggers.com/2010/07/latent-variable-analysis-package-for-r/	July 17, 2010	Shige		 0 Comments
What (Search Engines Think) People Want	https://www.r-bloggers.com/2010/07/what-search-engines-think-people-want/	July 17, 2010	C		 0 Comments
CoRe in CiRM [end]	https://www.r-bloggers.com/2010/07/core-in-cirm-end/	July 17, 2010	xi'an	Back home after those two weeks in CiRM for our “research in  pair” invitation to work on the new edition of Bayesian       Core, I am very grateful for the support we received from CiRM and through it from SMF and CNRS. Being “locked” away in such a remote place brought a considerable increase in concentration and decrease in stress levels. Although I was planning for more, we have made substantial advances on five chapters of the book (out of nine), including a completely new chapter (Chapter 8) on hierarchical models and a thorough rewriting of the normal chapter (Chapter 2), which along with Chapter 1 (largely inspired from  Chapter 1 of Introducing  Monte Carlo Methods with R, itself inspired from the first edition of Bayesian        Core,!). is nearly done. Chapter 9 on image processing is also quite close from completion, with just the result of a batch simulation running on the Linux server in Dauphine to include in the ABC section. As the only remaining major change is the elimination of reversible jump from the mixture chapter (to be replaced with Chib’s approximation) and from the time-series chapter (to be simplified into a birth-and-death process). Going back to the CiRM environment, I think we were lucky to come during the vacation season as there is hardly anyone on the campus, which means no car and no noise. The (good) feeling of remoteness is not as extreme as in Oberwolfach, but it is truly a quality environment. Besides, being able to work 24/7 in the math library is a major plus. as we could go and grab any reference we needed to check. (Presumably, CiRM is lacking in terms of statistics books, compared with Oberwolfach, still providing most of the references we were looking for.) At last, the freedom to walk right out of the Centre into the national park for a run, a climb or even a swim (in Morgiou, rather than Sugiton) makes working there very tantalising indeed! I thus dearly hope I can enjoy again this opportunity in a near future…   	 0 Comments
Distorting the Electoral Connection? Partisan Representation in Confirmation Politics	https://www.r-bloggers.com/2010/07/distorting-the-electoral-connection-partisan-representation-in-confirmation-politics/	July 17, 2010	Andrew Gelman		 0 Comments
More StackExchange sites	https://www.r-bloggers.com/2010/07/more-stackexchange-sites/	July 16, 2010	Rob J Hyndman	The StackExchange site on Statistical Analysis is about to go into private beta testing. This is your last chance to commit if you want to be part of the private beta testing. Don’t worry if you miss out — it will only be a week before it is then open to the public. There is also a StackExchange site proposal for TeX, LaTeX and friends. Presumably that means that most of the LaTeX questions on StackOverflow will then move to this new site. It still needs a couple of hundred more people to commit before it can be launched, so if you are interested in LaTeX, please commit to being part of it. Another site proposal that may be of interest to readers of this blog is the one on English language usage. A few proposals are already open to the public for beta testing. One that I’ve been using a little is Web Apps which is useful for questions on Gmail, Google reader, WordPress, etc.   	 0 Comments
Documenting R‘s Connections Internals	https://www.r-bloggers.com/2010/07/documenting-r%e2%80%98s-connections-internals/	July 16, 2010	Matt Shotwell	In studying R‘s connections source code, I’ve put together a series of notes about their workings. Rather than let the notes go where most of my notes go (I have no idea  ), I decided to do some proofreading and make the notes available to others who might be interested. These notes should make the task of extending or maintaining the connections source code easier for programmers unfamiliar with the code. R‘s connections code has been quite stable for 5 years (according to the Subversion log). However, I will do my best to keep these notes up-to-date with changes in to the connections internals, and periodically add notes on other features. The document is in HTML and PDF. I would be happy to provide the texinfo document also, on request. 	 0 Comments
Starting an EC2 Machine Then Setting Up a Socks Proxy… From R!	https://www.r-bloggers.com/2010/07/starting-an-ec2-machine-then-setting-up-a-socks-proxy%e2%80%a6-from-r/	July 16, 2010	JD Long	I do some work from home, some work from an office in Chicago and some work on the road. It’s not uncommon for me to want to tunnel all my web traffic through a VPN tunnel. In one of my previous blog posts I alluded to using Amazon EC2 as a way to get around your corporate IT mind control voyeurs service providers. This tunneling method is one of the 5 or so ways I have used EC2 to set up a tunnel. I used to fire these tunnels up manually using the Amazon AWS Management Console then opening a shell prompt and entering: the -i switch tells ssh to use my RSA identity file stored in ~/MyPersonalKey.pem the machine name (ec2-184-73-41-72.compute-1.amazonaws.com) I get from the AWS Management Console the -D is the magic. -D opens an dynamic port forwarding tunnel between my Linux box and the EC2 machine. This is, for all intent and purposes, an encrypted SOCKS4 proxy on port 9999 of localhost. Then I just have to change my proxy settings in Firefox to use use a SOCKS host. Now that’s all pretty easy. And I like easy. But it’s not easy ENOUGH. You see, I’m lazy. I’m not just lazy in the “I’ll do it mañana” sort of way, but in the “I’m too damn lazy to click my mouse 5 times” way. So I want this easier. Well, I can make the proxy settings in Firefox easier through the use of the Quick Proxy extension for Firefox. That’s a good start. It turns on and off the proxy with a single mouse click. But I still have to go into the AWS management web site, fire up a machine then log in via SSH. Let’s make that part easier! While it’s not simple to install and configure, the EC2 command line tools are going to be required in order to make a script that fires up EC2 and then connects to the instance with ssh. I struggled getting the tools to run until I found this tutorial. Your file locations and names may be different than the tutorial. Change appropriately. I followed the tutorial instructions but I created a key named ec2ApiTools which will come in handy later. After you get the EC2 tool up and running and you can do something like list the available AMIs without an error you can stop with the tutorial. I’ve been doing a lot of shell scripting lately so I said to myself, “Self, let’s script the ssh connection in R!” For the record, I always end my impredicative in an explanation point which I verbally pronounce as, “BANG!” As a result, when I talk to myself it sounds like two 10 year old boys playing cops and robbers. Anyhow, I did script it with R using Rscript. Because I’m a man who listens to myself. And since you were kind enough to slog through my channeling the drunken ghost of James Joyce, here’s my script: If you’re reading this in an RSS reader of for some other reason don’t see an R script above, here’s your link. The only two EC2 API commands I use in the script are  ec2-run-instances which starts the instance and ec2-describe-instances which gives me a list of running instances and their details.The rest of the script is simply parsing the output and figuring out which instances was started last. I’ve now set up a launcher panel item that starts the script. Then when I see the xterm window come up I click the little red button in the lower right corner of my browser which switches on the Firefox proxy. Then I’m safe to surf Soldier of Fortune Magazine without the interference of my corp firewall. 	 0 Comments
Because it’s Friday: Analytics of an Engagement	https://www.r-bloggers.com/2010/07/because-its-friday-analytics-of-an-engagement/	July 16, 2010	David Smith	"The always-interesting R blogger and political scientist Drew Conway — and self-styled luckiest man on earth — just announced some very happy news: he just got engaged to his future wife! Congratulations, Drew! (Drew proposed the traditional way, on a lovely European vacation. But if he’d just held off a couple of days he could have proposed via towel-clad adonis, instead.) Always the analyst, Drew did manage to capture some metrics on the impact of the engagement: the rate of posts to his fiancée’s facebook wall after she changed her relationship status to “Engaged” (click to enlarge): 
 Drew used the Facebook API to capture the data, R to create a kernel smooth of the post rate segmented by male and female friends, and the ggplot2 package to chart the results. Looks like the women are quicker to respond, but the men keep up the posts longer. (Which reminds me: I still have some Christmas cards to write around here somewhere…)   And the chart makes for a lovely design for a wedding invitation, too!  Zero Intelligence Agents: Anatomy of a Life-Milestone Announcement on Facebook  "	 0 Comments
Generating Balanced Incomplete Block Designs (BIBD)	https://www.r-bloggers.com/2010/07/generating-balanced-incomplete-block-designs-bibd/	July 16, 2010	Ralph	The Balanced Incomplete Block Design (BIBD) is a well studied experimental design that has various desirable features from a statistical perspective. The crossdes package in R provides a way to generate a block design for some given parameters and test wheter this design satisfies the BIBD conditions. For a BIBD there are v treatments repeated r times in b blocks of k observations. There is a fifth parameter lambda that records the number of blocks where every pair of treatment occurs in the design. We first load the crossdes package in our sessions: The function find.BIB is used to generate a block design with specific number of treatments, blocks (rows of the design) and elements per block (columns of the design). Consider an example with five treatments in four blocks of three elements. We can create a block design via: This design is not a BIBD because the treatments are not all repeated the same number of times in the design and we can check this with the isGYD function. For this example: This confirms what we can see from the design. Let us instead consider a design with seven treatments and seven blocks of three elements to see whether we can create a BIBD with these parameters: In this situation we are able to generate a valid BIBD experiment with the specified parameters. 	 0 Comments
Want to join the closed BETA of a new Statistical Analysis Q&A site – NOW is the time!	https://www.r-bloggers.com/2010/07/want-to-join-the-closed-beta-of-a-new-statistical-analysis-qa-site-%e2%80%93-now-is-the-time/	July 16, 2010	Tal Galili	The bottom line of this post is for you to go to: Stack Exchange Q&A site proposal: Statistical Analysis  And commit yourself to using the website for asking and answering questions. (And also consider giving the contender, MetaOptimize a visit) * * * * A month ago I invited readers of this blog to commit to using a new Q&A website for Data-Analysis (based on StackOverFlow engine), once it will open (the site was originally proposed by Rob Hyndman). And now, a month later, I am happy to write that over 500 people have shown interest in the website, and choose to commit themselves.  This means we we have reached 100% completion of the website proposal process, and in the next few days we will move to the next step. The next step is that the website will go into closed BETA for about a week.  If you want to be part of this – now is the time to join ( From being part in some other closed BETA of similar projects, I can attest that the enthusiasm of the people trying to answer questions in the BETA is very impressive, so I strongly recommend the experience. If you won’t make it by the time you see this post, then no worries – about a week or so after the website will go online, it will be open to the wide public. (p.s: thanks Romunov for pointing out to me that the BETA is about to open) I would like to finish this post with mentioning MetaOptimize.   This is a Q&A website which is of a more “machine learning” then a “statistical” community.  It also started out some short while ago, and already it has around 700 users who have submitted ~160 questions with ~520 answers given.  From my experience on the site so far, I have enjoyed the high quality of the questions and answers. When I first came by the website, I feared that supporting this website will split the R community of users between this website and the area 51 StackExchange website. But after a lengthy discussion (published recently as a post) with MetaOptimize founder, Joseph Turian, I came to have a more optimistic view of the competition of the two websites.  Where at first I was afraid, I am now hopeful that each of the two website will manage to draw a tiny bit of different communities of people (that would otherwise wouldn’t be present in the other website) – thus offering all of us a wider variety of knowledge to tap into. See you there… 	 0 Comments
Rcpp 0.8.4	https://www.r-bloggers.com/2010/07/rcpp-0-8-4-2/	July 15, 2010	Thinking inside the box	"

This release builds upon
release 0.8.3.
Highlights include changes to the sugar framework for highly expressive
C++ constructs which gained new vector function as well as a first set of
matrix function. As well, unit tests have been reorganised in such a way that
we end up with a lot fewer compilations (but of several files at once) which reaps
significant speed gains. Date calculation now use the same mktime()
function R itself uses (and which comes from Arthur Olson’s tzone library).

The NEWS entry follows below:
 
As always, even fuller details are in 
Rcpp Changelog page and the 
Rcpp page which also
leads to the downloads, the 
browseable
doxygen docs and zip files of doxygen output for the standard formats.
A local directory  has
source and documentation too. 
Questions, comments etc should go to the
rcpp-devel mailing list
off the R-Forge page

 "	 0 Comments
CoRe in CiRM [6]	https://www.r-bloggers.com/2010/07/core-in-cirm-6/	July 15, 2010	xi'an	This is the last day at CiRM for our “research in pair” working on the new edition of Bayesian      Core. I mostly completed the revision of the chapter on Bayesian imaging, including stuff on Markov random fields and mainly on ABC algorithms. All that remains to be done about this chapter is the processing of the Scottish lip cancer dataset from an MRF perspective. I included the comparison below of the ABC approximation (variability) with the truth in the case of the normal mean test.  Jean-Michel has almost finished the chapter on hierarchical modelling, before I get a go at the style of the chapter. Thus ends a very productive ten-days where we dealt with five chapters out of nine, the remaining four being less prone to deep modification in this new edition… 	 0 Comments
Anatomy of a Life-Milestone Announcement on Facebook	https://www.r-bloggers.com/2010/07/anatomy-of-a-life-milestone-announcement-on-facebook/	July 15, 2010	Drew Conway	As I have mentioned, I recently returned for a lovely trip to Europe.  While on vacation my brilliant, beautiful, funny, and all around perfect girlfriend accepted my invitation to be my wife.   Pause for shared overwhelming feeling of joy… While I am still basking in the glow of being the luckiest man on Earth, as a true data geek I could not let this opportunity to analyze a novel data set escape me. One of the most fascinating aspects of social media is how it has changed the way life-milestones, like getting engaged, are announced.  Facebook’s ‘Relationship Status’ feature allows users to inform all of their friends at once about these large life changes.  Such announcements are often met with a sudden deluge of comments and wall postings, so I thought: wouldn’t it be interesting to collect this data and analyze the frequency of decay of these postings? Though I am not on Facebook, my fiancée is, and with a little help from Facebook’s API and R’s ggplot2 library I was able to collect and analyze this data.  Below I present (with permission) the data on from my fiancée’s wall for the first 48 hours after she changed her Relationship Status from ‘In a Relationship’ to ‘Engaged’.  Interesting.  A huge spike in the first hour, a drop and flattening over the next two hours, and finally another large drop with sporadic spikes.  Women dominate the initial posts, while the gender difference vanish as posting frequency decreased more late-comers make posts. Of course, all of this is secondary to that fact that—Kristen—I love you and I cannot wait to spend the rest of my life with you! 	 0 Comments
Wanted: Big-data beta testers	https://www.r-bloggers.com/2010/07/wanted-big-data-beta-testers/	July 15, 2010	David Smith	We’re nearing completion of the package of statistical tools for very large data sets that I gave an early preview of at R/Finance 2010. It will be released for Revolution R Enterprise later this year, but we’re looking for some R users with big data sets to put the 1.0 version through its paces in the beta program and offer suggestions for subsequent versions.  For now, it’s a limited beta program — we’re looking for testers with some fairly specific attributes:      If you think you might fit the bill, send me an email at [email protected] with a brief description of your setup and we’ll see if we can get you in to the beta program. Thanks!    	 0 Comments
R: a “Rock Star” for Business Intelligence	https://www.r-bloggers.com/2010/07/r-a-rock-star-for-business-intelligence/	July 15, 2010	David Smith	TDWI (The Data Warehousing Institute) recently published a comprehensive article about R and increasing level of activity around it from commercial organizations, including Revolution Analytics. The article opens with: In statistical circles, “R” is the name of an open source programming language for statistical analysis. These days, it might also be shorthand for “rock star.” Many of the companies cited in the article look to R to enhance business intelligence and data warehousing offerings. Jaspersoft, in conjunction with Revolution R, delivers “Mashboards” combining advanced analytics with traditional Business Intelligence displays). IBI (Information Builders, Inc.) has developed a “WebFOCUS-like front-end GUI” for R. And Netezza integrates R with their data warehousing appliance. Of course, big-name players like SAS, SPSS (IBM) and Oracle also tout integrations with R these days, but the article suggests this is more of a catch-up play to the new-wave companies that pioneered integrations with R. As Michael Corcoran from IBI says: “The R technology is so well adopted, it’s almost a no-brainer for customers,” said Corcoran, in a interview this Spring. “If you look at it, SAS now integrates with R … because they have to. … It’s not just universities anymore that are using [R]; it’s financial services, it’s retail.” Revolution Analytics’ own efforts to bring R to the commercial world are featured heavily in the article. As Jeff Erhardt (Revolution’s COO) says, “With all of the growth in R, [companies are] starting to see their peers use R and … they’re also getting R creeping into their organizations by virtue of these recent grads who are using it,” Erhardt continues. “Our pitch is to come in and say, ‘Hey, the world and business is going in this direction, whether you like it or not. We have a solid product built on this base-level of R, which is so well accepted, and over the next six months we’ll be exponentially expanding our capabilities.” Read the complete article at the link below for more information from Jeff about our product roadmap, including the forthcoming capabilities for large data analysis, web services integration and the thin-client GUI. TDWI: Advanced Analytics on a Budget, R-Style  	 0 Comments
ggplot2 GSOC progress	https://www.r-bloggers.com/2010/07/ggplot2-gsoc-progress/	July 15, 2010	Tal Galili	(Written by Ian Fellows) The RForge build error has been fixed. the package can now be tried with: install.packages(“Deducer”,,”http://www.rforge.net“,type=”source”) 	 0 Comments
Maps, Geocoding, and the R User Conference 2010	https://www.r-bloggers.com/2010/07/maps-geocoding-and-the-r-user-conference-2010/	July 14, 2010	C		 0 Comments
Creating a Presentation with LaTeX Beamer – Tables	https://www.r-bloggers.com/2010/07/creating-a-presentation-with-latex-beamer-%e2%80%93-tables/	July 14, 2010	Ralph	Tables of information can be included in a LaTeX beamer presentation in the same way that they would be incorporated into any other LaTeX document. The tabular environment is used and, if necessary, the tables could be numbered but this probably doesn’t make as much sense as labelling and numbering tables within an article or book. Fast Tube by Casper For example, if we wanted to add a table of some of the common geom elements for ggplot2 to a slide we could use the following code: This is a basic example of creating a table and there are many examples of how the appearance of a table can be enhanced in LaTeX, see for example here or here. Other useful resources are provided on the Supplementary Material page. 	 0 Comments
R’s Normal Distribution Functions: rnorm and pals	https://www.r-bloggers.com/2010/07/rs-normal-distribution-functions-rnorm-and-pals/	July 14, 2010	dylan	The rnorm() function in R is a convenient way to simulate values from the normal distribution, characterized by a given mean and standard deviation. I hadn’t previously used the associated commands dnorm() (normal density function), pnorm() (cumulative distribution function), and qnorm() (quantile function) before– so I made a simple demo. The *norm functions generate results based on a well-behaved normal distribution, while the corresponding functions density(), ecdf(), and quantile() compute empirical values. The following example could be extended to graphically describe departures from normality (or some other distribution– see rt(), runif(), rcauchy() etc.) in a data set.  read more 	 0 Comments
Revolution at useR! 2010	https://www.r-bloggers.com/2010/07/revolution-at-user-2010/	July 14, 2010	David Smith	Revolution Analytics is a proud sponsor of this year’s annual R user conference, useR! 2010, and many members of the Revolution team will be there at Gaithersburg next week. We’ll be hosting a booth at the conference where you can come up and meet the team, and see some of the new features being developed for Revolution R in action. Several members of the development and leadership teams are also giving presentations at the conference:   Revolution Analytics will also be hosting a party to celebrate the official launch of inside-R.org (a community site for R sponsored by Revolution Analytics) with many new features. The party will be held on Wednesday night starting at 8PM at the Crowne Plaza in Rockville. Buses will be provided after the useR! poster session at NIST. All R users are welcome (you don’t need to be registered for the useR! conference), and admission is free. We’ll even be giving away an Xbox 360 and a Zune (with thanks to Microsoft). I’m really looking forward to seeing old R friends again and meeting new ones next week. Come and say hi! Revolution Analytics: useR! 2010   	 0 Comments
Homicide in North America	https://www.r-bloggers.com/2010/07/homicide-in-north-america/	July 14, 2010	Diego Valle-Jones		 0 Comments
Short Open Source Q&A with Revolution Analytics	https://www.r-bloggers.com/2010/07/short-open-source-qa-with-revolution-analytics/	July 14, 2010	Matt Shotwell	"I recently e-mailed David Smith of Revolution Analytics with a few questions about their relationship with the R-project, and how they handle R‘s source code. David mentioned, and I’m flattered that my email motivated an additional page on the Revolution website. Beyond this, I have no other relationship with the company. I’d like to thank David and Revolution for their openness and willingness to interact with the open-source R community. David kindly responded with the following (reproduced with permission): 
Q. Is Revolution R a “downstream” project of the R-project?
A. No. Our model is Open Core: we bundle proprietary components with the open-source R project. We do sometimes make minor changes to the R source code when we distribute R, but you couldn’t consider this a “downstream version”: we always update from the latest sources for R from the R Project (and re-incorporate any of our changes to the new version, where necessary). In general our preference is to contribute changes to R directly to the R project, which we’ve done several times in the past and will continue to do so.
 
Q. Can the community see Revolution’s changes/additions to R source code?
A. Certainly — all the changes are listed in the ChangeLog file in the GPL sources we distribute; we provide a link to those sources via email when you download Revolution R, and also from http://www.revolutionanalytics.com/downloads/
 
Q. How does Revolution sell programs that use R source code? Isn’t there a license issue?
A. We use an Open Core software model, which is an established business model around open-source software. Mark Radcliffe, the General Legal Counsel for the Open Source Initiative, helps us ensure we comply with all aspects of the GPLv2. He’s written about Open Core models here: http://lawandlifesiliconvalley.com/blog/?p=485
 The ‘Open Core’ model (in my shallow understanding), is an emerging business strategy for companies using open-source software. In contrast with the strategy of say, Canonical (Ubuntu), businesses operating under the Open Core model will offer an open-source ‘base’ package, and then a commercially licensed suite of enhancements to the base package. In the case of Revolution R, the base package is simply R (perhaps with minor changes of the type David mentions above). The commercial enhancements to Revolution include, for example, the use of Intel’s Math Kernel Library (MKL) and compiling under an Intel compiler.  I took a moment to browse Revolution’s modifications to version 2.10.1 of R‘s source code (obtained from the link above). The changes between the two trees involved 92 files, 1010 insertions, and 224 deletions (according to diff). Browsing through the changes, I found most to be related to the Intel compiler, its extensions, and the MKL. In addition to file changes, there were many additional files. The changes and additions involved a total 544 files, 208691 insertions, and 319 deletions (according to diff). The majority of additional files result from the inclusion of the open-source JPEG library by the Independent JPEG Group, libpng, libtiff, and several other Windows-related fixes and extensions. "	 0 Comments
QQ plot of p-values in R using base graphics	https://www.r-bloggers.com/2010/07/qq-plot-of-p-values-in-r-using-base-graphics/	July 14, 2010	Stephen Turner		 0 Comments
Multidimension bridge sampling (CoRe in CiRM [5])	https://www.r-bloggers.com/2010/07/multidimension-bridge-sampling-core-in-cirm-5/	July 13, 2010	xi'an	Since Bayes factor approximation is one of my areas of interest, I was intrigued by Xiao-Li Meng’s comments during my poster in Benidorm that I was using the “wrong” bridge sampling estimator when trying to bridge two models of different dimensions, based on the completion (for  and  missing from the first model)  When revising the normal chapter of Bayesian     Core,  here in CiRM, I thus went back to Xiao-Li’s papers on the topic to try to fathom what the “true” bridge sampling was in that case. In Meng and Schilling (2002, JASA), I found the following indication, “when estimating the ratio of normalizing constants with different dimensions, a good strategy is to bridge each density with a good approximation of itself and then apply bridge sampling to estimate each normalizing constant separately. This is typically more effective than to artificially bridge the two original densities by augmenting the dimension of the lower one”. I was unsure of the technique this (somehow vague) indication pointed at until I understood that it meant  introducing one artificial posterior distribution for each of the parameter spaces and processing each marginal likelihood as an integral ratio in itself. For instance, if  is an arbitrary normalised density on , and  is an arbitrary function, we have the bridge sampling identity on :  Therefore, the optimal choice of  leads to the approximation  when  and . More exactly, this approximation is replaced with an iterative version since it depends on the unknown . The choice of the density  is obviously fundamental and it should be close to the true posterior  to guarantee good convergence approximation. Using a normal approximation to the posterior distribution of  or a non-parametric approximation based on a sample from , or yet again an average of MCMC proposals are reasonable choices. The boxplot above compares this solution of Meng and Schilling (2002, JASA), called double (because two pseudo-posteriors  and  have to be introduced), with Chen, Shao and Ibragim (2001) solution based on a single completion  (using a normal centred at the estimate of the missing parameter, and with variance the estimate from the simulation), when testing whether or not the mean of a normal model with unknown variance is zero. The variabilities are quite comparable in this admittedly overly simple case. Overall, the performances of both extensions are obviously highly dependent on the choice of the completion factors,  and  on the one hand and  on the other hand, . The performances of the first solution, which bridges both models via , are bound to deteriorate as the dimension gap between those models increases. The impact of the dimension of the models is less keenly felt for the other solution, as the approximation remains local. 	 0 Comments
House Data: 41k finance summaries from 2200 candidates	https://www.r-bloggers.com/2010/07/house-data-41k-finance-summaries-from-2200-candidates/	July 13, 2010	jjh	"I’d like to announce a new project by Offensive Politics called House Data, launching today. House Data is a large-scale extract of FEC Form 3 Summary of receipts of disbursements  (pdf warning) of every US House campaign from mid-2001 onward.  The traditional source for campaign finance summaries is the Candidate Summary File, which is a single set of summary statistics for a campaign for an entire electoral cycle. But a campaign files a new F3 at least quarterly, and before and after every election they participate in. Each F3 provides insight into where a campaign stands, and with access to this intra-cycle data we can better compare campaigns and perform more sophisticated analysis. The House Data file is built from these F3 reports, all 41,050 reports from 2,241 candidates for the US House since 2002.  Campaigns often update previously filed reports with amendments, so the file contains only the latest summary provided by a campaign.  The file is compiled automatically using FECHell into a zipped CSV format. New releases will be made within 3 days of a new batch of electronic filings, according to the FEC 2010 Filing Deadline Schedule (pdf warning).  Here is a simple example of a quarterly summary of total receipts and total disbursements made by all house campaigns in 2008:


 The House Data Project is live today, with more examples and a data dictionary. The latest version can always be downloaded from http://offensivepolitics.net/data/housedata-latest.zip.  If you have any questions, comments, or suggestions about the house data file please don’t hesitate to contact me. "	 0 Comments
Norman Nie on Internet Evolution radio	https://www.r-bloggers.com/2010/07/norman-nie-on-internet-evolution-radio/	July 13, 2010	David Smith	Revolution CEO Norman Nie just recorded a live podcast with Terry Sweeney of Internet Evolution Radio. In the 30-minute interview, Norman talked about the history of R, his time with SPSS, development plans for Revolution R, and how predictive analytics is impacting businesses, the Web, and even political opinion. You can hear the recorded interview at the link below.  Internet Evolution Radio: Norman Nie, CEO, Revolution Analytics 	 0 Comments
Area Plots with Intensity Coloring	https://www.r-bloggers.com/2010/07/area-plots-with-intensity-coloring/	July 13, 2010	Allan Engelhardt	"I am not sure apeescape’s ggplot2 area plot with intensity colouring is really the best way of presenting the information, but it had me intrigued enough to replicate it using base R graphics. The key technique is to draw a gradient line which R does not support natively so we have to roll our own code for that.  Unfortunately, lines(..., type=""l"") does not recycle the colour col= argument, so we end up with rather more loops than I thought would be necessary. (The answer is not to use lines(..., type=""h"") which, confusingly, does recycle the colour col= argument.  This one had me for a while, but the type=h lines always start from zero so you do not get the gradient feature.) We also get a nice opportunity to use the under-appreciated read.fwf function. The result is a decent gradient: 
I deliberately omitted the scale legend on the right hand side following Allan’s First Law of Happy Graphics: Thou shall not present the same information twice.
 
For less dense information, you should increase the line width.  That is left to the reader. (Hint: it is hard to get just right in base graphics, but lwd <- ceiling(par(""pin"")[1] / dev.size(""in"")[1] * dev.size(""px"")[1] / length(x)) could be a starting point for an approximation. We really need gradient-filled polygons in base R.)
 Jump to comments. 

R code for Chapter 1 of Non-Life Insurance Pricing with GLM
 Insurance pricing is backwards and primitive, harking back to an era before computers. One standard (and good) textbook on the topic is Non-Life Insurance Pricing with Generalized Linear Models by Esbjorn Ohlsson and Born Johansson. We have been doing some work in this area recently. Needing a robust internal training course and documented methodology, we have been working our way through the book again and converting the examples and exercises to R , the statistical computing and analysis platform. This is part of a series of posts containing elements of the R code. 

R code for Chapter 2 of Non-Life Insurance Pricing with GLM
 We continue working our way through the examples, case studies, and exercises of what is affectionately known here as “the two bears book” (Swedish björn = bear) and more formally as Non-Life Insurance Pricing with Generalized Linear Models by Esbjörn Ohlsson and Börn Johansson (Amazon UK | US ). At this stage, our purpose is to reproduce the analysis from the book using the R statistical computing and analysis platform, and to answer the data analysis elements of the exercises and case studies. Any critique of the approach and of pricing and modeling in the Insurance industry in general will wait for a later article. 

Employee productivity as function of number of workers revisited
 We have a mild obsession with employee productivity and how that declines as companies get bigger. We have previously found that when you treble the number of workers, you halve their individual productivity which is mildly scary. We revisit the analysis for the FTSE-100 constituent companies and find that the relation still holds four years later and across a continent. 

R: Eliminating observed values with zero variance
 I needed a fast way of eliminating observed values with zero variance from large data sets using the R statistical computing and analysis platform . In other words, I want to find the columns in a data frame that has zero variance. And as fast as possible, because my data sets are large, many, and changing fast. The final result surprised me a little. 

Feature selection: Using the caret package
 Feature selection is an important step for practical commercial data mining which is often characterised by data sets with far too many variables for model building. In a previous post we looked at all-relevant feature selection using the Boruta package while in this post we consider the same (artificial, toy) examples using the caret package. Max Kuhn kindly listed me as a contributor for some performance enhancements I submitted, but the genius behind the package is all his. "	 0 Comments
Hierarchical Visualizations in R and the Javascript InfoVis Toolkit	https://www.r-bloggers.com/2010/07/hierarchical-visualizations-in-r-and-the-javascript-infovis-toolkit/	July 12, 2010	C		 0 Comments
A quantum leap (CoRe in CiRM [4])	https://www.r-bloggers.com/2010/07/a-quantum-leap-core-in-cirm-4/	July 12, 2010	xi'an	Today, as I was trying to install SpatialEpi to use the Scotland  lip cancer data in the last chapter of Bayesian    Core, I realised my version of R, R Version 2.6.1, was hopelessly out of date! As I am also using Hardy Heron, a somehow antiquated version of Ubuntu on my Mac, upgrading R took some effort as well. I eventually found that adding the line deb http://cran.cict.fr/bin/linux/ubuntu hardy/ in /etc/apt/sources.list worked nicely. So I now moved two years forward in time!!! On top is my first attempt at plotting the dataset with my modified version of mapvariable. As it happens, another blog appeared today on R-bloggers about color gradients using ggplot2.  	 0 Comments
Charting the World Cup	https://www.r-bloggers.com/2010/07/charting-the-world-cup/	July 12, 2010	David Smith	"Now that Spain has won the World Cup, it’s interesting to go back and
look at some metrics from the matches and see if we can tease out what
characteristics made for a winning Cup team this time around.
Fortunately, the Guardian’s Data Blog has made a wealth of World Cup statistics available, with data on every player of every team (position, shots at goal, passes, tackles made, and saves), plus aggregate statistics for each team (goals, % shots on target, fouls, and much more). The data are ripe for analysis in R, especially given that you can download the data directly from the cloud as an R object with the following commands: players  teams  The method I’ve described before for accessing a Google Spreadsheet from R didn’t quite apply here, as those instructions assume you own the document (and have access to the Publish menu). But some experimentation and tweaking of the spreadsheet URL made it work: the key parameters seem to be the “&gid=” (sheet number) and “%range=” (cell ranges, use %3A to encode the colon) and “&output=csv” to download in CSV format. It would be nice if Google published the specs to form URLs like these, but as far as I know they don’t.  Anyway, a couple of bloggers have used these data to great effect to express the results of the World Cup visually using R graphics. For example, the R Charts blog used ggplot2 to look at the number of fouls committed by each team during the tournament: 
   (Personally, I would have sorted the rows by descending number of fouls, rather than alphabetically.) Interesting to see that Cup champions Spain are in the middle of the pack on fouls, whereas runners-up Netherlands lead this table (boosted heavily by their performance in the final). Blogger Jason Priem also took a look at the data, this time with a scatterplot of goals per game by fouls per game, related to how far each team advanced in the competition: 
  (Download Jason’s code for this chart here.) Again it’s interesting to see the positions of the two finalists here, with Netherlands on the extreme frontier for both fouls and goals, while spain is moderate on goals per game and near the lowest on fouls per games. It’s a rich dataset and I’m sure other Revolutions readers could come up with some equally interesting visualizations. If you do, tell us about it in the comments. Guardian Data Blog: World Cup 2010 statistics: every match and every player in data  "	 0 Comments
Example 8.2: Digits of Pi, redux	https://www.r-bloggers.com/2010/07/example-8-2-digits-of-pi-redux/	July 12, 2010	Ken Kleinman		 0 Comments
Launch R document from Smultron / Fraise	https://www.r-bloggers.com/2010/07/launch-r-document-from-smultron-fraise/	July 12, 2010	Timothée	Perhaps you know of the defunct Smultron, a lightweight editor for Mac OS X. There exists a new version called Fraise, with the same features. Among them, R syntax highlighting and the possibility to interact with the R Gui console. To do so, when in Fraise, just push Cmd-B to open the Commands window, then create a new command, and type You can assign a keyboard shortcut to this command. Whenever you will hit the keys, R.app will activate, set the good working directory, and execute your code. Now, you see that it is straightforward to modify this code / create another so that you execute the selected text only. While Fraise is less powerful than TextMate and its R plugin, it is free, and really easier to instal that SciViews or StatEt. 	 0 Comments
A robust Hotelling test…	https://www.r-bloggers.com/2010/07/a-robust-hotelling-test/	July 12, 2010	Manos Parzakonis	"Recently I was in need of testing a mean vector. I wrote a few lines of code in R and had it done perfectly. Hotelling test is one of the least interesting test to me. never really figured out why… At that time I had some time to search more about it. One of the most common things to search for a test is a robust version of it (at least that’s what I search for!). A little search in the 3rd page of google results leads to the following : The classical Hotelling test for testing if the mean equals a certain value or if two means are equal is modiﬁed into a robust one through substitution of the empirical estimates by the MM-estimates of location and scatter. The MM-estimator, using Tukey’s biweight function, is tuned by default to have a breakdown point of 50% and 95% location efﬁciency. This could be changed through the control argument if desired. Performs one and two sample Hotelling T2 tests as well as robust  one-sample Hotelling T2 test. The first uses MM and S estimators while the latter a Minimum Covariance Determinant one. You can get info on those on the links in the end of the post. What might be crucial to you is that MM/S estimators would be more time comsuming compared to MCD. A little demonstation is the following.. Time consuming as it may is I would stick with the Bootstrap method. What would you do? Read more Roelant, E.,  Van Aelst, S., and Willems, G. (2008),  “Fast Bootstrap for Robust Hotelling Tests,”  COMPSTAT 2008: Proceedings in Computational Statistics (P. Brito, Ed.)  Heidelberg: Physika-Verlag, to appear. Willems G., Pison G., Rousseeuw P. and Van Aelst S.  (2002),  A robust hotelling test,  Metrika, 55, 125–138. 

 "	 0 Comments
World Cup 2010 Statistics Plotted with R	https://www.r-bloggers.com/2010/07/world-cup-2010-statistics-plotted-with-r/	July 11, 2010	C		 0 Comments
using R + ess-remote with screen in emacs	https://www.r-bloggers.com/2010/07/using-r-ess-remote-with-screen-in-emacs/	July 11, 2010	Vinh Nguyen	Dear list, I brought up this issue before but a good solution never arised: being able to use screen on a remote server (so if something goes wrong on my side I can always resume that R session) inside of emacs in order to utilize ESS.  The closest thing I found to a good work flow was to use ansi-term or multi-term and copying and pasting code in Emacs (ESS only worked nicely with shell-mode; reason at the end).  Some would advise to use screen and open emacs inside of that screen, and voila, you have the luxuries of screen (attaching the session anywhere) and emacs+ESS (keybindings, etc).  However, I prefer to use one main emacs session on my laptop/netbook (all my configurations are there), where I can have multiple projects and multiple R jobs opened at once. I would like to share what I have working  (for the time being), with the help of Michael Zeller in case others are interested. In xterm (or the likes), ssh to the remote server and start screen.  Detach it.  (Need to do this first as starting the initial screen in emacs shell-mode becomes very ugly with the printing; resuming the same screen session also becomes messy in xterm) In emacs, M-x shell.  Set: ssh to remote server.  screen -r to resume the screen session.  start R.  M-x ess-remote. Send R code from R source files like before! To detach or do anything screen related, precede EACH keybinding with C-l.  For example, C-a C-d to detach will now be C-l C-a C-l C-d.  Yes this is cumbersome, but I don’t imagine screen keybinding to be used much at this stage since we are developing and debugging R code for say a simulation study =]. I would also like to note (for archival reasons) that ess-remote does not work with ansi-term and multi-term because of the inferior-ess-mode command, which stems from comint-mode and inferior-ess-send-input.  If you remove this command in ess-remote, you don’t get an error but u can only send one line of code at a time from the R file. Hope this helps someone out there. 	 0 Comments
Area Plots with Intensity Coloring ~ el nino SST anomalies w/ ggplot2	https://www.r-bloggers.com/2010/07/area-plots-with-intensity-coloring-el-nino-sst-anomalies-w-ggplot2/	July 10, 2010	apeescape	"I see many economy indicator graphs that show emphasis by shading in the curve under the area (while x-axis is time). The shade is stronger at higher values (example). I did this in R below (ggplot2). This was a little more difficult that I’d expected. The color gradients are good to color each individual points depending on the strength, but not good (AFAIK) to color regions outside the dataset. The functions, geom_area, geom_ribbon geom_area, geom_linerange and the like can color themselves for different pieces, but we can’t specify color gradients within the piece. So I basically created a grid of points with different strengths dependent on the location of the y-axis. Data is here (NOAA, first three lines commented out + I put a space in front of all minus signs) + similar examples at Learn R and Climate Charts and Graphs. Plot below:  Simpler and faster to just do points, but was not what I wanted.  R code: 
Filed under: Climate Change, ggplot2, R        

 "	 0 Comments
CoRe in CiRM [3]	https://www.r-bloggers.com/2010/07/core-in-cirm-3/	July 10, 2010	xi'an	Still drudging along preparing the new edition of Bayesian   Core. I am almost done with the normal chapter, where I also changed the Monte Carlo section to include specific tools (bridge) for evidence/Bayes factor approximation. Jean-Michel has now moved to the new hierarchical model chapter and analysed longitudinal  datasets that will constitute the core of the chapter, along with the introduction of DAGs. (meaning some time “wasted” on creating DAG graphs, I am afraid!) He is also considering including a comparison with OpenBUGS and JAGS implementations, with the convincing motive that the hierarchical models are used in settings where practitioners have no time/ability to derive Gibbs samplers for a whole collection of models they want to compare… And we are  vaguely wondering whether or not using a pun in the title, from Bayesian    Core to Bayesian    CoRe, in order to emphasise the R link. This morning, it took me half an hour to figure out how resetting new counters (like our exercise environment) in LaTeX, but I eventually managed it, thanks to the UK TeX Archive. 	 0 Comments
World Government Data Store API (R and Ruby)	https://www.r-bloggers.com/2010/07/world-government-data-store-api-r-and-ruby/	July 10, 2010	C		 0 Comments
bibtex 0.2-1	https://www.r-bloggers.com/2010/07/bibtex-0-2-1/	July 10, 2010	romain francois	I’ve uploaded version 0.2-1 of my bibtex package to CRAN.  This release anticipates changes in R 2.12.0, and structures bibtex entries in object of the new class bibentry. The release also fixes various parser and lexer bugs 	 0 Comments
Creating a Presentation with LaTeX Beamer – Bullet Lists	https://www.r-bloggers.com/2010/07/creating-a-presentation-with-latex-beamer-%e2%80%93-bullet-lists/	July 10, 2010	Ralph	When writing a presentation we might want to use a bullet list to highlight some key points that might be lost if they are part of a large body of text. We can use the standard LaTeX environments for creating lists within a beamer presentation in a straightforward way. Fast Tube by Casper The bullet lists can be created using the itemize or enumerate environments depending on the type of list that we want to appear on the slides. The itemize list uses symbols and is not numbered while the enumerate list is a numbered list so the choice will depend on the elements covered by the list. Within our slide we would create the list environment in exactly the same was as over LaTeX documents and the item command. An example of creating a bullet list of assumptions for a linear statistical model is shown here: This can be easily changed to an enumerate environment to make it a numbered list. Other useful resources are provided on the Supplementary Material page. 	 0 Comments
Visualizing the census	https://www.r-bloggers.com/2010/07/visualizing-the-census/	July 9, 2010	David Smith	"Now that the 2010 survey is over, you might be wondering what we can learn from the data when the aggregated results are published. For a good guide to the kinds of questions you’ll be able to answer, take a look at StatJump, where you can see tables and charts of the results of the 2000 census: population data (e.g. the largest cities), social demographic data (where are the most high school graduates?), economic data (which cities have the longest average commute?), and housing data (where I learned I work in the city with the most houses valued over $1M — although the housing crash may change that when the 2010 data are out). Nicest of all, many of the variables are presented as choropleths (created with R, of course), so you can see maps of the US showing how the variables are distributed geographically, like this: Percent of People Born in U.S. by County: 
 Check out the StatJump website at the link below to check out dozens of other charts and tables from the 2000 Census. StatJump: Home Page   "	 0 Comments
R and Oracle HR Part II – Plotting a single variable	https://www.r-bloggers.com/2010/07/r-and-oracle-hr-part-ii-plotting-a-single-variable/	July 9, 2010	C		 0 Comments
Les estivales 2010	https://www.r-bloggers.com/2010/07/les-estivales-2010/	July 9, 2010	romain francois	Les estivales 2010 ont commencées à montpellier.  	 0 Comments
Chatfield’s Plots in S-Plus	https://www.r-bloggers.com/2010/07/chatfields-plots-in-s-plus/	July 9, 2010	gjabel	I have recently finished reading the sixth edition of The Analysis of Time Series: An Introduction by Chatfield in our Statistics reading group. Whilst enjoying most of the book I got a little confused when looking at Appendix D: Some MINITAB and S-PLUS Commands. In the S-Plus section the author gives the code below to replicate his Figure 1.2. I was not too sure what was going on with the code, which gave a tonne of error messages, some of which might well have been typo’s by the publisher (_ instead of )? In addition, the author stresses the effort required to construct nice plots in S-Plus. This got me thinking that 1) his code was excessive (not just because it does not work) and 2) he could have saved a lot of his effort by using R. The R code below proves my point Much Tidier! Here is the plot..Note, if you only wanted labels every other January, as in p.2 of the book (but not in the S-Plus code), then you can use.. 	 0 Comments
Funny Math in Governor Approval Ratings	https://www.r-bloggers.com/2010/07/funny-math-in-governor-approval-ratings/	July 9, 2010	Matt Shotwell	Andrew Gelman wrote today about some erroneous U.S. Governor approval ratings, noting that the ratings for Janet Napolitano sum to 108%. In fact most of these ratings do not sum to 100%. I prepared a clean CSV file of the ratings, making use of R‘s XML library and the readHTMLTable function. The ratings data file is here approval.csv. There is so much variability here, we could start to think about the sampling distribution, and the factors contributing to variability. I don’t know much about this survey, or about survey conventions when reporting percentages. But I know my advisors wouldn’t have let me report percentages like this. Is it common to report percentages that sum to less that 100% when there is nonresponse? Or are these typos too? Also, where are the ratings for Hawaii, Idaho, Indiana, and Wyoming? (I knew learning the alphabetical state song in elementary school would be useful some day) 	 0 Comments
World Bank data plots	https://www.r-bloggers.com/2010/07/world-bank-data-plots/	July 9, 2010	prasoonsharma		 0 Comments
100 Prisoners, 100 lines of code	https://www.r-bloggers.com/2010/07/100-prisoners-100-lines-of-code/	July 9, 2010	Matt Asher	 In math and economics, there is a long, proud history of placing imaginary prisoners into nasty, complicated scenarios. We have, of course, the classic Prisoner’s Dilemma, as well as 100 prisoners and a light bulb. Add to that list the focus of this post, 100 prisoners and 100 boxes. In this game, the warden places 100 numbers in 100 boxes, at random with equal probability that any number will be in any box. Each convict is assigned a number. One by one they enter the room with the boxes, and try to find their corresponding number. They can open up to 50 different boxes. Once they either find their number or fail, they move on to a different room and all of the boxes are returned to exactly how they were before the prisoner entered the room. The prisoners can communicate with each other before the game begins, but as soon as it starts they have no way to signal to each other. The warden is requiring that all 100 prisoners find their numbers, otherwise she will force them to listen to hundreds of hours of non-stop, loud rock musician interviews. Can they avoid this fate? The first thing you might notice is that if every prisoner opens 50 boxes at random, they will have a  probability of finding their number. The chances that all of them will find their number is , which is approximately as rare as finding a friendly alien with small eyes. Can they do better? Of course they can. Otherwise I wouldn’t be asking the question, right? Before I explain how, and go into a Monte Carlo simulation in R, you might want to think about how they can do it. No Googling! All set? Did you find a better way? The trick should be clear from the code below, but if not skip on to the explanation. Here is what one of my plots looked like after running the code:  Out of the 1000 times I ran the experiment, on 307 occasions every single prisoner found his number. The theoretical success rate is about 31%. So, if it’s not clear from the code, what was the strategy employed by the prisoners and how does it work? One way to look at the distribution of numbers in boxes is to see it as a permutation of the numbers from 1 to 100. Each permutation can be partitioned into what are called cycles. A cycle works like this: pick any number in your permutation. Let’s say it’s 23. Then you look at the number the 23rd place (ie the number in the 23rd box, counting from the left). If that number is 16, you look at the number in the 16th place. If that number is 87, go open box number 87 and follow that number. Eventually, the box you open up will have the number that brings you back to where you started, completing the cycle. Different permutations have different cycles. The key for the prisoner is that by starting with the box that is the same place from the left as his number, and by following the numbers in the boxes, the prisoner guarantees that if he is in a cycle of length less than 50, he will eventually open the box with his number in it, which would complete the cycle he began. One way to envision cycles of different lengths is to think about the extreme cases. If a particular permutation shifted every single number over one to the left (and wrapped number 1 onto the end), you would have a single cycle of length 100. Box 1 would contain number 2, box 2 number 3 and so on. On the other hand, if a permutation flipped every pair of consecutive numbers, you would have 50 cycles, each of length 2: box 1 would have number 2, box 2 would have number 1. Of course if your permutation doesn’t change anything you have 100 cycles of length 1. As you can see from the histogram, when using this strategy you can never have between 50 and 100 winning prisoners. Anytime you have a single cycle of length greater than 50, for example 55, then all 55 prisoners who start on that cycle will fail to find their number. If no cycles are longer than 50, everyone wins. Just how rare are different cycles of different lengths? For the math behind that check out this excellent explanation by Peter Taylor of Queen’s University. Before moving on I wanted to visualize these cycles. Try running the code below: You can adjust the “Sys.sleep()” parameter to make the animation faster. I recommend running the code to see how the cycles “develop” over time, but here’s a snapshot of what I got:  	 0 Comments
TripleR/ BlockR: Working on mixed effect models …	https://www.r-bloggers.com/2010/07/tripler-blockr-working-on-mixed-effect-models/	July 9, 2010	felixschoenbrodt	At the moment I’m working on the implementation of full block designs (e.g., every member of group A rates each member from group and vice versa. A typical example is speed dating: every man rates each woman and vice versa). These designs can be analyzed with mixed effect models, and now I’m a bit confused whether I should use lme4 or lme4a … 	 0 Comments
Core in CiRM [2]	https://www.r-bloggers.com/2010/07/core-in-cirm-2/	July 8, 2010	xi'an	We are making slow progress on the normal and regression chapters as we decided to write the package at the same time we revise the chapters… Jean-Michel transformed the variable selection and model choice R codes of the regression chapter into generic functions that will fit within the package. I rewrote the section on testing, removing all connections to classical testing, 0-1 loss functions and 5% errors, towards a purely model-choice perspective. This does not modify the computational aspects but it makes the discourse more coherent and bypasses the debate about whether or not posterior probabilities are commensurable to p-values and all that. Last morning, I went running to the two creeks (locally called calanques) of Morgiou and Sugiton on a very nice costal trail. 	 0 Comments
Just updated the list of the new blog de…	https://www.r-bloggers.com/2010/07/just-updated-the-list-of-the-new-blog-de/	July 8, 2010	Tal Galili	Just updated the list about the new blog design and purpose.  Hope it will prove useful. 	 0 Comments
Competition: Predicting traffic	https://www.r-bloggers.com/2010/07/competition-predicting-traffic/	July 8, 2010	David Smith	Here’s an interesting competition that may well lend itself to R: the IEEE International Conference on Data Mining is running a contest to find the best way of predicting traffic problems. There are three separate contests:  Prizes worth $5,000 will be awarded to the winners, and the competition closes on September 30. For details, see the link below. IEEE ICDM Contest: Road Traffic Prediction for Intelligent GPS Navigation 	 0 Comments
R and Oracle HR Part I – Set Up and Connect	https://www.r-bloggers.com/2010/07/r-and-oracle-hr-part-i-set-up-and-connect/	July 8, 2010	C		 0 Comments
Subsampling for dummies	https://www.r-bloggers.com/2010/07/subsampling-for-dummies/	July 8, 2010	Timothée	A couple months ago, I was writing code for a paper that required some intense subsampling (something along the lines of several thousands of replicates on several thousands matrices). I decided to do the whole thing in R (I must confess that I don’t know how to live without it…), and as you can guess, it is really simple. The whole idea was to pick at random submatrices from within one bigger matrix. For the sake of simplicity, I assume that my matrix will be square (i.e. same number of rows and columns), with 20 rows and 20 columns, and that approximately 40 percent of the positions will be filled with something that is not a zero. Also, the maximal value is 1. It is easy to simulate a matrix with these properties in R, using a binomial distribution to fill the matrix at random places: What I want to do is to see if my current sampling is large enough to get a correct idea of the « real » situation. It means that even if I decrease the sample size a little bit, I will have a comparable result. Let’s say that I really want to know how many non-zero interactions are in each row, something we will call generality, and define as a one-liner : The general code for subsampling my big matrix will be And voilà. This function simply randomly sample our matrix, and apply the function we want (here on each line). At the end, we change the matrix in a data.frame, that we can plot with lattice, for example. The result is not exactly good looking because the data are as devoid of structure as possible, but the goal was just to illustrate how easy it is to build a subsampling routine. I’m not sure this is the best way to proceed, but I was happy with the speed of the code, and really happy with the time I spent on implementing the whole thing. 	 0 Comments
R/Finance 2010 presentations	https://www.r-bloggers.com/2010/07/rfinance-2010-presentations/	July 7, 2010	David Smith	The presentations from April’s successful R/Finance 2010 conference in Chicago are now available online. (Revolution Analytics is a proud sponsor of the conference.) There’s some amazing content here for anyone looking for the cutting-edge of financial engineering, with presentations from practitioners and researchers at institutions like Invesco Asset Management, Black Mesa Capital, and some of the leading academic institutions from around the world. Here’s just a selection of titles available for download:  R/Finance 2010: Agenda   	 0 Comments
All code on GGD is Free (Open Source BSD)	https://www.r-bloggers.com/2010/07/all-code-on-ggd-is-free-open-source-bsd/	July 7, 2010	Stephen Turner		 0 Comments
Navigate the Bermuda Triangle of Mediation Analysis	https://www.r-bloggers.com/2010/07/navigate-the-bermuda-triangle-of-mediation-analysis/	July 6, 2010	dan	"MYTHS AND TRUTHS ABOUT AN OFTEN-USED, LITTLE-UNDERSTOOD STATISTICAL PROCEDURE  If you go to a consumer research conference, you will hear tales of how experiments have undergone particular statistical rites: the attainment of the elusive crossover interaction, the demonstration of full mediation through Baron and Kenny’s sacred procedure, and so on. DSN has nothing against any of these ideas, but is opposed to subjecting all ideas to the same experimental designs, to the same tests, the same alternative hypotheses (typically a null of no difference), and the same rituals. Zhao, Lynch, and Chen point out in their recent Journal of Consumer Research article that Baron & Kenny’s Mediation Analysis is incredibly popular (ca 13,000 cites between 1986 and 2010), prescribed reflexively, though flawed in ways its users probably aren’t aware of. This article was invited by the journal “to serve as a tutorial on the state of the art in mediation analysis”. ABSTRACT
Baron and Kenny’s procedure for determining if an independent variable affects a dependent variable through some mediator is so well known that it is used by authors and requested by reviewers almost reflexively. Many research projects have been terminated early in a research program or later in the review process because the data did not conform to Baron and Kenny’s criteria, impeding theoretical development. While the technical literature has disputed some of Baron and Kenny’s tests, this literature has not diffused to practicing researchers. We present a nontechnical summary of the flaws in the Baron and Kenny logic, some of which have not been previously noted. We provide a decision tree and a step-by-step procedure for testing mediation, classifying its type, and interpreting the implications of findings for theory building and future research. REFERENCES
Baron, Reuben M. and David A. Kenny (1986), Moderator-Mediator Variables Distinction in Social Psychological Research: Conceptual, Strategic, and Statistical Considerations, Journal of Personality and Social Psychology, 51(6), 1173–82. Bullock, J. G., Green, D. P, & Ha, S. E. (2010). Yes,  But What’s the Mechanism? (Don’t Expect an Easy Answer), Journal of  Personality and Social Psychology, Vol. 98, No. 4, 550–558. Zhao, X., Lynch, J. G., Chen, Q. (2010).Reconsidering Baron and Kenny: Myths and Truths about Mediation Analysis. Journal of Consumer Research, 37, 197-206. R Package for Causal Mediation Analysis SPSS Code (see the Zhao, Lynch, and Chen article) "	 0 Comments
Drawing a trophic network (part 2)	https://www.r-bloggers.com/2010/07/drawing-a-trophic-network-part-2/	July 6, 2010	Timothée	Following discussion on the comments of the previous post, I thought about how it was possible to draw links going in several directions (i.e. there are no ‘clear’ differences between the levels, and species from level n can interact with species of level n, n+1, n-1, n±k, etc).  This is now done, with a code that is as inelegant as humanly possible. Also, the way to lay the different nodes on the graph (i.e. their ‘trophic level’) should be calculated with a proper method, but (i) I am not a specialist of these metrics, and (ii) it is way too late to do any decent bibliographical research to look them up. The procedure I use here is quite horrible. I randomly reorder the matrix, and compute the trophic position by elevating each species whenever it eats another species. I repeat this a couple time, and then average the trophic position for each species. The x position is a simple random value, which means that you will probably need to do the launch the code a couple times to get a decent looking network. The R code can be downloaded here. 	 0 Comments
Core in CiRM [1]	https://www.r-bloggers.com/2010/07/core-in-cirm-1/	July 6, 2010	xi'an	Jean-Michel Marin and myself have thus started our “research  in pair” in CIRM,  Luminy, for a fortnight. We are working on the second edition of Bayesian  Core and, despite working round the clock on the project (except for a one hour run around Mont Puget this morning), we are not going as fast as planned… Today, we worked in parallel on the normal and the regression chapters, looking for a sexy normal dataset to replace the larceny (normaldata) and the large and delicate CMB datasets. We eventually settled for a modern version of the Michelson-Morley dataset (available in R as morley), produced by K.K. Illingworth in 1927. I hope the spectral data and the relevance of the experiment will not be lost on the readers.  	 0 Comments
Draw a trophic network with n levels	https://www.r-bloggers.com/2010/07/draw-a-trophic-network-with-n-levels/	July 6, 2010	Timothée	I published a short function to draw trophic networks with several levels, with links going from level n+1 to level n only. It will only be of interest for people working in ecology (but if you see another possible use, please let me know). It is available here – with examples. If I continue to publish at this rhythm, expect a brand new post around november! 	 0 Comments
Chaos in the Financial Markets?	https://www.r-bloggers.com/2010/07/chaos-in-the-financial-markets/	July 6, 2010	Intelligent Trading		 0 Comments
Ross Ihaka in Sunday Star Times	https://www.r-bloggers.com/2010/07/ross-ihaka-in-sunday-star-times/	July 6, 2010	David Smith	"New Zealand’s Sunday Star Times last weekend featured a profile on Ross Ihaka, co-creator of R: The down-to-earth associate statistics professor and his fellow researcher Robert Gentleman are famous around the world for developing R programming – a “glorified calculator” that crunches data. R programming allows for statistical computing and graphics and is used by thousands of companies worldwide – and all of them do it free. 
 Photo: Sunday Star Times  I’m not sure about the characterization of R as a “glorified calculator” — see here for a more in-depth description of R — but it’s some nice home-town coverage about the project nonetheless. Sunday Star Times: Global giants crunch numbers on Kiwi creation "	 0 Comments
New versions of plyr, ggplot2 released	https://www.r-bloggers.com/2010/07/new-versions-of-plyr-ggplot2-released/	July 6, 2010	David Smith	Hadley Wickham has announced that new versions of his popular grammar-of-graphics charting package ggplot2 and his general-purpose data reshaping tool plyr for R are now available.  plyr boasts several new features, most notably a new join function which should simplify what can sometimes be a difficult process in R: merging two data sets. A simplified SQL-like terminology should make it much easier to specify how the datasets should be combined. But the most significant changes to plyr may be behind the covers — this version offers a number of performance improvements that should make the process of restructuring large data frames much easier. The ggplot2 package will also benefit from these performance improvements, and also includes a number of bug fixes to improve upon the previous version. Hadley Wickham: ggplot2 and plyr 	 0 Comments
Example 8.1: Digits of Pi	https://www.r-bloggers.com/2010/07/example-8-1-digits-of-pi/	July 6, 2010	Ken Kleinman		 0 Comments
R goes to StackExchange	https://www.r-bloggers.com/2010/07/r-goes-to-stackexchange/	July 6, 2010	Ben Mazzotta	“What’s the big deal? We already have the r-help mailing list.” No, it’s a big deal. Really. Have you forgotten the joys of being a first-year R user, either begging advice off of friends or using Google to search archives of the R-help list? (Firefox has a dedicated search add-on for the R-help archives.) Yes, it gets the job done, but it’s kludge. If you’re a self-taught R programmer you know what I’m talking about. StackExchange for Statistical Analysis will let experienced users answer statistics questions, presented in a legible format, and good answers are promoted to the top of the list. Questions can be tagged by subject matter and by package. Proper formatting for code swatches, and for discussion. It’s modeled on StackOverflow. A well-designed, user generated site for statistics might make it possible for newbies to learn without posting a thousand redundant and ill-posed questions to the help list. It might make it more palatable for experts to monitor discussions and lend a hand from time to time, rather than having to offer up a personal email address as a sacrificial lamb to the greater good of teaching statistics. Young experts don’t want to have to monitor email all day to be part of the discussion. Their answers belong on a website with a normal content management system, with good search functions and user interactions. Go sign up. Or read Tal Galili’s post on why you should sign up. Compare it to what’s on offer at MetaOptimize and I think you’ll agree this is a much more general-interest statistics tool. 	 0 Comments
oro.nifti 0.2.0	https://www.r-bloggers.com/2010/07/oro-nifti-0-2-0/	July 6, 2010	Brandon Whitcher		 0 Comments
New versions for ggplot2 (0.8.8) and plyr (1.0) were released today	https://www.r-bloggers.com/2010/07/new-versions-for-ggplot2-0-8-8-and-plyr-1-0-were-released-today/	July 6, 2010	Tal Galili	As prolific as the CRAN website is of packages, there are several packages to R that succeeds in standing out for their wide spread use (and quality), Hadley Wickhams ggplot2 and plyr are two such packages.  And today (through twitter) Hadley has updates the rest of us with the news: just released new versions of plyr and ggplot2. source versions available on cran, compiled will follow soon #rstats Going to the CRAN website shows that plyr has gone through the most major update, with the last update (before the current one) taking place on 2009-06-23.  And now, over a year later, we are presented with plyr version 1, which includes New functions, New features some Bug fixes and a much anticipated Speed improvements. ggplot2, has made a tiny leap from version 0.8.7 to 0.8.8, and was previously last updated on 2010-03-03. Me, and I am sure many R users are very thankful for the amazing work that Hadley Wickham is doing (both on his code, and with helping other useRs on the help lists).  So Hadley, thank you! Here is the complete change-log list for both packages:  (taken from the CRAN website)  New functions: * arrange, a new helper method for reordering a data frame. * count, a version of table that returns data frames immediately and that is much much faster for high-dimensional data. * desc makes it easy to sort any vector in descending order * join, works like merge but can be much faster and has a somewhat simpler syntax drawing from SQL terminology * rbind.fill.matrix is like rbind.fill but works for matrices, code contributed by C. Beleites Speed improvements * experimental immutable data frame (idata.frame) that vastly speeds up subsetting – for large datasets with large numbers of groups, this can yield 10-fold speed ups. See examples in ?idata.frame to see how to use it. * rbind.fill rewritten again to increase speed and work with more data types * d*ply now much faster with nested groups New features: * d*ply now accepts NULL for splitting variables, indicating that the data should not be split * plyr no longer exports internal functions, many of which were causing clashes with other packages * rbind.fill now works with data frame columns that are lists or matrices * test suite ensures that plyr behaviour is correct and will remain correct as I make future improvements. Bug fixes: * **ply: if zero splits, empty list(), data.frame() or logical() returned, as appropriate for the output type * **ply: leaving .fun as NULL now always returns list (thanks to Stavros Macrakis for the bug report) * a*ply: labels now respect options(stringAsFactors) * each: scoping bug fixed, thanks to Yasuhisa Yoshida for the bug report * list_to_dataframe is more consistent when processing a single data frame * NAs preserved in more places * progress bars: guaranteed to terminate even if **ply prematurely terminates * progress bars: misspelling gives informative warning, instead of uninformative error * splitter_d: fixed ordering bug when .drop = FALSE (taken from the CRAN website) Bug fixes: * coord_equal finally works as expected (thanks to continued prompting from Jean-Olivier Irisson) * coord_equal renamed to coord_fixed to better represent capabilities * coord_polar and coord_polar: new munching system that uses distances (as defined by the coordinate system) to figure out how many pieces each segment should be broken in to (thanks to prompting from Jean-Olivier Irisson) * fix ordering bug in facet_wrap (thanks to bug report by Frank Davenport) * geom_errorh correctly responds to height parameter outside of aes * geom_hline and geom_vline will not impact legend when used for fixed intercepts * geom_hline/geom_vline: intercept values not set quite correctly which caused a problem in conjunction with transformed scales (reported by Seth Finnegan) * geom_line: can now stack lines again with position = “stack” (fixes #74) * geom_segment: arrows now preserved in non-Cartesian coordinate system (fixes #117) * geom_smooth now deals with missing values in the same way as geom_line (thanks to patch from Karsten Loesing) * guides: check all axis labels for expressions (reported by Benji Oswald) * guides: extra 0.5 line margin around legend (fixes #71) * guides: non-left legend positions now work once more (thanks to patch from Karsten Loesing) * label_bquote works with more expressions (factors now cast to characters, thanks to Baptiste Auguie for bug report) * scale_color: add missing US spellings * stat: panels with no non-missing values trigged errors with some statistics. (reported by Giovanni Dall’Olio) * stat: statistics now also respect layer parameter inherit.aes (thanks to bug report by Lorenzo Isella and investigation by Brian Diggs) * stat_bin no longer drops 0-count bins by default * stat_bin: fix small bug when dealing with single bin with NA position (reported by John Rauser) * stat_binhex: uses range of data from scales when computing binwidth so hexes are the same size in all facets (thanks to Nicholas Lewin-Koh for the bug report) * stat_qq has new dparam parameter for specifying distribution parameters (thanks to Yunfeng Zhang for the bug report) * stat_smooth now uses built-in confidence interval (with small sample correction) for linear models (thanks to suggestion by Ian Fellows) * sta 	 0 Comments
Creating a Presentation with LaTeX Beamer – Basic Slides	https://www.r-bloggers.com/2010/07/creating-a-presentation-with-latex-beamer-%e2%80%93-basic-slides/	July 5, 2010	Ralph	In a previous post we looked at using the LaTeX beamer package to create presentations. Given a title page and outline of a presentation the next stage will be to create the content that appears in various slides of the presentation. Fast Tube by Casper To create a slide we make use of the frame environment and the first argument is the title for the slide, which usually appears at the top of the slide. For example a basic slide with a title and a sentence of body text would be created with the following commands: The text in a slide may look plain and if we wanted to highlight a particular word(s) then there are various options for doing this. The emph is used to provide emphasis that is usually indicated by an italic font. The alert command highlights a word in a colour, often red, to make it stand out from the rest of the text. There are other options that can be used to change for example the colour of the text or other features of the font. Other useful resources are provided on the Supplementary Material page. 	 0 Comments
Ajax using JQuery to Sinatra (and a Barplot using R)	https://www.r-bloggers.com/2010/07/ajax-using-jquery-to-sinatra-and-a-barplot-using-r/	July 5, 2010	C		 0 Comments
Go Guerrill… R on Your Data in August	https://www.r-bloggers.com/2010/07/go-guerrill-r-on-your-data-in-august-2/	July 5, 2010	Neil Gunther		 0 Comments
Prime Parallels for Load Balancing	https://www.r-bloggers.com/2010/07/prime-parallels-for-load-balancing/	July 5, 2010	Neil Gunther		 0 Comments
Hacker News Religion Poll	https://www.r-bloggers.com/2010/07/hacker-news-religion-poll/	July 5, 2010	C		 0 Comments
Make R speak SQL with sqldf	https://www.r-bloggers.com/2010/07/make-r-speak-sql-with-sqldf/	July 5, 2010	C		 0 Comments
Convergence diagnostics with MCMCglmm	https://www.r-bloggers.com/2010/07/convergence-diagnostics-with-mcmcglmm/	July 5, 2010	Shige		 0 Comments
Excellent R Code Format Package	https://www.r-bloggers.com/2010/07/excellent-r-code-format-package/	July 4, 2010	Quantitative Finance Collector		 0 Comments
Happy 4th of July	https://www.r-bloggers.com/2010/07/happy-4th-of-july/	July 3, 2010	C		 0 Comments
"Europe
Data set:> eurodist                 Athens Barcelona…"	https://www.r-bloggers.com/2010/07/europedata-set-eurodist-%c2%a0%c2%a0%c2%a0%c2%a0%c2%a0%c2%a0%c2%a0%c2%a0%c2%a0%c2%a0%c2%a0%c2%a0%c2%a0%c2%a0%c2%a0-athens-barcelona/	July 3, 2010	Human Mathematics	"Data set:> eurodist
                 Athens Barcelona Brussels Calais Cherbourg Cologne Copenhagen
Barcelona         3313                                                       
Brussels          2963      1318                                             
Calais            3175      1326      204                                    
Cherbourg         3339      1294      583    460                             
Cologne           2762      1498      206    409       785                   
Copenhagen        3276      2218      966   1136      1545     760           
Geneva            2610       803      677    747       853    1662       1418
Gibraltar         4485      1172     2256   2224      2047    2436       3196
Hamburg           2977      2018      597    714      1115     460        460
Hook of Holland   3030      1490      172    330       731     269        269
Lisbon            4532      1305     2084   2052      1827    2290       2971
Lyons             2753       645      690    739       789     714       1458
Madrid            3949       636     1558   1550      1347    1764       2498
Marseilles        2865       521     1011   1059      1101    1035       1778
Milan             2282      1014      925   1077      1209     911       1537
Munich            2179      1365      747    977      1160     583       1104
Paris             3000      1033      285    280       340     465       1176
Rome               817      1460     1511   1662      1794    1497       2050
Stockholm         3927      2868     1616   1786      2196    1403        650
Vienna            1991      1802     1175   1381      1588     937       1455
                Geneva Gibraltar Hamburg Hook of Holland Lisbon Lyons Madrid
Barcelona                                                                   
Brussels                                                                    
Calais                                                                      
Cherbourg                                                                   
Cologne                                                                     
Copenhagen                                                                  
Geneva                                                                      
Gibraltar         1975                                                      
Hamburg           1118      2897                                            
Hook of Holland    895      2428     550                                    
Lisbon            1936       676    2671            2280                    
Lyons              158      1817    1159             863   1178             
Madrid            1439       698    2198            1730    668  1281       
Marseilles         425      1693    1479            1183   1762   320   1157
Milan              328      2185    1238            1098   2250   328   1724
Munich             591      2565     805             851   2507   724   2010
Paris              513      1971     877             457   1799   471   1273
Rome               995      2631    1751            1683   2700  1048   2097
Stockholm         2068      3886     949            1500   3231  2108   3188
Vienna            1019      2974    1155            1205   2937  1157   2409
                Marseilles Milan Munich Paris Rome Stockholm
Barcelona                                                   
Brussels                                                    
Calais                                                      
Cherbourg                                                   
Cologne                                                     
Copenhagen                                                  
Geneva                                                      
Gibraltar                                                   
Hamburg                                                     
Hook of Holland                                             
Lisbon                                                      
Lyons                                                       
Madrid                                                      
Marseilles                                                  
Milan                  618                                  
Munich                1109   331                            
Paris                  792   856    821                     
Rome                  1011   586    946  1476               
Stockholm             2428  2187   1754  1827 2707          
Vienna                1363   898    428  1249 1209      2105 Multi-dimensional scaling of the distances: > cmdscale(eurodist)
                        [,1]        [,2]
Athens           2290.274680  1798.80293
Barcelona        -825.382790   546.81148
Brussels           59.183341  -367.08135
Calais            -82.845973  -429.91466
Cherbourg        -352.499435  -290.90843
Cologne           293.689633  -405.31194
Copenhagen        681.931545 -1108.64478
Geneva             -9.423364   240.40600
Gibraltar       -2048.449113   642.45854
Hamburg           561.108970  -773.36929
Hook of Holland   164.921799  -549.36704
Lisbon          -1935.040811    49.12514
Lyons            -226.423236   187.08779
Madrid          -1423.353697   305.87513
Marseilles       -299.498710   388.80726
Milan             260.878046   416.67381
Munich            587.675679    81.18224
Paris            -156.836257  -211.13911
Rome              709.413282  1109.36665
Stockholm         839.445911 -1836.79055
Vienna            911.230500   205.93020 Plot      require(stats)
     loc <- cmdscale(eurodist)
     rx <- range(x <- loc[,1])
     ry <- range(y <- -loc[,2])
     plot(x, y, type=""n"", asp=1, xlab="""", ylab="""")
     abline(h = pretty(rx, 10), v = pretty(ry, 10), col = ""light gray"")
     text(x, y, labels(eurodist), cex=0.8) "	 0 Comments
Get up and running with R, Sweave, and LaTex	https://www.r-bloggers.com/2010/07/get-up-and-running-with-r-sweave-and-latex/	July 2, 2010	--	"There are a lot of great references on the web on how to levarage LaTeX for reporting and presentations (beamer), but as someone who is completely new to R and reproducible research, I was having a pretty hard time figuring out simply what to do (what tools, where do you edit, etc.).  Eclipse as a programming environment has been growing on me, but I simply could not get it to render me a PDF. In this post, I am going to link to a few resources and attempt to explain it to you in a way that hopefully will get you up and running.  It really is pretty straight forward.  Learning how to modify commands, however, thats another post for another day. Assumptions:
You have R up and running (I am using 2.10.1)
You are just trying to create a document – I tried to create a presentation (Beamer) and got an error when trying to open the PDF….I will try to figure this out later First, save the following text below as a .rnw file Here is the code: One Note:  You may get prompted to install some additional packages…I said Yes. "	 0 Comments
Visualization of regression coefficients (in R)	https://www.r-bloggers.com/2010/07/visualization-of-regression-coefficients-in-r/	July 2, 2010	Tal Galili	Update (07.07.10): The function in this post has a more mature version in the “arm” package.  See at the end of this post for more details. * * * * Imagine you want to give a presentation or report of your latest findings running some sort of regression analysis.  How would you do it? This was exactly the question Wincent Rong-gui HUANG has recently asked on the R mailing list. One person, Bernd Weiss, responded by linking to the chapter “Plotting Regression Coefficients” on an interesting online book (I have never heard of before) called “Using Graphs Instead of Tables” (I should add this link to the free statistics e-books list…) Letter in the conversation, Achim Zeileis, has surprised us (well, me) saying the following I’ve thought about adding a plot() method for the coeftest() function in the “lmtest” package. Essentially, it relies on a coef() and a vcov() method being available – and that a central limit theorem holds. For releasing it as a general function in the package the code is still too raw, but maybe it’s useful for someone on the list. Hence, I’ve included it below.  (I allowed myself to add some bolds in the text) So for the convenience of all of us, I uploaded Achim’s code in a file for easy access.  Here is an example of how to use it: Here is the resulting graph:  I hope Achim will get around to improve the function so he might think it worthy of joining his“lmtest” package.  I am glad he shared his code for the rest of us to have something to work with in the meantime  * * * Update (07.07.10): Thanks to a comment by David Atkins, I found out there is a more mature version of this function (called coefplot) inside the {arm} package.  This version offers many features, one of which is the ability to easily stack several confidence intervals one on top of the other. It works for baysglm, glm, lm, polr objects and a default method is available which takes pre-computed coefficients and associated standard errors from any suitable model. Example: (Notice that the Poisson model in comparison with the binomial models does not make much sense, but is enough to illustrate the use of the function) (hat tip goes to Allan Engelhardt for help improving the code, and for Achim Zeileis in extending and improving the narration for the example) Resulting plot   * * * Lastly,  another method worth mentioning is the Nomogram, implemented by Frank Harrell’a rms package. 	 0 Comments
The case of the missing zeroes	https://www.r-bloggers.com/2010/07/the-case-of-the-missing-zeroes/	July 2, 2010	David Smith	"Political polling is a big industry these days, especially here in the US, and both mainstream news outlets and many of the bigger political blogs commission their own polls to measure (for example) the popularity of a sitting or candidate politician or policy. In the last week though, a very public spat has erupted between the left-leaning political blog Daily Kos and the polling firm Research 2000, commissioned by Daily Kos to produce a series of polls for the site. An investigation by statistician-readers of Daily Kos suggested some unusual features of the polling data provided by Research 2000, mainly that the results from tracking polls appear to be underdispersed compared to what you’d expect given the sample sizes. Other results suggest data drawn from unexpected distributions, as illustrated for example by this histogram of week-to-week changes in a measure of Obama’s favorability over a period of 60 weeks: 
 This and other anomalies in the data have led Daily Kos to sue Research 2000, claiming that they “handed Daily Kos fiction and claimed it was fact”. (On the other hand, Nate Silver of fivethirtyeight.com, while having his own reservations about Research 2000, suggests a less nefarious explanation for modifying data by hand.) In any case, this story is an illustrative example of how statistical concepts (randomness, distributions, samples, data) have increasing prominence in political discourse today — largely as a result of this increased use of polling — and how the stakes have been raised by the data they generate. Daily Kos: Research 2000: Problems in plain sight "	 0 Comments
Creating a Presentation with LaTeX Beamer – Getting Started	https://www.r-bloggers.com/2010/07/creating-a-presentation-with-latex-beamer-%e2%80%93-getting-started/	July 2, 2010	Ralph	The LaTeX beamer package can be used to create appealing presentations for many applications. A working knowledge of LaTeX is required but once the initial learning curve and transition is made from a visual word processor to a markup based approach the benefits are worth the initial investment of time. Title Pages In a previous post we considered creating a title page for a presentation which follows the same general approach to creating titles in other LaTeX document types. Fast Tube by Casper An example of the commands needed to create the title of a presentation are shown below, where the square brackets are used to indicate the short information that is used in places like the running headers of a document: Then we need to create a frame in the document to display the title page via the frame environment: Document Structure Most LaTeX documents make use of the sectioning commands to create a logical structure to the contents of a document. Headings can be defined at various levels starting from chapter moving down to sections then subsections. Fast Tube by Casper These sections commands can be used in beamer and behave slightly differently to other document classes and the display of the section headings depends on the theme and the use of tables of contents which can be interspaced around the presentation, for example appearing at the start of each section to remind the audience of progress through the presentation. The sections are defined in the same way as with other LaTeX documents: Table of Contents It is a simple task to add a slide with the table of contents to a presentation using the following commands: We can specify a title for the slide with the table of contents using the frametitle command. Fast Tube by Casper We can generate a recurring table of contents at the start of each section with the AtBeginSection command like this: Other useful resources are provided on the Supplementary Material page. 	 0 Comments
An experiment in A/B Testing my Résumé	https://www.r-bloggers.com/2010/07/an-experiment-in-ab-testing-my-resume/	July 1, 2010	Paul Butler	"I’ll admit it: my résumé doesn’t stand out. I’ve had some great internships, but also a tendency to work for companies that aren’t (yet!) household names. And though I’m doing fine academically, it’s not well enough to stand  out on my marks alone. On the other hand, my blog lets me stand out. I’ve had a few opportunities to meet and interview with some great people and companies because they read my blog. Naturally, then, the primary goal of my résumé is to get people to visit my blog. Since I don’t quite have the audacity to make my résumé a note telling people to visit my blog, I’m faced with the problem of how to optimize my résumé to ensure people see my blog. That’s where this experiment comes in. I started thinking about variables in my résumé that could affect the rate at which people viewed my blog. I narrowed it down to three that I could easily test. The first is the length of the résumé. My friends Rajesh and Shams are adamant about keeping their résumés down to a single page. Their arguments are sound, but I wanted to see if the data would back up their beliefs. I created a “short” version of my résumé which I squeezed into one page by omitting the Awards section and removing some skills. Second, I wanted to know how my grades affected the résumé. Obviously I couldn’t start making things up, but since my major average differs from my overall average by a good margin, I had two numbers that I could use truthfully with a subtle change in wording. Résumé variations with different grades  Résumé variation with a link to GitHub I wanted to track three things: how many people downloaded the résumé, how many people scrolled to the bottom of the landing page, and how many people visited my blog. The first I accomplished by logging downloads. The second I accomplished with jQuery and an Ajax callback. The third I accomplished with a tracking image, just like hit counters in the 90s. I used IP address and cookies to match up actions with the associated résumé. The only remaining problem was how to get hundreds of people to see my résumé in a short period of time. Fortunately Google offered me $110 in AdWords credits as a Google Analytics user, so I took advantage of that and ran ads on Google searches. Here is one of the half-dozen variations I ran:  One of the Google ads I ran After less than a week, I managed to exhaust my AdWords budget and gather a fair bit of data. I wrote a few hadoop jobs with my new toy and then brought the data into R for analysis and visualization. As you might expect, the people who encountered the short resume were much more likely to scroll to the bottom. Just over half did, versus just under a third of those presented with the long resume. This makes sense because there is less to scroll through, but it was nice to have the data confirm my suspicions. Note that in the following graph, and all others in this post, the grey lines indicate the 90% confidence interval. 
The short résumé also resulted in more downloads and blog views, but not enough to be statistically significant with the amount of data I collected. The grades shown on the résumé didn’t affect any of the metrics I was measuring in a statistically significant way. I was surprised to find that the non-blog link shown on my résumé affected the frequency of click-throughs to my blog. Even adding a link to my GitHub profile more than halved the frequency of a clickthrough to my blog. LinkedIn and twitter were even worse. 
I created a heatmap-like visualization from the relative significances of each link to each other. For example, the upper leftmost cell means that it is 97.2% likely that if a sufficiently large group of people were exposed to each of the LinkedIn and blog-link-only versions of my résumé, the group that saw the blog-link-only version would visit my blog more. Jesse E. Farmer has written more about the details of how this is calculated. 
Oddly, the effect was reversed when you consider downloads rather than blog views. The résumés without any social media links were far less likely to be downloaded than those with. Even a résumé with a twitter profile did better than one without, though not by enough to be statistically significant. 

The additional links also reduced the frequency of readers scrolling to the bottom of the page. 
 There are two main things I learned from this experiment. First, I’m going to keep social network links off of my résumé. Although they increased the download rate, they decreased visits to my blog. Since the latter is my priority, I’m not going to start adding social networks to my résumé. Second, the short résumé did better in every way. However, the improvement in blog views was not statistically significant. For now, I’m keeping my online résumé at two pages, but I will use the one-page version in print. There’s a number of disclaimers I should make here. For one, even if my findings are true of my résumé, they might not be true of other résumés. Maybe a change in layout would diminish the effect of linking to social media profiles, or make the longer résumé convert better. I should also point out that I have no way of knowing who my audience was. They probably weren’t all in a position to hire a programmer or data scientist, so the factors that make them visit my blog may or may not have the same effect on those who are. And finally, a shameless plug. I’m looking for an interesting data science internship this fall (September to December 2010). If you’re doing cool things with data, I’d be glad to hear from you. My contact information is in the sidebar. "	 0 Comments
Maps without map packages	https://www.r-bloggers.com/2010/07/maps-without-map-packages/	July 1, 2010	dan	"LATITUDE + LONGITUDE + OVERPLOTTING FIX = MAPS  Decision Science News is always learning stuff from colleague, physicist, mathlete, and all-around computer whiz Jake Hofman. Today, it was a quick and clean way to make nice maps in R without using any map packages: just plot the latitude and longitude of your data points (e.g. web site visitors) along with the “alpha” parameter to allow for layering of coincident points. It’s duh in hindsight. Above we see a how it looks with a little data. Below is the result with more data and a lower alpha:  In the words of James Taylor, all you have to do is call: library(ggplot2)
qplot(long,lat,data=us,alpha=I(.1)) To get the Decision-Science-News-approved framing and aspect ratio for the USA: qplot(long,lat,data=wtd,alpha=I(.1),
xlim=c(-125-10/2,-65),ylim=c(23.5,50.5)) +
opts(aspect.ratio = 3.5/5) As we are certain that there are readers who will want to show that there are much nicer ways to do this, we say: download the data and show us. "	 0 Comments
UseR! 2010 registration extended until July 4	https://www.r-bloggers.com/2010/07/user-2010-registration-extended-until-july-4/	July 1, 2010	David Smith	Registration for the worldwide R user conference, useR! 2010, closed on June 20. But it seems like a few folks weren’t aware of the early deadline, and missed out on registering. Never fear! Due to overwhelming demand, registration has re-opened until midnight on July 4. You’ll need to hurry — there are only 30 spots left — so get in quick and register at the link below. useR! 2010: Registration 	 0 Comments
"Call for Papers: Special Issue in JSS for ""Magnetic Resonance Imaging in R"""	https://www.r-bloggers.com/2010/07/call-for-papers-special-issue-in-jss-for-magnetic-resonance-imaging-in-r/	July 1, 2010	Brandon Whitcher		 0 Comments
Can you spot the Error?	https://www.r-bloggers.com/2010/07/can-you-spot-the-error/	July 1, 2010	martin	Peter Huber referred to “the rawness of raw data”, a kind of data we would not expect to find in a textbook. The book of Fahrmeir and Tutz on multivariate modelling refers to the visual impairment data from Liang et al., 1992 in table 3.12: Visual Impairment Data from Liang et al. as found in Fahrmeir and Tutz Nothing wrong here at first sight; but how would you tell? There are some people who are actually able to look at non-trivial table data and spot “the round peg in the square hole”, but that just won’t work for the rest of us. As you might guess, I am going to make a case for graphics here. Let’s start with what the mainstream would do: plot the data in a dotplot like thing using the trellis paradigm of conditioning. I used ggplot2 to make sure to trellis state-of-the-art. A simple gives me: (I still have a hard time to find that syntax intuitive …) Surprisingly this plot already is sufficient to spot the “problem” in the data, although some important properties of the data can’t be seen here. A mosaic plot makes the whole thing even easier:  (impairment cases highlighted, left and right is left and right) The left and right cases are (what a surprise) always of the same size, except for the 70+, black – hard to believe that in this group 110 cyclops show up not having a right eye. In the mosaic plot the higher proportion of the impaired right eyes for 70+ blacks jumps immediately to ones eyes, but what reveals the error is the missing independence between race and side for 70+. That implies that we have too few cases here, and what is ’226′ in the table should actually be ’336′. Here is the (corrected) data. 	 0 Comments
Simple Dummy R GUI Generator	https://www.r-bloggers.com/2010/07/simple-dummy-r-gui-generator/	July 1, 2010	Quantitative Finance Collector		 0 Comments
Using XML package vs. BeautifulSoup	https://www.r-bloggers.com/2010/08/using-xml-package-vs-beautifulsoup/	August 31, 2010	Ryan	"A while back I posted something about scraping a webpage using the BeautifulSoup module in Python.  One of the comments to that post was by Larry — a blogger over at IEORTools — suggesting that I take a look at the XML library in R.  Given that one of the points of this blog is to become more familiar with some of the R tools, it seemed like a reasonable suggestion — and I went with it. I decided to replicate my work in python for scraping the MLS data using the XML package for R.  OK, I didn’t replicate it exactly because I only scraped five years worth of data.  I figured that five years would be a sufficient amount of time for comparison purposes.  The only major criterion that I enforced was that they both had to export nearly identical .csv files.  I say “nearly” because R seemed to want to wrap everything in parenthesis and it also exported the row names (1, 2, 3, etc.) as default options in write.table().  Neither of these defaults are an issue, so I didn’t bother changing them.  I wrote in a few print statements for commenting purposes in both scripts to show where a difference (if any) in timing might exist.  The code can be found in my scraping repository on github. I don’t really know much about how system.time() works in R to be honest.  However, I used this function as the basis of my comparison.  Of course, I was source’ing an R file and using the system() function in R to run the python script, i.e., system(“path/to/py_script.py”).  The results can be summarized in the following graph.  
 
 As you can see in the figure, there is about a 3x speedup in using the XML package relative to using BeautifulSoup!  This is not what I was expecting.  Further, it appears that the overall “user” speedup is approximately 5x.  In fact, the only place where python seems to beat the R package is in the user.self portion of the time….whatever the hell this means. As I said before, I decided to print out some system times within each script because scraping this data is iterative.  That is, I scrape and process the data for each year within the loop (over years).  So I was curious to see if there each option was scraping and processing at about the same speed.  It turns out that XML beat BeautifulSoup here as well. Results:  ## From system call to python:

Sun Aug 29 18:07:57 2010 -- Starting

Sun Aug 29 18:08:00 2010 -- Year: 2005

Sun Aug 29 18:08:02 2010 -- Year: 2006

Sun Aug 29 18:08:04 2010 -- Year: 2007

Sun Aug 29 18:08:06 2010 -- Year: 2008

Sun Aug 29 18:08:08 2010 -- Year: 2009

Sun Aug 29 18:08:08 2010 -- Finished 

 and in R:


[1] ""2010-08-29 18:10:29 -- Starting""

[1] ""2010-08-29 18:10:29 -- Year: 2005""

[1] ""2010-08-29 18:10:29 -- Year: 2006""

[1] ""2010-08-29 18:10:29 -- Year: 2007""

[1] ""2010-08-29 18:10:30 -- Year: 2008""

[1] ""2010-08-29 18:10:30 -- Year: 2009""

[1] ""2010-08-29 18:10:30 -- Finished  ""

 What do I conclude from this?  Well, use R damnit!  The XML package is super easy to use and it’s fast.  Will I still use python?  Of course!  I would bet thatpython/BeautifulSoup would be a superior option if I had to scrape and process huge amounts of data — which will happen sooner rather than later. My computer’s technical specs: 2.66 GHz Intel Core 2 Duo, 8 GB RAM (DDR3), R version 2.11.0, XML v3.1-0, python 2.6.1, and BeautifulSoup v3.1.0.1. Preview of upcoming post: I am going to compare my two fantasy football drafts with the results similar drafts that are posted online!  Exciting stuff…you know, if you’re a nerd and like sports. "	 0 Comments
Better than Average	https://www.r-bloggers.com/2010/08/better-than-average/	August 31, 2010	C		 0 Comments
apply functions in R	https://www.r-bloggers.com/2010/08/apply-functions-in-r/	August 31, 2010	Samuel Brown		 0 Comments
Birds of a feather shop together	https://www.r-bloggers.com/2010/08/birds-of-a-feather-shop-together/	August 31, 2010	dan	PREDICTING CONSUMER BEHAVIOR FROM SOCIAL NETWORKS  This week, Decision Science News is doing a special cross-posting with Messy Matters. The post below is by Sharad Goel and describes work that he and your Decision Science News editor Dan Goldstein are jointly undertaking at Yahoo! Do you know what the #$*! your social media strategy is? Perhaps it’s “to facilitate audience conversations and drive engagement with social currency”? Or maybe, “to amplify word of mouth by motivating influencers”? Well, given all the lies and damned lies being told about social, fellow yahoo Dan Goldstein and I decided to enter the fray with statistics. We measured the extent to which your friends’ behavior predicts your own, and found that in several consumer domains the effect is substantial, complementing traditional demographic and behavioral predictors. That friends are similar along a variety of dimensions is a long-observed empirical regularity—a pattern sociologists call homophily. As McPherson et al. write in their canonical review on the subject, “homophily limits people’s social worlds in a way that has powerful implications for the information they receive, the attitudes they form, and the interactions they experience.” Turning this statement around, where there is homophily, one can in principle predict an individual’s behavior based on the attributes and actions of his or her associates. To assess the quality of such network-based predictions, we merged a large social network (based on email and IM exchanges) with offline sales data at an upscale, national department store chain. Thus, for each of over one million users, we had their past purchase amounts in dollars, and had the same information for each of their network contacts. Think about this for a minute: we not only know how much these individuals themselves spent at an offline retailer, but also how much their social contacts spent, a testament to how profoundly the Internet is changing the way we study human behavior. (Despite bolstering social science research, these newfound tools raise serious privacy issues. We left the matching to a third party that specializes in doing this securely, so neither we nor the department store had access to the other’s complete customer database.) The plot below summarizes our findings. First, as indicated by the top line, consumers whose friends spent a lot, also spent a lot themselves, consistent with the hypothesis that homophily extends to consumer behavior. When friends (alters) on average spent $400 during the six-month observation period, the consumer herself (ego) spent nearly $600, more than twice the typical consumer (indicated by the dotted line). As our aim is prediction, however, the relevant question is not just whether friends are similar in their purchasing behavior, but rather how much information is conveyed by social ties relative to other attributes. One might conjecture that ties simply indicate demographic (i.e., age and sex) similarity, that those who spend a lot are more likely to be middle-aged women—the primary market segment for this department store—and that friends of middle-aged women tend also to be middle-aged women. To test this hypothesis, we first paired each individual with a randomly chosen consumer of identical age and sex. The bottom line shows that this demographically matched group is, perhaps surprisingly, pretty ordinary. In other words, looking only at age and sex, you can’t identify consumers whose friends spend a lot (and who we know spend a lot themselves).   Though it’s standard marketing practice to target consumers based on their demographics, it’s an admittedly noisy profiling technique. So, to put social through the wringer, we next took the “socially select” group—consumers whose friends spent a lot—and matched them to random consumers with identical age, sex, and past purchase amounts. Each social candidate, that is, was matched to a consumer not only of the same age and sex, but one who spent approximately the same amount as the social candidate during the previous six months. Even relative to this formidable baseline, social cues still provide considerable information. As the middle line indicates, knowing a consumer’s age, sex and past purchases, but not that their friends are shopaholics, one would still underestimate their future sales.[1] We repeated this analysis for two other domains—examining signups for Yahoo! Fantasy Football, and clicks on ten online banner ads for movies, apparel, government programs, and beyond—again finding that the predictive power of social persists even after adjusting for age, sex, and past behavior. Lest you run off to rejigger your social strategy, we should mention a couple of caveats. First, we have shown that consumers with big-spending friends tend to spend a lot—more, in fact, than demographics and past purchases alone would suggest. But since most people, even premium customers, don’t have shopaholic friends, social cues do not substantially boost average predictive performance. Second, though social signals help predict how much consumers spend, they don’t always help identify which consumers will spend the most. Those who recently spent fifty grand on sartorial elegance are likely to be habitual top spenders, regardless of what you know about their friends. Assessing the value of social, as with most things, is a messy affair. On the one hand, network ties convey information not captured by the usual egocentric metrics, a conclusion that at the very least we find scientifically interesting. On the other hand, it’s not immediately obvious how to use that knowledge to take over the world. Well, rest assured that an army of social strategy gurus are waiting in the wings with a game-changing, technology-disrupting way to, you know, “leverage the social graph to deliver personalized experiences” or something. N.B.Thanks to Randall Lewis and David Reiley for acquiring the sales data, Jake Hofman for assembling the email data, and Duncan Watts and Dan Reeves for comments. For related work in the telecom domain, check out the paper, “Network-Based Marketing: Identifying Likely Adopters via Consumer Networks,” by Shawndra Hill, Foster Provost, and Chris Volinsky. Illustration by Kelly Savage [1] It’s perhaps tempting to conclude from these results that shopping is contagious (i.e., to assert causation where only correlation has been shown). Though there is probably some truth to that claim, establishing such is neither our objective nor justified from our analysis. 	 0 Comments
R is indispensable, because it’s reproducible	https://www.r-bloggers.com/2010/08/r-is-indispensable-because-its-reproducible/	August 31, 2010	David Smith	Maria Wolters, self-styled “Science-Mum of two” and speech and language technology researcher, has a great blog post about the one tool she couldn't live without: R. Maria says R is her “favourite tool for analysing experimental results and modelling the resulting patterns of behaviour and preferences”, and explains why: R is a programming language for everything statistical. It’s free, it’s open source, and it’s being maintained by statisticians for statisticians. Its origin means that it is a pain to learn. It takes a while until one has cleared a path through the data structures, including the various conventions for extracting information from objects that store the results of painstaking statistical analyses, and I am still often baffled myself. But the payoff is magnificent. Clear (modulo coding ability), open, replicable analyses. R is the ultimate in replicable research. If you give people your data set and your source code, they can repeat every single step of your reasoning. There are no paywalls, no limits of affordability, no packages that are indispensable for the analysis, but that your department hasn’t paid for.  This issue of “replicable analysis” is an important one: the ability to know that you can re-run your analysis at any time in the future (assuming you still have access to the same hardware, or at least a virtual instance of it) and verify the results, without having to worry about the software no longer being available, is crucial. It also means that third parties can reproduce your results where necessary. The fact that it really is necessary to support good science is the topic that Fritz Leisch covered in this excellent keynote speech at this year's UseR! conference. Speech and Science: The one tool I couldn’t live without 	 0 Comments
Soil Properties Visualized on a 1km Grid	https://www.r-bloggers.com/2010/08/soil-properties-visualized-on-a-1km-grid/	August 31, 2010	dylan	Fresno Area Urban Areas vs Irrigated LCC: grey regions are current urban areas A couple of maps generated from a 1km gridded soil property database, derived from SSURGO data where available with holes filled with STATSGO data. Soil properties visualized at this scale illustrate several important soil-forming factors operating within California: sediment source in the Great Valley, the interplay between precipitation and ET, and removal of salts. This database and the details on its creation should be available within a couple of months. This builds on a related post highlighting some of these maps packaged in KMZ format. Check back in a couple of weeks of updates. read more 	 0 Comments
Namespaces and name conflicts	https://www.r-bloggers.com/2010/08/namespaces-and-name-conflicts/	August 31, 2010	Michal	R packages ‘igraph’ and ‘network’ are good examples of two R packages providing similar but complementary functionalities for which there are a lot of name conflicts. As for now the ‘igraph’ package has a namespace while the ‘network’ package (version 1.4-1) does not. This became an issue when I was working on the ‘intergraph‘ package. Below is a note on how the fact that ‘igraph’ does and ‘network’ currently does not have a namespace affects using them simultaneously during an R session. Loading and attaching ‘network’ and ‘igraph’ simultaneously (in this case ‘network’ first, ‘igraph’ second) gives So let’s see how does it affect our work. One of the conflicting functions is ‘add.edges’. When we call it from the console R starts looking for it in several places in a particular order. This can be inspected with ‘search’ function: So R will look in global environment (aka Workspace) first and then in the code provided by the attached packages as shown. Consequently: Because ‘network’ does not have a namespace some of its functions are masked by the copies from ‘igraph’. A call to some function from ‘network’ that uses one of those conflicting functions will use the version from the ‘igraph’ package (so trigger an error most likely) and not the correct version from ‘network’. For example: This tries to use ‘set.vertex.attribute’ from ‘igraph’ and not the correct one from ‘network’ because ‘igraph’ comes first on the search path as returned by ‘search()’. First some clean-up then loading the packages Now the network version of ‘add.edges’ is on top Version from ‘igraph’ can be called with ‘::’ Given that we attached ‘igraph’ first and ‘network’ second based on the previous section we could expect that this will brake the functioning of the ‘igraph’ package. Let’s see this with ‘V’ function in ‘igraph’ which calls ‘set.vertex.attribute’ (a name conflict with ‘network’). Let’s check that: And this works OK because for packages with namespaces R uses a little different searching mechanism. It searches in package namespace first, then among the functions imported by that package, then R base, and lastly in the “normal” search path (as from ‘search()’). See “Writing R Extensions”, last paragraph before the end of sub-section. Consequently, the correct version of ‘set.vertex.attribute’ is used. If both packages have namespaces the errors like the one with ‘as.network’ shown above cannot happen. The code provided in the packages will work correctly as they will use their own copies of conflicting functions. Using the namespaced ‘network’ I made while creating ‘intergraph‘: So, works OK even though we attached ‘network’ first and ‘igraph’ second. 	 0 Comments
Writing my Thesis – Follow me on Twitter	https://www.r-bloggers.com/2010/08/writing-my-thesis-follow-me-on-twitter/	August 31, 2010	Stephen Turner		 0 Comments
Even Simpler Multivariate Correlated Simulations	https://www.r-bloggers.com/2010/08/even-simpler-multivariate-correlated-simulations/	August 31, 2010	JD Long	So after yesterday’s post on Simple Simulation using Copulas I got a very nice email that basically begged the question, “Dude, why are you making this so hard?” The author pointed out that if what I really want is a Gaussian correlation structure for Gaussian distributions then I could simply use the mvrnorm() function from the MASS package. Well I did a quick ?mvrnorm and, I’ll be damned, he’s right! The advantage of using a copula is the ability to simulate correlation structures where the correlation is different for different levels of values. So that gives the flexibility to make the tails of the distributions more correlated, for example. But my example yesterday was purposefully simple… so simple that a copula was not even needed. After creating my sample data all I really needed to do was this: myDraws  So I  took my example from yesterday and updated it using the mvrnorm() code and, as is my custom, put up a Github gist. The code is embedded below as well. I added a little ggplot2 code at the end that will create a facet plot of the 4 distributions showing the shape of the distributions of both the starting data and the simulated data. The plot in the upper left of this post is the ggplot output. EDIT: The email hipping me to this was sent by Dirk Eddelbuettel who’s been very helpful to me more times than I can count. I had omitted his name initially. However after confirming with Dirk, he told me it was OK to mention him by name in this post. 	 0 Comments
Zurich 2010: R Course for Students	https://www.r-bloggers.com/2010/08/zurich-2010-r-course-for-students/	August 31, 2010	Rmetrics blogs		 0 Comments
Map colors	https://www.r-bloggers.com/2010/08/map-colors/	August 31, 2010	Steven Mosher	Reader P was kind enough to make us a new color map so I promptly played around with it and other parameters. Need to figure out how to drop the labels and ticks on the “map”  map.axes() is no help. In anycase, I had a day long struggle with my R set up,  its all fixed. old packages stuck in a user library. So, Someone else asked for a 1 file version. See the drop box: sstcomplete. It has the new graphics in it. One complete script.  I put that together and added the “install” to the file. Delete the calls to “install.packages” IFF you already have them installed. ( I need to test for this ) NEXT, I’m going back to the Land. Some project clean up and making it all download from one file.  	 0 Comments
NppToR 2.4.0 Adds Auto-Completion	https://www.r-bloggers.com/2010/08/npptor-2-4-0-adds-auto-completion/	August 30, 2010	andrew	I’ve had a wonderful summer, very busy, but now I’ve finally had some time to sit down and program some thing on NppToR that I’ve been wanting to get out.  Thanks to Yihui Xie and his wonderful R script for generating auto-completion files, NppToR now has a dynamic Auto-Completion feature like the Dynamic Syntax generation feature.   Special thanks to Yihui for allowing it’s inclusion in NppToR.  Auto completion files can be installed from a menu command, but it usually requires admin privileges. There were also some big changes in the installation.  The installer now allows for a global install option.  I had a few requests for it so I’ve added it.  BTW if the installer is run with the global (i.e. with admin privileges) option, the auto-completion files will be installed as part of the installation process. For those of you who like the silent evaluation option, that has been updated to work more effectively and I’m looking forward to hearing feedback either positive or negative. There were quite a few changes deep in the code this release and so any regressions (things not working the way that they should), please notify me right away. You can do that at the forums. 	 0 Comments
Econometrics and R	https://www.r-bloggers.com/2010/08/econometrics-and-r/	August 30, 2010	Rob J Hyndman	Econometricians seem to be rather slow to adopt new methods and new technology (compared to other areas of statistics), but slowly the use of R is spreading. I’m now receiving requests for references showing how to use R in econometrics, and so I thought it might be helpful to post a few suggestions here. There are of course dozens of books on R with a more statistical perspective, including several on time series. But I will leave them for another post.   	 0 Comments
Hyper-g priors	https://www.r-bloggers.com/2010/08/hyper-g-priors/	August 30, 2010	xi'an	Earlier this month, Daniel Sabanés Bové and Leo Held posted a paper about g-priors on arXiv. While I glanced at it for a few minutes, I did not have the chance to get a proper look at it till last Sunday. The g-prior was first introduced by the late Arnold Zellner for (standard) linear models, but they can be extended to generalised linear models (formalised by the late John Nelder) at little cost. In Bayesian        Core, Jean-Michel Marin and I do centre the prior modelling in both linear and generalised linear models around g-priors, using the naïve extension for generalised linear models,  as in the linear case. Indeed, the reasonable alternative would be to include the true information matrix but since it depends on the parameter  outside the normal case this is not truly an alternative. Bové and Held propose a slightly different version  where W is a diagonal weight matrix and c is a family dependent scale factor evaluated at the mode 0. As in Liang et al. (2008, JASA) and most of the current literature, they also separate the intercept  from the other regression coefficients. They also burn their “improperness joker” by choosing a flat prior on , which means they need to use a proper prior on g, again as Liang et al. (2008, JASA), for the corresponding Bayesian model comparison to be valid. In Bayesian        Core, we do not separate  from the other regression coefficients and hence are left with one degree of freedom that we spend in choosing an improper prior on g instead. (Hence I do not get the remark of Bové and Held that our choice “prohibits Bayes factor comparisons with the null model“. As argued in Bayesian        Core, the factor g being an hyperparameter shared by all models, we can use the same improper prior on g in all models and hence use standard Bayes factors.) In order to achieve closed form expressions, the authors use Cui and George ‘s (2008) prior  which requires the two hyper-hyper-parameters a and b to be specified. The second part of the paper considers computational issues. It compares the ILA solution of Rue, Martino and Chopin (2009, Series B) with an MCMC solution based on an independent proposal on g resulting from linear interpolations (?). The marginal likelihoods are approximated by Chib and Jeliazkov (2001, JASA) for the MCMC part. Unsurprisingly, ILA does much better, even with a 97% acceptance rate in the MCMC algorithm. The paper is very well-written and quite informative about the existing literature. It also uses the Pima Indian dataset  (The authors even dug out a 1991 paper of mine I had completely forgotten!) I am actually thinking of using the review in our revision of Bayesian        Core, even though I think we should stick to our choice of including  within the set of parameters… 	 0 Comments
The Chosen One	https://www.r-bloggers.com/2010/08/the-chosen-one/	August 30, 2010	Matt Asher	 Toss one hundred different balls into your basket. Shuffle them up and select one with equal probability amongst the balls. That ball you just selected, it’s special. Before you put it back, increase its weight by 1/100th. Then put it back, mix up the balls and pick again. If you do this enough, at some point there will be a consistent winner which begins to stand out. The graph above shows the results of 1000 iterations with 20 balls (each victory increases the weight of the winner by 5%). The more balls you have, the longer it takes before a clear winner appears. Here’s the graph for 200 balls (0.5% weight boost for each victory).  As you can see, in this simulation it took about 85,000 iterations before a clear winner appeared.  I contend that as the number of iterations grows, the probability of seeing a Chosen One approaches unity, no matter how many balls you use. In other words, for any number of balls, a single one of them will eventually see its relative weight, compared to the others, diverge. Can you prove this is true? BTW this is a good Monte Carlo simulation of the Matthew Effect (no relation). Here is the code in R to replicate: After many trials using a fixed large number of balls and iterations, I found that the moment of divergence was amazingly consistent. Do you get the same results?  	 0 Comments
Stochastic Simulation With Copulas in R	https://www.r-bloggers.com/2010/08/stochastic-simulation-with-copulas-in-r/	August 30, 2010	JD Long	A friend of mine gave me a call last week and was wondering if I had a little R code that could illustrate how to do a Cholesky decomposition. He ultimately wanted to build a Monte Carlo model with correlated variables. I pointed him to a number of packages that do Cholesky decomp but then I recommended he consider just using a Gaussian Copula  and R for the whole simulation. For most of my copula needs in R, I use the QRMlib package which is a code companion to the book Quantitative Risk Management: Concepts, Techniques and Tools by Alexander J. McNeil, Rudiger Frey and Paul Embrechts. The book is only loosely coupled (pun intended) with the code in the QRMlib package. I really wish the book had been written with code examples and tight linkage between the book and the code. Of course I’m the type of guy who prefers code snip-its to mathematical notation. I had some code where I used the QRMlib package, but it was really messy and fairly specific to my use case. So I whipped up very simple example of how to create correlated random draws from a multivariate distribution. In this example I used normally distributed marginals and Gaussian correlation to keep things simple and easy to follow. Rather than blogging through the code, I added a shit load (metric ass ton, if you’re in Canada) of comments. The code is designed to be stepped through. So don’t just run the whole blob and wonder what happened. Walk through the code and if you find any errors be sure and let me know. The code is embedded in a Github gist below, but if you are reading this in an aggregator (shout out to R-Bloggers) you’ll need to manually go to the gist. 	 0 Comments
Where to Start with PDQ?	https://www.r-bloggers.com/2010/08/where-to-start-with-pdq/	August 30, 2010	Neil Gunther		 0 Comments
Taking R to the Limit: Large Datasets; Predictive modeling with PMML and ADAPA	https://www.r-bloggers.com/2010/08/taking-r-to-the-limit-large-datasets-predictive-modeling-with-pmml-and-adapa/	August 30, 2010	Ryan	During the first part of our meeting, Ryan Rosario presented on the topic of large datasets in R. Video, slides and code of the talk “Taking R to the Limit: Large Datasets” by Ryan Rosario at the Los Angeles area R Users Group in August 2010 are below. Video  Slides are also available for PDF download here. R code is available here. More information about the talk can be found here. During the second part, Trividesh Jena presented on creating models in R for use with the Zementis ADAPA product in the cloud. The video of his talk is below.  	 0 Comments
Sweet bar chart o’ mine	https://www.r-bloggers.com/2010/08/sweet-bar-chart-o%e2%80%99-mine/	August 30, 2010	richierocks	"Last week I was asked to visualise some heart rate data from an experiment.  The experimentees were clothed in protective suits and made to do a bunch of exercises while various physiological parameters were measured.   Including “deep body temperature”. Gross.  The heart rates were taken every five minutes over the two and a half hour period.  Here’s some R code to make fake data for you to play with.  The heart rates rise as the workers are made to do exercise, and fall again during the cooling down period, but it’s a fairly noisy series.
interval <- 5

heart_data <- data.frame(

+++time = seq.int(0, 150, interval)

)

n_data <- nrow(heart_data)

frac_n_data <- floor(.7 * n_data)

heart_data$rate = runif(n_data, 50, 80) +

+++c(seq.int(0, 50, length.out = frac_n_data),

+++seq.int(50, 0, length.out = n_data - frac_n_data)

)

heart_data$lower <- heart_data$rate - runif(n_data, 10, 30)

heart_data$upper <- heart_data$rate + runif(n_data, 10, 30) The standard way of displaying a time series (that is, a numeric variable that changes over time) is with a line plot.  Here’s the ggplot2 code for such a plot. library(ggplot2)

plot_base <- ggplot(heart_data, aes(time, rate))

plot_line <- plot_base + geom_line()

plot_line Using a line isn’t always appropriate however.  If you have missing data, or the  data are irregular or infrequent, then it is misleading to join them together with a line.  Other things are happening during the times that you have no data for.  ggplot2 will automatically removes lines that have a missing value between them (as represented by NA values) but in the case of irregular/infrequent data you don’t want any lines at all.  In this case, using points rather than lines is the best option, effectively creating a scatterplot. plot_point <- plot_base + geom_point()

plot_point Since heart rate can change dramatically over the course of five minutes, the data generated by the experiment should be considered infrequent, and so I opted for the scatterplot approach. The experimenters, however, wanted a bar chart. plot_bar <- plot_base +

+++geom_bar(aes(factor(time), rate), alpha = 0.7) +

+++opts(axis.text.x = theme_text(size = 8))

plot_bar I hadn’t considered this use of a bar chart before, so it was interesting to think about the pros and cons relative to using points.  First up, the bar chart does successfully communicate the numeric values, and the fact they they are discrete.  The big difference is that the bars are forced to stretch down to zero, squeezing the data into a small range near the top of the plot.  Whether or not you think this is a good thing depends upon the questions you want to answer about the heart rates. If you want to be able to say “the maximum heart rate was twice as fast as the minimum heart rate”, then bars are great for this.  Comparing lengths is what bars are made for.  If on the other hand, you want to focus on the relative differences between data (“how much does the heart rate go up by when the subject did some step-ups?”), then points make more sense, since you are zoomed in to the range of the data. There are a couple of other downsides to using a bar chart.  Bars have a much lower data-ink ratio than points.  Further, if we want to add a confidence region to the plot, it gets very busy with bars.  Compare plot_point_region <- plot_point +

+++geom_segment(aes(

++++++x = time, xend = time, y = lower, yend = upper),

++++++size = 2, alpha = .4)

plot_point_region plot_bar_region <- plot_bar +

+++geom_segment(aes(

++++++x = as.numeric(factor(time)),

++++++xend = as.numeric(factor(time)),

++++++y = lower,

++++++yend = upper), size = 2, colour = ""grey30"")

plot_bar_region The big deal-breaker for me is that a bar chart seems semantically wrong.  Bar charts are typically used to visualise a numeric variable split over several categories.  This isn’t the case here: time is not categorical. Something about this analysis was bugging me though, and I started wondering “Is it ever appropriate to use bars in a time series?”.  Last night, as I was watching Guns ‘N’ Roses headline the Leeds Festival, the answer came to me.  GNR were at least an order of magnitude more awesome than expected, but damn, some of those power ballads go on a long time, which allowed my mind to wander.  Here’s their set list, with song lengths.  (Solos and instrumentals omitted, and I wasn’t standing there with a stopwatch so data are taken from the album versions.) songs <- c(

+++""Chinese Democracy"",

+++""Welcome To The Jungle"",

+++""It's So Easy"",

+++""Mr. Brownstone"",

+++""Sorry"",

+++""Live And Let Die"",

+++""This I Love"",

+++""Rocket Queen"",

+++""Street Of Dreams"",

+++""You Could Be Mine"",

+++""Sweet Child O' Mine"",

+++""November Rain"",

+++""Knockin' On Heaven's Door"",

+++""Nightrain"",

+++""Paradise City""

) albums <- c(

+++""Appetite for Destruction"",

+++""G 'N' R Lies"",

+++""Use your Illusion I"",

+++""Use your Illusion II"",

+++""""The Spaghetti Incident?"""",

+++""Chinese Democracy""

) gnr <- data.frame(

+++song   = ordered(songs, levels = songs),

+++length = c(283, 274, 203, 229, 374, 184, 334, 373, 286, 344, 355, 544, 336, 269, 406),

+++album  = ordered(albums[c(6, 1, 1, 1, 6, 3, 6, 1, 6, 4, 1, 3, 4, 1, 1)], levels = albums)

) plot_gnr <- ggplot(gnr, aes(song, length, fill = album)) +

geom_bar() +

opts(axis.text.x = theme_text(angle = 90, hjust = 1))

plot_gnr Here we have a “categorical time series”.  The data are ordered in time, but form discrete chunks.  As a bonus,  the album colouring tells you which tunes have stood the test of time.  In this case, the band’s debut Appetite for Destruction was played even more than the current miracle-it-arrived-at-all Chinese Democracy .  G ‘N’ R Lies and “The Spaghetti Incident?”, by contrast,  didn’t feature at all. "	 0 Comments
Example 8.3: pyramid plots	https://www.r-bloggers.com/2010/08/example-8-3-pyramid-plots/	August 30, 2010	Nick Horton		 0 Comments
Wanted: R Analysis of New Scientist Covers	https://www.r-bloggers.com/2010/08/wanted-r-analysis-of-new-scientist-covers/	August 30, 2010	David Smith	"Peter Aldhous and Jim Giles — from New Scientist's San Francisco bureau — are looking for a statistician and R user to take part in an interesting data analysis challenge, and also be part of a future article in the magazine. They were inspired by this rather tongue-in-cheek presentation where Sebastian Wernicke analyzed videos, transcripts and ratings of TED talks to conclude, for example, that a talk about how “French coffee spreads happiness in your brain” would be the “ultimate TED talk”. As the hook for an upcoming article about analytics and predictions, New Scientist will soon be running a competition along similar lines: can you use information extracted from the covers of New Scientist magazines (headline, subheadline, photograph, main colors, article titles, etc.) to predict newsstand sales? Competitors will be given historical sales data to build models, and can use the extracted cover details (or extract their own details from provided cover images), plus any other publicly-accessible data (weather records for examples), to help boost their predictions. Teams will be ranked on a weekly basis on their ability to generate the best predictions for future newsstand sales in the leadup to the published article. Several teams with expertise in domains ranging from machine learning to sentiment analysis and neural networks — and even a pair of trained pigeons! — have been assembled already, but as yet no-one's using the power of R for the analysis. This is a problem that R is ideally suited for, so if you'd like to give it a shot, and can spend some time over the next few months building novel models that can outperform the other teams, New Scientist would like to hear from you. I've offered to help Peter and Jim find a candidate (or team) for the competition, so send me an email if you'd like to participate with some notes about how you'd tackle the problem. C'mon, you just know you can beat those pigeons. Incidentally, New Scientist is no stranger to R: previous articles have featured analyses or graphics done in the R environment. A few examples appear after the jump; most of the links require a New Scientist subscription to view. In the article “Inside the Stem Cell Wars“, Cox proportional hazards regression in R was used to examine differences in time-to-acceptance and time-to-publication between papers from US and Non-US based scientists in the hottest area of stem cell research. The Kaplan-Meier curve in the article (below) was created in R and enhanced with Adobe Illustrator. 
  Another example comes from the article “Hey green spender: The truth about eco-friendly brands“. The interactive infographic in the screenshot below was based on scatterplots in R. R was also for Kruskal-Wallis tests and subsequent multiple comparisons, and for Spearman rank correlations: see the exact methods used. 
   "	 0 Comments
US House Election Results Visualized Five Ways	https://www.r-bloggers.com/2010/08/us-house-election-results-visualized-five-ways/	August 30, 2010	jjh	"I have been working on an analysis, using OP HouseData, of what effect esoteric campaign finance variables might have on election returns in the US House. To kickoff this project I need a baseline idea of how the Democratic vote share in the US House changed during my target period of 2002 to 2008. With this information I could look for intra-year trends or inter-year clusters that could inform which financial variables I’d include in my analysis.  For the baseline summary I considered using a color-coded map (like CQ, CNN) but I care more about aggregates than individual districts or states. Instead I created five non-map visualizations of the same vote share data, using R and ggplot2. Each visualization helped me better understand my data and refine my assumptions and expectations, even if I eventually discarded the output. The interactive nature of R allowed me to experiment and iterate very quickly until I got what I needed. The R code and data are available at the end of the post.  Figure 1: Democratic Vote Share US House 2002-2008 (scatterplot)  Figure 2: Democratic Vote Share US House 2002-2008 (scatter + jitter) The scatter plots helped me realize what I really wanted was a summary of the distribution of the democratic vote share, not the raw values themselves. That lead me to the following:   Figure 3: Democratic Vote Share US House 2002-2008 (histogram) Since a histogram was too raw I decided to switch back to a box-and-whisker plot.  Figure 4: Democratic Vote Share US House 2002-2008 (box and whisker plot) This lead me to use the established seats-votes plot from theoretic political science literature (Kastellec, Gellman, Chandler (2006), and Jackman, etc (PDF).   Figure 5: Democratic Vote Share USE House 2002-2008 (Seats-Votes plot) It is no surprise the seats-votes plot proved to be the most useful for my purposes since it was specifically designed, by very smart social and political scientists, to look at this type of data. The seats-votes plot is very versatile and can be adapted to a single election by looking at all precincts within a single district. I performed this type of analysis in Aggregate electoral targeting blog post: Democratic vote share, by precinct in VA HOD 13. Even though the other plots aren’t as useful, they do provide some diagnostic information. The box and whisker plot is probably easier to read if you only cared about median vote share, and the histogram plot was excellent in finding uncontested seats. For fewer than 435 points even the scatter plots could be very useful. Please email me or comment with ideas or alternative visualizations of vote share data.   R code
CSV file "	 0 Comments
Graphing Highly Skewed Data	https://www.r-bloggers.com/2010/08/graphing-highly-skewed-data/	August 30, 2010	Thomas Hopper	Recently Chandoo.org posted a question about how to graph data when you have a lot of small values and a few larger values. It’s not the first time that I’ve come across this question, and I’ve seen a lot of answers, many of them really bad. While all solutions involve trade-offs for understanding and interpreting graphs, some solutions are better than others. Data graphs tell stories by revealing patterns in complex data. Good data graphs let the data tell the story by revealing the patterns, rather than trying to impose patterns on the data. As William Cleveland discusses in The Elements of Graphing Data and his 1993 paper A Model for Studying Display Methods of Statistical Graphics, there are two basic visual operations that people employ when looking at and interpreting graphs: pattern perception and table look-up. Pattern perception is where we see the geometric patterns in a graph: groupings; relative differences (larger/smaller); or trends (straight/curved or increasing/decreasing). Table look-up is where we explore details of values and names on a graph. These two operations are distinct and complimentary, and it is through these two operations that the data’s story is told. So suppose that we have some data like that at right, where we are interested in the patterns of smaller, individual values, but there are also a few extremely large values, or outliers. We describe such data as being skewed.  How do we plot this data?  First, for such a small data set, a simple table is the best approach. People can see the numbers and interpret them, there aren’t too many numbers to make sense of and the table is very compact. For more complicated data sets, though, a graph is needed. There’s a few basic options: A bar chart with all data, including outliers, plotted on the same scale. This is the simplest solution, and if you’re only interested in knowing about the outliers (Dec ’09 and Jan ’10) then it will do. However, it completely hides whatever is happening in the rest of the months. Pattern recognition tells us that two months near the end of the series have the big numbers. Table-lookup tells us the approximate values and that these months are around December ’09 and February ’10, but the way the labels string together and overlap the tick marks, it’s not clear exactly what the labels are, let alone which label applies to which bar (which months are those, precisely? Is that “09 Dec” and “09 Feb?” Do the numbers even go with the text, or are they separate labels?). For all but the simplest of messages, this rendition defeats both pattern recognition and table look-up. We definitely need a better solution. Excel gives us an easy solution: break the data into two columns  (“small” numbers in one and “large” numbers in the other) and plot them  on separate axes. Now we can see all the data, including the patterns in  all the months. Bar chart, with outliers plotted using a secondary axis. Unfortunately, pattern recognition tells us that the big-sales months are about the same as all the other months. It’s only the table look-up that tells us how big of a difference there is between the two blue columns and the rest of the data. This is why I’ve added data labels to the two columns: to aid table look-up. Even if we tweaked around with the axes to set the outliers off from the rest of the data, we’d still have the same basic problem: pattern recognition would tell us that there is a much smaller difference than there actually is. By using a secondary axis, we’ve set up a basic conflict between pattern recognition and table look-up.  Worse, it’s easy to confuse the axes; which bars go with which axis? Reproduction in black and white or grayscale would make it impossible to correctly connect bars to the correct axis. Some types of color blindness would similarly make it difficult to interpret the graph. Table look-up is easily defeated with secondary axes. The secondary axis presents so many problems that I always advise against using it. Stephen Few, author of Show Me The Numbers and Information Dashboard Design, calls graphs with secondary axes “dual-scaled graphs.” In his 2008 article Dual-Scaled Axes in Graphs, he concludes that there is always a better way to display data than by using secondary axes. Excel makes it easy to create graphs like this, but it’s always a bad idea. In scientific applications, skewed data is common, and the usual solution is to plot the logarithm of the values. Bar chart plotting skewed with logarithmic axis. With the logarithm, it is easy to plot, and see, all of the data. Trends  in small values are not hidden. Pattern perception immediately tells us  the overall story of the data. Table look-up is easier than with  secondary axes, and immediately tells us the scale of the differences.  Plotting the logarithm allows pattern perception and table look-up to  compliment each other. Below, I’ve created the same graph using a dot plot instead of a bar chart. Dot plots have many advantages over bar charts: most obviously, dot plots provide a better arrangement for category labels (e.g. the months); also, dot plots provide a clearer view of the data by plotting the data points rather than filling in the space between the axis and the data point. There are some nice introductions to dot plots, including William Cleveland’s works and a short introduction by Naomi Robbins. The message is clear: any data that you might present with a bar chart (or pie chart) will be better presented using dot plots. Skewed data plotted on a dot plot using a logarithmic scale. Another approach, which might be better for audiences unfamiliar with logarithmic scales, is to use a scale break, or broken axis. With some work, we can create a scale break in Excel or OpenOffice.org. Bar chart with outliers plotted by introducing a subtle scale break on the y-axis. There are plenty of tutorials for how to accomplish this in Excel. For this example, I created the graph in OpenOffice.org Spreadsheet, using the same graph with the secondary axis, above. I adjusted the two scales, turned off the labels for both y-axes and turned off the tick marks for the secondary y-axis. Then I copied the graph over to the OpenOffice.org Draw application and added y-axis labels and the break marks as drawing objects. That pretty much highlights the first problem with this approach: it takes a lot of work. The second problem is that those break marks are just too subtle; people will miss them. The bigger problem is with interpretation. As with the secondary axis, this subtle scale break sets up a basic conflict between the two basic operations of graph interpretation. Pattern recognition tells us that the numbers are comparable; it’s only table look-up that tells us what a large difference there is. Cleveland’s recommendation, when the logarithm won’t work, is to use a full-panel scale break. In this way, pattern recognition tells that there are two distinct groups of data, and table look-up tells us what they are. Dot plot with a full scale break to show outliers. The potential disadvantage of this approach is that pattern perception might be fooled. While the scale break visually groups the “large” values from the “small” ones, the scale also changes, so that the broader panel on the left actually represents a much narrower range of values (about 1100 dollars range) than the narrower panel on the right (about 17000 dollars range). Our audience might have difficulties interpreting this correctly. Edward Tufte has popularized the idea of small multiples, the emphasis of differences by repeating a graph or image with small changes from one frame to the next. In this case, we could show the full data set, losing fidelity in the smaller values, and then repeat the graph while progressively zooming in on a narrower and narrower slice with each repetition. The full data, with outliers, is plotted on the left. On the right, a zoomed view showing detail in the smaller values. This shares many similarities to Cleveland’s full scale break, but provides greater flexibility. With this data, there are two natural ranges: 0 – 100000 and 0 – 1200. If there were more data between 1200 and 85000, we might repeat the graph several times, zooming in more with each repetition to show lower levels of detail. I think there are two potential pitfalls. As with the full scale break, the audience might fail to appreciate the effect of the changes to scale. Worse, the audience might be fooled into thinking that each graph represented a different set of data, rather than just a different slice of the same data. Some care  in preparing such graphs will be needed for successful communication. When presenting data that is, like the data above, arranged by category, use a dot plot instead of bar charts. When your data is heavily skewed, the best solution is to graph the logarithm of the data. However, if your audience will be unable to correctly interpret the logarithm, try a full scale break or small multiples. 	 0 Comments
GEO database: curation lagging behind submission?	https://www.r-bloggers.com/2010/08/geo-database-curation-lagging-behind-submission/	August 30, 2010	nsaunders	"GSE and GDS records in GEOmetadb by date Is the curation effort keeping up with user submissions?  A little difficult to say, since GEOmetadb curation seems to have its own issues:  (1) why do GDS records stop in 2008?  (2) why do GDS (curated) records begin earlier than GSE (submitted) records?
 "	 0 Comments
MCMC Diagnostics in R with the coda Package	https://www.r-bloggers.com/2010/08/mcmc-diagnostics-in-r-with-the-coda-package/	August 29, 2010	John Myles White	This is a follow up to my recent post introducing the use of JAGS in R through the rjags package. In the comments on that post, Bernd Weiss encouraged me to write a short addendum that describes diagnostic functions that you should use to assess the output from an MCMC sampler. I’ve only been using these diagnostics for a week now for an academic project of my own, so I’ll summarize my understanding of their use as it stands today. Please correct me if I’m spreading misinformation. As I see it, all diagnostics used to analyze the output of an MCMC sampler try to answer a simple question: has the sampler been given a sufficient adaptation (“burn-in”) period to justify your claim that the samples you draw from the chains approximate the posterior distribution of interest to you? This question in turn leads one to analyzing the samples drawn after the burn-in period for obvious warning signs. To test for these potential warning signs, it’s easiest to use the diagnostic functions that are included in coda. To make the use of coda clear, I’m going to follow up on the linear regression example that I used in my last post. First, we generate some data from a linear model: Then we set up our BUGS/JAGS model in example.bug: All of this is copied exactly from my earlier post. Now we change things by replacing our call to jags.samples() with a call to coda.samples(). This will provide output from JAGS in the format necessary for using the two diagnostic functions I understand best: plot() and gelman.plot(). Because we have used such a small number of adaptive samples (only 10), our call to jags.model will produce  this warning message: Thankfully, this message is not the only evidence that we’ve used too few adaptive samples: you can also tell from the output of our call to plot(): The density plots for a and b in the right column are very suspicious: they are extremely pointed and seem to include a large number of outlier values that don’t belong if the values for a and b are normally distributed. You can also see these extreme values in the trace plots in the left column as well: they are the extreme values at the start of the trace for each chain. Simply using 1,000 adaptive samples instead of 10 makes a world of difference: In this image, you can see that the trajectory of the chain is consistent over time and that its distribution looks appropriately normal. So the first takeaway message is simple: check the traces and distributions of your variables using plot() to make sure that they are reasonable and don’t indicate clear deficiencies in the length of your adaptation period. When you know the distributional form of your posteriors, this is particularly effective, as in our example here, where we know to expect normal distributions. Another diagnostic tool is the Gelman plot, which has a simple logic to it: you test how similar the various chains you’re using are as a way to detect whether they’ve hit the target distribution. This similarity is what’s called mixing. To start, it’s easier to simply repeat our examples above using the Gelman plot: And then you can try to see how the plot looks different when you switch to a longer adaptive period: Unfortunately, given our current call to jags.model(), it’s quite hard visually to identify convergence using Gelman plots, since the scales of these plots are not identical across our two examples, and the most prominent visual patterns are likely to be the results of random noise. There is a reason for this difficulty: we’re not properly initializing our sampler’s starting values separately for each chain. Both chains start from identical positions, which means that we don’t have enough power to really see the size of the space a theoretical chain might pass through before settling down. To fix that, we change our call to jags.model() to include an inits value, for which we provide an anonymous function that provides random values consistent with the prior we specified in example.bug. First, let’s repeat our previous approach again: Now let’s add proper random initialization for the starting position of each chain: Now you should be able to easily see that the chains are converging near the end of the sampling period, and that we would do well to give ourselves a greater adaptation period before using any of the samples we’ve generated, since the early samples in the chain are far too dispersed. With these two techniques you should be able to diagnose obvious deficiencies in the length of your adaptive period. 	 0 Comments
Beta translation done!	https://www.r-bloggers.com/2010/08/beta-translation-done/	August 29, 2010	xi'an	Once my team of four translators had handed back to me all the chapters of the French version of Introducing  Monte Carlo Methods with R  to me, I had to go over the book to ensure some minimal consistency between the chapters. I started the editing in the plane to Vancouver but did not get much done until last Monday when arriving in Long Beach. After three days of hard work, here at home, I am now done with the beta version of the translation and I have sent it to the French Springer editor… I hope he will not ask for deep changes as I have absolutely no time left in my schedule for the coming months! 	 0 Comments
SST with Raster. Complete	https://www.r-bloggers.com/2010/08/sst-with-raster-complete/	August 29, 2010	Steven Mosher	"Update: new zip, correcting bug found by Steve  McIntyre: if(!file.exists(HadSST2ncdf)) downloadHADSST2() if(!file.exists(HadSST2ncdf)) downloadHadSST2() issue pending with another line as well. Checking raster versions. I’ve also, added some code into “downloadHadSST2″ that corrects for the “NA” problem with HadSST. (currently commented out). There is an issue with “ncdf” handling CF standards, which has been addressed in raster (need to test) until the ncdf folks address it.  So, you can try uncommenting that code to do the fix manually,like I used to do. UPDATE: try out the box.net on the right hand sidebar to get the two scripts. UPDATE: NA issue with the maps.. the little red dots on Decade maps are actually NAs. Tracking that down.  UPDATE: Windows and Linux users should switch to 1.4.10. Available on CRAN, MAC is coming. (or get it from Forge) Two lines will change as applyStack() will now take na.rm as a parameter. less filling and tastes great! Annual  Annual  Decade  Decade 
 Update: Admiring my own stuff, I found this: decadeIndex  decadeIndex 
 Headed out for bit, test on MAC when I return. Latest version is in the box to the right. Analysis of SST processing with R’s ‘raster’ package is now complete.  The process has gone really smoothly primarily because of some help by the key package developer. My goal was to complete SST processing using only raster objects and only raster functions. When you download the zip file and unzip it, you will see the following:  
 Those are the only two files you should have in your directory. Make this folder your working directory and make sure that you have all the packages installed. This will not work properly unless you have the lastest version of raster from  R Forge. 1.4.9. The other pacakges you need should be clear from the library load statements in the R Script: SSTVerify.R. With all the packages installed, just execute the script, SSTVerify.R That script will download and unzip all the files you need. It will download the SST datafile ( which takes a while ), a land/ocean mask, and the latest results from Hadley that we will validate against. When the program completes you will se your directory populated like so:  While the program runs you can watch the progress as files fill up.  I’ll go over the files in order as they are written ( the ascci files are self explanatory. Arrg,doubled the date columns.. easy fix). The first calculation we do is a simple unweighted average. This is compared to the figures from Hadley.Next we weight the temperatures by the area of grid cells. small difference is primarily from a difference in how they calculate a global average. There average is the mean of the NH and SH. Moshtemp calculates a global average. Next I take the unweighted temps and generate Annual area maps for every year in the series ( and yes my colors suck ). Then I collate by decade.Finally I test applying a weight for  the actual area  of water in a cell. This would ONLY be applied when doing a complete land/ocean analysis. There we would have temperature from land stations and from Ocean data sets in the cells that are coastal. So, don’t make too much of the difference between these series. What is shows is that coverage is increasing in cells that are coastal. To show you how powerful raster is, We can calculate the monthly SST like this in 3 lines SST  Weights  Monthly  and Monthly will contain the mean monthly SST for 1850 to 2010/7 OR in 2 SST  Monthly  What this means is that when the land is rewritten we will be able to combine a land series and SST series in a few simple lines. 
 
 
 
 
 
 
  
 
 
 
 
 
  
 
 
 
 
 
 
  
 
 
 
 
 
  
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 "	 0 Comments
Subset views in R	https://www.r-bloggers.com/2010/08/subset-views-in-r/	August 28, 2010	ellbur	"I don’t know how to do this in R. So let me just say why I can’t. I wanted something akin to Boost‘s sub-matrix views, where you can have indexes map back to the original matrix, so you don’t create a new object. Sounds straightforward, just overload ‘[[‘ to subtract the offset and check the length. Alas, no dice. R zealously copies objects to the point this is not (as far as I know, which isn’t much) possible. To demonstrate, the following function executes and times expressions operating on a vector called “M”. Like Then see, does the size of M affect the time of the operation? (obviously) And here’s why we know it’s copying: Or with attributes Good luck making a subset without copying! And here’s the relevant parts of the R code. Making a list (main/builtin.c) Note the repeated calls to ""duplicate"". And yes, duplicate does copy, and it is deep (main/duplicate.c): "	 0 Comments
Blegging for Data	https://www.r-bloggers.com/2010/08/blegging-for-data/	August 28, 2010	John Myles White	I’m in the middle of a new project that involves analyzing the packages that are currently on CRAN. As part of my work, I could really benefit from information about which packages are installed on people’s computers. If you’re willing to part with a bit of your time and privacy, I’d very much appreciate you running the following script in R, and sending me the output file my_installed_packages.csv by e-mail to [email protected] 	 0 Comments
Patrick Burns is blogging	https://www.r-bloggers.com/2010/08/patrick-burns-is-blogging/	August 28, 2010	Joshua Ulrich	"
 "	 0 Comments
Mike’s CNC 2010-08-27 18:36:00	https://www.r-bloggers.com/2010/08/mikes-cnc-2010-08-27-183600/	August 27, 2010	Mike Messner		 0 Comments
Fractals in R	https://www.r-bloggers.com/2010/08/fractals-in-r/	August 27, 2010	C		 0 Comments
Because it’s Friday: How Machines Work	https://www.r-bloggers.com/2010/08/because-its-friday-how-machines-work/	August 27, 2010	David Smith	"Ever wondered how a sewing machine seemingly manages to knot stitches without ever releasing the thread? Well, wonder no more: 
  Find this and other animations of marvels of engineering, including the universal velocity joint and the rotary engine, at the link below. World Of Technology: Complicated Mechanisms Explained in simple animations (via)   http://mytechnologyworld9.blogspot.com/2010/08/complicated-mechanisms-explained-in.html   "	 0 Comments
Poll: Half of SAS users considering a switch	https://www.r-bloggers.com/2010/08/poll-half-of-sas-users-considering-a-switch/	August 27, 2010	David Smith	A recent poll of KD Nuggets readers suggests that of those using SAS today, almost half (49.6%) are considering switching to a different system for statistical analysis. The poll was prompted by the recent high court decision in the UK, that affirmed that “WPS is lawful clone of SAS system” (as stated in a WPS press release). The exact poll question was, “Are you a current SAS user and are you considering a switch, in light of a recent WPS court victory vs SAS?”. The poll also allowed respondents to indicate one or more solutions they're considering switching to, with the following responses:   KD Nuggets Polls: Switching from SAS to WPS, R, … 	 0 Comments
Matlab-style multiple assignment in R	https://www.r-bloggers.com/2010/08/matlab-style-multiple-assignment-in%c2%a0r/	August 26, 2010	ellbur	"R again! You know how in Matlab you can do? I like that. R generic functions makes this possible. First, let’s genericize assignment. I feel like regular “=” and “
 Now the next step is a bit tricky. We need to group several variables on the left of %=% The trick is they have to stay unevaluated. Luckily R uses (what IIRC is called) “pass by name”, so we can do this. Let’s start with a function to take a, b, … The hard part is grabbing the ‘…’ without evaluating it. won’t work, nor will alist(…). I have NO IDEA why, but the following expression (stolen from data.frame()) works: So now we have our function And we can add a specific implementation for our generic %=%: Which just treats the objects “a”, “b” … as strings, and assigns into the caller’s environment. That’s what I had first, but it can be made better. With the above implementation: It doesn’t play nice with assignment functions. It can be modified to call ‘
 And now all is good. "	 0 Comments
ProjectTemplate	https://www.r-bloggers.com/2010/08/projecttemplate/	August 26, 2010	John Myles White	As many people already know, I’ve recently uploaded a new R package called ProjectTemplate to GitHub and CRAN. The ProjectTemplate package provides a function, create.project(), that automatically builds a directory for a new R project with a clean sub-directory structure and automatic data and library loading tools. My hope is that standardized data loading, automatic importing of best practice packages, integrated unit testing and useful nudges towards keeping a cleanly organized codebase will improve the quality of R coding. My inspiration for this approach comes from the rails command from Ruby on Rails, which initializes a new Rails project with the proper skeletal structure automatically. Also taken from Rails is ProjectTemplate’s approach of preferring convention over configuration: the automatic data and library loading as well as the automatic testing work out of the box because assumptions are made about the directory structure and naming conventions that will be used in your code. You can customize your codebase however you’d like, but you will have to edit the automation scripts to use your conventions instead of the defaults before you’ll get their benefits again. In what follows, I try to highlight the state of the package as of today. ProjectTemplate is available on CRAN and can be installed using a simple call to install.packages(): If you would like access to changes that are not available in the current version on CRAN, please download the contents of the GitHub repository and then run, To create a new project called my-project, open R and type: To enter that project’s home directory and start working, type: Once you have code worth testing, you can also type, to automatically run all of the unit tests in your tests directory. If you’re interested in these last two functions, you should know that load.project() is essentially a mnemonic for calling source('lib/boot.R'), which automatically loads all of your libraries and data sets. Similarly, run.tests() is essentially a mnemonic for calling source('lib/run_test.R'), which automatically runs all of the ‘testthat’ style unit tests contained in your tests directory. As far as ProjectTemplate is concerned, a good project should look like the following: To do work on such a project, enter the main directory, open R and type source('lib/boot.R'). This will then automatically perform the following actions: Within your project directory, ProjectTemplate creates the following directories and files whose purpose is explained below: I would love to hear feedback about things that ProjectTemplate is missing or should do differently. Please leave any and all comments you have. 	 0 Comments
Oh (de)bugger!	https://www.r-bloggers.com/2010/08/oh-debugger/	August 26, 2010	richierocks	"By number of questions asked, R passed MATLAB for the first time on Stack Overflow today.  Thus it seems an appropriate time to write my first R-based post. This post concerns what to  do when your R-code goes pear shaped.  Back in June there were a couple of very good videos on R debugging that came out of an R meetup in New York.  Jay Emerson talked about basic debugging functions like print and browser and Harlan Harris talked about more advanced techniques like trace and debug.  These R meetups sound like a great idea but I suspect that we don’t have the critical mass of R users here in Buxton, UK.  I digress … There are two obvious cases where you need to debug things.  If you are lucky, you have the case of an error being thrown.  It sounds like it should be the worst case, but at least you get to know where the problem is occurring; the more difficult situation is simply getting the wrong answer. When I get an error, then traceback() is my usual instinctive response to find the location of the error.  If the ggplot2 package is loaded, then tr() provides a way to save a few keystrokes.  This function isn’t infallible though, and seems to have particular trouble with code in try blocks.  To see this, compare 

throw_error <- function() stop(""!!!"")

throw_error()

traceback()


with


throw_error_in_try <- function() try(stop(""!!!""))

throw_error_in_try()

traceback() #Same as before; new error did not register

 In the ‘hard’ case, where you have a wrong answer rather than an error, or where traceback has let you down, you’ll have to step through your code to hunt down the problem.  This entails using the debug function, or it’s graphical equivalent mtrace (from the debug package).  I don’t want to spend time on those functions here (another post perhaps), so if you’re desperate to know how they work I recommend Harlan’s video tutorial. After I’ve found the function that is causing the problem, my next step is usually to stick a call to browser in my code and rerun it.  This lets me explore my environment and test out alternative code.  The following example is just a wrapper for sum.  The nesting gives us a call stack to look at. 

first <- function(...) second(...)

second <- function(...) sum(...)

first(2, 4, ""6"")

 The call to traceback tells us where the error is. 

2: second(...)

1: first(2, 4, ""6"")

 Let’s suppose that the error wasn’t as obvious as this.  (That’s the character input, for those of you asleep at the back.)  The traceback output has shown us that the error occurred in the function second.   Technically, it occurred in sum, but the contents of that are C code rather than R code and traceback is smart enough to know it is of no use to us.  As I described above, the next step is to call browser, just before the problem occurs. 

second <- function(...)

{

   browser()

   sum(...)

}

 Now when we rerun the code, execution halts at the browser() line, and we get the chance to dig about into what is going on. In this case, since all the arguments are contained in the ellipsis, we can see everything with list(...).  The more common circumstance is to have at least some named arguments in your function call.  In those cases ls.str() is the quickest way to see what is going on.  list(...) returns 

[[1]]

[1] 2 [[2]]
[1] 4 [[3]]
[1] “6″ The real strength of the browser function is that if the problem wasn’t obvious now, we could go on to execute additional code from within the function environment.  Nevertheless, there are some limitations with the approach.  The first issue is that we need to be able to get at the source code.  There are two main cases where we can’t do this: firstly, when external code is called and secondly, when the code is tucked away in a package.  The external case is sadly quite insoluble.  I don’t know of any easy ways to debug C or Fortran code from R.  For debugging in packages, if you have an error being thrown, setting options(error = recover) is an excellent alternative to adding browser statements.  If there is no error thrown, then debug and trace are the best solutions, but they are beyond the scope of this post. Another issue is that the problem with our code may not be caused in the same function that the error is thrown in.  It could occur higher up the call stack.  In that sort of situation, you need a way to see what is happening in all the functions that you’ve called.  And that is what I’m going to talk about in part two of this post. "	 0 Comments
New R User Groups in Seoul, Denver	https://www.r-bloggers.com/2010/08/new-r-user-groups-in-seoul-denver/	August 26, 2010	David Smith	We have two new local R user groups to report this week. In Seoul, South Korea R user Chel Hee Lee is the organizer of the GNU R User's Group and Open Statistics Project in Korea. My translate-fu isn't quite up to figuring out when the next meeting is, but you can contact the group organizers here or here. In Denver, Colorado computational statistician Ryan Elmore (recently featured here on the blog) has teamed up with several other local R users to form the Denver R User Group. The first meetup on September 14 will feature Ryan talking about using R to analyze baseball data, in addition to planning for future meetups. R users in Denver and Seoul — get to it! Revolutions: Local R User Group Directory  	 0 Comments
Producing grids of plots in R with ggplot2: A journey of discovery	https://www.r-bloggers.com/2010/08/producing-grids-of-plots-in-r-with-ggplot2-a-journey-of-discovery/	August 26, 2010	Robin Wilson	I’ve just gone through a bit of a ‘journey of discovery’ in R while trying to plot a grid of plots for one of the research projects I’m doing. I wanted to write a simple function which could produce this grid of plots from a CSV file, allowing me to easily view the trends of the dune metrics produced by my DunesGIS project. I first started by loading the data into R and producing a simple ggplot2 graph, which I then customised using the standard ggplot2 commands. At this point I had the following code: I wanted to produce a number of these plots to show all of the key dune metrics, so I attempted to arrange these plots in to a grid. This is where it got difficult… I could easily put the plots in a grid using the arrange function (available here), but I had to write lots of repetitive code to create the graphs. I couldn’t see a way to easily put this into a function, as I needed to take in a variable name as an argument and then use it as a variable. Luckily, some of Steve Yegge’s posts on lisp must have stuck in my brain, as I realised I could do this using a macro. It happens that R is a nice language that has support for macros, and I managed to write one quite easily: This uses the defmacro function, and takes a variable name (for example, mean_len) and some text to use as the title, and then returns the result of the qplot() call. Combining this with the call to arrange led to the code below: Now, I thought I'd been pretty clever by now, and was pleased with the result (see below). However, I realised it could do with improving...  Grid of plots (arrange-based method) The graphs weren't aligned very well, and the whole thing looked rather amateurish - not what I want if I end up publishing in a journal. So, I contacted the ggplot2 mailing list for help. You can read the whole thread here if you want, but I'll be explaining how I progressed below. Members of the mailing list suggested that I used the faceting feature to make a better grid of plots. I had considered this, as I knew faceting created grids of plots, but I'd never really understood this whole facet thing. Still after some help from the mailing list I found I could easily create a 'dummy faceting variable' to get this to work. Originally, my data frame looked like this: To use faceting I needed to reshape this data frame so that I had a variable field giving the name of the variable, repeated as many times as needed to get all of the data in. That sounds complicated, but comparing the example below to the example above should explain it: This can be easily accomplished using the melt command in the reshape package. The command I used was: This performs a melt operation on the data frame df using name and t as ID variables (that is, variables that identify each row - in this case the time and the name of the model run). Once you've melted the data frame you can plot it using the faceting feature of ggplot2, for example, in the code below: This code, combined with the data frame loading and melting above produced the following output:  Grid of plots (facet-based method) That's a lot better, but still needs some tweaking. Members of the mailing list advised how to change the order of the plots, and I also found out how to remove variables that I didn't want. I then tweaked the appearance of the plot. The final task was to wrap the whole lot in a function called plot_graphs which takes an argument of the path to a CSV file and then plots the graphs based on it. The final code is below, followed by the final output:  Final Grid of Plots (facet-based method) You'd think that would be the end of it...but the most useful part of this whole process was the advice given by members of the mailing list after I'd succeeded with my task. This advice was a suggestion of a way of working within R and ggplot2 that will bring dividends: that is, to do as much of the data processing as possible outside ggplot2, and then just use simple graph plotting functions. R has loads of useful packages (like reshape which provides the melt command used above), and can do a lot of very clever processing very easily. Also, the ability of R to link to databases was mentioned, as SQL queries can often be a very good way to extract data for visualisation (something I may use to store dune metrics in a later version of DunesGIS). So, thanks are due to all the members of the ggplot2 group who contributed to this 'journey of discovery': Baptiste, Dennis, Hadley, Brandon and Mark - thanks guys! 	 0 Comments
In Search of Power-laws: WikiLeaks Edition	https://www.r-bloggers.com/2010/08/in-search-of-power-laws-wikileaks-edition/	August 26, 2010	Drew Conway	Yesterday, a commenter reminded me of the very popular hobby among scientists of searching for power-law distributions in large event data.  While the commonality of scale invariance in event data is quite well known—particularly with respect to conflict data—this has not prevented many researchers from seeking and finding these patterns in data. As the commenter notes, it is likely that the WikiLeaks data will soon be annexed into this line of research.  Before other researchers examine the distributional properties of these data more thoroughly, it is worth doing a quick exploration to show some of the issues in power-law fishing, and how to avoid them.  First, we begin by plotting the WikiLeaks casualty data using the traditional log-log transformation and fitting a linear regressio.   The search for power-law distributions often focuses on the scaling parameter: .  Scaling parameters where 2 <  < 3 are generally accepted as fitting a power-law, thus the search is for values in that range. When using linear regression to fit the distribution this parameter is calculated simply as the slope of the linear fit to the logged data.  The above panels show this analysis for two different version of the data.  On the left, the data are restricted to only observations with, in the words of Lewis Richardson, “deadly quarrels.”  That is, WikiLeaks events where a death occurred, which accounts for friendly, enemy, host nation and civilian deathsg.  Interestingly, we find that for the KIA data the scaling parameter falls just outside the necessary range to be classified as a power-law.   At this point we might conclude that the data does not fit our assumptions and move on to test other distributions.  If we were particularly motivated to find a power-law in this data, however, one option would be to go back and loosen our restriction on the data to include not just KIA’s but all casualties, i.e., non-deadly quarrels.  The assumption being that with more data points the “tail” of the distribution would be longer and thus more likely to fit a power-law.  The right panel above illustrates this analysis, and as you can see in this case we find that the data do fit a power-law, with . Unfortunately, even if suspending disbelief enough to accept the altogether dubious inclusion of more data points to force-fit a power-law, everything we have done up to this point is wrong.  As was brilliantly detailed by Clauset, et al in “Power-law distributions in empirical data,” linear fits to log transformed data are extremely error-prone.  As such, rather than rely on the above findings we will use the method detailed by these authors for properly fitting power-law on the WikiLeaks data. In this case we need to do three things: 1) find the appropriate lower-bound for the value of  for our data, which in this case are events with casualties; 2) fit the scaling parameter with ; 3) perform a goodness-of-fit test to test whether our empirical observations actually fit the parameterization of the distribution. For the first step we are fortunate, as we know the appropriate minimum value , since these are discrete event data and we are counting the number of observed casualties in the data.  Equally convenient, this allows for a straightforward maximum-likelihood estimation of the scaling parameter via a variant of the well-known Hill estimator.  This functionality is built-into R’s igraph package so we can compute the new scaling parameters easily.   Using this more accurate methods for estimating the scaling parameter reveals that—in fact—neither set of data on the frequency and magnitude of violent events in Afghanistan fit a power-law. As a result, goodness-of-fit tests for power-law with this data are unnecessary, but as described in Clauset, et al. using a Kolmogorov–Smirnov test to measure the distance between theorized and observed distributions is a useful tool for checking fits to other distributions.  There are several alternative distributions that may better fit these data, many of which are specified for simulation in the degreenet R package, but I leave that as an exercise to the reader. There are two primary things to take away from this exercise: 1) power-laws are much less frequently observed than is commonly thought, and careful estimation of scaling parameters and goodness-of-fit should be performed to check; 2) it appears that the WikiLeaks data fall well short of proving, or even reinforcing, previous conclusions about the underlying dynamics of violent conflict. As always, the code used to generate this analysis is available on Github. 	 0 Comments
Mike’s CNC 2010-08-26 06:37:00	https://www.r-bloggers.com/2010/08/mikes-cnc-2010-08-26-063700/	August 26, 2010	Mike Messner		 0 Comments
Louis: A Silent Film with Live Music	https://www.r-bloggers.com/2010/08/louis-a-silent-film-with-live-music/	August 26, 2010	Thinking inside the box	"

The film, which is written, directed and producted by Dan Pritzker, is based
loosely on the early years of Louis Armstrong in New Orleans.  The movie is
shot beautifully by Vilmos Zsigmond in blend of colour and black-and-white
which works very well for invoking the early days of film.  A key part of the
production is of course the score, and the live music with both a
thirteen-piece orchestra featuring Wynton Marsalis as well as piano solo
recitals by Cecile Licad with an emphasis on pieces by 19th-century composer
Louis Moreau Gottschalk.  The combination of a silent movie with a stong live
band is something to behold — if you can catch the movie and performance in
a city nearby, go!

 "	 0 Comments
Labor vote share across different types of balloting	https://www.r-bloggers.com/2010/08/labor-vote-share-across-different-types-of-balloting/	August 26, 2010	jackman	We’re starting to get more divisions reporting two-candidate preferred numbers by vote type.  The emerging picture (literally) is one in which Labor’s performance on the pre-polls and postals is lagging its performance in ordinary votes.  On the other hand, Labor seems to be doing well among absentee voters (the regression line sitting above the 45 degree line). These numbers are in flux, and the graph below will change a little. Details: each panel shows a scatterplot between Labor’s 2PP percentage of the vote in ordinary votes (horizontal axis) and other forms of voting (absentee, pre-poll, postal).  Each plotted point is an electoral division (in which the Labor candidate is one of the two-candidate-preferred candidates).  The gray line is a 45 degree line; if Labor’s 2PP vote shares were the same irrespective of the type of ballot, we’d have the data falling on this line.  If the points tend to lie below that line, we have evidence of Labor doing better in the “ordinary” voting. The dark line is a linear regression.  Continuously updated PDF; preview JPG below.  	 0 Comments
Global Temperature Proxy Reconstructions ~ now with CO2 forcing	https://www.r-bloggers.com/2010/08/global-temperature-proxy-reconstructions-now-with-co2-forcing/	August 26, 2010	apeescape	Previously, I did a simple Bayesian projection of recent temperature using proxy data and the methods shown in McShane and Wyner (2010). I showed that when you take out the last 30 years of data (1969~1998), the projection does not track the recent uptick in temperatures well. The “projection” is a simple unparametric bootstrap which draws parameter values from the posterior and fits them to the predictors (proxies). Now let’s try this for the reconstruction period. The strategy is the same as before. Estimate the posterior using JAGS and backcast by starting at 1998 and iteratively move forward given parameter draws. Do this 100 times, and we get the 95% credible interval. I did this over the instrumental period as well, for two calibration periods: one for the CRU dataset, and another that took out the most recent 30 year period. (click for source code: probably has to be combined with the code in the previous article). (data @ McShane’s website)  The orange line is the frequentist estimate directly from MW2010 (w/o AR2 structure), the green line is the Bayesian estimate with 95% credible interval, and the purple line is the Bayesian interval w/o the last 30 years of data to calibrate. One thing you’ll notice is that both projections underestimate the most recent warming. Not surprisingly, the green (which uses the whole instrumental period) does better in this period. What’s even more interesting is the deviation in the early years between the two projections. When I saw the results of M&W2010, I noticed the acute angle the “hockey stick” has compared to other recent reconstructions (Mann et al. 2008; Li et al. 2010; Li et al. 2007; Hopcroft et al. 2009; Smith 2010; Tingley 2009; Hegerl et al. 2007; Lee and Zwiers 2008). The method seems to be fairly sensitive to certain assumptions. The paper pushed the hardest for the model with 10 principal components and an AR(2) structure. Given that the data that’s used is from Mann et al. (2008), I thought it was a little weird that there was a large discrepancy between the two. Now, there’s obviously a ton of things to be done to check (and much of it has been done), but I was interested in the inability to predict the short run-ups in recent periods. My hypothesis was, the tremendous increase in greenhouse gases has created an atmosphere (no pun intended) that is wholly different from anything in the past. I could test this by borrowing ideas from Li et al. (2010) — to use forcing data in the model. Traditionally, reconstructions of past climate have come from aggregating proxy data. But Li et al. (2010) created a Bayesian hierarchical model (BHM) that enables separating out error between residuals, process and priors (instead of just two out of three). Climate forcing (data) can predict temperature, so Li et al. used forcing data as the temperature driver, while measurement/observation error was articulated through the proxies. The BHM is still incomplete (only uses CO2, volcano, sun as forcings), but it’s shown to reduce both the bias and variance in the models. I might do a BHM later, but for now, an conceptually easy compromise is to collapse this hierarchy, and just use it as a linear predictor to the original model.  I only include a CO2 predictor for simplicity, and the usual uninformative priors. I did the same simple projection process as before, and overlaid the results in blue: data @ Institute for Mathematics Applied to Geosciences (IMAGe)  This model with the CO2 predictor predicts the recent upswing in temperature the best. Not only that, the predictions of past temperatures are lower than the other two models. The purple line omits the last thirty years, while the blue line does not omit the last thirty years worth of data, but they are still the most similar. My guess why this happens, is that the hidden CO2 effect “corrupts” the green reconstruction because you’re parameterizing against something that has not happened before. The purple line has less data, but it’s at least calibrating against data where CO2 forcing is not as strong, similar to anything before the modern period. The blue model with the CO2 predictor corrects for both, and predicts both the end and beginning of the series well. Now, the usual disclaimers: this is not a rigorous analysis. The stationarity assumption may be violated easily with the CO2 forcing (or any of the other PCs), and I’ve obviously not considered other forcing that will certainty change the proportional effect of log(CO2). So don’t go bonkers! Refs: 	 0 Comments
Online Certificate Courses in Computational Finance with R	https://www.r-bloggers.com/2010/08/online-certificate-courses-in-computational-finance-with-r/	August 25, 2010	David Smith		 0 Comments
Excel Errors and Other Numerical Nightmares	https://www.r-bloggers.com/2010/08/excel-errors-and-other-numerical-nightmares/	August 25, 2010	Neil Gunther		 0 Comments
Slides and replay for “Big Data with Revolution R”	https://www.r-bloggers.com/2010/08/slides-and-replay-for-big-data-with-revolution-r/	August 25, 2010	David Smith	Thanks to everyone who attended our webinar this morning, Big Data Analysis for R Using Revolution R Enterprise, and in particular thanks for all the thoughtful questions during the Q&A session. If you missed the live broadcast, a replay is now available (requires the ability to view WMV files), and you can also download the slides in PDF format. Revolution Analytics: Big Data Analysis for R Using Revolution R Enterprise 	 0 Comments
Creating a Presentation with LaTeX Beamer – Using Overlays	https://www.r-bloggers.com/2010/08/creating-a-presentation-with-latex-beamer-%e2%80%93-using-overlays/	August 25, 2010	Ralph	Overlays can be used in a LaTeX beamer presentation to reveal parts of a slide sequentially, which can be a useful way of imparting information to your audience. There are a number of different ways that overlays can be created depending on the sophistication required in the presentation. Fast Tube by Casper The pause command is the easiest way to create simple overlays that reveal consecutive parts of a slide. Commands such as <1-> can be added after various beamer environments to allow them to appear on different slides of the overlay set to provide greater control over the display. The onslide command is used to indicate a range of slides in the overlay where the information on that line should be displayed. There are ways to create more complicated overlays to reveal information on a slide. Other useful resources are provided on the Supplementary Material page. 	 0 Comments
From igraph to network and back again	https://www.r-bloggers.com/2010/08/from-igraph-to-network-and-back%c2%a0again/	August 25, 2010	Michal	The Hobbit, or There and Back Again (by Alan Lee) In an effort to achieve this (last paragraph), I created a couple of functions to coerce networks as ‘igraph’ objects to networks as ‘network’ objects and vice versa. I wrapped them into a package called ‘intergraph’ which I just uploaded to my personal miniCRAN. Please mind, this is still an experimental version! Might be bug-infested. The package depends on ‘igraph’ and ‘network’ packages (big thanks to Gabor Csardi and Tamas Nepusz for ‘igraph’ and to Carter Butts and the Statnet team for ‘network’!). At the time of writing this the ‘network’ package available on CRAN (version 1.4-1) does not have a namespace. To be able to work with both ‘igraph’ and ‘network’ together reliably I had to create a version of the ‘network’ package with the namespace. To try ‘intergraph’ you will need to install the namespaced version. Installation instructions are at the bottom. The package contain two major functions ‘as.igraph’, and a  ‘as.network’ method for ‘igraph’ objects. Consequently, you can go back  and forth between the two representations as in: Because  ‘network’ objects define some additional attributes (like ‘na’ etc.)  which are not used by ‘igraph’ objects ‘ig’ and ‘ig2′ will not be exectly  identical (in the sense of ‘identical()’) . The only potential differences though is due to these attributes. Fooling around further: which should produce a figure as shown.  I’m going to test this a bit still, and you are welcome to do that too.Bug reports, comments etc. are more than welcome.  If everything is OK the package will be available through CRAN. To install namespaced version of the ‘network’ package (tagged with version 1.4-1-1) call: To install a Windows binary version use one of depending on your R version. Sorry, no other R Windows versions  supported at this time. I don’t have the facilities to build Mac  binaries either. If you don’t want to overwrite the version of the ‘network’ package you already have installed you can install it to a different directory (essentially separate R library tree) following these steps: I do hope the next version of ‘network’ will have a namespace. To install the ‘intergraph’ package itself use 	 0 Comments
Mike’s CNC 2010-08-25 07:54:00	https://www.r-bloggers.com/2010/08/mikes-cnc-2010-08-25-075400/	August 25, 2010	Mike Messner		 0 Comments
What the hell is a variance matrix?	https://www.r-bloggers.com/2010/08/what-the-hell-is-a-variance-matrix/	August 25, 2010	Pat	"When I first came to finance, I kept hearing about “risk models”. I  wondered, “What the hell is a risk model?” Of course, I didn’t say this  out loud — that would have given the game away.  My wife has strict  instructions that she is to be the only one to know that I’m an idiot. Finally enlightenment came: “Oh, you mean a variance matrix.” So  let’s look at what a variance matrix (also called a covariance matrix  or a variance-covariance matrix) is, or a risk model if you prefer. The  big idea is that we want something that shows the relationship between  each pair of returns. Note “returns” and not “prices”.  You can do the  calculations with prices instead of returns, but you end up with useless  garbage. The variance matrix is square with a row and a column — in our case — for  each asset.  In practice the number of assets can range from a few to a few thousand. The diagonal elements of the matrix are the variances  of the assets.  If the variance matrix is annualized, then these  diagonal elements are the squared volatilities.  It would be nicer if we  just had volatilities instead of their squares.  Contrary to popular  belief statisticians are not in general sadists — or masochists.  The  volatilities are squared because, unpleasant as it is, that is actually  the easiest path. The off-diagonal elements are covariances.   Again, if we have the scaling on an annual basis, then a covariance is  the volatility of the first asset times the volatility of the second  asset times the correlation of the two assets.  The matrix is symmetric,  and hence redundant — the covariance of the first asset with the  second asset is the same as the covariance of the second asset with the  first asset. Correlations go from -1 to 1. Here is some R code that will show you what a sample of random normals looks like that have a particular correlation:


> require(MASS)

> mycor <- 0.5

> plot(mvrnorm(100, c(0,0), matrix(c(1,mycor,mycor,1), 2), empirical=TRUE))


Figure 1: 100 random normals with sample correlation = 0.5
Figure 1 shows an example of the plot command in action.  (The figure cheats slightly by setting xlab and ylab to the empty string.)  You can recall the plot command and re-execute it several times to get a sense of how variable the same sample correlation can look.  You can then change the correlation or the sample size. Now that we know what a variance matrix is, how do we go about getting one?  We have to estimate it. In the abstract statistical setting we estimate a variance matrix using a matrix of data.  The rows are observations, the columns are variables.  The result is a variance matrix that is number of variables by number of variables.  The assumption is that each observation is independent from the others, and they all have the same distribution.  In particular they all come from the same variance matrix. We’ll start with a matrix of returns: times (trading days, for instance) in the rows, assets in the columns.  But here is where we and textbook statistics part ways. The means of returns at different times are (presumably) not the same, like statisticians demand.  But they are close enough that this issue is not worth worrying about. In market data there is the phenomenon of volatility clustering.  Volatility jumps up during crises and then gradually dies back down again.  The dying down is decidedly non-smooth. Figure 2: Approximate S&P 500 volatility for 2000 days up to 2010-Aug-23
Figure 2 provides a feel for volatility clustering.  The estimate of volatility was done with a rather crude model, but if we magically had a picture of the true volatility, the overall features would be very similar. Correlations are dynamic as well.  In general they also increase during crises. For these reasons using as long of a history as possible to estimate the variance matrix is not a good plan.  We only want to use current data.  What does “current” mean?  Excellent question. The classical statistics set-up demands that we have more observations than variables (more time periods than assets in our case).  It isn’t that we can’t do the calculation if there are fewer times than assets, it’s that we might not like what we get.  The technical terms for what we get is a singular matrix — one that is not positive-definite.  The practical effect is that the variance matrix says that there exist portfolios with zero volatility. That’s a place not to go — there be dragons. There are several ways to get around this problem.  We’ll save that discussion for another post or two. One more caution that you’ll find in the statistics literature is that correlation is only about linear relationships.  There is the possibility of zero correlation, but a strong relationship.  Perhaps there are other opinions, but I don’t think we need to worry about this. Relationships are continuously in flux so we can only ever get an approximation anyway.  A linear approximation is about all we can hope for. Let”s review: The first point suggests that what we want (most often) is a prediction for some future time period rather than an estimation of the past.  The second suggests that that prediction is going to be noisy.  Very noisy to a statistician’s eye. Using a financial perspective, the prediction error of the variance matrix is rather modest though.  Predicting returns is close to impossible, so relatively speaking the variance matrix is predicted with brilliant accuracy. "	 0 Comments
informality, the 2010 edition	https://www.r-bloggers.com/2010/08/informality-the-2010-edition/	August 24, 2010	jackman	"For the last two cycles I’ve done some simple regression analysis of the informal vote.  I saw Possum have his go at it, using a specification that is virtually the same as what I’ve run in the past (2007, 2004). The 2010 edition follows.  As usual, electorate-level informality in House of Reps voting increases with (a) the number of candidates on the ballot; (b) the percentage of the electorate residing in non-English-speaking households (NESH); (c) does the state have optional preferential voting in their state legislative elections (NSW & QLD); but decreases with (d) percentage of the electorate with tertiary qualifications. The basic linear spec gets you: Not too shabby for 4 linear, additive predictors. You can do a little better with semi-parametric terms (thin-plate smoothing splines, via the mgcv package in R) in the NESH and tertiary predictors, and an interaction with OP/non-OP: Update: by request, the four smooth terms from the GAM.
PDF "	 0 Comments
How Safe is Your Money?	https://www.r-bloggers.com/2010/08/how-safe-is-your-money/	August 24, 2010	C		 0 Comments
Almost There	https://www.r-bloggers.com/2010/08/almost-there/	August 24, 2010	awaiting assimilation	It’s been a long wait, but it’s almost over… Ever felt like you started a term paper over and over? Never really getting the mojo to finish it? That’s how I felt creating an ubuntu package for rApache. They should be available in a few hours on launchpad.net, i386 and amd64 at least. Here’s the sources.list entries: and check out apt-cache show libapache2-mod-rstats! 	 0 Comments
Portfolio Cycle Charts	https://www.r-bloggers.com/2010/08/portfolio-cycle-charts/	August 24, 2010	wuertz		 0 Comments
R and Analytics: A good career choice	https://www.r-bloggers.com/2010/08/r-and-analytics-a-good-career-choice/	August 24, 2010	David Smith	According to Microsoft, the hottest three new tech majors are:  Tim O’Reilly concurs. Of course, R features prominently in most of these areas — follow the links I added above for examples — and is growing rapidly, so learning R makes for a smart career choice.  Microsoft Careers JobsBlog: The Top Three hottest new majors for a career in technology 	 0 Comments
Rmetrics at useR! 2010	https://www.r-bloggers.com/2010/08/rmetrics-at-user-2010/	August 24, 2010	wuertz		 0 Comments
Demonstration of {estout}	https://www.r-bloggers.com/2010/08/demonstration-of-estout/	August 24, 2010	Ben Mazzotta	"I wrote a short talk demonstrating the use the R package {estout} for tonight’s New England R Users Group meeting.  NB this is not a discussion of the econometric model, but rather a demonstration of how to get publication-quality results out of R efficiently. The basic functions of {estout} are modeled on the Stata package estout. Once the R user has a dataset and a regression format in memory, {estout} will All the normal bells and whistles for econometrics are in there: reporting both coefficient estimates and their standard errors, asterisks for alpha=0.10, 0.05, and 0.01 significance levels, R-squared and number of observations. Options to customize are clearly marked in the documentation. 
 "	 0 Comments
Tools for Hacking R: Git + Subversion	https://www.r-bloggers.com/2010/08/tools-for-hacking-r-git-subversion/	August 24, 2010	Matt Shotwell	In an earlier post, I discussed how to use Subversion to download, edit, and generate a patch against R‘s source code. Since most of us can’t commit our code changes back to R‘s repository, we can consider alternatives to store and maintain our patch, until it is eventually incorporated into R. Of course, our changes may never be incorporated. We still ought to have a record of our work! The biggest problem in maintaining a patch, is ensuring compatibility with upstream changes. In other words, once we’ve written a patch, we need to ensure that subsequent changes to R‘s main development branch don’t conflict with our changes. The Git version control software can help us here.  Git is similar in purpose to Subversion; it’s used to track changes to source code. Git has features that make it easy to maintain a patch against a larger project. In contrast with Subversion, a complete Git repository is designed to be stored locally. In addition, Git is often distributed with tools that make it easy to interact with Subversion repositories. This is a blog post, so lets just see an example: We first need to install the Git and Git-Subversion packages. In Debian GNU Linux or Ubuntu, we can use aptitude: We can then use git svn to download and initialize a Git repository from the R Subversion repository: This command tells Git to download the the Subversion repository at http://svn.r-project.org/R/trunk, at revision 52760, and use it to initialize a Git repository locally in directory R-patch. The -r argument here is critical. If the revision is not provided, the entire revision history is downloaded from the Subversion repository (all ~53k revisions)! It’s also important to select a revision that is current, because when the Git repository is updated, all subsequent revisions are downloaded. Now we have a local Git repository in R-patch, we can modify this code and keep track of our changes under the normal Git conventions. Say we want to increase the number of available R connections. We can modify src/main/connections.c such that the resulting diff is: and commit our changes locally with something like: Now that we have a patch against the revision 52760, we need to ensure that subsequent changes in the Subversion trunk don’t conflict with our code. The Git-Subversion software has a special command to deal with this, called rebase. The rebase command ‘unwinds’ our local work, applies the changes from the Subversion trunk, and then ‘replays’ our work on top of those changes. If there are conflicts, Git-Subversion will issue a notification and mark the areas in each file where a conflict occurs. At this point the rebase operation is incomplete, and you must manually resolve the conflicting code. When all conflicts are resolved, the rebase --continue command completes the rebase operation, and our patch maintenance is complete.  To illustrate: Clearly, our local modifications did not result in a conflict, and so we have successfully maintained this trivial patch. In addition, our local commit is now at the top of the Git commit log, just after the latest Subversion commit by Peter Dalgaard, of the R core team: We can generate a new patch file against the latest (Subversion trunk) revision using git diff, and specifying only the revision(s) we had committed locally: where be0b532 is the (partial) Git hash code of the latest Subversion trunk revision, and be0b532.. selects the commits since this revision, i.e our local changes. 	 0 Comments
Webinar: Big Data Analysis with Revolution R	https://www.r-bloggers.com/2010/08/webinar-big-data-analysis-with-revolution-r/	August 24, 2010	David Smith	Don’t forget that I’ll be hosting a webinar tomorrow talking about the new RevoScaleR package included with the forthcoming Revolution R Enterprise 4.0. The webinar will also feature a live demonstration from Joseph Rickert. The full details are below, and you can register for the webinar here. Big Data Analysis for R Using Revolution R EnterpriseDate: Wednesday, Aug 25, 2010Time: 9am – 10am Pacific Time Presenters:David Smith, Revolution AnalyticsJoseph Rickert, Revolution Analytics The R language is well-established as the modern language for predictive analytics. However, given the deluge of data that must be processed and analyzed today, some organizations have been reluctant to deploy R beyond research into production applications. Additionally, R’s in-memory design offers great flexibility, but can be limiting when processing multi-gigabyte or terabyte-class datasets.  Attend this webinar to see how Revolution R Enterprise now extends the reach of R into the realm of ‘Big Data’ data analysis!   Revolution Analytics webinars: Big Data Analysis for R using Revolution R Enterprise 	 0 Comments
Packing everything into a data.frame	https://www.r-bloggers.com/2010/08/packing-everything-into-a%c2%a0data-frame/	August 23, 2010	ellbur	OK, I know I talk about R too much, but I like R, so I’m going to talk about it some more. Common situation: repeat a procedure many times; each time generates some large wadge of awful-structured data, and in the end you’d like to go back and look at it all. OK, sounds reasonably simple, just and you’ve got a list of structs containing that data. It works, but I find it undesirable for a few reasons: So to get a data.frame, we can use the magic of sapply. Like this: I have to admit I don’t actually know why sapply is smart enough to do this, but it turns the whole shebang into a matrix of mode “list”. t() transposes that matrix so the fields A, B… become the columns. as.data.frame() makes the whole thing a data frame. Excellent. Well, there’s a little problem here. I didn’t realize this at first, but a data.frame is just a list() of columns plus some attributes() attached. And those columns are welcome to be of mode “list”, as they will be here. In one way that’s actually really convenient, because you can stick complex stuff inside a data.frame, as in, like anything, even whole other data.frames. But you can’t call mean() or sd() or acf() on a vector of mode “list”. Inconvenient. (By the way, is there any other language in which every object has a type, a mode and a class, all of which mean different things? What is up with that?) So the solution is this “clean” function, to convert, where possible, vectors of mode “list” to numeric or character vectors. Basically, check to see that all the elements are atomic vectors (ie not lists) of length 1; if so, flatten (“unlist”). And lastly, how about automatically grabbing everything you created along the way? Just end each loop with Putting this all together, we have: 	 0 Comments
A Rule Change in Major League Soccer?	https://www.r-bloggers.com/2010/08/a-rule-change-in-major-league-soccer/	August 23, 2010	Ryan	"I have to admit that working with my Major League Soccer data set has been slow going.  There are a few reasons:  (1) I have a full-time job at the National Renewable Energy Lab and (2) the data isn’t quite as “rich” as I initially thought.  As an example, the MLS site doesn’t list the wins and losses for each team by year.  That seems to be a fundamental piece of “sports”-type data, right?  In any case, I did come across something that I can’t seem to answer.  If you know somebody that works with MLS, send ‘em my email address and tell them that I want answers, damnit! So following up on my previous MLS-related post, I wanted to see if I could pinpoint why goals per game has been decreasing in recent years.  My first thought was that with MLS expanding, more US-based players transferring overseas, etc., that the overall talent level in MLS has suffered a bit in the more recent years.  One way that this might manifest itself in the data is by having less shots “on target” or “on goal”.  Therefore, I looked at the number of shots on goal vs the number of shots and also vs the number of goals over the years.  The two figures are given next.   Based on the first figure, one could argue that the shooters are becoming a little less accurate.  That is, the number of shots on target per shot has decreased by about 10% over the course of the league’s lifetime.  Shots on goal per goal seems relatively steady over this same time period.  This might suggest that the league’s strikers are getting slightly worse whereas the quality of the keepers is holding steady.  That, of course, could contribute to the decline of goals per game. I also decided to look at the number of assists per goal.  Why?  Well, my logic is that if there are more assists per goal, then there might be better overall team play.  Conversely, a decrease in this number might be a result of teams having one or more stars (hence, more individual brilliance) and less of the quality, build-up-type goals.  Make sense?  C’mon, I’m trying here!  Anyway, here is the resulting graph.  Whoa, what in the hell happened there?  The data look a bit suspicious.  Specifically, there seems to be a serious change between the 2002 and 2003 seasons.  So I made a similar graph, but I separated by the different time periods.  Here ya go.  What does this mean?  My hypothesis is that there was a fundamental change to the rules in how assists were recorded between the 2002 and 2003 seasons.  Unfortunately, I can’t confirm this.  I’ve searched the web, read the MLS Wikipedia page, read a good amount of the MLS website, and can’t seem to find anything related to a rules change that might result in this sort of phenomenon.  Sooooo, if you have any ideas, send ‘em my way! This will likely be the last MLS-specific post for a while.  Unless I can find some more data, I’m giving up — their data is just not that interesting.  Notice that I didn’t say that this would be my last soccer post.  Hopefully I can scrape some EPL (England’s Premiership) data.  Given that their league has been around for more than 15 years, it should be a bit more interesting than mine. If you’re interested in taking a look at the data and/or code yourself, I’ve created a github repository for your perusal.  Feel free to pass along your comments and/or questions regarding any code — I have thick skin. So what’s next?  I am thinking about comparing my current workflow of (a) scrape with Python and (b) analyze with R to just doing everything in R (e.g., using the xml package).  Hopefully, I can post some time comparisons soon! Addendum:  According to at least one blogger, the recording of “secondary assists” was changed after the 2002 season.  I’m not sure why they record secondary assists in the first place — I guess MLS wanted to appeal to the hockey people in the early years.  Here is the bloggers take on secondary assists:  
  

 "	 0 Comments
GRASS Can Make Pretty Maps	https://www.r-bloggers.com/2010/08/grass-can-make-pretty-maps/	August 23, 2010	dylan	I have posted a couple examples in the past on the topic of high quality map production from GRASS GIS— usually via the Generic Mapping Tools. I am not sure why, but I have previously avoided using the traditional cartographic output module that is bundled with GRASS (ps.map). This is despite the fact that there is now an excellent collection of examples and a very detailed manual page… I now realize that I have been missing out. I needed a map with several “zoomed” insets, on rather short notice, with all of the data derived from work that had previously done in GRASS. Not looking forward to exporting all of the data into a GMT-compatible format, I gave ps.map a try. The following image is a JPG (i.e. degraded) version of the Postscript file generated by ps.map, with no manual intervention (other than page layout). Obviously the labels aren’t readable, but that can be fixed within a drawing program like Inkscape. Overall, the results were much better than I was expecting. I’ll post some notes and the script used to generate the map next time. Example map produced with ps.map read more 	 0 Comments
What’s for lunch? Private browsing.	https://www.r-bloggers.com/2010/08/whats-for-lunch-private-browsing/	August 23, 2010	David Smith	"Over at the Mozilla Metrics blog, Mozillan Hamilton Ulmer uses R and ggplot2 to look at when people (or at least, Firefox users that volunteered to share their usage data) enable private browsing. Turns out it isn’t just “porn mode” after all: the main use turns out to be lunchtime browsing away from the employer’s prying eyes: 
 Follow the link below for more analysis of the habits of private browsers.  Mozilla Metrics: Understanding Private Browsing "	 0 Comments
Tips for the R beginner (a 5 page overview)	https://www.r-bloggers.com/2010/08/tips-for-the-r-beginner-a-5-page-overview/	August 23, 2010	Tal Galili	In this post I publish a PDF document titled “A collection of tips for R in Finance”. It is a basic 5 page introduction to R in finances by Arnaud Amsellem (linked in profile). The article offers tips related to the following points: This article is well articulated, and offers a perspective of someone who is experienced in the field and touches points that I can imagine beginners might otherwise overlook.  I hope publishing it here will be of use to some readers out there. Update: as some readers have noted to me (by e-mail, and by commenting), this document touches very lightly on the topic of “finances” in R.  I therefore decided to update the title from “R in finance – some tips for beginners”, to it’s current form. Lastly: if you (a reader of this blog) feel you have an article (“post”) to contribute, but don’t feel like starting your own blog, feel welcome to contact me, and I’ll be glad to post what you have to say on my blog (and subsequently, also on R bloggers). Here is the article:  Download (PDF, 418.09KB) 	 0 Comments
Taking R to the Limit: Parallelism and Big Data	https://www.r-bloggers.com/2010/08/taking-r-to-the-limit-parallelism-and-big-data/	August 23, 2010	David Smith	In a two-part series at the Los Angeles R User Group[*], Ryan Rosario took a look at the many ways you can take the R language to the limits of high-performance computing.  In Part I (see video at this link; slides and code also available), Ryan focuses on the various methods of parallel computing in R. There’s some great material here on explicit parallelism, especially if you’re looking to get into the nuts and bolts of the Rmpi package. Ryan also gives several examples of using the snow and snowfall packages for fine-grained parallel computing. If you don’t want to think too hard about the details of parallel programming, but just want to use the power of your hardware to speed up “embarrassingly parallel jobs”, Ryan also covers implicit parallelism with the multicore package, and shows how to simplify things even further with foreach[**]. Part I wraps up with a brief look at high-performance computing with GPUs: computations can be very fast, but the tools available still aren’t very user-friendly. If you’re thinking about getting into parallel computing with R, Part I of Ryan’s talk gives a great overview of the possibilities available. It also includes some advice about when not to try parallel computing: “Each iteration should execute computationally-intensive work. Scheduling tasks has overhead, and can exceed the time to complete the work itself for small jobs.”  This sage advice is worth taking to heart. My personal (but unscientific) rule of thumb is that it’s worth trying parallelism only when each iteration takes longer than the time it takes to get up and pour a cup of coffee. (Then again, the coffee pot is less than 5m from my desk.) In Part II (video coming soon, slides and code available now), Ryan looks at the various tools available to break the constraints of R storing all data in memory, and perform analysis of very large data sets from within the R environment. Much of the presentation is focused on the bigmemory and ff packages, which use different techniques to store data on disk instead of in memory. In the former case, there’s an interesting example of combining both foreach and bigmemory to speed up processing of the airline delay data set, along with an example of doing linear regression on the data. (Revolution’s Joseph Rickert does a similar analysis using the forthcoming RevoScaleR package in this white paper, where the computation is automatically parallelized and runs somewhat faster. I’ll be talking more about RevoScaleR and showing a demonstration of this analysis in a webinar on Wednesday.) Ryan compares ff and bigmemory and finds that performance-wise they’re much the same, but does note an interesting aspect of ff: if you need to create extremely long vectors, it can help. The goal of ff is to get rid of the following message: > x rep(0, 2^31 – 1)Error: cannot allocate vector of length 2147483647 If you’ve been thinking about getting into MapReduce and/or Hadoop, Ryan has some great introductory materials beginning at slide 49. He gives several examples of using parallel programming tools to speed up map/reduce processing with the mapReduce package, and if you want to play with Hadoop but don’t program in Java, Ryan also shows how to use the HadoopStreaming package to drive Hadoop directly from R. If you want more power in controlling Hadoop, Ryan also touches briefly on the Rhipe package.  Thanks go to Ryan for making these useful materials available! Byte Mining: Taking R to the Limit, Part I & Part II [*] Revolution Analytics is a proud sponsor of the Los Angeles Area R User Group. [**] foreach is an open-source package developed by Revolution Analytics. 	 0 Comments
Leveraging the Wisdom of Crowds for Fantasy Football	https://www.r-bloggers.com/2010/08/leveraging-the-wisdom-of-crowds-for-fantasy-football/	August 23, 2010	Drew Conway	WARNING: This has nothing to do with national security, but is nonetheless awesome. This evening I will be participating in that great annual tradition which marks the transition from Summer to Fall: the fantasy football draft. A large part of having a successful fantasy football draft is being able to adjudicate the value of a player more accurately than your opponent.  This year, I decided to take the “wisdom of the crowds” approach to evaluating players by analyzing large amounts of mock draft data from FantasyFootbalCalculator.com.  This website is a great repository of draft data, which may be viewed as a proxy for on how the “market” of fantasy managers value different players in aggregate. Using a short R script I collected a large sample of mock drafts with the same number of managers as my league, then crunched some very simple statistics on the data to assist me in my own draft strategy.  Specifically, I collected the most recent 2,000 completed mock drafts with the same number of managers as my league, and computed the following statistics: While very simple, these statistics can provide interesting insight into how different players are being judged.  For example, the top ten players are very stable over time, however, there are slight changes in the variance of draft position for these players that are telling.  Consider the results for Maurice Jones-Drew and Ray Rice in my most recent data pull.  These players are consistently ranked third and fourth in overall draft position, but the standard deviation for Jones-Drew in aggregate is slightly higher.  These differences in variance can be interpreted as the market having slightly less confidence that Jones-Drew’s true value is the third overall pick than it does Rice is the fourth. Another interesting insight that can be drawn from this data is identifying the players that the market has the most difficultly evaluating.  Because the distributions in these data are rarely single-peaked, I use the median absolute deviation (MAD) of draft position to identify the players the market has most inconsistently ranked.  Below, I have plotted players’ median draft position against their MAD, and labeled those players whose MAD is in the 95th percentile.  This chart is interesting for several reasons.  First, the smoothed fit line shows a gradual decrease in the confidence of a player’s market valuation (depicted by the upward slope) the later a player is drafted, which peaks around the 110th player drafted and then confidence goes back up in later rounds.  This makes sense, as players drafted early are those the market has a more confidence valuation for, then in later rounds that confidence goes down until the final rounds where late draft picks are more consistent.  Also, as of this morning it appears Braylon Edwards is the player most difficult for managers to evaluate; perhaps this is evidence of a “Hard Knocks Effect.” There is much more that could be done with the data, and I won’t give away all my secrets here—I have to some something for comparative advantage.  As an added bonus, however, the R code also pulls individual player performance data from AdvancedNFLStats.com and outputs them as CSV files.  Now, get out there and own your league with data! 	 0 Comments
TripleR round-up	https://www.r-bloggers.com/2010/08/tripler-round-up/	August 23, 2010	felixschoenbrodt	"GSoC 2010 is over – here’s the harvest from my project: TripleR 0.4.3 is the current stable version – and it is a major milestone in its development. Now for the first time social relations round robin designs can be analyzed in R. All results have been cross-checked with TripleR’s DOS predecessor SOREMO.exe, and all results are identical. Currently, we’re working on simulations about missing values in RR-designs. The results look very promising, nonetheless, the active development of TripleR will continue after GSoC. During the project I changed the repository from R-Forge to RForge – Simon (who runs the latter) is very helpful and responsive. I really appreciated that. Bug fixing and a proper documentation took much more time than I’ve planned – maybe that’s a hint for future GSoC students. These steps (in particular the documentation) really take their time. Overall, GSoC 2010 was a good experience for me. I was happy to have the funding to complete this project. Finally, I want to thank my mentor, Stefan, and Dirk for organizing everything! Felix
________
Project: TripleR – Social Relations Analyses in R
Student: Felix Schönbrodt, LMU Munich, Germany
Mentor: Stefan Schmukle, University of Münster "	 0 Comments
Tools for Hacking R: Subversion	https://www.r-bloggers.com/2010/08/tools-for-hacking-r-subversion/	August 23, 2010	Matt Shotwell	The development version of R is stored in a Subversion repository at the URL http://svn.r-project.org/R/trunk/. In fact, you can browse the source code by clicking the link. Subversion is software for source code revision control. That means it keeps track of changes, who made them, when they were made, and any comments about the change. By convention, the source code is usually kept in three sub-directories of the main repository, named trunk, branches, and tags. The trunk directory contains the main development tree (currently R 2.12.0). The branches directory contains several additional copies of the source tree, and these copies are used for experimental purposes. That is, if someone wants to try out a radical idea that might break something, he creates a new branch, a copy of trunk, and tries his ideas out there. If the ideas are successful, then the changes may be merged back into trunk. Otherwise, the branch may be deleted, or simply fall into dereliction. When the R core team decides it’s time to release a new version of R, a new tree is created in the tags directory, corresponding to the release number. This directory is filled with a snapshot of the current source tree in trunk and ought not to be modified further. R‘s source code may be ‘checked out’ from the repository using a Subversion client. In Debian or Ubuntu Linux, we can install Subversion with aptitude: We may then use the Subversion command checkout to download a copy of the trunk repository: This command will download the trunk tree of the source code to the local directory R-devel. The local copy of the R source tree may then be edited freely. Suppose we want to make a small change in a source file, say R-devel/src/main/connections.c, that increases the limit on the number of concurrent R connections. That is, at line 63, I change to read Hence, the my local copy of the R source will now allow up to 256 connections, rather than 128. If changes made to R‘s source are good, they ought to be shared with the R community. The traditional mechanism for sharing code changes is with a patch. The Subversion software provides a mechanism to easily generate a patch of the changes to a Subversion repository. Continuing with our example, we can issue the Subversion diff command: The output from this command is in a ‘diff’ format. That is, lines that were removed are marked with the ‘-‘ symbol, and lines that were added are marked with the ‘+‘ symbol. The output of this command may be redirected to a file The file myedits.patch is a patch for the changes we made. When talking about patches, it’s also prudent to describe the origin of the changed code. In this case, we say the myedits.patch is a patch against the development version of R, at revision 52769. We can share our changes via the R mailing lists by attaching the patch, or by copy-and-pasting the patch into the email text.  	 0 Comments
Abstract word clouds using R	https://www.r-bloggers.com/2010/08/abstract-word-clouds-using-r/	August 23, 2010	nsaunders	"A recent question over at BioStar asked whether abstracts returned from a PubMed search could easily be visualised as “word clouds”, using Wordle. This got me thinking about ways to solve the problem using R.  Here’s my first attempt, which demonstrates some functions from the RCurl and XML packages. update: corrected a couple of copy/paste errors in the code

First, install a couple of packages:  snippets, which provides the cloud() function for plotting a word cloud and tm, a text-mining library: Next, the code to search PubMed, fetch abstracts and generate a list of words:  Word cloud for abstracts To retrieve the abstracts:  define an EUtils Efetch URL, fetch the XML and parse as before (lines 14-17).  This time, the NodeSet object, abstracts, contains the AbstractText tags and their contents.  We can run sapply on each abstract to pull out the text between the tags (line 20).  Finally, we split each abstract into words by looking for spaces (” “), put all of the words in one big list and convert them all to lower-case, using the “one-liner” on line 20.  Conversion to lower-case ensures that words are not counted twice (e.g. “The” and “the”). That’s a good start, but there is still some work to do.  For a start, many of the words are not strictly words, because they include punctuation symbols.  We can get rid of the symbols using grep: We’re probably not interested in “words” composed solely of numerals: We’re definitely not interested in commonly-used words such as:  “a, and, the, we, that, which, was, those…” and so on.  These are referred to as stopwords – and this is where the tm package is useful.  It provides a list of stopwords, to which we can compare our word list and remove matches: OK – we are just about ready to plot the word cloud.  Count them up using table(), remove those that occur only once and plot: Result:  see the graphic, above-right (click on it for the full-size version). It’s a start, if not quite so attractive as a Wordle.  The tm package looks worthy of further investigation; it contains many more functions than the simple use of stopwords() illustrated here. "	 0 Comments
A small and lonely sea urchin…	https://www.r-bloggers.com/2010/08/a-small-and-lonely-sea-urchin%e2%80%a6/	August 22, 2010	Timothée	A few weeks ago, a paper on which I am a co-author was accepted for publication in the french ecological journal Life & Environment. In this paper, we evaluate the consequences of recreative harvesting on three populations of sea urchins in the Golfe du Lion, the northwestern part of the Mediterranean sea (at the border between France and Spain). In a nutshell, we conclude that at the most important anthropogenic pressure, populations of urchins (we focused on two edible species, Arbaxia lixula and Paracentrorus lividus) decrease in abundance, and the mean size of individuals decrease as well. This result bears important ecological consequences. Below are the boxplot of density for P. lividus at three sites, subjected to an increasing gradient of harvesting.  First, a little context about the paper. Two years ago, a small group of researchers from the south of France, backed up by an association of biodiversity watchers, started a program to evaluate biodiversity in the Mediterranean sea, and to track changes that can be introduced by an ever increasing anthropogenic pressure. This program [1] relies heavily on the participation of eco-volunteers, that can help measuring the diversity of different populations through non-invasive, non-destructive methods (a part of our paper is dedicated to the benchmarking of such a method with regard to previous results). On with the implications of our results. As you may not know, urchins have quite an important part in the coastal ecosystem of shallow areas, in that they feed on different type of algae, and are fed upon by fishes. Additionally, the two different species we studied exploit different algal types, that can act to maintain diversity both for the algae and their consumers [2]. The main point of the paper is that an increase of harvesting at the more exploited site may ultimately lead to a loss of stock. Fishermen tend to pick the larger individuals, which are also the sexually mature one. Clearly, over-exploiting the sites can lead to a loss of reproductive individuals, triggering inbreeding depression [3]. All hope is not lost, though, as populations can be maintained if dispersing larvae can recolonize the exploited site at least as far as the local population is decimated. Should the urchins become locally extinct, that can lead to a switch in the algal communities, that will in turn impact the availability of food for other grazing organisms. The next part is more relevant to people analyzing data, and is not focused on the ecological consequences of what we found. Part of my implication on this project was to design the statistical analysis (I was not the lucky one that got to dive to count the urchins…). One of the problems we encountered is that we needed a permutational two-way ANOVA with random effects [4]. This particular test requires a balanced design, i.e. there might be the same count of individual in each possible combination of factors. Due to a limited time to carry the whole sampling (that was a lot of work, given that only one of the authors, Anne, was actually diving), one of our sample comprised only 28 individuals (of a species at a site). We had to work a little magic to apply the test. The obvious solution was to randomly subsample the samples of a greater size, to reduce them to 28 individuals. In order to make sure that the subsamples were comparable to the original population, we checked that their mean and variance were not significantly different. Using R, it was really easy, and I might even post the code (assuming that I find some time to clean it…). This is a good example of what R can do for you: I did not see myself doing this procedure in Statistica or JMP (I’m not even sure this is possible at all). 	 0 Comments
Newcomb, Benford, and their Dirty, Dirty Logarithms	https://www.r-bloggers.com/2010/08/newcomb-benford-and-their-dirty-dirty-logarithms/	August 22, 2010	Ethan Brown	Tom Taverner introduced me to Benford’s Law as we were eating lunch together at a statistical computing conference: If you look at the first digits of data in many naturally-occuring datasets, a startling 30 percent of them are ones. “Pah!” I said. “That belies intuition! Why would one digit occur any more than another? I’d expect each digit to occur with about equal frequency–1/9 of the time. Why isn’t the probability 11 percent? Eh?” A hip, bespeckled biostatistician nearby joined the fray with the help of his iPhone. He was also skeptical, but he looked at the distribution of leading digits in a fancy gene database he was analyzing, and indeed 1 occurred about 30 percent of the time, with 2, 3, 4, and so on occuring with decreasing frequency. You prove me wrong so good, statistics! Benford’s Law, generally, states that the probability of the first digit d in base b is:  This turns out to give a 30 percent chance for starting with 1, 18 percent for starting with 2, and so on. It has even has wide applicability outside of entertaining lunch conversations–including fraud detection and computer disk space allocation. Several clever folks in the R world have recently used Benford to assess whether data is actually naturally occuring: Drew Conway decided there was not strong evidence of numerical tampering with the Wikileaks Afganistan War Logs, and Diego Valle discussed problems in homocide-reporting by the Mexican government. Rattle, a graphical interface for R, has a function to overlay plots of leading digits in base 10 of different subsets of the data to evaluate where funny business may be occuring in a dataset; also, as I found out from his  comment below, Kevin Wright has posted some R distribution and plot functions for Benford’s law on the R wiki. Does Benford’s law seem unintuitive? Well maybe it’s because kids these days are just too darn LAZY to look at a good book of logarithms like we did in the good old days! (They’re also too lazy to walk to school barefoot over barbed wire uphill both ways in the snow.) Logarithmic tables are where you look up the first several digits of a number to see what the logarithm of that number is–then you can simply add to the log of the number to represent greater powers of ten, or add the logs of two numbers to get the product; then you can look up that result in a table that will convert back to familiar numbers. It’s a lot easier to add numbers than multiply them by hand, so this was a huge time-saver. Not only does using logarithm books build character, but, in the words 19th-century science fiction author and astronomer Simon Newcomb, you can discover important scientific laws: “That the ten digits do not occure with equal frequency must must be evident to any one making much use of logarithmic tables, and noticing how much faster the first pages wear out than the last ones.” In other words, people have simply been looking up more numbers that begin with the smaller digits, so those pages get dirty faster. Now you know what you’ve been missing, you technology-dependent slacker! Newcomb continues his “Note on the Frequency of Use of the Different Digits in Natural Numbers” to precisely state the law, which was later independently discovered, popularized, and demonstrated in the 30s on all sorts of different kinds of data by a bright Schenectadian, Frank Benford. As an illustration of the similar paths that human minds can follow, Benford also was inspired by logarithmic books, citing in his “The Law of Anomalous Numbers” how “the logarithms of the low numbers 1 and 2 are apt to be more stained and frayed by use than those of the higher numbers 8 and 9.” So next time you spill coffee on or drool all over a library book, don’t feel guilty–your detritus may just inspire scientists of future generations. Coming up next: My functions for Benford analysis in R, and using them to look at baby names, MA property values, dinosaur bone lengths, traffic data, and Congressional lobbyists! 	 0 Comments
Traffic prediction contest closing soon	https://www.r-bloggers.com/2010/08/traffic-prediction-contest-closing-soon/	August 22, 2010	David Smith	A quick reminder that the IEEE traffic-prediction competition closes soon. If you’re thinking of entering you’ll need to get the description of your R-based solution in by September 13. IEEE ICDM Contest: Road Traffic Prediction for Intelligent GPS Navigation  	 0 Comments
Global Temperature Proxy Reconstructions ~ Bayesian extrapolation of warming w/ rjags	https://www.r-bloggers.com/2010/08/global-temperature-proxy-reconstructions-bayesian-extrapolation-of-warming-w-rjags/	August 22, 2010	apeescape	Update: fixed projection. There are a bunch of “hockey sticks” that calculate past global temps. through the use of proxies when instrumental data is absent. There is a new one out there by McShane and Wyner (2010) that’s creating quite a stir in the blogosphere (here, here, here, here). The main take out being, that the uncertainty is too great for the proxies to be any good. I thought it would be a fun exercise to recreate their analysis (code and data @ McShane’s website for now; @ Annals of Applied Statistics in the future). Broadly speaking, the paper has two parts: constructing PCAs to come up with the predictors, and the Bayesian analysis to construct uncertainty bounds). I need to refreshen my Bayesian instincts anyways. I might do the PCAs, but because I lack knowledge in this area, it’ll probably take a little more time. The general model is as follows (assuming all the PCAs are done):  The following are the uninformative priors:  This is basically an AR(2) model with principal components as covariates. Thanks to the Bayes Theorem, we can put uncertainty bounds around the parameters in addition to the uncertainty around the data (residuals). Now, to estimate this, I decided to use JAGS. This is basically the command-line version of WinBUGS, a program that uses the Gibbs sampler to generate values from the posterior distribution. I was thinking of using PyMC (a Python package), but because I had so much problems with it, I resorted to rjags (WinBUGS not an option!). For a casual intro, see Simon Jackson’s or John Myles White’s blog. What’s nice about rjags (as opposed to using JAGS itself) is the R interface. The only thing outside of R that you need to do, is to specify the model (and save it as a .bug file). The data, parameter, initial value specification, grabbing the results, inference, all that could be done through the R interpreter. Download the M&W2010 data and code, and you can find the .ocd file (WinBUGS). This file has the model designation, so extract that, and put it into a .bug file: Everything else, we put in the R code (click below to expand). Just be cognizant of where to put the files, and the above code plots the results (also get rid of the superfluous last two lines in the original data). I first plot  the Bayesian estimates for the whole dataset. Uncertainty bounds are for all of the parameters (which means it’s not a confidence interval, but a prediction interval, in frequentist language). The model fits pretty well in general, and I got good estimates without too many samples. M&W2010 also did a test, where they held a 30 year block away from the data, in order to project the temperature time series given the model and the limited data. This process wasn’t described well in the paper, but I am presuming they just did a nonparametric bootstrap off the posterior distributions (Update: the way I calculate the projection is flawed, hopefully I’ll have a post on this later). Update2: I was bootstrapping over the actual data of the most recent 30 year period. This was obviously wrong, and I’ve replaced it with the projected values. New graph is reflected in the code, just search for part deux. Interestingly enough, the projection looks even crappier in terms of predicting recent warming. M&W2010 interprets this as crappy proxies, climatologists interpret this as a regime change in the atmospheric dynamics (old graph here). Update3: oops, didn’t consider for the backcast (old graph here). See part three in code. Update4: last update, did the projection over the whole interval (old graph here). See part four in code. The following is the result:  I think this is pretty much identical to Fig. 18b in the paper. One thing to notice is that I smoothed over the estimates and the 95% intervals. M&W2010 weirdly draws over all instances of the simulations (notice the wide grey intervals). Since all the realizations of the model are overlapping each other, the graph effectively shows the maximum and minimum of the 95% interval. Where my interval will probably be interpreted as about +/- 0.4 on each side, M&W2010′s paper could be interpreted as +/- 0.5 degrees. This is a large difference especially for a paper that espouses large uncertainties in the proxy data. OTOH, Mann et al. (2008) I think produces mean uncertainty bounds which might be confusing depending on one’s perspective (or I guess I should say it’s confusing if you’re going to compare the two!). Apparently there is another paper (w/ code and data) coming out soon that uses Bayesian hierarchical models that I might recreate as well (Bo Li1, Douglas W. Nychka2 and Caspar M. Ammann. 2010. The Value of Multi-proxy Reconstruction of Past Climate). More fun to chew on. Refs: 	 0 Comments
Dump R datasets into a single file	https://www.r-bloggers.com/2010/08/dump-r-datasets-into-a-single-file/	August 21, 2010	danganothererror	Should you need datasets that come with R and additional packages (you can access them via data()) in one single file, here’s what I did to dump the entire workspace into one file: This code can easily be adapted to dump individual dataset into its own file. 	 0 Comments
Using R for Introductory Statistics, Chapter 3.4	https://www.r-bloggers.com/2010/08/using-r-for-introductory-statistics-chapter-3-4/	August 21, 2010	Christopher Bare	"…a continuing journey through Using R for Introductory Statistics, by John Verzani. Linear regression is a kooky term for fitting a line to some data. This odd bit of terminology can be blamed on Sir Francis Galton, a prolific victorian scientist and traveler who saw it as related to his concept of regression toward the mean. Calling it a linear model is a little more straight-forward, and linear modeling through the lm function is bread-and-butter to R. For example, let’s look at the data set diamonds to see if there’s a linear relationship between weight and cost of diamonds.  We start by creating the formula f using the strange looking tilde operator. That tells the R interpreter that we’re defining a symbolic formula, rather than an expression to be evaluated immediately. So, our definition of formula f says, “price is a function of carat”. In the plot statement, the formula is evaluated in the context given by data=diamond, so that the variables in our formula have values. That gives us the scatter plot. Now let’s fit a line using lm, context again given by data=diamond, and render the resulting object as a line using abline. Looks spiffy, but what just happened? The equation of a line that we learned in high school is:  Minimizing squared error over our sample gives us estimates of the slope and intercept. The book presents this without derivation, which is a shame. 

 Maybe later, I’ll get brave an try to insert a derivation here. There’s a popular linear model that applies to dating, which goes like this: It’s OK for a man to date a younger woman if her age is at least half the man’s age plus seven. In other words, this:  Apparently, I should be dating a 27 year old. Let me go ask my wife if that’s OK. In the meantime, let’s see how our rule compares to results of a survey asking the proper cutoff for dating for various ages. 
 That's a nice correspondence. On second thought, this is statistical proof that my daughter is not allowed to leave the house 'til she's 30. Somehow related to that is the data set Animals, comparing weights of body and brain for several animals. The basic scatterplot not revealing much, we put the data on a log scale and find that it looks much better. As near as I can tell, the I or AsIs function does something like the opposite of the tilde operator. It tells the interpreter to go ahead and evaluate the enclosed expression. The general gist is to transform our data to log scale then apply linear modeling.  Now the problem is, the line doesn't seem to fit very well. Those three outliers on the right edge have high body weights but less than expected going on upstairs. That seems to unduly influence the linear model away from the main trend. R contains some alternative algorithms for fitting a line to data. The function lqs is more resistant to outliers, like the large but pea-brained creatures in this example.  That's better. Finally, you might use identify to solve the mystery of the knuckleheaded beasts. Problem 3.31 is about replicate measurements, which might be a good idea where measurement error, noisy data, or other random variation is present. We follow the by now familiar procedure of defining our formula, doing a scatterplot, building our linear model, and finally plotting it over the scatterplot.  We are then asked to look at the variance of measurements at each particular voltage. To do that, we'll first split our data.frame up by voltage. The result is a list of vectors, one per voltage level. Next, let's compute the variance for each component of the above list and build a data.frame out of it. This split-apply-combine pattern looks familiar. It's basically a SQL group by in R. It's also the basis for Hadley Wickham's plyr library. Plyr's ddply function takes breakdown, a data.frame, and splits it on values of the voltage column. For each part, it computes the variance in the time column, then assembles the results back into a data.frame. While that's not directly related to linear modeling, this kind of exploratory data manipulation is what R is made for. More fun Previous episode of Using R for Introductory Statistics "	 0 Comments
Map of Upcoming Ruby Conferences	https://www.r-bloggers.com/2010/08/map-of-upcoming-ruby-conferences/	August 21, 2010	C		 0 Comments
Managing Market Studies in R	https://www.r-bloggers.com/2010/08/managing-market-studies-in-r/	August 21, 2010	Milk Trader		 0 Comments
swing graph	https://www.r-bloggers.com/2010/08/swing-graph/	August 21, 2010	jackman	I’m updating a swing dotplot PDF every 10 minutes as the count progresses (and the cool part is that the updates continue even as I’m flying Heathrow to SFO). 	 0 Comments
Weekend art in R (Part 3)	https://www.r-bloggers.com/2010/08/weekend-art-in-r-part-3/	August 21, 2010	Matt Asher	 I have a few posts nearing completion, but meanwhile a weekend break for art. Big thanks to Simon Urbanek and Jeffrey Horner, creators of Cairo, a library for the programming language R. Have you noticed how R can’t anti-alias (fancy way for saying smooth out lines and curves when creating a bit-mapped image)? Cairo can. Make sure to click the image above for the full version. Here’s my code: 	 0 Comments
Using JAGS in R with the rjags Package	https://www.r-bloggers.com/2010/08/using-jags-in-r-with-the-rjags-package/	August 20, 2010	John Myles White	I’m going to assume that you have access to a machine that will run JAGS. If you don’t, then you should be able to use WinBUGS, which is very easy to get set up. Unfortunately, the details of what follows may not help you as much if you’re using WinBUGS. To set up your system for using JAGS, there are two very easy steps: Once you’ve done that, a simple call to library('rjags') will be enough to run JAGS from inside of R. You’ll want to do everything except model specification in R. You’ll specify the model in a separate file using BUGS/JAGS syntax. Let’s assume that you’ve got a bunch of data points from a normal distribution with unknown mean and variance. This is arguably the simplest data set you can analyze with JAGS. So, how do you perform the analysis? First, let’s create some simulation data that we’ll use to test our JAGS model specification: We don’t actually need to write out the data since ‘rjags’ automatically does this for us (and in another format at that), but it’s nice to be able to check that JAGS has done something reasonable by analyzing the raw inputs post hoc. With your simulated data in hand, we’ll write up a model specification in JAGS syntax. Put the model specification in a file called example1.bug. The complete model looks like this: In every model specification file, you have to start out by telling JAGS that you’re specifying a model. Then you set up the model for every single data point using a for loop. Here, we say that x[i] is distributed normally (hence the dnorm() call) with mean mu and precision tau, where the precision is simply the reciprocal of the variance. Then we specify our priors for mu and tau, which are meant to be constant across the loop. We tell JAGS that mu is distributed normally with mean 0 and standard deviation 100. This is meant to serve as a non-informative prior, since our data set was designed to have all measurements substantially below 100. Then we specify tau in a slightly round-about way. We say that tau is a deterministic function (hence the deterministic <- instead of the distributional ~) of sigma, after raising sigma to the -2 power. Then we say that sigma has a uniform prior over the interval [0,100]. With this model specified in example1.bug, we can write more R code to invoke it and perform inference properly: Obviously, we have to import the 'rjags' package. Then we need to set up our model object in R, which we do using the jags.model() function. We specify the JAGS model specification file and the data set, which is a named list where the names must be those used in the JAGS model specification file. Finally, we tell the system how many parallel chains to run. (If you don't understand what the chains represent, I'd suggest just playing around and then reading up about the issue of mixing in MCMC.) Finally, we tell the system how many samples should be thrown away as part of the adaptive sampling period for each chain. For this example, I suspect that we could safely set this parameter to 0, but it costs so little that I've used 100 just as a placeholder. After calling jags.model(), we receive a JAGS model object, which we store in the jags variable. After all of that set up, I've chosen to have the system run another 1000 iterations of the sampler just to show how to use the update() function, even though it's completely unnecessary in this simple problem. Finally, we use jags.sample() to draw 1000 samples from the sampler for the values of the named variables mu and tau. When you call jags.sample(), you'll see the output provides proposed values for mu and tau. These should be close to 0 and 0.04 if JAGS is working properly, since those were the mean and precision values we used to create our simulation data. (At the risk of being pedantic: we used a standard deviation of 5, which gives a variance of 25 and a precision of 1 / 25 = 0.04.) Of course, they'll be even closer to the sample mean mean(x) and the sample precision 1 / var(x), so you should not forget to compare the inferred values to these values. The sample size, 1,000, isn't large enough to guarantee that the mean will be all that close to 0. Moving on to a slightly more interesting example, we can perform a simple linear regression in JAGS very easily. As before, we set up simulation data from a theoretical linear model: We then set up the Bayesian model for our regression in example2.bug: Here, we've said that every data point is drawn from a normal distribution with mean a + b * x[i] and precision tau. We assign non-informative normal priors to a and b and a non-informative uniform prior to the standard deviation sigma, which is deterministically transformed into tau. Then, we run this model using the same exact approach as we used earlier: After running the chain for a good number of samples, we draw inferences for a and b, which should be close to the proper values of 0 and 1. I've ignored tau here, though there's no reason not to check that it was properly inferred. Finally, it's good to see a model that's harder to implement without a good deal of knowledge of optimization tools unless you use a sampling technique like the one JAGS automates. For that purpose, I'll show how to implement logistic regression. Here we set up a simple one-dimensional predictor for our binary outcome variable and assume the standard logistic model: Then we set up our Bayesian model in example3.bug, where y[i] is Bernoulli distributed (or binomial distributed with 1 draw, if you prefer that sort of thing) and the linear model coefficients a and b are given non-informative normal priors: Finally, we perform our standard inference calls in R to run the model through JAGS and extract predicted values for a and b. As always, you should check that the outputs you get make sense: here, you expect a to be approximately -5 and b to be around 0.01. This inference problem should take a good bit longer to solve: there are other tools for handling logistic regressions in JAGS that are faster, but I find this approach conceptually simplest and best for highlighting the similarity to a standard linear regression. Here are a few errors that stumped me for a bit as I got started using JAGS today: My first attempt to run a linear regression didn't work. I still don't entirely understand why, but here is the alternative code that failed if you ever make the same type of mistake and find yourself puzzled: My assumption is that it's more difficult to infer values for all the epsilon's at the same time as tau, which makes this harder than the earlier call without any explicit epsilon values. If that's wrong, please do correct me. Another hypothesis I entertained is that it's a problem to ever set y[i] to be a deterministic node, though this doesn't seem really plausible to me. 	 0 Comments
Automatic Differentiation in R	https://www.r-bloggers.com/2010/08/automatic-differentiation-in-r/	August 20, 2010	chillu	"project outcomes
—————-
radx: forward automatic differentiation in R
tada: templated automatic differentiation in C++ development summary
——————-
During the summer of 2010, under the Google Summer of Code program,
I was assigned the project of implementing an engine for Automatic
Differentiation in R. The implementation involved building a fully
functional system for computing numerical derivatives (specifically
of the first and second degrees — jacobian and hessian matrices)
in an efficient manner. Numerical derivatives are extremely useful in
optimization. For example, Newton’s method for unconstrained optimization
requires both the hessian and the gradient of the objective function
and an efficient method to solve the resulting linear system Hx = -g,
usually a preconditioned conjugate gradient method. Before the mid-term deadline, I had completed the most of the features
that are part of radx as of now. This includes univariate and multivariate
differentiation of vector functions. Computation of first and higher
order pure and partial derivatives are carried out through univariate
Taylor propagation and exact interpolation. Please see documentation
for references and technical material. Also are included a handful of
demos that illustrate what radx can do. Functionally radx has fulfilled
most of the goals that it set out to achieve, except for the support
for reverse mode automatic differentiation. Post mid-term, according to the timeline I had set for myself, I
was to build the reverse mode differentiation in radx. While easy on
paper this involved, however, the building of a computational tracer
within R. As I had no previous experience with building such tracers,
I chose to instead use my time to build a more powerful backend for
the automatic differentiation engine using C++ for performance and
flexibility. Technically, while this counts as missing functionality,
it is not as serious as one might think as derivatives can still
be computed using the existing forward mode engine within radx. The
difficulty arises only when we require the computation of gradients
for functions that have lot of independent variables. Unfortunately,
however, most real world problems involve such kind of functions and
computing their sensitivities to the independent variables accurately
and quickly is important. Although this hasn’t been done thus far, it
is expected that tada will be fully interfaced with radx so that radx
will use the computational kernels inside of tada to compute derivatives. tada is now a fast and extensible AD engine written in C++ that can
compute derivatives using a great many scalar types. While building
a fast and correct system tada has been built with an emphasis on
loose coupling between the derivative algorithms and the actual data
types that it uses to compute the derivatives. This has enabled the
computation using a great many scalar types like arbitrary precision
floats, complex numbers and even rational numbers for exact results
(provided operations involve only non-transcendental functions). Support
for interval arithmetic is an important missing feature and it’s inclusion
is on the cards. Correctness of every propagation step in the univariate
Taylor propagation algorithm has been checked by cross checking results
with the symbolic differentiation program, SymPy in unit tests written
with CPPUnit. tada is expected to be actively developed even past the end of the GSoC
program to continue adding features, building on its strong support for
varied data types. The development summary and future for tada can be
seen in the TODO file in the included folder. people
——
mentor: Prof. John Nash
student: Chidambaram Annamalai follow
——
You can follow development of the projects here:
radx: http://github.com/quantumelixir/radx
tada: http://github.com/quantumelixir/tada "	 0 Comments
Taking R to the Limit, Part II – Large Datasets in R	https://www.r-bloggers.com/2010/08/taking-r-to-the-limit-part-ii-large-datasets-in-r/	August 20, 2010	Ryan Rosario	 For Part I, Parallelism in R, click here. Tuesday night I again had the opportunity to present on high performance computing in R, at the Los Angeles R Users’ Group. This was the second part of a two part series called “Taking R to the Limit: High Performance Computing in R.” Part II discussed ways to work with large datasets in R. I also tied in MapReduce into the talk. Unfortunately, there was too much material and I had originally planned to cover Rhipe, using R on EC2 and sparse matrix libraries. Slides My edited slides are posted on SlideShare, and available for download here.    Topics included: Code The corresponding demonstration code is here. Data Since this talk discussed large datasets, I used some, well, large datasets. Some demonstrations used toy data including trees and the famous iris dataset included in base R. To load these, just use the call library(iris) or library(trees). Large datasets: Video  The video was created with Vara ScreenFlow and I am very happy with how easy it is to use and how painless editing was. For Part I, Parallelism in R, click here. 	 0 Comments
How extreme is the Russian heatwave?	https://www.r-bloggers.com/2010/08/how-extreme-is-the-russian-heatwave/	August 20, 2010	David Smith	"The devastating heatwave in Russia now seems to be over, but not before killing thousands, causing extensive wildfires, and destroying crops. But how severe was this heatwave, compared to past summers? Physicist and climate scientist Joe Wheatley looks at the record of temperature and rainfall in Russia over the last 60 years and places the last 3 months in context: 
 The last two months are clear outliers in both temperature and rainfall. Joe created this chart with the smoothScatter function in R. It’s a function I haven’t seen used very much, but it gives a nice representation of the envelope of climate extremes for this chart. Biospherica: Extreme conditions in Russian croplands  "	 0 Comments
Phylogenetic trees online	https://www.r-bloggers.com/2010/08/phylogenetic-trees-online/	August 20, 2010	Samuel Brown		 0 Comments
Speeding up parentheses (and lots more) in R	https://www.r-bloggers.com/2010/08/speeding-up-parentheses-and-lots-more-in-r/	August 19, 2010	Radford Neal	As I noted here, enclosing sub-expressions in parentheses is slower in R than enclosing them in curly brackets.  I now know why, and I’ve modified R to reduce (but not eliminate) the slowness of parentheses.  The modification speeds up many other operations in R as well, for an average speedup of something like 5% for programs that aren’t dominated by large built-in operations like matrix multiplies. I looked at the source code for the latest version of R, 2.11.1, and figured out what’s going on.  The difference between parentheses and curly brackets comes about because R treats curly brackets as a “special” operator, whose arguments are not automatically evaluated, but it treats parentheses as a “built in” operator, whose arguments (just one for parentheses) are evaluated automatically, with the results of this evaluation stored in a LISP-style list.  Creating this list requires allocation of memory and other operations which seem to be slow enough to cause the difference, since the curly bracket operator just evaluates the expressions inside and returns the last of them, without creating such a list.  Furthermore, the implementation allocates one more “CONS” cell than is necessary when creating this list, apparently because the programmer found this slightly easier than writing code to avoid creating the extra cell. Since this is time critical code, not just for parentheses but for most operators, allocating a cell unnecessarily is a bad idea.  I modified the program to avoid this, changing the evalList and evalListKeepMissing procedures in the eval.c source file in the src/main directory.  Here are the old and new versions of these procedures (probably of interest only to R hackers). Here are some timing results.  First with the R 2.11.1 unmodified, on an Intel Linux system: Parentheses are 8.7% slower than curly brackets. Now here are the results with my modified version: Now parentheses are only 2.2% slower than curly brackets.  Furthermore, the curly bracket version is 2.7% faster than before, and the parenthesis version is 8.5% faster. Here’s a test on a program that implements EM for a simple model.  First, with the unmodified version of R (running far more iterations of EM than really needed): And now my modified version of R: My new version is 6% faster than the old version of R. The size of the improvement will vary a lot from one R program to another, of course. (The nature of the change is such that I don’t think any programs will be slower.)  But one can see that the simple modification I made has produced a significant improvement — a 6% speedup is a lot for such a small change.  From looking at the source code for R, I think a number of other improvements can be made without great effort.  In particular, arithmetic on large vectors can be speeded up substantially, including the squaring operation discussed here.  The changes to do this will also speed up arithmetic on short vectors, including scalars, but I don’t yet know  how significant the improvement will be in that context. The fundamental reason for the slowness of the current R implementation seems to be a lack of attention by the implementors to efficiency in code that is on the critical interpretive pathway.  This is the opposite fault to what is often seen among hackers — an obsessive focus on efficiency even where it doesn’t matter.  One should focus on efficiency where it matters, and focus on other things where it doesn’t matter (which is typically 95% of a program).  On the plus side, the R source code is fairly readable, even though it lacks much in the way of informative comments.  It was readable enough that I was able to figure out what’s going on and modify the program in less than a day.  My next task is to figure out how to get the modification into the next release of R… UPDATE: I forgot to mention that the real solution to speeding up parentheses is to get rid of them entirely.  The only reason for them to be preserved in R’s internal expression tree is so that they can be printed when an expression is deparsed.  But for that, one could just have a field in each expression node saying how many pairs of parentheses directly enclose it (usually 0 or 1, except for people who really like redundant parentheses), which can be accessed when deparsing the expression, but which would not slow down execution of the program. 	 0 Comments
A brief introduction to “apply” in R	https://www.r-bloggers.com/2010/08/a-brief-introduction-to-%e2%80%9capply%e2%80%9d-in-r/	August 19, 2010	nsaunders	"At any R Q&A site, you’ll frequently see an exchange like this one: 
Q:  How can I use a loop to […insert task here…] ?
A:  Don’t.  Use one of the apply functions.
 So, what are these wondrous apply functions and how do they work?  I think the best way to figure out anything in R is to learn by experimentation, using embarrassingly trivial data and functions.

If you fire up your R console, type “??apply” and scroll down to the functions in the base package, you’ll see something like this: Let’s examine each of those. 1.  apply
Description: “Returns a vector or array or list of values obtained by applying a function to margins of an array or matrix.” OK – we know about vectors/arrays and functions, but what are these “margins”?  Simple:  either the rows (1), the columns (2) or both (1:2).  By “both”, we mean “apply the function to each individual value.”  An example: That last example was rather trivial; you could just as easily do “m[, 1:2]/2″ – but you get the idea. 2.  by
Description: “Function ‘by’ is an object-oriented wrapper for ‘tapply’ applied to data frames.” The by function is a little more complex than that.  Read a little further and the documentation tells you that “a data frame is split by row into data frames subsetted by the values of one or more factors, and function ‘FUN’ is applied to each subset in turn.”  So, we use this one where factors are involved. To illustrate, we can load up the classic R dataset “iris”, which contains a bunch of flower measurements: Essentially, by provides a way to split your data by factors and do calculations on each subset.  It returns an object of class “by” and there are many, more complex ways to use it. 3.  eapply
Description:  “eapply applies FUN to the named values from an environment and returns the results as a list.” This one is a little trickier, since you need to know something about environments in R.  An environment, as the name suggests, is a self-contained object with its own variables and functions.  To continue using our very simple example: I don’t often create my own environments, but they’re commonly used by R packages such as Bioconductor so it’s good to know how to handle them. 4.  lapply
Description:  “lapply returns a list of the same length as X, each element of which is the result of applying FUN to the corresponding element of X.” That’s a nice, clear description which makes lapply one of the easier apply functions to understand.  A simple example: The lapply documentation tells us to consult further documentation for sapply, vapply and replicate.  Let’s do that.     4.1  sapply
Description: “sapply is a user-friendly version of lapply by default returning a vector or matrix if appropriate.” That simply means that if lapply would have returned a list with elements $a and $b, sapply will return either a vector, with elements [['a']] and [['b']], or a matrix with column names “a” and “b”.  Returning to our previous simple example:     4.2  vapply
Description: “vapply is similar to sapply, but has a pre-specified type of return value, so it can be safer (and sometimes faster) to use.” A third argument is supplied to vapply, which you can think of as a kind of template for the output.  The documentation uses the fivenum function as an example, so let’s go with that: So, vapply returned a matrix, where the column names correspond to the original list elements and the row names to the output template.  Nice.     4.3  replicate
Description: “replicate is a wrapper for the common use of sapply for repeated evaluation of an expression (which will usually involve random number generation).” The replicate function is very useful.  Give it two mandatory arguments:  the number of replications and the function to replicate; a third optional argument, simplify = T, tries to simplify the result to a vector or matrix.  An example – let’s simulate 10 normal distributions, each with 10 observations: 5.  mapply
Description: “mapply is a multivariate version of sapply.  mapply applies FUN to the first elements of each (…)  argument, the second elements, the third elements, and so on.” The mapply documentation is full of quite complex examples, but here’s a simple, silly one: Here, we sum l1$a[1] + l1$b[1] + l2$c[1] + l2$d[1] (1 + 11 + 21 + 31) to get 64, the first element of the returned list.  All the way through to l1$a[10] + l1$b[10] + l2$c[10] + l2$d[10] (10 + 20 + 30 + 40) = 100, the last element. 6.  rapply
Description: “rapply is a recursive version of lapply.” I think “recursive” is a little misleading.  What rapply does is apply functions to lists in different ways, depending on the arguments supplied.  Best illustrated by examples: So, the output of rapply depends on both the function and the how argument.  When how = “list” (or “replace”), the original list structure is preserved.  Otherwise, the default is to unlist, which results in a vector. You can also pass a “classes=” argument to rapply.  For example, in a mixed list of numeric and character variables, you could specify that the function act only on the numeric values with “classes = numeric”. 7.  tapply
Description: “Apply a function to each cell of a ragged array, that is to each (non-empty) group of values given by a unique combination of the levels of certain factors.” Woah there.  That sounds complicated.  Don’t panic though, it becomes clearer when the required arguments are described.  Usage is “tapply(X, INDEX, FUN = NULL, …, simplify = TRUE)”, where X is “an atomic object, typically a vector” and INDEX is “a list of factors, each of same length as X”. So, to go back to the famous iris data, “Species” might be a factor and “iris$Petal.Width” would give us a vector of values.  We could then run something like: Summary
I’ve used very simple examples here, with contrived data and standard functions (such as mean and sum).  For me, this is the easiest way to learn what a function does:  I can look at the original data, then the result and figure out what happened.  However, the “apply” family is a much more powerful than these illustrations – I encourage you to play around with it. The things to consider when choosing an apply function are basically: It’s the usual input-process-output story:  what do I have, what do I want and what lies inbetween? "	 0 Comments
R/Rmetrics at BaselR	https://www.r-bloggers.com/2010/08/rrmetrics-at-baselr/	August 19, 2010	rmetrics		 0 Comments
Myths about Ciudad Juarez	https://www.r-bloggers.com/2010/08/myths-about-ciudad-juarez/	August 18, 2010	Diego Valle-Jones		 0 Comments
Integrating PHP and R	https://www.r-bloggers.com/2010/08/integrating-php-and-r/	August 18, 2010	Lee	"“How can I integrate PHP and R?” I know I’m not the only one who’s asked this question.  After all, with great content management systems like Drupal, it would very cool to be able to drop an R module into some PHP code and instantly have a web app popping out some snazzy looking ggplot graphics.  After spending some time Google searching for an easy implementation and finding very little in the PHP + R space, I was able to piece together a method for integrating the two.  It uses no JavaScript… no AJAX… just plain old PHP.  Disclaimer:  There are many reasons why you shouldn’t actually stick the following code on the web for all to see.  It is a very stripped down example, which is great for experimenting on a localhost, but has some serious security flaws. This implementation of PHP and R consists of only two files.  One written in PHP, and the other an R script.  The PHP returns a form which uses the GET method to send a variable N to the server.  When the form is submitted, the PHP will then execute an R script from the shell using a combination of the PHP command exec() and the Rscript shell command.  This command will pass the variable N to the R script.  The R script will then execute and save a histogram plot of N normally distributed values to the filesystem.  Finally, when the R script is complete, the PHP will return the HTML tag containing the saved images path. First, the PHP file… and the R script… Finally, to help you visualize the whole process a bit better, below is a screenshot of the results…
 This is a very simple example that works fine on a localhost, but there are some BIG issues that need to be resolved before this code is opened up to the web. The bottom line is that with less than 20 lines of code we are able to generate R graphics in a browser… everything else is just details. "	 0 Comments
New R User Group in Raleigh-Durham	https://www.r-bloggers.com/2010/08/new-r-user-group-in-raleigh-durham/	August 18, 2010	David Smith	New local R user groups keep on popping up on a regular basis, which is great to see. The latest one is deep in SAS territory: it’s the Raleigh-Durham-Chapel Hill R Users Group in They don’t have any meetings scheduled just yet (but when they do their gracious meetup hosts are Carrboro Creative Coworking). So if you’re in the area why not sign up and suggest a topic for that first meetup? meetup.com: Raleigh-Durham-Chapel Hill R Users Group 	 0 Comments
R be dragons	https://www.r-bloggers.com/2010/08/r-be-dragons/	August 18, 2010	Timothée	"Hic sunt dracones used to be placed on maps, as a way to denote a dangerous or otherwise unexplored territory. We might as well write it all over R-related material used in introductory classes, because students seems to be really afraid of what they will find. Last year, I had the chance to teach a bunch of master students some fundamentals of R. The course was called « Analysis of biological data » or something along these lines. It was a joint course in biostatistics and an introduction to R, and I was in charge of two groups of students for this last part. I will be doing it again this winter. I was really surprised by the fact that these students were not really looking forward to the first class. After asking them what they expected from the class and why they were anxious about it, they told me they feared that the combination of statistics
(statistical thinking seems to be something we lose, because it is obviously active as soon as 18 months!) and computing will result in too much new concepts to handle. Yes, R has a steep learning curve, a fact that some used to call it an epic fail [what is an epic fail?]. So does LaTeX, so does C, and so does almost any other language. If we wanted to use something with a smooth learning curve, we would use Statistica or JMP. That would allow the students to go into a button-clicking frenzy, they would get test statistics and p-values by the dozen, and…  well, they will likely do the same thing that I did when I was a student using Statistica and JMP : perform a bunch of tests that are not necessarily valid given their data, have no global understanding of how things work, and learn roughly no statistics at all (or, it can be that I was a disastrous student…). Because no matter how difficult it might seem to be proficient in R, you end up learning a lot of things in statistic in your daily use. Just the help pages for most tests are rich in information. So, the goal is not to make the learning of R easy (actually, I guess it is, but it will never be natural to some people).  It is to design the introduction to R so that is will maximize the interaction with statistics, and help learn things jointly in the two fields. To this regard, I think that a document like R for beginners (PDF) is not optimal (while it is great and I used it many times, I always had the feeling that it expected you to know R in order to learn R), while simpleR (PDF) or IPSUR do a great job. Does any of you have experiences in teaching simultaneously R and statistics? I’ll be curious to know how you approached the situation… "	 0 Comments
Distributions in R	https://www.r-bloggers.com/2010/08/distributions-in-r/	August 18, 2010	David Smith	One of the R language’s most powerful features is its ability to deal with random distributions: not just generating random numbers from various distributions (based on a very powerful pseudo-random number generator), but also calculating densities, probabilities, and quintiles. John Cook provides a handy reference chart listing all of the distributions supported by standard R (reproduced below — and there are many other distributions supported by contributed packages), and also explains the elegant naming scheme for the various functions.  Updated Aug 20: added the ncp parameter to beta, chisq, f, and t with thanks to Doug Bates’ comment below. John D Cook: Distributions in R and S-PLUS 	 0 Comments
Bookshelf remodelling	https://www.r-bloggers.com/2010/08/bookshelf-remodelling/	August 18, 2010	Manos Parzakonis	"I found time and read Gelman and Hill’s “Data Analysis Using Regression and Multilevel / Hierarchical Models“…Now, please do yourself a favour and get it (of course the paperback version  ). Even for experienced or intermediate (myself) this will be a treat for your eyes and neurons. 

 "	 0 Comments
Twifficiency Scores	https://www.r-bloggers.com/2010/08/twifficiency-scores/	August 18, 2010	John Myles White	Neil Kodner wrote a great post this morning about yesterday’s Twifficiency scores outbreak. He grabbed all the auto-tweeted scores he could find and plotted their distribution. I was struck by the asymmetry of the resulting distribution, which you can see below: Thankfully, Neil handed me the raw data for his plot, so I was able to run a K-S test for normality, which rejected normality pretty easily, though I’m coming up with a tie that I’m surprised by: I suppose that I’m a bit worried that the p-value is simply a reflection of sample size here, since there are 7089 measurements. Would it be more compelling to bootstrap the D score from the K-S test on samples of 500 scores at a time to confirm that the non-normality is present even in small groups of scores? Assuming that the data really has a skewed distribution, does anyone understand the scoring system well enough to say what produces the asymmetry? 	 0 Comments
state-by-state pendulum	https://www.r-bloggers.com/2010/08/state-by-state-pendulum/	August 17, 2010	jackman	By popular demand (!), my state-by-state pendulum (pendula?) for 2010 is up (big PDF), just in time for the election.  550px wide JPG version is inline, below. This follows the same formatting I used in the 2007 edition. We start with the 2PP ALP vote shares recorded at the last election (incorporating changes from electoral redistributions, as computed by the AEC), subtract those from 50 percent so as to get the results interpretable as swing (i.e., how much 2PP swing is required for the seat to change hands). Color code by party, in the usual way (red is Labor, blue is Liberal/Country Liberal, green is National, and black is independent), with party reflecting the party of the present incumbent. I have also scaled the vertical axis non-linearly, so as to devote more of the graph to the politically interesting region around the no-swing mark. There are also reference lines showing Labor’s best showing (since 1949) in each of the states.   Note how close VIC & QLD in 2007 are to Labor’s best ever showing in those states.  VIC is threatening to swing towards Labor and perhaps post a “best ever” result for Labor there.  Contrast QLD, which seems like it is reverting to type.  	 0 Comments
Ed Burnette on Software Patents	https://www.r-bloggers.com/2010/08/ed-burnette-on-software-patents/	August 17, 2010	Matt Shotwell	"Ed Burnette  makes a point that hits home, with regard to software patents, and how engineers and programmers of modern companies are now being asked to write them: 
Unfortunately, the joke is on all of us. It’s on our economy, as we let patents choke down innovation and increase fear, uncertainty, and doubt in an already uncertain time. It’s on our bottom lines, as we make busy-work for our expensive lawyers with their sparkling eyes  instead of investing for the future. And it’s on our collective consciousness, as we force good and decent people to act against the better angels of their nature.
 I wonder if the R Foundation has considered the implications of software patents for the R software. "	 0 Comments
Programming Language Popularity: StackOverflow and Ohloh	https://www.r-bloggers.com/2010/08/programming-language-popularity-stackoverflow-and-ohloh/	August 17, 2010	C		 0 Comments
Unit Testing in R: The Bare Minimum	https://www.r-bloggers.com/2010/08/unit-testing-in-r-the-bare-minimum/	August 17, 2010	John Myles White	This week I decided to start unit testing my R code, so I taught myself the bare minimum about the RUnit and testthat packages to be able to use them. Here’s what I found necessary to get started writing tests with both packages. I’m going to assume that you’ve got a bunch of functions in sample.R that you want to test. For example, sample.R might contain a definition of the naïve factorial function: To test your functions, create a directory called tests that will store all of your test cases. In tests, create a file called 1.R that will contain your first set of tests. Each set of tests will go in a function inside of 1.R, named according to the convention test.*. For example, you might have this: To run this set of tests, we need to create a file called run_tests.R that will act as a test suite and invoke all of the tests in your tests directory: Here you inform the defineTestSuite() function that you’re creating a test suite called “example” and that the test files are located in a directory called “tests” where all of the files match the regular expression ‘^\\d+\\.R’. Then you run the suite explicitly and print out the results in a text format. That’s it. With those ideas, you can write your own test suite for your R code. In general, with RUnit, you use a function named something like check* to test the following conditions: In addition to these functions, there’s also a DEACTIVATED() function that lets you turn off a test function during its execution if you need to do that. As above, I’m going to assume that you’ve got a bunch of functions in sample.R that you want to test. And, as before, to test your functions, you should create a directory called tests that will store all of your test cases. In tests, create a file called 1.R that will contain your first set of tests. We’ll use expect_that() for all of our tests, as in the example below: To run this set of tests, we need to create a file called run_tests.R that will act as a test suite and invoke all of the tests in your tests directory: To get output, you have to inform test_dir() to use the SummaryReporter, which provides more than enough information for my purposes. See the testthat docs for other reporters you could use. In general, you can ask expect_that() to test the following conditions: There are some other tricks that testthat can do as well. It can automatically rerun tests on a directory of code whenever the code is edited using a function called auto_test() and it can set up contexts to separate tests using a function context(). I haven’t really explored either, so I can’t comment more on them. While I don’t think the arguments on behalf of either RUnit or testthat are unquestionable, I’m inclined to think that testthat has a brighter future, especially since it’s written by ggplot2′s author, Hadley Wickham. 	 0 Comments
Animated Heatmap of WikiLeaks Report Intensity in Afghanistan	https://www.r-bloggers.com/2010/08/animated-heatmap-of-wikileaks-report-intensity-in-afghanistan/	August 17, 2010	Drew Conway	" Visualisation of Activity in Afghanistan using the Wikileaks data from Mike Dewar on Vimeo.  The latest visualization of the WikiLeaks data compiled by our group is an animation of the intensity of report observations in Afghanistan over the six year period in the WikiLeaks data.  Team member Mike Dewar did the vast majority of work for this visualization, and has provided a brief description of how to interpret it: 
 The intensity of the heatmap represents the number of events logged. The colour range is from 0 to 60+ events over a one month window. We cap the colour range at 60 events so that low intensity activity involving just a handful of events can be seen – in lots of cases there are many more than 60 events in one particular region. The heatmap is constructed for every day in the period from 2004-2009, and the movie runs at 10 days per second.  The orange lines represent the major roads in Afghanistan, and the black outlines are the individual administrative regions.  To me, Mike’s animation is the best visual representation of the WikiLeaks data to date, and provides a very nuanced account of how the war unfolded from 2004 to 2009.  In addtion it gives clear indication as to the areas where coalition activity was consistently high, such as the capital region and eastern border with Pakistan, and the areas where activity ebbed and flowed, such as the south and central areas of Afghanistan.   The persistent level of heavy reporting at the Pakistani border is also quite striking.  By 2007, the entire eastern portion of the map is blanketed with a high intensity of activity on roads leading from Kabul to Pakistan. Finally, it is clear that coalition activities are highly constrained by the main “Ring Road” in Afghanistan, as the animation shows activity spread along this path steadily over time. As always, all of the code required to make this animation, along with several other analyses, are available at our github repository, and the original Vimeo link to the animation is here.  I look forward to reading your thoughts on this visualization! "	 0 Comments
New R User Group in Singapore	https://www.r-bloggers.com/2010/08/new-r-user-group-in-singapore/	August 17, 2010	David Smith	There’s yet another R user group starting, this time in Singapore. Their first meetup is next week, on Wednesday August 25. If you’re in Singapore, come along and meet your fellow R users! meetup.com: R User Group – SG 	 0 Comments
Deducer: R and ggplot2 GUI	https://www.r-bloggers.com/2010/08/deducer-r-and-ggplot2-gui/	August 16, 2010	Stephen Turner		 0 Comments
IEOR Tools Tutorial:  Learning XML with R	https://www.r-bloggers.com/2010/08/ieor-tools-tutorial-learning-xml-with-r/	August 16, 2010	Larry D'Agostino		 0 Comments
Goals per Game in MLS	https://www.r-bloggers.com/2010/08/goals-per-game-in-mls/	August 16, 2010	Ryan	"I promised something related to Major League Soccer and here it is.  Caveat:  It’s not much.  Why so sparse?  (1) The data is a bit messy due to teams folding, expansion, name changes, etc.  (2)  I was backpacking all weekend and didn’t have time to work on this side project.  Yes, I have a real job and working during the work week is a bit difficult. My first step was to scrape the “stats” section of the MLS site to get all of their public data.  Or at least all of the data that is relatively easy to find and easy to scrape.  I’ll post the code soon once I setup the master repository on github.  Needless to say, I think it looks a bit better than my initial foray into beautifulsoup as posted here. I decided to look at goals per game by team and year.  Most people who like soccer like goals, so this seems like a good starting point.  Here is the initial figure.  As you can see, there are a lot of blank spaces.  The reason for this is because a lot of teams changed their name and/or relocated (e.g., San Jose), some teams folded (e.g., Tampa Bay), and MLS added teams over the years (e.g., Chivas USA).  The bottom line is that it makes for an ugly graph.  In an attempt to clean it up a bit, I tried to consolidate some of the names.  Here is the new figure.  It still doesn’t look great, but I do think that you can learn a bit from this figure.  Overall, I would say that the goals per game for each team is decreasing over time.  Is it a statistically significant decline?  I dunno.  I’m not writing a paper here — it’s a freaking blog, i.e., speculation reigns supreme!  In any case, this raises more questions.  For example, I would hypothesize that it’s just that the quality of the league has improved significantly over the years.  Hence, the teams are holding possession more and not just firing shots whenever they get a chance.  As a result, I will look into attendance numbers, shots, shots on goal, etc. in the upcoming days or possibly weeks.  I believe that some interesting questions can be answered with these data.  However, I am still trying to discover what these questions might be.  If you have any ideas for questions, let me hear about them in the comments section. The R code for this project isn’t too interesting, so I won’t post it below — it will be on the github repository in time though.  One thing that I did learn about R is that reading in numeric data measured in the thousands (e.g., attendance figures) can be problematic if the numbers have commas.  It took me a while to find the workaround and it’s given below. mls.reg.dat$h_tot <- as.numeric(gsub("","", """", mls.reg.dat$h_tot))

mls.reg.dat$h_avg <- as.numeric(gsub("","", """", mls.reg.dat$h_avg))

mls.reg.dat$a_tot <- as.numeric(gsub("","", """", mls.reg.dat$a_tot))

mls.reg.dat$a_avg <- as.numeric(gsub("","", """", mls.reg.dat$a_avg))  "	 0 Comments
Rose plot using Deducers ggplot2 plot builder	https://www.r-bloggers.com/2010/08/rose-plot-using-deducers-ggplot2-plot-builder/	August 16, 2010	Tal Galili	The (excellent!) LearnR blog had a post today about making a rose plot in ggplot2. Following today’s announcement, by Ian Fellows, regarding the release of the new version of Deducer (0.4) offering a strong support for ggplot2 using a GUI plot builder,  Ian also sent an e-mail where he shows how to create a rose plot using the new ggplot2 GUI included in the latest version of Deducer.  After the template is made, the plot can be generated with 4 clicks of the mouse. Here is a video tutorial (Ian published) to show how this can be used:  The generated template file is available at: http://neolab.stat.ucla.edu/cranstats/rose.ggtmpl I am excited about the work Ian is doing, and hope to see more people publish use cases with Deducer. 	 0 Comments
Charting the performance of cricket all-rounders – IT Botham	https://www.r-bloggers.com/2010/08/charting-the-performance-of-cricket-all-rounders-%e2%80%93-it-botham/	August 16, 2010	Ralph	Cricket is a sport that generates a large volume of performance data and corresponding debate about the relative qualities of various players over their careers and in relation to their contemporaries. The cricinfo website has an extensive database of statistics for professional cricketers that can be searched to access the information in various formats. As an initial example we will consider the English legend Sir Ian Botham who played 102 test matches for England between his debut in 1977 until his final game in 1992. The first obvious breakdown is to consider how Botham performed against the six countries that he played against during his test career. A summary of his statistics are shown here: Botham only played three matches against Sri Lanka so it is difficult to properly assess his performance against them. If the above table is stored in a data frame itb.opp then we can create a histogram of the total runs (or wickets) by opposition country: This code produces the following graph: IT Botham Total Runs by Opposition The total wickes graph is produced by the next code: IT Botham Total Wickets by Opposition We may now want to delve deeper into the performance against different nations to take into account the number of games or innings where Botham batted or bowled. The traditional way to assess performance is to calculate batting and bowling averages and we can do this by opposition which provides the following data frame: This can be converted into a dot plot so we can see whether Botham had a high batting average than bowling average, which is often taken to be one of the signs of an all-rounder. The graph is shown here: IT Botham Batting and Bowling Averages by Opposition We can see the differences in performance based on the opposition. Botham’s performance against the West Indies, by far the strongest team during most of his international career, were worse than against the other countries. However, his averages were far from embarassing when compared to other players at the time. The graph also shows that Botham enjoyed batting and bowling against India. We can divide this data further based on whether the matches were played in England or outside of England and this data is shown here: A dot plot is created from this data with a separate panel for each of the six opposition countries and the averages divided into batting and bowling performances. The coloured dots in the graph indicated whether the average is for matches at home or away. This graph is shown below: IT Botham Batting and Bowling Averages by Country and Home/Away We can see that the difference between home and away peformance is, in general, not very large for bowling averages but in some cases there is a noticeable difference in batting averages. When looking at Botham’s performances against the West Indies his statistics at home are much better than his away performance, suggesting that his main struggles against the strong West Indies team were in the Caribbean. This might be due to his swing bowling being more suitable to English conditions compared to pitches in the West Indies. To round off this brief look at the career of IT Botham let us consider some other important statistics, in particular games where he performed with the bat and ball. Individual matches of excellence include five games with a century and at least five wickets: These performances and others show why Botham was considered such a great player as he produced some sustained periods of excellent all-round cricket rather than having one discipline more dominant for a long period of time. 	 0 Comments
GillespieSSA 0.5-4 is released	https://www.r-bloggers.com/2010/08/gillespiessa-0-5-4-is-released/	August 16, 2010	Mario Pineda-Krch	I just uploaded GillespieSSA 0.5-4 to CRAN. It should be  just a matter of days before it has propagated itself across all mirrors. This release consists of minor revisions with no (intended) changes in functionality. The main change (and it is also minor) is a fix for warning messages in recent versions of R. I owe a thanks to Thomas Petzoldt who took upon him to do the actual leg work on this one. My only contribution was to delay the fix for so long that CRAN dropped the package and moved it to the morgue. Reports of the death of GillespieSSA were, however, grossly exaggerated and now that the package passess all the checks again it will hopefully appear on CRAN in due time. Note that these days the development of the package (albeit slow) is taking place on GitHub, where the most recent version is also available. Interestingly the very same day CRAN relegated the package to their formerly available packages list I started to receive emails from distressed users from all parts of the world. I was quite taken back that so many people were interested in the package (which of course is not the same as people actually using it), and once Thomas (out of the blue) fixed the warning issue (which was the cause of the relegation in the first place) I truly started feeling guilty. Just to address a some of the questions I received: Feedback, bug reports, ideas, code contributions are most welcome and will move things forward more expeditiously. This is from the “Mario’s Entangled Bank” blog (http://pineda-krch.com) of Mario Pineda-Krch, a theoretical biologist at the University of Alberta. 	 0 Comments
GPU Computing with R	https://www.r-bloggers.com/2010/08/gpu-computing-with-r/	August 16, 2010	rtutor.chiyau	"

Statistics is computationally intensive. Routine statistical tasks such as data
extraction, graphical summary, and technical interpretation all require heavy use of
modern computing machinery. Obviously, these tasks can benefit greatly from a
parallel computing environment where multiple calculations can be performed
simultaneously.
 read more "	 0 Comments
ggplot2 plot builder is now on CRAN! (through Deducer 0.4 GUI for R)	https://www.r-bloggers.com/2010/08/ggplot2-plot-builder-is-now-on-cran-through-deducer-0-4-gui-for-r/	August 16, 2010	Tal Galili	Ian fellows, a hard working contributer to the R community (and a cool guy), has announced today the release of Deducer (0.4) to CRAN (scheduled to update in the next day or so). This major update also includes the release of a new plug-in package (DeducerExtras), containing additional dialogs and functionality. Following is the e-mail he sent out with all the details and demo videos.  Deducer is designed to be a free easy to use alternative to proprietary data analysis software such as SPSS, JMP, and Minitab. It has a menu system to do common data manipulation and analysis tasks, and an excel-like spreadsheet in which to view and edit data frames. The goal of the project is two fold. Provide an intuitive interface so that non-technical users can learn and perform analyses without programming getting in their way. Increase the efficiency of expert R users when performing common tasks by replacing hundreds of keystrokes with a few mouse clicks. Also, as much as possible the GUI should not get in their way if they just want to do some programming. Deducer is designed to be used with the Java based R console JGR, though it supports a number of other R environments (e.g. Windows RGUI and RTerm). For those not familiar with Deducer, an online manual is available at: http://www.deducer.org/pmwiki/pmwiki.php?n=Main.DeducerManual An introductory tour of Deducer (4.5 min):  There is also an “expert users introsuction” (8 min)  The major change to Deducer is the inclusion of a new plotting GUI built on the ggplot2 package. This Google Summer of Code project provides an easy to use system to make anything from simple histograms, to custom publication ready graphics. Feel free to check out the video introduction: Part 1 (6 min):  Part 2 (6 min):  Additional videos: Templates (5 min):  Extending the Builder (4 min):  The DeducerExtras package is an add-on package containing a variety of additional analysis dialogs. These include: Introduction to Deducer Extras (~2 min):  I would like to take this opportunity to thank the R community for choosing this project for a Google Summer of Code grant, and for the support and encouragement. In particular I would like to thank Hadley Wickham for mentoring the Plot Builder GUI, and Dirk Eddelbuettel for his organization of students and mentors. 	 0 Comments
Intraday volatility of OMX Baltic stocks	https://www.r-bloggers.com/2010/08/intraday-volatility-of-omx-baltic-stocks/	August 16, 2010	kafka	Usually, intraday volatility exhibits a “smile” – it is high at open and close and it is lower during the trading day.﻿ DJI index, 5 min. intervals, CET time:  MOS stock, 5 s. intervals, CET time:  Because many readers of this blog are trading Nasdaq OMX Baltic stocks, it is worth to share my findings about volatility in these markets. It was surprising for me, that none of the stocks in OMX Baltic markets follow “smile” pattern as it was show above. APG1L, 30 min. intervals, CET time:  CTS1L, 30 min. intervals, CET time:  TEO1L, 30 min. intervals, CET time:  SRS1L, 30 min. intervals, CET time:  MRK1T, 30 min. intervals, CET time:  OEG1T, 30 min. intervals, CET time:  TAL1T, 30 min. intervals, CET time:  UKB1L, 30 min. intervals, CET time:  TVEAT, 30 min. intervals, CET time:  TKM1T, 30 min. intervals, CET time:  I can find only one reason, why these stocks do not follow the “smile” pattern – liquidity. During the summer these markets are very dry and the quotes are 1.5 months long. Let me know, if you are interested in this report with longer period, then I will repeat it later. 	 0 Comments
Gone Guerrill_ R on Our Data	https://www.r-bloggers.com/2010/08/gone-guerrill_-r-on-our-data/	August 16, 2010	Neil Gunther		 0 Comments
Consultants’ Chart in ggplot2	https://www.r-bloggers.com/2010/08/consultants-chart-in-ggplot2/	August 16, 2010	learnr	"Excel Charts Blog posted a video tutorial of how to create a circumplex or rose or dougnut chart in Excel. Apparently this type of chart is very popular in the consulting industry, hence the “Consultants’ Chart”. It is very easy to make this chart in Excel 2010, but it involves countless number of clicks and formulas to format both the source data and the chart itself. In ggplot2 the same can be achieved with around 10 lines of code, as can be seen below.  
 Dougnut chart is essentially a bar chart using a polar coordinate system, so we start off with a simple bar chart. And to make life a bit more colourful will fill each slice with a different colour. Next the coordinate system is changed and some of the plot elements are removed for a cleaner look. Update 17 August 2010 Several commentators asked how to draw gridlines on top of the slices as in the original example. Putting the gridlines/guides on top of the plot is accomplished by adding a new variable that is purely used for drawing the borders of each slice element. Essentially, it stacks the required number of slices with a white border on top of each other. "	 0 Comments
A quick analysis of the trends in the number of weddings in France (1975–2010)	https://www.r-bloggers.com/2010/08/a-quick-analysis-of-the-trends-in-the-number-of-weddings-in-france-1975%e2%80%932010/	August 15, 2010	Timothée	I’m currently planning my wedding, and my fiancée and I were discussing wether there were more or less couples getting married over time. It turns out that this information is quite easy to get via INSEE, a french institute that deals with all demographic and economic questions. Additionally, they provide a csv file with monthly data, so we can look at the intra-annual trends. The short answer is: the number of celebrated weddings decreased since 1975, although there seem to be some kind of stabilization since 1985.  What is interesting is that the monthly trend seems to be less clear:  While some months like Oct. to Apr. consistently decreased over time, the summer time seem less easily predictable. Looking at the data for only May to Sep. (including Oct. as a reference) makes it clear that these months show both a long term and short term (~5 years) fluctuations:  Moreover, it seems that if at year n, there were a high number of weddings in june, there will by a high number of weddings in july at year n+1 (but this is not significant). So, if any reader has an interesting hypothesis about why some years decline but some other fluctuate, I would be really glad to know! Edit (Aug. 19, 2010) : Baptiste Coulmont offers the solution to the 5-years fluctuation  (in french): as it turns out, when there are 5 saturdays in a month, it has more weddings than the years around it. Which makes complete sense. 	 0 Comments
Downloading DNA sequences into R	https://www.r-bloggers.com/2010/08/downloading-dna-sequences-into-r/	August 15, 2010	Samuel Brown		 0 Comments
Two Surpising Things about R	https://www.r-bloggers.com/2010/08/two-surpising-things-about-r/	August 14, 2010	Radford Neal	I see that it’s been over a year since my last post!  I have a backlog of blog post ideas, but something else always seems to have higher priority.   Today, though, I have some interesting (and useful) things to say about R, which I discovered in the last few days, and which shouldn’t take long to blog about.  Of course, some other people may already be quite familiar with these things.  Or maybe not… First up, a useful feature of R that I hadn’t realized existed, which comes with a surprising gain in efficiency.  Second, something surprisingly slow about R’s implementation of a very common operation. First, the good thing I discovered about R.  In complex mathematical expressions, it’s common to use more than one type of bracket, so that it’s easier to pair them up visually.  Typical programming languages use only parentheses, other brackets having been appropriated for other uses.   But it turns out that in R you can use both parentheses and curly brackets!  The curly brackets are normally used to group statements, but an expression is one type of statement, and the last (or only) in the group provides the value.  I’m not sure that this was always the case — I vaguely recall otherwise with some earlier version (perhaps an early version of S).  But it works now. Here’s the even more surprising thing.  It occurred to me that before rushing out and using this feature, I should check that it doesn’t introduce some horrible inefficiency, as might be the case if curly brackets were optimized for their more common use in grouping statements.  So I did a little test, as follows: Using curly brackets speeds the program up by about 6%! That’s with R version 2.9.2 on a Windows XP machine.  Of course I ran it several times to be sure that results were consistent.  And I ran it on two other versions of R, on Intel Solaris and Sun SPARC machines, with similar results. I’m having difficulty imagining how curly brackets can be more efficient than parentheses.  Could there be a dispatch operation somewhere in which a curly bracket operator gets recognized faster than a parenthesis operator?  But surely any such search wouldn’t be done by linearly, or by any other method where this could happen.  I could imaginge some strange accidental effect of caching, except that it’s consistent over different versions of R and different machine architectures. The second surprising thing is how long it takes R to square all the numbers in a vector.  Consider the following: So multiplying the vector by itself is about 3.6 times faster than squaring it the obvious way with ^2.  This is again R version 2.9.2, released 2009-08-24. My first thought was that R treats ^2 as a general exponentiation operation, requiring taking a log, multiplying by two, and then exponentiating.  But no, as seen below, a general exponentiation takes even longer So my guess is that R does check for an exponent being exactly two, and treats that specially, but that it does this check again and again, for every element of the vector. The speed gain from replacing a^2 with a*a is enough to justify this replacement in time-critical code, even though it makes the program less readable.  But perhaps squaring will be (has already been?) made faster in a later version. 	 0 Comments
Hard drive occupation prediction with R – The linear regression	https://www.r-bloggers.com/2010/08/hard-drive-occupation-prediction-with-r-the-linear-regression/	August 14, 2010	Avulsos by Penz - Articles tagged as R	"
On some environments, disk space usage can be pretty predictable. In this post,
we will see how to do a linear regression to estimate when free space will reach
zero, and how to assess the quality of such regression, all using
R – the
statistical software environment.
 
The first thing we need is the data. By running a simple
(date --utc; df -k; echo) >> /var/dflog.txt
everyday at 00:00 by cron, we will have more than enough, as that will store the
date along with total, free and used space for all mounted devices.
 
On the other hand, that is not really easy to parse in R, unless we learn more
about the language. In order to keep this post short, we invite the reader to
use his favorite scripting language (or python) to process that into a file with
the day in the first column and the occupied space in the second, and a row for
each day:
 
This format can be read and parsed in R with a single command.
 
This is the data file we will use as source for the results
provided in this article. Feel free to download it and repeat the process.
All number in the file are in MB units, and we assume an HD of 500GB. We will
call the date the free space reaches 0 as the df0.
 
After running R in the shell prompt, we get the usual license and basic help
information.
 
The first step is to import the data:
 
The variable duinfo is now a list with two columns: day and usd. The
attach command allows us to use the column names directly. The
totalspace variable is there just for clarity in the code.
 
We can check the data graphically by issuing:
 
That gives us an idea on how predictable the usage of our hard drive is.
 
From our example, we get:
 
We can now create and take a look at our linear model object:
 
The second coefficient in the example tells us that we are consuming about 559 MB of disk space per day.
 
We can also plot the linear model over our data:
 
The example plot, with the line:
 
R provides us with a very generic command that generates statistical information
about objects: summary. Let's use it on our linear model objects:
 
To check the quality of a linear regression, we focus on the residuals, as
they represent the error of our model. We calculate them by subtracting the
expected value (from the model) from the sampled value, for every sample.
 
Let's see what each piece of information above means: the first is the
five-number summary
of the residuals. That tells us the maximum and minimum error, and that 75% of
the errors are between -1.4 GB and 1.3 GB. We then get the results of a
Student's t-test of
the model coefficients against the data. The last column tells us roughly how
probable seeing the given residuals is, assuming that the disk space does not
depend on the date - it's the
p-value. We usually accept an
hypothesis when the p-value is less than 5%; in this example, we have a large
margin for both coefficients. The last three lines of the summary give us more
measures of fit: the
r-squared values - the closest
to 1, the better; and the general p-value from the f-statistics, less than 5%
again.
 
In order to show how bad a linear model can be, the summary bellow was generated
by using 50GB as the disk space and adding a random value between -1GB and 1GB
each day:
 
It's easy to notice that, even though the five-number summary is narrower, the
p-values are greater than 5%, and the r-squared values are very far from 1. That
happened because the residuals are not normally distributed.
 
Now that we are (hopefully) convinced that our linear model fits our data
well, we can use it to predict hard-disk shortage.
 
Until now, we represented disk space as a function of time, creating a model
that allows us to predict the used disk space given the date. But what we really
want now is to predict the date our disk will be full. In order to do that, we
have to invert the model. Fortunately, all statistical properties (t-tests,
f-statistics) hold in the inverted model.
 
We now use the predict function to extrapolate the model.
 
But... when is that? Well, that is the numeric representation of a day in R:
the number of days since 1970-01-01. To get the human-readable day, we
use:
 
There we are: df0 will be at the above date if the
current pattern holds until then.
 
The linear model can give us the predicted hard disk space usage at any future
date, as long as collected data pattern is linear. If the data we collected
has a break point - some disk cleanup or software installation - the model will
not give good results. We will usually see that in the analysis, but we should
also always look at the graph.
 
This article is focused on teaching R basics - data input and plotting. We skip
most of the formalities of science here, and linear regression is certainly not
a proper df0 prediction method in the general case.
 
On the other hand, in the next part of this
article we will see a more robust method for df0 prediction. We will also
sacrifice our ability to see the used space vs time to get a
statistical distribution for the date of exhaustion, which is a lot more useful
in general.
 "	 0 Comments
Hard drive occupation prediction with R	https://www.r-bloggers.com/2010/08/hard-drive-occupation-prediction-with-r/	August 14, 2010	Avulsos by Penz - Articles tagged as R	"
On some environments, disk space usage can be pretty predictable. In this post,
we will see how to do a linear regression to estimate when free space will reach
zero, and how to assess the quality of such regression, all using
R – the
statistical software environment.
 
The first thing we need is the data. By running a simple
(date --utc; df -k; echo) >> /var/dflog.txt
everyday at 00:00 by cron, we will have more than enough, as that will store the
date along with total, free and used space for all mounted devices.
 
On the other hand, that is not really easy to parse in R, unless we learn more
about the language. In order to keep this post short, we invite the reader to
use his favorite scripting language (or python) to process that into a file with
the day in the first column and the occupied space in the second, and a row for
each day:
  
This format can be read and parsed in R with a single command.
 
This is the data file we will use as source for the results
provided in this article. Feel free to download it and repeat the procedures.
All number in the file are in MB units, and we assume an HD of 500GB. We will
call the date the free space reaches 0 as the df0.
 
After running R in the shell prompt, we get the usual license and basic help
information.
 
The first step is to import the data:
 
The variable duinfo is now a list with two columns: day and usd. The
attach command allows us to use the column names directly. The
totalspace variable is there just for clarity in the code.
 
We can check the data graphically by issuing:
 
That gives us an idea on how predictable the usage of our hard drive is.
 
From our example, we get:
  
We can now create and take a look at our linear model object:
  
The second coefficient in the example tells us that we are consuming about 559 MB of disk space per day.
 
We can also plot the linear model over our data:
  
The example plot, with the line:
  
R provides us with a very generic command that generates statistical information
about objects: summary. Let's use it on our linear model objects:
  
To check the quality of a linear regression, we focus on the residuals, as
they represent the error of our model. We calculate them by subtracting the
expected value (from the model) from the sampled value, for every sample.
 
Let's see what what each piece of information above means: the first is the
five-number summary
of the residuals. That tells us the maximum and minimum error, and that 75% of
the errors are between -1.4 GB and 1.3 GB. We then get the results of a
Student's t-test of
the model coefficients against the data. The last column tells us roughly how
probable seeing the given residuals is, assuming that the disk space does not
depend on the date - it's the
p-value. We usually accept an
hypothesis when the p-value is less than 5%; in this example, we have a large
margin for both coefficients. The last three lines of the summary give us more
measures of fit: the
r-squared values - the closest
to 1, the better; and the general p-value from the f-statistics, less than 5%
again.
 
In order to show how bad a linear model can be, the summary bellow was generated
by using 50GB as the disk space and adding a random value between -1GB and 1GB
each day:
 
It's easy to notice that, even though the five-number summary is narrower, the
p-values are greater than 5%, and the r-squared values are very far from 1. That
happened because the residuals are not normally distributed.
 
Now that we are (hopefully) convinced that our linear model fits our data
well, we can use it to predict hard-disk shortage.
 
Until now, we represented disk space as a function of time, creating a model
that allows us to predict the used disk space given the date. But what we really
want now is to predict the date our disk will be full. In order to do that, we
have to invert the model. Fortunately, all statistical properties (t-tests,
f-statistics) hold in the inverted model.
  
We now use the predict function to extrapolate the model.
 
But... when is that? Well, that is the numeric representation of a day in R,
which is the number of days since 1970-01-01. To get the human-readable day, we
use:
 
There we are: df0 will be at the above date if the
current pattern holds until then, with 95% statistical significance.
 
The linear model can give us the predicted hard disk space usage at any future
date, as long as collected data pattern is linear. If the data we collected
has a break point - some disk cleanup or software installation - the model will
not give good results. We will usually see that in the analysis, but we should
also always look at the graph.
 
In the next part of this article we will see a more
robust method for df0 prediction. We will also sacrifice our ability
to see the used space vs time in order to get a statistical distribution for the
date of exhaustion, which is a lot more useful.
 "	 0 Comments
Auto-completion in Notepad++ for R Script	https://www.r-bloggers.com/2010/08/auto-completion-in-notepad-for-r-script/	August 14, 2010	Yihui Xie	Auto-completion is fancy in a text editor. Notepad++ does not support auto-completion for the R language, so I spent a couple of hours on creating such an XML file to support R: Put it under ‘plugins/APIs‘ in the installation directory of Notepad++ (you can see several other XML files there supporting different languages such as C), and make sure you have enabled auto-completion in Notepad++ (Settings --> Preferences --> Backup/Auto-completion). Open an R script and start typing a familiar function (e.g. paste()), you will see some candidates in a drop-down list like this: Show parameters of R functions in Notepad++ Hit the Enter key if the function name selected in the list is correct for you, then type ‘(‘ and you will see hints for parameters: Auto-completion in Notepad++ for R script The file R.xml was actually generated from R; it contains almost all visible R objects in base R packages as well as recommended packages like MASS. You may create an extended XML file (containing keywords from other packages) by yourself after loading the packages you need into your current workspace, and run: 	 0 Comments
Introducing visualVaR.com	https://www.r-bloggers.com/2010/08/introducing-visualvar-com/	August 13, 2010	Lee	 After a month of on-again, off-again coding, I’ve finally completed a web site geared towards calculating the Value at Risk of the average investor’s portfolio.  The site is visualvar.com.  The big idea was to combine the statistical and visualization tools of R (especially ggplot2) with the web interface of Drupal.  While I’m happy with the results, I think this may only be the tip of the iceberg in mashing up these technologies.  As a side note, I took a bit of a shortcut and I don’t actually have R running directly on the web server, which means I had to settle for ‘overnight’ calculations rather than ‘On-Demand’.  But I still think it is a good proof of concept for combing Drupal with R.    For all the data lovers out there, below are some of my favorite visualizations that visualVaR produces using the users inputted portfolio.  This one showing the rank correlation between the daily returns of the stocks in a portfolio (obviously the higher the dependency between two stocks, the greater the VaR).  And this one showing the 4 years of daily returns (the amount of data used in the VaR model) for a single stock in a portfolio, BP in the case.   I was a little hesitant to turn the site loose on the web, but I figured it would be best to just ’see what happens’.  Please feel free to try it out, just cross your fingers that the server holds! 	 0 Comments
Presentations and video from useR! 2010 available	https://www.r-bloggers.com/2010/08/presentations-and-video-from-user-2010-available/	August 13, 2010	David Smith	For anyone who missed the useR! 2010 conference in Gaithersburg last month (or just wants to revisit some of the amazing talks), you can now find slides from many of the contributed presentations available for download. (Look for the [S] link to download slides.) The presentations from the Revolution team members are there, including:   Even better, in a first for the useR! conference, video is available for the invited talks (or audio, in the case of Richard Stallman’s talk) and the panel discussion, “Challenges of Bringing R into Commercial Environments“. And for anyone thinking of starting up a user group, check out the video of the panel discussion, Local R User Groups (with thanks to Drew Conway). Finally, slides and exercise materials are also available for many of the tutorials. Thanks to Kate Mullen and her team once again for putting on such an outstanding conference. On a related note, next year’s conference, useR! 2011, has been announced. See you next year in Warwick, UK.   	 0 Comments
Apologies and Style Guides	https://www.r-bloggers.com/2010/08/apologies-and-style-guides/	August 13, 2010	Ryan	I have to say that it’s pretty exciting to watch your blog go from a few hits over its lifetime to getting almost 200 in a single day.  I am currently negotiating with Google over the purchase of this blog.  Or maybe not.  Again, thanks be to @revodavid for posting to the Revolution Analytics Blog.  Anyway, I just wanted to apologize for the format of the code snippets.  Reading the python script and the R code again was painful to even me.  I’m going to try to follow the Google style guides for python and R and hopefully the code will be a bit more readable in future posts.  Further, I will try to host my projects on github so that I might find some collaborators. I’ve added a few interesting links to the Blogroll on the right.  I would recommend subscribing to the RSS feed for r-bloggers.com.  I see some good stuff on there almost daily. Next up:  An analysis of some MLS data!  All I’ve found so far is that the teams like to change their names.  Hopefully something useful will come of this project. 	 0 Comments
Rcpp svn revision 2000	https://www.r-bloggers.com/2010/08/rcpp-svn-revision-2000/	August 13, 2010	romain francois	I commited the 2000th revision of Rcpp svn today, so I wanted to look back at what I did previously with the 50 000th R commit.  Here are the number of commits per day and month … the same thing, but focused on the period since I joined the project … and now split by contributor here are the month where each of us have been the most active and the most active day The code to reproduce the graphs is here 	 0 Comments
Scrape Web data using R	https://www.r-bloggers.com/2010/08/scrape-web-data-using-r/	August 13, 2010	--	Plenty of people have been scraping data from the web using R for a while now, but I just completed my first project and I wanted to share the code with you.  It was a little hard to work through some of the “issues”, but I had some great help from @DataJunkie on twitter. As an aside, if you are learning R and coming from another package like SPSS or SAS, I highly advise that you follow the hashtag #rstats on Twitter to be amazed by the kinds of data analysis that are going on right now. One note.  When I read in my table, it contained a wierd set of characters.  I suspect that it is some sort of encoding, but luckily, I was able to get around it by recoding the data from a character factor to a number by using the stringr package and some basic regex expressions. Bring on fantasy football! 	 0 Comments
Rcpp at LondonR, oct 5th	https://www.r-bloggers.com/2010/08/rcpp-at-londonr-oct-5th/	August 12, 2010	romain francois	I’ll be presenting Rcpp at the next LondonR, which is currently scheduled for october 5th Here is one picture I found on flickr, searching for london speed bus, … there are many other 	 0 Comments
Fun with the proto package: building an MCMC sampler for Bayesian regression	https://www.r-bloggers.com/2010/08/fun-with-the-proto-package-building-an-mcmc-sampler-for-bayesian-regression/	August 12, 2010	Michael Bedward		 0 Comments
Tuning Notepad++	https://www.r-bloggers.com/2010/08/tuning-notepad/	August 12, 2010	Karsten W.	Here are some tricks I collected for making Notepad++ a more comfortable text editor for me in general in for the R programming language in particular. Notepad++’s default behaviour is to use Ctrl+(Shift)+Tab for tabbing between different text files. This was very annoying to me,because other programs I use, such as Chrome and Excel, bind these functions to Ctrl+PageUp/PageDown. Today I found a blog post by Yawar Amin which shows how change this behaviour.  Yihui Xie shows on his blog how to add autocompletionfor R functions and symbols to notepad++. The Npp2R does something similar but less customizable, and does also containadditional functionality like code passing, but I did not look into it in detail. 	 0 Comments
R’s role in the national response to the BP Oil Spill	https://www.r-bloggers.com/2010/08/rs-role-in-the-national-response-to-the-bp-oil-spill/	August 12, 2010	David Smith	In the early days of the Deepwater Horizon oil spill in the Gulf of Mexico, the rate of flow of oil from the spill was of great concern: estimating it accurately was key to coordinating the scale and scope of the response to the emergency. Unfortunately, estimates from independent sources varied widely, and BP’s own estimates varied widely over time.  Antonio Possolo, Division Chief of Statistical Engineering at the National Institute of Science and Technology (NIST), was charged with making sense of these varied estimates to help the government coordinate the national response to the spill. As described in this video testimonial (starting at 2:20), Possolo was sitting in the company of the Secretaries of Energy and the Interior, when he broke out R on his laptop to run uncertainty analysis and harmonize the estimates from the various sources. In the video, Possolo is adamant in his confidence in his statistical analysis using the open-source R system. As he says to the crowd (including many of the developers of R), “The quality that you have built into R, through public, open examination, is the greatest strength and source of confidence I could have asked for.”  Blip.tv: useR talks, video #1  	 0 Comments
useR! 2010 conference videos	https://www.r-bloggers.com/2010/08/user-2010-conference-videos/	August 12, 2010	Szilard	Videos of the invited talks of the useR! 2010 conference as follows (courtesy by Kate Mullen and NIST). This site also aims at collecting the materials (video, slides, R code) of local R users group (RUG) meetings and various other R talks and bringing them to the larger R community. See more videos here, and if you’d like to contribute with materials, see more information here. Welcome by NIST and Frank E. Harrell Jr: Information Allergy  Mark S. Handcock: Statistical Modeling of Networks in R and Panel Discussion: Challenges Bringing R into Commercial Environments  Luke Tierney: Some possible directions for the R engine and Diethelm Würtz: The Hull, the Feasible Set, and the Risk Surface: A Review of the Portfolio Modeling Infrastructure in R/Rmetrics  Friedrich Leisch: Reproducible Statistical Research in Practice and Uwe Ligges: Prospects and Challenges for CRAN – with a glance on 64-bit Windows binaries  Also, audio of Richard M. Stallman: Free Software in Ethics and in Practice can be found here (the main talk) and here (the Q&A).  More details about the talk and the Q&A session is available on the original blog post link. 	 0 Comments
Baseball games: getting longer?	https://www.r-bloggers.com/2010/08/baseball-games-getting-longer/	August 11, 2010	David Smith	"ESPN’s Bill Simmons (aka The Sports Guy) recently suggested that the primary cause of dwindling interest in Red Sox games by fans is that baseball games these days are too long. “It’s not that fun to spend 30-45 minutes driving to a game, paying for parking, parking, waiting in line to get in, finding your seat … and then, spend the next three-plus hours watching people play baseball”, he says. But this is a relatively new phenomenon: are the games really any longer today than they used to be? Inspired by the column and having recently learned about the power of R‘s ggplot2 package to visualize data, R user (and baseball fan) Ryan Elmore decided to find out. He wrote code to scrape from the Web data on the lengths of all MLB games since 1970, and then created some lovely visualizations using ggplot 2 and R. For example, here’s a scatterplot of the lengths of games from all teams over time, with a smooth trend indicating a clear lengthening of the game, at least through the turn of the century: 
The peak average game length comes during the “Steroids Era” of Baseball, which Ryan delineates on the chart with the dashed red lines. Beyond that peak, there appears to be a slow but steady decline in the length of the games (although still more than 20 minutes longer than games in the 70’s). So how does this explain the disaffectation of Red Sox fans suggested by The Sports Guy? Ryan takes it one step further, and break out Red Sox games separately: 
 So it does seem that Boston games are indeed getting longer, lending credence to the Sports Guy’s claim. However: I’m no baseball expert, but I’m pretty sure there have been more than 3 Red Sox games since 2004, and the apparent increase may just be a fluke. Given that Ryan has posted his code in R to create the chart, maybe someone with access to more data can verify the result? If you do, let us know in the comments.  The Log Cabin: Are MLB games getting longer? "	 0 Comments
What would a 25th, 50th, and 75th percentile soil profile look like?	https://www.r-bloggers.com/2010/08/what-would-a-25th-50th-and-75th-percentile-soil-profile-look-like/	August 11, 2010	dylan	I have mentioned the AQP package in previous entries. One of the functions in this package generates aggregate soil profile data, from a collection of soil profiles that are related by some factor: common lithology, common landscape position, and so on. Typically the mean, or median (50th percentile) is used to generate a new aggregate profile, that is representative of the original collection. Extending this idea, I thought that it would be interesting to generate aggregate profiles that are representative of the 25th and 75th percentiles as well. For the sake of clarity, lets call these three new profiles (25th, 50th, and 75th percentiles) Q25, Q50, and Q75. A 10 cm slicing interval was used as the basis upon which soil properties were aggregated.  read more 	 0 Comments
Using R for Introductory Statistics 3.3	https://www.r-bloggers.com/2010/08/using-r-for-introductory-statistics-3-3/	August 11, 2010	Christopher Bare	…continuing our way though John Verzani’s Using R for introductory statistics. Previous installments: chapt1&2, chapt3.1, chapt3.2 If two data series have a natural pairing (x1,y1),…,(xn,yn), then we can ask, “What (if any) is the relationship between the two variables?” Scatterplots and correlation are first-line ways of assessing a bivariate data set. The Pearson’s correlation coefficient is calculated by dividing the covariance of the two variables by the product of their standard deviations. It ranges from 1 for perfectly correlated variables to -1 for perfectly anticorrelated variables. 0 means uncorrelated. Independent variables have a correlation coefficient close to 0, but the converse is not true because the correlation coefficient detects only linear dependencies between two variables. [see wikipedia entry on correlation]  Question 3.19 concerns a sampling of 1000 New York Marathon runners. We’re asked whether we expect a correlation between age and finishing time. We discover a low correlation – good news for us wheezing old geezers. A scatterplot might show something. And we have the gender of each runner, so let’s use that, too. First, let’s set ourselves up for a two panel plot. Next let’s set up colors – pink for ladies, blue for guys – and throw in some transparency because a lot of data points are on top of each other. In the first panel, draw the scatter plot. And in the second panel, break it down by gender. It's a well kept secret that outcol and outpch can be used to set the color and shape of the outliers in a boxplot. Now return our settings to normal for good measure.  Sure enough, there doesn't seem to be much correlation between age and finishing time. Gender has an effect, although I'm sure the elite female runners would have little trouble dusting my slow booty off the trail. It looks like we have fewer data points for women. Let's check that. We can use table to count the number of times each level of a factor occurs, or in other words, count the number of males and females. I'm still a little skeptical of our previous result - the low correlation between age and finishing time. Let's look at the data binned by decade.  It looks like you're not washed up as a runner until your 50's. Things go down hill from there, but, it doesn't look very linear, so we shouldn't be too surprised about our low r. Coarser bins, old and young using 50 as our cut-off, reveal that there's really no correlation in the younger group. In the older group, we're starting to see some correlation. I suppose you could play with the numbers to find an optimum cut-off that maximized the difference in correlation. Not sure what the point of that would be. I ran a marathon once in my life. I think I was 30 and my time was a pokey 270 or so. My knees hurt for days afterwards, so I'm not sure I'd try it again. I do want to do a half, though. Gotta get back in shape for that... 	 0 Comments
Converting R contingency tables to data frames	https://www.r-bloggers.com/2010/08/converting-r-contingency-tables-to-data-frames/	August 11, 2010	toddjobe	"A contingency table presents the joint density of one or more
categorical variables.  Each entry in a contingency table is a count
of the number of times a particular set of factors levels occurs in
the dataset.  For example, consider a list of plant species where
each species is assigned a relative seed size (small, medium, or
large) and a growth form (tree, shrub, or herb).
 
A contingency table will tell us how many times each combination of
seeds.sizes and growth.forms occur.
 
The output contingency table are of class table.  The behaviour of
these objects is not quite like a data frame.  In fact, trying to
convert them to a data frame gives a non-intuitive result.
 
Coercion of the table into a data frame puts each factor of the
contingency table into its own column along with the frequency,
rather than keeping the same structure as original table object.
If we wanted to turn the table into a data frame keeping the
original structure we use as.data.frame.matrix.  This function is
not well-documented in R, and this is probably the only situation in
which it would be used.  But, it works.
 "	 0 Comments
Which chart is better?	https://www.r-bloggers.com/2010/08/which-chart-is-better/	August 10, 2010	dan	CHART CRITICS, GRAPHICS CURMUDGEONS, COME ONE COME ALL  Once upon a time there was this graph (graph 1).  Andrew Gelman went all graphics curmudgeon on it, calling it an “ugly, sloppy bit of data graphics“, so it became this graph (graph 2).  Now the question is, which is better: graph 2 or graph 3?  Please use the comments and logic. Thank you. ADDENDUM As a result of all the feedback here. The following chart was chosen for use in the publication (Proceedings in the National Academy of Sciences):  Photo credit: http://www.flickr.com/photos/emeryjl/2104152944/. Graphs 1 and 3 have four categories and graph 2 has five categories. Also, there is a missing label on graph 3′s horizontal axis. Assume you are deciding among graphs of these basic forms that have equivalent numbers of groups and identical axis labeling. 	 0 Comments
R Environments for Gibbs Sampler State	https://www.r-bloggers.com/2010/08/r-environments-for-gibbs-sampler-state/	August 10, 2010	Matt Shotwell	"I recently decided to revisit some R code that implements a Gibbs sampler in an attempt to decrease the iteration time. My strategy was to implement the sampler state as an R environment rather than a list. The rationale was that passing an environment to and from functions would reduce the amount of duplication (memory copying). In my experiments, this results in only marginal improvement in iteration time. The following is just a toy example to illustrate. Consider the logistic regression model
 For Gibbs sampling, it’s convenient to store and manupulate a model state. In the past, I had represented this in R with a list, for example Then, to perform a Gibbs update on beta, I would pass the old state to a function that would return the updated state. For example: Hence, the Gibbs sampler would be used as follows Alternatively, the Gibbs sampler state may be stored and manipulated asan R environment. For example, to initialize the state: Then rewrite the Gibbs sampler function: The Gibbs sampler is used in an almost identical manner: The percent decrease in iteration time is fairly constant for a variety of n and maxiter values. Presumably, this modest gain in speed results from less memory duplication. However, in some experiments with tracemem the story appears to be more complicated… For now, it doesn’t appear that utilizing environments is worth the trouble when the sampling algorithm is much slower than memory allocation. "	 0 Comments
Conditioning Systems on Regime Variables	https://www.r-bloggers.com/2010/08/conditioning-systems-on-regime-variables/	August 10, 2010	Intelligent Trading		 0 Comments
A Twitter feed for R links	https://www.r-bloggers.com/2010/08/a-twitter-feed-for-r-links/	August 10, 2010	David Smith	India-based data scientist Harsh Singhal has compiled “State of the R”: a list of more than 50 links to R-related websites, which has generated much discussion on the R Project group on LinkedIn. Now, even if you’re not on LinkedIn, you can find the list at the new Links4R Twitter profile, and get updates about new links by following @Links4R. Thanks, Harsh! Twitter: Links4R   	 0 Comments
Just for Fun: Using R to Create Targets	https://www.r-bloggers.com/2010/08/just-for-fun-using-r-to-create-targets/	August 10, 2010	dylan	OK, not really science or soil-related, but a fun 5 minute use of R to make something you can use to improve your hand-eye coordination. read more 	 0 Comments
Homogeneity analysis of hierarchical classifications	https://www.r-bloggers.com/2010/08/homogeneity-analysis-of-hierarchical-classifications/	August 10, 2010	Michael Bedward		 0 Comments
RQuantLib 0.3.4	https://www.r-bloggers.com/2010/08/rquantlib-0-3-4/	August 9, 2010	Thinking inside the box	"

This follows the 0.3.3 release from last week
and has again a number of internal changes.  All uses of objects from
external namespaces are now explicit as I removed the remaining using
namespace QuantLib;. This makes things a little more verbose, but should
be much clearer to read, especially for those not yet up to speed on whether
a given object comes from any one of the Boost, QuantLib or Rcpp
namespaces. We also generalized an older three-dimensional
plotting function used for option surfaces — which had already been used in the
demo() code — and improved the code underlying this: arrays of
option prices and analytics given two input vectors are now computed at the
C++ level for a nice little gain in efficiency.  This also illustrates the possible
improvements from working with the new 
Rcpp API that is
now used throughout the package,

 
Full changelog details, examples and more details about this package are at
my RQuantLib page.


 "	 0 Comments
An HSV colour wheel in R	https://www.r-bloggers.com/2010/08/an-hsv-colour-wheel-in-r/	August 9, 2010	respiratoryclub	If you’ve read any of my previous posts, you’ll notice that they’re rather scanty on colour.  There’s a reason for this.  Mainly, that to get a good colour output takes some time.  I recently read a commentary in Nature methods (sorry if you don’t have access to it, but this looks like it may be the first part of an interesting series of articles), which discusses colour in graphics.  The author suggests a colour wheel, and I thought I’d have a go in R:  You have to click on it to read the text, sorry.  There’s probably much easier ways to do it, and it takes a silly amount of time to render (several seconds! – all those nested loops), but this code below makes the colour wheel.  If you set the variables t.hue, t.sat and t.val, the bottom right box is the resulting colour (the box just to the bottom right of the colour wheel is the hue with sat and val set to 1.0).  Then on the right is the plot of val, and below is the plot of sat.  As you go anti-clockwise from the x axis round your hue increases from 0.0 to 1.0. So you can play around with colour, see what works and what doesn’t.  This uses the HSV approach, which seemed okay for my purposes.  rgb2hsv() converts rgb into hsv (obviously), if you are more familiar with the RGB approach.  There are lots of other resources for colour in R, one of my favourites is here, and of course you can always search R-bloggers. 	 0 Comments
R unfolds the history of the Afghanistan war	https://www.r-bloggers.com/2010/08/r-unfolds-the-history-of-the-afghanistan-war/	August 9, 2010	David Smith	"Drew Conway continues his analysis of the Wikileaks data. Having concluded that the data appear legitimate (except perhaps in one region, based on a Benford’s Law analysis of the numbers in the documents), Drew follows up with a spatio-temporal analysis of activity within Afghanistan, based on the datelines of the documents themselves (click to enlarge): 
 Each panel represents a year of documents; each dot is a document color-coded by the event type. Over time, you can see a steady increase in the number of reported incidents and also a change in their character, such as the perplexing number of neutral/unknown incidents in 2007. (If I were to request one change of Drew for this chart, it would be for a four-way, for-color segmentation of incidents as friendly, neutral, enemy and unknown, to make it easier to distinguish the event types.) Wired’s Danger Room column gives an in-depth look at Drew’s analysis; you should take a look at is as well as Drew’s original post below. Drew’s R code for all his analysis is also available at this github repository. Zero Intelligence Agents: Wikileaks Attack Data by Year and Type Projected on Afghanistan Regional Map "	 0 Comments
Quickly Find the Class of data.frame vectors in R	https://www.r-bloggers.com/2010/08/quickly-find-the-class-of-data-frame-vectors-in-r/	August 9, 2010	Stephen Turner		 0 Comments
Useful functions for data frames	https://www.r-bloggers.com/2010/08/useful-functions-for-data-frames/	August 9, 2010	Ralph	The R software system is primarily command line based so when there are large sets of data it is not easy to browse the data frames. There are various useful functions for working with data frames. For example, after loading data from a text file we might want to view the first few lines of a set of data. The functions head and tail return the first or last parts of a vector, matrix, table, data frame or function. Consider the Orange data set that is available in R. We can view the first few lines or the last few lines: Another useful function is str, which compactly displays the internal structure of an R object. On this set of data we get: There is quite a bit of additional information attached to this data frame, mainly due to it having more than one class. 	 0 Comments
GitHub Stats on Programming Languages	https://www.r-bloggers.com/2010/08/github-stats-on-programming-languages/	August 9, 2010	C		 0 Comments
R has the best models	https://www.r-bloggers.com/2010/08/r-has-the-best-models/	August 9, 2010	David Smith	"
  We had a great time at the JSM conference, and I really enjoyed meeting with all the useRs at the Revolution mixer on Wednesday evening (where this photo was taken). Hope everyone had a great time — thanks for coming!    "	 0 Comments
New R Package ‘aqp’: Algorithms for Quantitative Pedology [updates]	https://www.r-bloggers.com/2010/08/new-r-package-aqp-algorithms-for-quantitative-pedology-updates/	August 9, 2010	dylan	"  
Soils are routinely sampled and characterized according to genetic horizons (layers), resulting in data that are associated with principal dimensions: location (x,y), depth (z), and property space (p). The high dimensionality and grouped nature of this type of data can complicate standard analysis, summarization, and visualization. The aqp package was developed to address some of these issues, as well as provide a useful framework for the advancement of quantitative studies in soil genesis, geography, and classification.  read more "	 0 Comments
Installing RApache on Mac OS X Snow Leopard	https://www.r-bloggers.com/2010/08/installing-rapache-on-mac-os-x-snow-leopard/	August 9, 2010	:)-k		 0 Comments
Handling Large CSV Files in R	https://www.r-bloggers.com/2010/08/handling-large-csv-files-in-r/	August 9, 2010	Quantitative Finance Collector		 0 Comments
Iris Data Set Visualization Web App in < 100 LOC	https://www.r-bloggers.com/2010/08/iris-data-set-visualization-web-app-in-100-loc/	August 7, 2010	C		 0 Comments
Wikileaks Attack Data by Year and Type Projected on Afghanistan Regional Map	https://www.r-bloggers.com/2010/08/wikileaks-attack-data-by-year-and-type-projected-on-afghanistan-regional-map/	August 7, 2010	Drew Conway	Below is a visualization of the Wikileaks data produced in collaboration with Michael Dewar.  This plot shows attacks in the data set by year and type, projected onto a map of Afghanistan with district boundaries.  This visualization is certainly not perfect, i.e., some colors are difficult to discern, but it does provide added insight to the level and location of fighting over six years o the war represented by the data. R code available here. 	 0 Comments
Creating a Presentation with LaTeX Beamer – Including Images from Graphics Files	https://www.r-bloggers.com/2010/08/creating-a-presentation-with-latex-beamer-%e2%80%93-including-images-from-graphics-files/	August 7, 2010	Ralph	It will often be more efficient to generate graphics in an external software package and then include these files in a LaTeX beamer presentation. The standard LaTeX approach to including graphics can be utilised to perform this task. Fast Tube by Casper The graphicx is useful for including graphics files in a presentation and this package has a command includegraphics where we specify the name of the image file to be included and (optionally) some formatting information such as the dimensions of the image in our document. As an example if we wanted to include a jpg file then we would include a command like this: and if we wanted to scale in the horizontal or vertical dimension then we can specify the height and width to use: There are other options as part of the graphicx package that can be used to adjust the display of the image as required. Other useful resources are provided on the Supplementary Material page. 	 0 Comments
Julian Besag 1945-2010	https://www.r-bloggers.com/2010/08/julian-besag-1945-2010/	August 7, 2010	xi'an	I have just learned that Julian Besag passed away last morning in Bristol after being admitted to the hospital two weeks ago. He was a leading figure of our field, a fiercely independent thinker, a brilliant statistician, and undoubtedly the clearest pretendent to having fathered MCMC. His influence on the field of spatial statistics will be felt for years, but he will be sorely missed… My first meeting with Julian was in 1993 in Laramie, Wyoming, and my last memory of him will be this visit George Casella and I made to Bath and Bristol in October 2008, and where Julian attended both of our talks as well as the dinner in Bath and the after-seminar beer in Bristol… He will be missed. 	 0 Comments
RInside release 0.2.3	https://www.r-bloggers.com/2010/08/rinside-release-0-2-3/	August 6, 2010	Thinking inside the box	"
This is the first release since March when we released
0.2.2. 
A few things got added to
Rcpp in the
meantime, and
RInside is
taking advantage of some of these as illustrated in several of the included examples. 

 
More details and the changelog are on the
RInside page
which also leads



 "	 0 Comments
Because it’s Friday: Foxes on a Trampoline	https://www.r-bloggers.com/2010/08/because-its-friday-foxes-on-a-trampoline/	August 6, 2010	David Smith	They’re foxes. They’re on a trampoline. And they’re jumping. I don’t think there’s much more for me to add:   	 0 Comments
Finding out (fast) the classes of data.frame vectors	https://www.r-bloggers.com/2010/08/finding-out-fast-the-classes-of-data-frame-vectors/	August 6, 2010	Aviad Klein	Sometimes it’s useful to write down the various classes of vectors inside your data.frame objects for documentation and other people to use it. I’ve searched for a quick way to find out all the classes of vectors inside a data.frame. Since I’ve found no reference for such a function/process I made one up. I’d like to hear what people have to say about the following use of the “class” function on data.frames a simple call : trying to use the “apply” function to know what classes are the columns in the data.frame yeilds the following unwanted result : For some reason the apply function returns “character” on all vectors regardless of their true content (any ideas why?). Anyhow, after some thought I’ve come up with the following function : Compact, fast and quite useful. Of course the control flow needs more work to fit other classes and recognize when x is not a data.frame. Comments are welcome. 	 0 Comments
How to animate Google Earth with R	https://www.r-bloggers.com/2010/08/how-to-animate-google-earth-with-r/	August 6, 2010	David Smith	"We’ve looked before at how you can annotate geographical maps using R, but what if you want to overlay data onto a globe of the Earth, using Google Earth? The RKML package for R (from the OmegaHat project) allows you to do just that, by providing a high-level interface from R to generate KML files, which in turn are used to drive animations in Google Earth. Here’s one very cool example of its use: at the blog Mind of a Markov Chain you can find an animation of the Deepwater Horizon oil spill, overlaid with the tracks of tuna migrations. Here’s a screenshot: 
  Follow the link below for the animation, where you’ll see that the migration paths intersect with the spill (in both time and space). This animation was created with fewer than 30 lines of R code, which you can also find at the link. Mind of a Markov Chain: R and Google Earth ~ comparing tuna tracks vs. Gulf of Mexico oil spill extent  "	 0 Comments
Twenty rules for good graphics	https://www.r-bloggers.com/2010/08/twenty-rules-for-good-graphics/	August 6, 2010	Rob J Hyndman	One of the things I repeatedly include in referee reports, and in my responses to authors who have submitted papers to the International Journal of Forecasting, are comments designed to include the quality of the graphics. Recently someone asked on stats.stackexchange.com about best practices for producing plots. So I thought it might be helpful to collate some of the answers given there and add a few comments of my own taken from things I’ve written for authors. The following “rules” are in no particular order. The classic books on graphics are:  These are both highly recommended. (If you can’t see the books above, turn off your ad-blocker.)   	 0 Comments
Combinadics in R	https://www.r-bloggers.com/2010/08/combinadics-in-r/	August 5, 2010	Mark Fredrickson	A question on stats.stackexchange.com reminded me of some code I wrote earlier this summer. The code provides a correspondence between the natural numbers 1 to (N choose K) and all the unique K sized combinations one could draw from N items. This relationship is know as the combinadic of an integer (and my code is pased on the reference implementation). Generating combinations is useful for permutation tests, in which one applies a test statistic on all possible allocations of treatment to an experimental pool. Since the number of possible combinations grows extremely rapidly, realizing all possible combinations at once can be extremely memory intentsive. Using combinadics, one can trade increased execution time for lower memory usage. Since they are indexed by integers, keeping track of which combination is currently used is trivial. But since the number of combinations grows very quickly, we still need to handle extremely large integers, perhaps larger than the default integer type in R accepts. Luckily, the GMP package provides for “big ints,” with which we can write a “N Choose K” algorithm for arbitrarily large numbers: Here are two functions to turn an integer into a vector representing a combination (a process I call decoding) and to turn a combination into an integer (encoding). As I expect that these operations will be frequent for a given N and K, these functions produce functions that take integers and combinations, respectively. Finally, as an illustration, the classic Lady Tasting Tea problem. There are 8 cups, 4 of which have the milk added first (for concreteness, say these are the first 4 cups). What is the distribution of correctly labeling cups as having milk added first? To answer the question, we need a test statistic to indicate how many cups were correctly labeled. Apply this test statistic to each possible allocation of cups, which corresponds to a combination of size 4 taken from 8 possible units. (NB: I’ve encountered strange results when creating lists/vectors of “big integers,” the result of encoding a combination. Use for and while loops instead.)  The plot shows that while a guess by chance of one, two, or three cups are not unlikely, a random guess that results in zero or four correct cups would be extremely rare. While astute readers will notice that this result can be found analytically, it still serves as a simple demonstration of a permutation test. The test statistic employed in this example leads to an analytical result. Other test statistics may not be as simple to solve. Permutation tests using all possible combinations always generates an exact distribution for any test statistic. This code was originally written for inclusion in RItools. Subsequently, I decided that only rare cases does one need to generate the entire null distribution of the test statistic, and in most cases sampling from possible combinations is sufficient. Future posts will address approximate approaches to permutation tests. Code for this post. 	 0 Comments
RcppArmadillo 0.2.5	https://www.r-bloggers.com/2010/08/rcpparmadillo-0-2-5/	August 5, 2010	Thinking inside the box	"

This release upgrades the included Armadillo version to Conrad’s just-released version
0.9.60. This overcomes some of minor issues we had with ‘older’ compilers such as g++ 4.2.x with x being 1 or 2.
No other changes were made from our end.

 
The short NEWS file extract follows:

 
More information is on the 
RcppArmadillo page.
Questions, comments etc should go to the
rcpp-devel mailing list
off the R-Forge page.

 "	 0 Comments
R and Google Earth ~ comparing tuna tracks vs. Gulf of Mexico oil spill extent	https://www.r-bloggers.com/2010/08/r-and-google-earth-comparing-tuna-tracks-vs-gulf-of-mexico-oil-spill-extent/	August 5, 2010	apeescape	There is a lot of interest in how the Gulf of Mexico oil gusher will affect the ecosystem and its marine species. One such species is the Western Atlantic bluefin tuna that holds the Gulf of Mexico as one of its major spawning grounds. Recent tag data show that the location of the gusher is also very close to its spawning ground both spatial and temporally (Teo and Block 2010). Some other effects (pdf). I thought it would be cool to plot this on Google Earth, using tools from RKML. I have 2003 and 2004 tracking data of Gulf bluefin tuna by GTOPP, along with the layers provided by NOAA-NESDIS, collected into one .kmz file by Google. RKML does easy translations from lat-lon data into KML. Using kmlTime(), I can create an animation by specifying exact dates. I equalized the years of the tracking data to 2010 since the layers come from different years, although they are labelled correctly. Below is the animation that shows both data on a 2-week time span. We see that there is some overlap in early June. embedded screencast w/ Jing. (note: if movie doesn’t play, that means it has reached its monthly bandwidth limit)  A screenshot:  Code: Refs: Add to: Facebook | Digg | Del.icio.us | Stumbleupon | Reddit | Blinklist | Twitter | Technorati | Yahoo Buzz | Newsvine 	 0 Comments
Where have all the Hacker News old-timers gone?	https://www.r-bloggers.com/2010/08/where-have-all-the-hacker-news-old-timers-gone/	August 5, 2010	David Smith	"Nostalgia ain’t what it used to be. As slashdot slowly loses its relevance and digg heads for a more general audience to head off competition from Twitter, loyal readers of uber-technical news aggregator Hacker News wonder if it’s heading the same way. Seems like the long-standing users aren’t posting links to deep, interesting articles anymore, and are instead being drowned out by swarms of new users with more of a social media agenda. But is there any reality to this perception? An analysis of posting rates by new and longtime users published at the R-Charts blog (and done, of course, in R) indicates there might be some truth to it: 
 The Y axis is a count of users, not number of posts, and as R-charts notes: The chart does indicate that – as of [late July] – a a given user who posted was more likely to be someone who signed up in the last year or two than a veteran. You can update the chart with the latest data from HN by running the R script at the link below.  R-Charts: Hacker News User Base Changed? "	 0 Comments
Internal Migration Estimation in England and Wales	https://www.r-bloggers.com/2010/08/internal-migration-estimation-in-england-and-wales/	August 5, 2010	gjabel	"During my MS.c. I worked on methods for combining internal migration data in England and Wales. Migration data is often represented in square tables of origin-destination flows. These are of particular interest to analysing migration patterns when they are disaggregated by age, sex and some other variable such as illness, ethnicity or economic status. In England and Wales the data within these detailed flow table are typically missing in non-census years. However, row and column (origin and destination) totals are regularly provided from the NHS patient registers (see the first two columns of the hypothetical data situation below). I worked on a method to estimate the detailed missing flow data to sum to the provided totals in non-census years (see the third column of the hypothetical data situation below). This method is particularly useful for estimating migration flow tables disaggregated by detailed characteristics of migrants (such as illness, ethnicity or economic status) that are only provided by the ONS for census years.

Hypothetical Example of Data Set Situation (where migrant origins are labelled on the vertical axis and destinations on the horizontal axis). The estimated values maintain some properties (various cross product ratios) of the Census data whilst updating marginal totals to more current data. For more details see my MS.c. dissertation (which I have put online here). This contains the R/S-Plus code to conduct the estimation in the Appendix. Note, there is also a published paper based on my MS.c. (abstract and links here) that uses a slightly modified R code. "	 0 Comments
New R User Group in Portland, OR	https://www.r-bloggers.com/2010/08/new-r-user-group-in-portland-or/	August 5, 2010	David Smith	There’s another new R user group on the US West Coast, this time in Portland, Oregon. The Portland R User Group will hold their first meeting at the Lucky Labrador Brew Pub on Tuesday, August 17. If you’re in the Portland/Vancouver area, come along for beeRs and conversation with other useRs in your area.  Incidentally, there are so many new user group announcements these days that I’ve added a new category to the blog to keep up with them. Check out the user groups section for all the R user group news. meetup.com: Portland R User Group 	 0 Comments
Are MLB Games Getting Longer?	https://www.r-bloggers.com/2010/08/are-mlb-games-getting-longer/	August 5, 2010	Ryan	"On July 29, 2010, I had a flight from Denver to Cincinnati.  About an hour before boarding, I went to ESPN’s website and found a new article by Bill Simmons, a.k.a The Sports Guy (@sportsguy33 on Twitter).  The basic premise of this article is that a core group of fans is losing interest in Red Sox games this season.  So he decides to assign percentages to his reasons why people are losing interest (he’s a writer, not a statistician).  Anyway, he states that the “biggie”, the “killer”, etc. is the time of games (his 55%), i.e., baseball games are too damn long.  He gives some data from baseball-reference.com to back up his claim. So what does this have to do with me flying to Cincinnati from Denver?  Well, being the nerd that I am, I immediately went to baseball-reference.com to see if I could download more data!  As an aside, I’ve been obsessing over learning how to use ggplot2 since my return from the useR2010 conference about two weeks ago.  This seems like a good time to start learning.  Ah shit…it appears that this project is going to be a little harder than I expected.  I could download the data for one team and one season before I boarded, but I wanted the past 30 seasons for all teams that played every season.  I suppose that I would have to write a scraper to collect the data from about 750 web pages.  Sweet, now I have something to do on the flight rather than obsess over whether or not the person next to me will spill over into my seat. Now I’m on the plane.  I’ve downloaded the html of one webpage so that I can test my python scraper (using BeautifulSoup) and I have the data that Simmons used in his article.  The first thing that I do is make a few graphs using his data.  Here is the first.  He essentially discretized the length of each game into five bins:  A – less than or equal to two hours, B – more than two but less than 2 hours and 30 min, and so on.  It certainly looks like the relative size of the blue and purple rectangles together is increasing and the golden rectangle seems to be decreasing.  Maybe Simmons is onto something.  Note that this isn’t the only figure that I made.  There were quite a few others, but I want to get to the good stuff. (Back onboard the plane.  I have two or three terminals open, Textmate with some python code open, R is open, and the dude sitting next to me is sufficiently freaked out by what’s going on.) OK, this is getting too long.  It turns out that I didn’t finish the scraper on the flight to Cincinnati.  Fortunately there is the return flight and a few ‘down’ hours to work on this project while visiting KY.  Success!  Upon landing at DIA on Monday, I had the scraper (for the test page that I downloaded on Friday) working.  Now I needed to write a script to iterate across all teams and every year since 1970.  I wrote the script, processed the ‘soup’ for all 700+ team/season combos and I give you the following figure!  So are baseball games getting longer?  Well, the preceding figure gives the median length of games (in minutes) for all teams over each season since 1970.  It looks like it is going from blue to red, suggesting an increase in the median length of games.  However, you might also notice that I’ve added two vertical lines from 1994 through 2004.  This roughly corresponds to the “Steroids Era” in MLB.  It looks like that the game times have been decreasing a bit since the middle of this era or so.  Can we look at the data in another way?  Of course, I give you Figure 3!  Here’s a different way of displaying the same data as given in the second figure.  I didn’t separate by team in this figure, however.  I’m just looking at the median length of games across all teams for each season and added a smooth curve to show a trend to the data.  Note that the peak on the curve corresponds to roughly 1998.  If you recall, Mark McGuire and Sammy Sosa were hitting HRs at record rates and teams were scoring a lot of runs.  And it looked as if their heads might suddenly explode from all of the ‘roids that they were taking. So what do I take from this?  Well, overall, I would say that they length of baseball games in general seems to be on the decline in the most recent years.  Is the same true for just the Red Sox?  Ah ha, I can make that figure.  Here you go.  Interestingly enough, it looks like the smooth trend line for Red Sox games is increasing in recent years whereas it’s decreasing for all other teams.  So maybe this Simmons character is onto something for his beloved Red Sox. What do I think?  I don’t really care.  I just wanted an interesting data set to use so that I can learn a bit more about ggplot2, learn a bit more about scraping data using python/beautifulsoup, and kill some time on a flight.  So that’s my story. Note that all of the data was obtained from baseball-reference.com.  Figures were made using the ggplot2 package in the R software.   Further, I know that I could do more with this data set from a statistical perspective.  That’s what I do, I’m a statistician.  But I have a full-time job and just wanted to learn some stuff! Some final thoughts.  I’m happy to share the r and/or python code for this project.  Just send me a message on twitter or gmail (twitter name @ gmail) and I’ll send it your way.  Also, you’ll notice some missing data for the Red Sox, NYY, LAD, and CHC.  Why?  I’m not sure.  Apparently the layout of the html code is a bit different than the rest of the pages and the scraper was returning missing values.  I should also say that I scraped the appropriate pages when teams switched cities.  For example, I was scraping Montreal prior to 2005 for the Washington Nationals data. Ryan @rtelmore on Twitter! Here is the R code: 
 "	 0 Comments
R helps reproducible research	https://www.r-bloggers.com/2010/08/r-helps-reproducible-research/	August 5, 2010	Timothée	While reading a paper on mutualistic networks in press at Ecology, by Jimena Dorado et al., I found the following sentence in the Materials & methods section: Correlation analyses were done using the cor.test function in the basepackage of R statistical software (https://www.r-project.org/) Shortly followed by It is implemented in the nestednof function of the bipartite package (Dorman et al. 2008) of R statistical software (https://www.r-project.org/) I already mentioned my opinion on developing and sharing R packages to facilitate the reproducibility of analyses. It seems to me that these authors are taking the next step forward, in that they clearly indicate, not only the kind of analysis they conducted (i.e. correlation and nestedness with the NODF metric), but allow the reader to see by himself with what tool these analyses were made. Explicitly writing in a paper that « I used R to do so and so » goes with two important consequences. First, it allows anyone to reproduce your analysis, given that most trophic networks are deposited to the NCEAS InteractionWeb database (and that facilitates meta-analyses). Second, it establishes the fact that R is a lingua franca in ecology. One can only rejoice to notice that the reproducibility of research in increased, and that analyses are done with a common tool to which anyone can contribute. These are all very good news. 	 0 Comments
Big data for R	https://www.r-bloggers.com/2010/08/big-data-for-r/	August 5, 2010	Allan Engelhardt	"
Revolutions Analytics recently announced their “big data” solution for R.  This is great news and a lovely piece of work by the team at Revolutions.
 
However, if you want to replicate their analysis in standard R, then you can absolutely do so and we show you how.
 
First you need to prepare the rather large data set that they use in the Revolutions white paper.  The preparation script shown  below does two passes over alal the files which is not needed: changing it to a single pass is left as an exercise for the reader….  Note that the following script will take a while to run and will need some 30-odd gig of free disk space (another exercise: get rid of the airlines.csv file), but once it is done the analysis is fast.
 
All done now.  Sample analysis:
 
Just like the Revolutions paper.  You can now use biglm.big.matrix and bigglm.big.matrix for basic regression and there are also k-means clustering and other functions.
 
I must admit here that I do not understand the Revolutions regression example, so I have not attempted to replicate it here.  It seems kind of sad if they change the syntax to be incompatible with standard R formulas, which is what appears to be happening.
 
Credit to Michael Kane and Jay Emerson of Yale who showed much of this in their poster The Airline Data Set... What's the big deal?.
 Jump to comments. 

Employee productivity as function of number of workers revisited
 We have a mild obsession with employee productivity and how that declines as companies get bigger. We have previously found that when you treble the number of workers, you halve their individual productivity which is mildly scary. We revisit the analysis for the FTSE-100 constituent companies and find that the relation still holds four years later and across a continent. 

Getting started with the Heritage Health Price competition
 The US$ 3 million Heritage Health Price competition is on so we take a look at how to get started using the R statistical computing and analysis platform . 

Benchmarking feature selection with Boruta and caret
 Feature selection is the data mining process of selecting the variables from our data set that may have an impact on the outcome we are considering. For commercial data mining, which is often characterised by having too many variables for model building, … 

A warning on the R save format
 The save() function in the R platform for statistical computing is very convenient and I suspect many of us use it a lot. But I was recently bitten by a “feature” of the format which meant I could not recover my data. I recommend that you save data in a d… 

R: Eliminating observed values with zero variance
 "	 0 Comments
Plotting texts as graphs with R and igraph	https://www.r-bloggers.com/2010/08/plotting-texts-as-graphs-with-r-and-igraph/	August 4, 2010	cornelius	"I’ve plotted several word association graphs for this New York Times article (1st paragraph) using R and the igraph library. #1, random method  #2, circle method  #3, sphere method  #4, spring method  #5, fruchterman-reingold method  # 6, kamada-kawai method   #7, graphopt method  The red vertices mark cliques. Here’s the (rough) R code for plotting such graphs: rm(list=ls());
library(""igraph"");

library(""Cairo"");
# read parameters

print(""Text-as-Graph for R 0.1"");

print(""------------------------------------"");
print(""Path (no trailing slash): "");

datafolder <- scan(file="""", what=""char"");
print(""Text file: "");

datafile <- scan(file="""", what=""char"");
txt <- scan(paste(datafolder, datafile, sep=""/""), what=""char"", sep=""\n"", encoding=""UTF-8"");
print(""Width/Height (e.g. 1024x768): "");

res <- scan(file="""", what=""char"");

rwidth <- unlist(strsplit(res, ""x""))[1]

rheight <- unlist(strsplit(res, ""x""))[2]
words <- unlist(strsplit(gsub(""[[:punct:]]"", "" "", tolower(txt)), ""[[:space:]]+""));
g.start <- 1;
g.end <- length(words) - 1;
assocs <- matrix(nrow=g.end, ncol=2)
for (i in g.start:g.end)

{

	assocs[i,1] <- words[i];

	assocs[i,2] <- words[i+1];

	print(paste(""Pass #"", i, "" of "", g.end, "". "", ""Node word is "", toupper(words[i]), ""."", sep=""""));

}
print(""Build graph from data frame..."");

g.assocs <- graph.data.frame(assocs, directed=F);
print(""Label vertices..."");

V(g.assocs)$label <- V(g.assocs)$name;
print(""Associate colors..."");

V(g.assocs)$color <- ""Gray"";
print(""Find cliques..."");

V(g.assocs)[unlist(largest.cliques(g.assocs))]$color <- ""Red"";
print(""Plotting random graph..."");

CairoPNG(paste(datafolder, ""/"", ""text-igraph-random.png"", sep=""""), width=as.numeric(rwidth), height=as.numeric(rheight));

plot(g.assocs, layout=layout.random, vertex.size=4, vertex.label.dist=0);

dev.off();
print(""Plotting circle graph..."");

CairoPNG(paste(datafolder, ""/"", ""text-igraph-circle.png"", sep=""""), width=as.numeric(rwidth), height=as.numeric(rheight));

plot(g.assocs, layout=layout.circle, vertex.size=4, vertex.label.dist=0);

dev.off();
print(""Plotting sphere graph..."");

CairoPNG(paste(datafolder, ""/"", ""text-igraph-sphere.png"", sep=""""), width=as.numeric(rwidth), height=as.numeric(rheight));

plot(g.assocs, layout=layout.sphere, vertex.size=4, vertex.label.dist=0);

dev.off();
print(""Plotting spring graph..."");

CairoPNG(paste(datafolder, ""/"", ""text-igraph-spring.png"", sep=""""), width=as.numeric(rwidth), height=as.numeric(rheight));

plot(g.assocs, layout=layout.spring, vertex.size=4, vertex.label.dist=0);

dev.off();
print(""Plotting fruchterman-reingold graph..."");

CairoPNG(paste(datafolder, ""/"", ""text-igraph-fruchterman-reingold.png"", sep=""""), width=as.numeric(rwidth), height=as.numeric(rheight));

plot(g.assocs, layout=layout.fruchterman.reingold, vertex.size=4, vertex.label.dist=0);

dev.off();
print(""Plotting kamada-kawai graph..."");

CairoPNG(paste(datafolder, ""/"", ""text-igraph-kamada-kawai.png"", sep=""""), width=as.numeric(rwidth), height=as.numeric(rheight));

plot(g.assocs, layout=layout.kamada.kawai, vertex.size=4, vertex.label.dist=0);

dev.off();
#CairoPNG(paste(datafolder, ""/"", ""text-igraph-reingold-tilford.png"", sep=""""), width=as.numeric(rwidth), height=as.numeric(rheight));

#plot(g.assocs, layout=layout.reingold.tilford, vertex.size=4, vertex.label.dist=0);

#dev.off();
print(""Plotting graphopt graph..."");

CairoPNG(paste(datafolder, ""/"", ""text-igraph-graphopt.png"", sep=""""), width=as.numeric(rwidth), height=as.numeric(rheight));

plot(g.assocs, layout=layout.graphopt, vertex.size=4, vertex.label.dist=0);

dev.off();
print(""Done!""); "	 0 Comments
Hacking the CISEN homicide data	https://www.r-bloggers.com/2010/08/hacking-the-cisen-homicide-data/	August 4, 2010	Diego Valle-Jones		 0 Comments
Recent arXiv postings	https://www.r-bloggers.com/2010/08/recent-arxiv-postings/	August 4, 2010	xi'an	With too little time, too many things to do on my plate, here are some recent arXiv postings I won’t have time to comment: 	 0 Comments
Integrating R with Geany	https://www.r-bloggers.com/2010/08/integrating-r-with-geany/	August 4, 2010	Shige		 0 Comments
ggplot2 gui: Major feature set complete	https://www.r-bloggers.com/2010/08/ggplot2-gui-major-feature-set-complete/	August 4, 2010	Tal Galili	"(Written by Ian Fellows) There has been quite a bit of progress on the ggplot2 graphical user interface since the last post. All of the major features have been implemented, and are outlined in the vlog links below. What remains is to fix bugs, improve interface elements, and listen to feedback from users (that’s you). Please give it a try by installing the development version of Deducer
install.packages(“Deducer”,,”http://www.rforge.net“,type=”source”) . It is best used with the R console JGR which you can find at http://rforge.net/JGR/ . Feature tour:
http://neolab.stat.ucla.edu/cranstats/vlog4.mov Development and extension:
http://neolab.stat.ucla.edu/cranstats/vlog5.mov Ian "	 0 Comments
U.S. Unemployment Data: Animated Choropleth Maps	https://www.r-bloggers.com/2010/08/u-s-unemployment-data-animated-choropleth-maps/	August 4, 2010	C		 0 Comments
use R! 2010 conference — reflections	https://www.r-bloggers.com/2010/08/use-r-2010-conference-reflections/	August 4, 2010	Karsten W.	From July 20-23, this year’s use R! conference took place in Gaithersburg near Washington. I attended the conference as part of my holidays in the U.S. and had a good time there. I met some people, even though that is not the easiest thing for me to do, and I got some inspirations and ideas I outline below: One speaker mentioned „stat apps” as the new buzz. This caught my attention. One „stat app” example that comes to my mind isGapminder, which is both a web app and a desktop app. „Stat apps” seen at UseR 2010 include 	 0 Comments
R Creators Accept Statistical Computing Award	https://www.r-bloggers.com/2010/08/r-creators-accept-statistical-computing-award/	August 4, 2010	David Smith	In a dedicated session at the JSM conference here in Vancouver, the creators of R, Robert Gentleman and Ross Ihaka, accepted their prize (announced in January, but awarded today) from the American Statistical Association for “innovation in computing, software, or graphics that has had a great impact on statistical practice or research”. It’s a well-justified award: as stated by the President of the Statistical Compution section of the ASA, it’s hard to think of any software other than the R Project that has had more impact on statistical practice or research. Today, R is the lingua franca by which statisticians around the work converse. Congratulations, Robert and Ross! ASA Sections on Statistical Computing and Statistical Graphics: Statistical Computing and Graphics Award 	 0 Comments
How Google and Facebook are using R	https://www.r-bloggers.com/2010/08/how-google-and-facebook-are-using-r/	August 4, 2010	Szilard	This is an older (2009) video from the kickoff meeting of the San Francisco Bay Area R Users Group. It was a panel discussion within the Predictive Analytics World conference. Video courtesy by Ron Fredericks of LectureMaker (click on the image below to see the video on LectureMaker’s site).   	 0 Comments
RQuantLib 0.3.3	https://www.r-bloggers.com/2010/08/rquantlib-0-3-3/	August 4, 2010	Thinking inside the box	"

Many of the changes in this new version are internal. The code was re-written using the new
Rcpp API throughout, and the build system was
further simplified using the LinkingTo: mechanism. The arithmetic average-price asian option pricer
was added. A few other code updates were made as well.

 
Full changelog details, examples and more details about this package are at
my RQuantLib page.


 "	 0 Comments
JSM 2010 [day 3]	https://www.r-bloggers.com/2010/08/jsm-2010-day-3/	August 4, 2010	xi'an	The same pattern as yesterday occurred, namely that the sessions that most interested me were all together at the 10:30 and 2pm slots, while there was no talk (besides the official ones) after 4pm… After another run around Stanley Park, I took the opportunity of being late for the first session to sit outside and to read the beginning of Francisco Samaniego’s book comparing Bayesian and frequentist inferences. The morning session was SAMSI sum-up about the spatio-temporal program they ran last year. I liked Noel Cressie’s analysis of a huge satellite data problem and the immense dimension reduction it brought. The most promising session of the day was however the afternoon’s Latent space models for network analysis, where Peter Hoff, David Bank and Purnamrita  Sarkar gave different perspectives on this quite interesting modelling technique. (I was reflecting during the talks that this could bring a modelling revival of the old French “analyse de données” school, in that it was translating dependencies into distances…) After meeting the other editors of StatProb and grabbing a Revolution Analytic water-bottle at their party, I attended the SBSS mixer where I had a great time (except for the $9 beers) talking to old friends and meeting new ones. 	 0 Comments
Meeting in the middle; or fudging model II regression with nls	https://www.r-bloggers.com/2010/08/meeting-in-the-middle-or-fudging-model-ii-regression-with-nls/	August 3, 2010	Michael Bedward		 0 Comments
Use Rapache: It Works!	https://www.r-bloggers.com/2010/08/use-rapache-it-works-2/	August 3, 2010	bayareaR	A half hour lecture by Jeffery Horner on RApache.  	 0 Comments
Suggestions for R-Chart iPhone App?	https://www.r-bloggers.com/2010/08/suggestions-for-r-chart-iphone-app/	August 3, 2010	C		 0 Comments
RODM Article on the Oracle Technology Network	https://www.r-bloggers.com/2010/08/rodm-article-on-the-oracle-technology-network/	August 3, 2010	C		 0 Comments
Using R and r.mapcalc (GRASS) to Estimate Mean Topographic Curvature	https://www.r-bloggers.com/2010/08/using-r-and-r-mapcalc-grass-to-estimate-mean-topographic-curvature/	August 3, 2010	dylan	"Recently I was re-reading a paper on predictive soil mapping (Park et al, 2001), and considered testing one of their proposed terrain attributes in GRASS. The attribute, originally described by Blaszczynski (1997), is the distance-weighted mean difference in elevation applied to an n-by-n window of cells: 
 Equation 4 from (Park et al, 2001)
  
where n is the number of cells within an (odd-number dimension) square window excluding the central cell, z is the elevation at the central cell, z_{i} is the elevation at one of the surrounding cells i, d_{i} is the horizontal distance between the central cell and surrounding cell i. I wasn’t able to get a quick answer using r.neighbors or r.mfilter, so I cooked up a simple R function to produce a solution using r.mapcalc. The results are compared with the source DEM below; concave regions are blue-ish, convex regions are red-ish. The magnitude and range are almost identical to mean curvature derived from v.surf.rst, with a Pearson’s correlation coefficient of 0.99. I think that it would be of general interest to add functionality to r.neighbors so that it could perform distance-weighted versions of commonly used focal functions. Elevation surface (left) and resulting mean curvature estimate (right) read more "	 0 Comments
countrycode R package	https://www.r-bloggers.com/2010/08/countrycode-r-package/	August 3, 2010	jackman	"I am digging this countrycode package in R I just found, written by a UM grad student Vincent Arel-Bundock.  Very nice.  Thanks! 

countrycode(""2"",""cown"",""iso3c"")

[1] ""USA""

 It would be better if the package could handle failed matches a little more gracefully: e.g., 

> countrycode(c(2,730,818),""cown"",""iso3c"")

[1] ""USA""


when the user was expecting to get back a vector of length 3 (including, possibly, NAs when matches aren’t found).  This just needs a little bit of care in the internals of the program, but otherwise this is a nice utility. "	 0 Comments
Image Data and Classification with R	https://www.r-bloggers.com/2010/08/image-data-and-classification-with-r/	August 3, 2010	» R	"03.08.2010 In this post i would like to demonstrate the ability of R to handle and classify image data with the help of ImageJ and Rserve bundled and implemented in Bio7. In general R is a very useful application for image analysis and plenty of “pure” R packages for image analysis are already available. But the combination of a scientific image analysis tool like ImageJ with R offers much more possibilities to edit and analyze images. With the correct image datatype (raw, integer, double) R  is also able to handle and process images >2000*2000 pixels (in combination with java!).
For my own curiosity i tried to find out the limits on a 32-bit and 64-bit OS with the image data present in the virtual maschine and after transfer (by means of Rserve) in R, too. Hardware: AMD Turion 2.0 GHz Dual core, 3Gb RAM Windows Vista 32-bit
Java enabled memory:  1024Mb
Tested:
Cluster Analysis (clara): Max 5000*5000*3(RGB) -> 6 centers: 70sec., 12 centers: 100s (byte transfer!).
Transferred 7 Landsat images 7457*6991 (byte transfer) and  created a new image from the sum of the 7 images (integer conversion!  with a triggering of the garbage collector).
Transferred 8000*8000 image as integer data to R in one shot.
Transferred and returned a 16000*16000 image as byte data (grayscale) to R and ImageJ in one shot. And with 64-bit (with a personal OSX port of Bio7).
Mac OSX 10.6, 2.93 GHz, 4GB RAM
Java enabled memory:  2512Mb
Cluster Analysis (clara) with R: Max 7000*6157*3(RGB) -> 6 centers: 168sec. (byte transfer!). R with ImageJ can be used perfectly to classify images. Two examples are shown below. The first example uses the clara algorithm which is generally more appropriate for images >2000*2000. Example 1: Unsupervised classification with the Bio7 interface (clara). Example 2: Supervised classification (rpart) with the Bio7 interface.
 This examples also demonstrate the general use of R for image analysis. Some more examples of image analysis with R, ImageJ and the Bio7 interface can be found here: Transfer images from and to R with ImageJ in Bio7 1.3 Analyse videos with ImageJ and R Transfer selected pixels of all opened images "	 0 Comments
Announcing Big Data for Revolution R	https://www.r-bloggers.com/2010/08/announcing-big-data-for-revolution-r/	August 3, 2010	David Smith	I’ve hinted this was coming a few times before, but with today’s press release the announcement is official: the next release of Revolution R Enterprise will include “Big Data” capabilities thanks to the new RevoScaleR package. We’re pretty excited at how it’s turned out: it’s kinda amazing to be able to use R’s formula syntax like this:  arrDelayLm2   and be able to do a regression on 120+million rows (more than 13Gb) of data in just a few seconds using an ordinary laptop. With the more powerful multicore machines now on the market, the parallel processing algorithms in RevoScaleR really scream. You can see some of the details of how the RevoScaleR package works in the white paper “Big Data Analysis with Revolution R Enterprise“, and I’ll be giving a presentation about the new Big Data capabilities of Revolution R Enterprise in a live webcast on August 25. One aspect of the announcement that really seems to have generated attention is how you can use Revolution R Enterprise with Hadoop to process super-massive data sets. (As I write this, before the press release is even on the wires, this article by Dave Rosenberg at Cnet has already been retweeted over 100 times.) Hadoop and RevoScaleR complement each other well: like a freight train, Hadoop can do the heavy lifting of preprocessing a distributed data set to get it ready for statistical analysis, and then, like a race car, RevoScaleR fits the statistical model. We’ll be coming out very soon with a white paper authored by Saptarshi Guha (author of the the Rhipe integration between Hadoop and R) demonstrating how he used Hadoop to extract out individual conversations from packet-level VOIP data, and then used RevoScaleR to perform a regression analysis on those calls. We’ll have more information about that analysis here in the blog in the next couple of weeks. Revolution Analytics: Revolutionary New Levels of Performance and Scalability for Big Data Analysis 	 0 Comments
Quantitative Candlestick Pattern Recognition (Part 2 — What’s this Natural Language Processing stuff?)	https://www.r-bloggers.com/2010/08/quantitative-candlestick-pattern-recognition-part-2-whats-this-natural-language-processing-stuff/	August 3, 2010	Intelligent Trading		 0 Comments
JSM 2010 [day 2]	https://www.r-bloggers.com/2010/08/jsm-2010-day-2/	August 3, 2010	xi'an	After a very good early run in Stanley Park, I went to a morning session on new statistical challenges in genetics, but unfortunately could not keep focussed enough (due to a very short night, still being not tuned to Pacific time!) so I ended up chatting with Sid Chib at the Springer booth about the future of R and the drawback of it running too slowly… The second session of the morning I attended was the I.J. Good memorial session (although there were many alternative choices I could have made at the same time!) where Steve Fienberg, Jim Berger, Adrian Raftery and David Banks gave different perspectives on the life and influence of this leading figure. After his work in Bletchley Park along Alan Turing during the war, already using Bayes factors introduced a few years earlier by Harold Jeffreys, I.J. Good contributed very much to the Bayesian revival of the 50′s. (A fact not mentioned this morning is that he was a consultant for 2001: A Space Odyssey!) The afternoon session on Bayesian processing of massive data systems was somehow compulsory since I was talking in this session! While the talks were interestingly diverse, there were however again very people in the room, making me feel the attendance was much lower than last year. As the day ended earlier to let free space to the presidential address, this eventually came as a less exciting day (but left me time for an early evening swim plus two mixers!)… 	 0 Comments
RcppExamples 0.1.1	https://www.r-bloggers.com/2010/08/rcppexamples-0-1-1/	August 2, 2010	Thinking inside the box	"
But at least this package now joins
RcppArmadillo
is using the highly-recommened LinkingTo: Rcpp directive in the
DESCRIPTION file to let R find the
Rcpp
headers, making the build process a little more robust.

 
A few more details on the page are on the
RcppExamples page.


 "	 0 Comments
inline 0.3.6	https://www.r-bloggers.com/2010/08/inline-0-3-6-2/	August 2, 2010	Thinking inside the box		 0 Comments
Integrating R and Gedit	https://www.r-bloggers.com/2010/08/integrating-r-and-gedit/	August 2, 2010	Shige		 0 Comments
useR! 2010 – Local R User Group Panel	https://www.r-bloggers.com/2010/08/user-2010-%e2%80%93-local-r-user-group-panel/	August 2, 2010	drewconway	 	 0 Comments
Advanced debugging techniques in R	https://www.r-bloggers.com/2010/08/advanced-debugging-techniques-in-r-2/	August 2, 2010	drewconway	 	 0 Comments
Map-reduce in R with Amazon EMR	https://www.r-bloggers.com/2010/08/map-reduce-in-r-with-amazon-emr-3/	August 2, 2010	drewconway	 	 0 Comments
On the Palin effect	https://www.r-bloggers.com/2010/08/on-the-palin-effect/	August 2, 2010	David Smith	Do Vice-Presidential candidates have any effect on the outcome of Presidential elections in the US? Some past research suggested their impact was negligible, but the 2008 election seems to have changed all that, at least in the case Sarah Palin. As reported on the Everyday Politics blog, a new paper (co-authored by Revolution Analytics’ CEO Norman Nie) looks at how voter recognition and opinion of vice-presidential candidates influenced the outcome of the election. They conclude: We estimate the “Palin effect,” based on individual-level changes in favorability towards the vice-presidential nominee, and conclude that her campaign performance cost McCain just under 2% of the final vote share. Follow the link below for more discussion about the paper. Everyday Politics: More evidence of the Palin effect 	 0 Comments
Summer hiatus	https://www.r-bloggers.com/2010/08/summer-hiatus/	August 2, 2010	Ken Kleinman		 0 Comments
JSM 2010 [day 1]	https://www.r-bloggers.com/2010/08/jsm-2010-day-1/	August 2, 2010	xi'an	The first day at JSM is always a bit sluggish, as people slowly drip in and get their bearings. Similar to last year in Washington D.C., the meeting takes place in a huge conference centre and thus there is no feeling of overcrowded [so far]. It may also be that the peripheric and foreign location of the meeting put some regular attendees off (not to mention the expensive living costs!). Nonetheless, the Sunday afternoon sessions started with a highly interesting How Fast Can We Compute? How Fast Will We Compute? session organised by Mike West and featuring Steve Scott, Mark Suchard and Qanli Wang. The topic was on parallel processing, either via multiple processors or via GPUS, the later relating to the exciting talk Chris Holmes gave at the Valencia meeting. Steve showed us some code in order to explain how feasible the jump to parallel programming—a point demonstrated by Julien Cornebise and Pierre Jacob after they returned from Valencia—was, while stressing the fact that a lot of the processing in MCMC runs was opened to parallelisation. For instance, data augmentation schemes can allocate the missing data in a parallel way in most problems and the same for independent data likelihood computation. Marc Suchard focussed on GPUs and phylogenetic trees, both of high interest to me!, and he stressed the huge gains—of the order of hundreds in the decrease in computing time—made possible by the exploitation of laptop [Macbook] GPUs. (If I got his example correctly, he seemed to be doing an exact computation of the phylogeny likelihood, not an ABC approximation… Which is quite interesting, if potentially killing one of my main areas of research!) Qanli Wang linked both previous with the example of mixtures with a huge number of components. Plenty of food for thought. I completed the afternoon session with the Student Paper Competition: Bayesian Nonparametric and Semiparametric Methods which was discouragingly empty of participants, with two of the five speakers missing and less than twenty people in the room. (I did not get the point about the competition as to who was ranking those papers. Not the participants apparently!) 	 0 Comments
Summary plots	https://www.r-bloggers.com/2010/08/summary-plots/	August 2, 2010	respiratoryclub	"So, when you first look at some data, it’s helpful to get a feel of it.  One way to do this is to do a plot or two.   I’ve found myself continuously doing the same series of plots for different datasets, so in the end I wrote this short code to put all the plots together as a time saving device.  Not pretty, but gets the job done.   The output looks like this:
 So on the top a histogram with a normal distribution plot.  On the right a QQ normal plot, with an Anderson Darling p value.  Then in the middle on the left is the same data put into different numbers of bins, to see how this affects the look of the data.  And on the right, we pretend that each value is the next one in a time series with equal time intervals between readings, and plot these.  Below this is the ACF and PACF plots.   Hope someone else finds this useful.  If there’s easier ways to do this, let me know.  To use the code – put your data into a text file as a series of numbers called data.txt in the working directory, and run this code:  "	 0 Comments
Using xBalance with MatchIt	https://www.r-bloggers.com/2010/08/using-xbalance-with-matchit/	August 1, 2010	Mark Fredrickson	In a previous post, I demonstrated how to create a propensity score matching, test balance, and analyze the outcome variable using the optmatch and RItools packages. The same strategy can be used with other matching algorithms, for example the various methods included in the MatchIt package. I’ll use the same basic question and data from my previous article. The MatchIt package wraps optmatch to provide its “full” and “optimal” matching methods, so I will the “full” option to maintain consistency with my previous article. The first step is loading the packages and the data: The interface for MatchIt is similar to optmatch for propensity score matches, except that the matchit() function compresses the process into a single step of specifying the propensity formula and producing the match, while fullmatch() allows a user to specify any number of distance matrices. In the end, the interface is fairly similar. As with the previous article, I match on a subset of the covariates. The all.mit object contains (among other items) a vector indicating each object’s matched set. For compatibility, save it as a factor: Unsurprisingly, as MatchIt uses optmatch the two matches are identical. Now that I have a factor listing the groups, I can run xBalance to assess the balance properties of the match: With a reported p-value of 0.82, there is little evidence against the null of balance, so we would fail to reject it. This walk through used the the “full” method for matchit(), but the same techniques will work with other matchit() methods, such as coarsened exact matching or nearest neighbor. If you are reasonably confident that you wish to use optimal matching, you should consider using the optmatch package directly, instead of using it through MatchIt. In future posts I will be demonstrating important techniques to speed up the matching process (which can be a great benefit to large datasets) and how you can create matches that incorporate more subject matter information than can be included in a simple logit model. 	 0 Comments
Margin Constraints with LSPM	https://www.r-bloggers.com/2010/08/margin-constraints-with-lspm/	August 1, 2010	Joshua Ulrich	"
 "	 0 Comments
NetLogo-R-Extension	https://www.r-bloggers.com/2010/08/netlogo-r-extension/	August 1, 2010	Shige		 0 Comments
R2HTML Package	https://www.r-bloggers.com/2010/08/r2html-package/	August 1, 2010	fernandohrosa	R2HTML is an R package, written and maintained by Eric Lecoutre  currently maintained by me. It provides functions and methods to output different R objects to HTML files. It also has a Sweave driver to allow parsing of HTML files containing R code and automatically write the corresponding outputs. Download source from CRAN: R2HTML_1.59.tar.gz. This package is being maintained on Sourceforge.net now. Please check its project page on http://sourceforge.net/projects/r2html/ for latest development snapshots and package releases. Anonymous SVN access is provided for read-only via Sourceforge.net now. The repository address is: https://r2html.svn.sourceforge.net/svnroot/r2html. To checkout a working copy of the current development version, try: The following RNews article by Eric Lecoutre is a good introduction to the package capabilities:   	 0 Comments
Revolution at JSM	https://www.r-bloggers.com/2010/08/revolution-at-jsm/	August 1, 2010	David Smith	If you’re in Vancouver for the Joint Statistics Meetings, come by and meet the Revolution team. (Revolution is a Silver Sponsor of JSM.) We’re at Booth 527 of the Exhibition Hall (near the poster displays) showing the new and upcoming features of Revolution R, and we’ve got lots of R-related goodies to give away. If you’re new to R, want to learn about the enhancements in Revolution R, or just want to get together with other R users over drinks and snacks, why not come along to our “Introduction to Revolution R” mixer? I’ll give a short presentation about R and Revolution R at 4PM on Wednesday in Ballroom A of the Fairmont Waterfront Hotel, and then it’s happy hour through 6PM. There’s also a special gift for the first 100 guests. On Wednesday, Revolution’s Bill Constantine will be giving a talk about a joint project with Merck, creating an open-source GUI for group-sequential trial design with the gsdesign package. I’ll also be attending the ASA Statistical Computing mixer on Monday night. This isn’t a Revolution event (we’re just providing a door prize) but it’s another great place to meet other R users. I hope to see you there — come along and say hi! Revolution Events: JSM 2010 	 0 Comments
Benford’s Law Tests for Wikileaks Data	https://www.r-bloggers.com/2010/08/benford%e2%80%99s-law-tests-for-wikileaks-data/	August 1, 2010	Drew Conway	In my first post on the WL Afghanistan data I provided a very high-level view of the data, and found that it generally met expectations for frequency given its context and presumed data generating process.  Next, I will look a bit deeper at this process and test if the observed frequencies of reports have properties consistent with a natural data generating process.  I will be using Benford’s Law to test if the leading digit of weekly report counts follow’s Benford’s distribution.  Benford’s Law is often used to test for fraud or tampering in count data.  In fact, two professor’s in the Politics department at NYU used the test to uncover fraud in the 2009 Iranian presidential election. Rather than vote counts, however, I will be counting the number of reports observed every week in the data set, which amounts to 318 weeks of count data. This is of particular interest to this data because we may be able to provide evidence that the data were altered from their original collection.  After the jump is a visualization of this test for the total data set, but before proceeding there are two important things to keep in mind.  First, this is not the most straightforward application of Benford’s Law, as the data had to be compacted and counted to get into a suitable form (e.g., split into weeks, etc.).  Second, given that these date are leaked intelligence reports, we should expect there to be so degree of selection, but the test is not able to show where that selection occurred—only that it did.   Using the weekly time-slices as counts, plots shows some tampering or selection may have occurred.  The Pr(1) in the observed data set is much lower than the theoretical expectation provided by Benford, and moving forward along the x-axis the observed data slowly return to the theoretical expectation.  Also, as suggested in the comments, I used a chi-square goodness-of-fit test to see if the deviation is statistically significant, but it was not; with a p-value of 0.2303.  Meaning we would fail to reject the null hypothesis: the observed data were a good fit for a Benford process.  That said, the p-value is not so large as to suggest total adherence. Also, the above analysis does not provide insight into where that deviation occurs within the data.  Also, One way to investigate the later question would be to split the data out by region, and then re-run the test.  This might help isolate where the tampering occurred, as would be the case if the test were being applied to vote counts by precinct, etc.  Below is a visualization of this more detailed test.  This test is much more revealing.  Here we can see that many of the regions follow Benford’s law very closely—particularly RC SOUTH and RC WEST.  There are, however, slight deviations in the RC EAST and UNKNOWN.  Though not nearly as strong a deviation as the total set, the observation from RC is interesting given that we know from the previous analysis that this is also the area with the heaviest reporting volume.  The chi-square test for these data also reject the null hypothesis. Overall, this test does not provide strong evidence for tampering with the data, but it does indicate that some may have occurred, perhaps disproportionately in data from RC EAST.  Finally, I have opened a Git repository for this analysis, so you may go there to see how these (and previous/future) analyses were performed. 	 0 Comments
Managing a statistical analysis project – guidelines and best practices	https://www.r-bloggers.com/2010/09/managing-a-statistical-analysis-project-guidelines-and-best-practices-2/	September 30, 2010	Vinh Nguyen	 Had to share this link today as I better read all the content it refers to and incorporate a lot of the recommended practices into my work flow. Thanks Tal Galili for compiling all those information.  	 0 Comments
Mandelbrot Set, evolved	https://www.r-bloggers.com/2010/09/mandelbrot-set-evolved/	September 30, 2010	David Smith	"The Mandelbrot Set is perhaps the most famous fractal of all time. It's simple in its definition: iterate the complex equation zn+1 = zn2 + c (starting with z0 = 0) for various values of c, and if doesn't go to infinity then c is part of the Mandelbrot Set. The result, however, is amazingly complex. Thinking of c as defining a 2-dimensional area, the boundary of set (between where z does and does not zoom to infinity) is infinitely crenellated. And if you decorate the interior of the set with colors defined by the absolute value of z after N iterations, beautiful patterns emerge. Because complex numbers are natural data types in the R language, it's easy to generate the Mandelbrot Set in just a few lines of code. And it's very quick, because each iteration is done over an entire grid of c values in a single statement. The code can be found in R's Wikipedia page (and reproduced after the jump). You've probably seen the Mandelbrot set before, but you may never have seen how it evolves from one iteration to the next. The R code below shows us by using the caTools package to creare an animated GIF. 
  Beautiful. Wikipedia: R Programming Language, Example 2    "	 0 Comments
Plotting Time Series data using ggplot2	https://www.r-bloggers.com/2010/09/plotting-time-series-data-using-ggplot2/	September 30, 2010	Ralph	There are various ways to plot data that is represented by a time series in R. The ggplot2 package has scales that can handle dates reasonably easily. Fast Tube by Casper As an example consider a data set on the number of views of the you tube channel ramstatvid. A short snippet of the data is shown here: The ggplot function is used by specifying a data frame and the aes maps the Date to the x-axis and the number of Views to the y-axis. The axis labels for the Date variable are created with the scale_x_date function where the format is specified as a Month/Year combination with the %b and %Y formatting strings. The graph that is produced is shown here: Time Series Plot Example with ggplot2 package Other useful resources are provided on the Supplementary Material page. 	 0 Comments
Moshtemp 5.1	https://www.r-bloggers.com/2010/09/moshtemp-5-1/	September 29, 2010	Steven Mosher	Time for another dump of the entire package. Get the zip file 5.1 in the box to the right. ( shortly). unzip and run the following files if you havent already: 1. downloadall.R 2. setup.R If you are running for the first time, You’ll note I added diagnostics. Sometimes the files get corrupted on download and these files will help me figure out where your problems are 3. If you have a old folder of “functions” from 5.0, replace it. 4. LandOcean.R should be run again to output data for the animation script. New in this release is Animation.R When you run “landocean.r” again it will output a Globe.Rdata object. Animation runs by loading this or ANY raster brick and then creating a html animation of the layers over time. Very simple. The parameters for the animation are set with “ani.options()” then, ani.start() is called, the the actual plots of each layer are drawn in animateWorldMap(). You pass that routine a brick and a color pallete. And everything else just happens. The folder also conatins all the source images as *.jpg. You can turn this into a move by using the tool Imagemagick or on the mac I just loaded the jpeg into iPhoto and exported as a movie. You get a 1320 frame  *.mov file with a 1 second frame rate and on playback you can control the speed. The package Animation actually allows you to “call”   saveMovie() but I wasnt able to get Imagemagick to compile on Osx 10.5. And they only have binaries for 10.6. Project for a rainy day. Next you can run “coolUrban.R” that program looks like this 	 0 Comments
Le Monde puzzle [38]	https://www.r-bloggers.com/2010/09/le-monde%c2%a0puzzle%c2%a038/	September 29, 2010	xi'an	Since I have resumed my R class, I will restart my resolution of Le Monde mathematical puzzles…as they make good exercises for the class. The puzzle this week is not that exciting: Find the four non-zero different digits a,b,c,d such that abcd is equal to the sum of all two digit numbers made by picking without replacement two digits from {a,b,c,d}. The (my) dumb solution is to proceed by enumeration taking advantage of the fact that the sum of all two-digit numbers is (30+4-1) times the sum a+b+c+d, but there is certainly a cleverer way to solve the puzzle (even though past experience has shown that this was not always the case!) 	 0 Comments
NewTwitter design based on a Golden Spiral	https://www.r-bloggers.com/2010/09/newtwitter-design-based-on-a-golden-spiral/	September 29, 2010	David Smith	"I finally got the new version of Twitter yesterday, and it looks great. And that's no accident: according to the designer, the layout of the new Twitter interface is based on the Golden Spiral: 
 You can describe the Golden Spiral by laying consecutive squares in a spiral fashion, each square being smaller than the last by a factor of the Golden Ratio, (1+sqrt(5))/2 or approximately 1.618. As a ratio of a length to a width, the Golden Ratio has been known by artists for millenia as an aesthetically pleasing aspect ratio for rectangular features in works of art (such as the dimensions of a painting). It's also known to mathematicians and statisticians as the number to which ratios of successive members of the Fibonacci sequence converge. In a classic case of using a sledgehammer to crack a walnut, I created a custom iterator in R to represent the Fibonacci sequence (code after the fold) and verified the ratio converges to the Golden Ratio at the R command line: See? Golden ratio. Twitter: @stop  "	 0 Comments
UCLA Statistics: Analyzing Thesis/Dissertation Lengths	https://www.r-bloggers.com/2010/09/ucla-statistics-analyzing-thesisdissertation-lengths/	September 29, 2010	Ryan Rosario	"As I am working on my dissertation and piecing together a mess of notes, code and output, I am wondering to myself “how long is this thing supposed to be?” I am definitely not into this to win the prize for longest dissertation. I just want to say my piece, make my point and move on. I’ve heard that the shortest dissertation in my program was 40 pages (not true). I heard someone from another school that their dissertation was over 300 pages. I am not holding myself to a strict limit, but I wanted a rough guideline. As a disclaimer, this blog post is more “fun” than “business.” This was just an analysis that I was interested in and felt that it was worth sharing since it combined Python, web scraping, R and ggplot2. It is not meant to be a thorough analysis of dissertation lengths or academic quality of the Department. The UCLA Department of Statistics publishes most of its M.S. theses and Ph.D. dissertations on a website. It is not complete, especially for the earlier years, but it is a good enough population for my use.  Using this web page, I was able to extract information about each thesis submitted for publishing on this website: advisor name, work title, year completed, and level (M.S. or Ph.D.). Student name was removed for some anonymity, although anyone can easily perform this analysis manually. The scraping part was easy enough but was only half the battle. I also had to somehow extract the length of each manuscript. To do this, I visited the directory for each manuscript (organized by paper ID number), downloaded it to a temporary directory, and used the Python library pyPdf to extract the number of pages in the document. I must note that the number of pages returned by pyPdf is the number of raw pages in the PDF document, not the number of pages of writing excluding references, appendices, figures etc. I also manually corrected inconsistencies, such as name formatting, use of nicknames, and mispellings. For example, “Thomas Ferguson” was standardized to “Thomas S. Ferguson.” In the event that two advisor names were given, only the full time Statistics professor’s name was retained. If both were names were full time Statistics faculty members, only the first one was chosen. Sorry about that. Naturally, I wanted to use a plot to see the distribution of thesis and dissertation lengths, but the one produced by base graphics was terrible: This hideous graphic gives rise to some questions… Although I respect the field of visualization, I am not huge on it, and I am usually content with the basics. This is one case where I had to step up my viz a notch. I had not used ggplot2 so there was no better time to learn. I will not attempt to explain what I am doing with the graphics, as there are already plenty of tutorials and write-ups from experts on the matter. Just look and be amazed…or just look. I wanted to give ggplot2 a spin, so I whipped this up as an example. Wow! Now it is obvious what each bar represents, and we can easily see the difference in lengths of M.S. theses and Ph.D. dissertations. We can easily see that M.S. theses were typically around 50 pages, and Ph.D. dissertations were typically about 110 pages with a long right tail. We can also see what tick labels represent, and the mesh grid gives a visual clue as to what the intermediate tick labels would be. We also see that there were two M.S. theses that was unusually long at 135 and 140 pages respectively. Their titles were Time Series Analysis of Air Pollution in the City of Bakersfield, California and Analysis of Interstate Highway 5 Hourly Traffic via Functional Linear Models, respectively. If you are from California, you can imagine why.  We can see that there is not much variance among lengths of Masters theses and much higher variance for Ph.D. dissertations. I hypothesized that there was an advisor and year effect. Based on hearsay, I had an idea of which advisors yielded the longest and shortest dissertations. My hunch does in fact appear to be true, but I am withholding those results. What I will say is that there does not seem to a be a “pattern.” It does not seem that the more accomplished professors yield longer (or shorter) dissertations. It also does not seem that certain fields, like Vision or Genetics, yield longer or shorter dissertations as a group. The following is a boxplot of the length of Ph.D. dissertations for your entertainment. But how has the length of dissertations changed over time? Or has it not?   This plot is beautiful, and interesting. It seems to suggest that overall, the mean length of a dissertation fell sharply between 1996-2000. However, there is a sample size effect here and there is not enough information to claim that there was in fact a drop during this period. If there in fact was a decrease in dissertation length, there could be several reasons. The Department became independent from the Department of Mathematics in 1998. Perhaps the academic climate was changing and dissertations were becoming shorter. Or, it could be that the Department of Mathematics historically had longer dissertations, and once the Department split off, its requirements waned from those of Mathematics. I bold the word mean because a better statistic here is the median since dissertation lengths do not follow a normal distribution; rather, they follow a right skewed distribution. Still though, using the median does not account for the sample size effect. From 2000 to 2006, dissertation lengths seemed to have leveled off. Then from 2006 to 2010, it appears that dissertation lengths increased. Not so fast though! Note that the number of dissertations filed from 2006-2010 is much larger than those submitted in other equivalent length periods of time — this bump is likely due to the number of observations. Based on my understanding of Department history, I believe that there probably was a decrease through the early years of the program as the Department established its own separate expectations. This may hold practically, but does it hold statistically? The geom_smooth() adds a curve to the plot representing a moving average over the data. It is not a trend line! geom_smooth() also adds some type of margin of error around this smoothing line (I admit that I have not looked deeply into the internals of ggplot2). If we interpret the margin of error loosely as a confidence interval, we can make a statistical conclusion of this graph. Recall that a basic one-sample confidence interval with population standard deviation known is  If we are a given a value  and it falls within the confidence interval, we must conclude that the true parameter  could possibly be . Take  pages. If we take the shaded region to be a confidence interval around  then we see that it is possible that  pages throughout the time period I studied. To make a long story short, it is possible that the length of dissertations has remained constant over time. So what is the purpose of this analysis? There is no purpose. It was just my curiosity, and thought that some of the coding was worth sharing.  With that said, after this extensive analysis, my goal is 110-115 pages. Some open questions for readers: Scraper code:
 "	 0 Comments
A Visualization of Soil Taxonomy Down to the Subgroup Level	https://www.r-bloggers.com/2010/09/a-visualization-of-soil-taxonomy-down-to-the-subgroup-level/	September 29, 2010	dylan	It turns out that you can generate a quasi-numerical distance between soil profiles classified according to Soil Taxonomy (or any other hierarchical system) using Gower’s generalized dissimilarity metric. For example, taxonomic distances computed from subgroup membership are based on the number of matches at the order, suborder, greatgroup, and subgroup level. This approach allows for the derivation of a quasi-numerical classification system from Soil Taxonomy, but it is severly limited by the fact that each split in the hierarchy is given equal weight. In other words, the quasi-numerical dissimilarity associated with divergence at the soil order level is identical to that associated with divergence at the subgroup level. Clearly this is not ideal. Gower’s generalized dissimilarity metric is conveniently implemented in the cluster package for R. I have posted some related material in the past, but left out some of the details regarding which clustering algorithms produce the most useful dendrograms. Divisive clustering best represents the step-wise splits within the hierarchy of Soil Taxonomy, as expressed in terms of pair-wise dissimilarities. Code examples are below, along with the data used to generate the figure of California subgroups. Discontinuities in figure below are caused by errors in the underlying data, e.g. mis-matches in soil order vs. suborder membership. Subgroups from California read more 	 0 Comments
Cooling stations. A UHI Hint	https://www.r-bloggers.com/2010/09/cooling-stations-a-uhi-hint/	September 29, 2010	Steven Mosher	Update: google earth files in the box: Personally I like to look at things backwards. Why are cool sites cool? So download the kml or kmz file and you can tour 62 sites: All with 90 years of data or more. All with a cooling trend. And all “supposedly” urban. what do you see at the meso scale. Anyway’s The next drop will have the animation code, the kml code, file download diagnostics, and a script to replicate the cool urban stations. Let’s recap where we are. I went in search of the the biggest warming trend and biggest cooling trend in the data.( for an entirely different reason ) And after restricting our view to stations with long records, stations with 90 years of data in the period 1900 to 2009, we landed on this distribution of trends. The trend per decade. Again, just looking at the distribution gives us some information. We’re seeing what appears “somewhat” normal, Just a quick look at the density,ECDF and QQ. Density ecdf QQplot By eye I was wondering if the trends were a gamma –err prolly not–  or log normal. A gamma sorta makes sense if one considers that a warming rate is going to take a while to appear. At some locations the “wait” time will be shorter while at other places the “wait” time will be longer. However, I didnt do any formal testing on this other than looking at the QQ– shrugs– just an observation to maybe come back to. As we see we have 1492 stations. Looking at the metadata for urban/small town/ rural we have this: Which shows us that  for stations with long records over half of them are “rural” by the designation in the inventory. Now, of course, that designation has it flaws, but this is just exploratory data analysis. The next step I took what to isolate just the stations that had negative warming. otherwise known as cooling. And  we pull up the most extreme case: Id                                              Name    Lat     Lon Altitude   Rural 4682 42572681004                  KETCHUM RS  43.68 -114.35          R  I noted a couple things. It’s a rural site in a Mountain valley. But seeing that drop in the later years and what may be a discontinuity ( undocumented station change) , I moved on to next station  42572438001      OOLITIC PURDUE EXP FM   38.88  -86.55      198      175     R A couple things: What we know from the distribution of all stations is that rural stations constitute half of the sample. Now, on the supposition that  there is no UHI, that rural and urban see the same warming over 110 years of data one can expect this. One can expect that the sample of cooling stations drawn from the whole sample will have the same distribution of urban/small town/rural: roughly 50:25:25. So, when the second station I drew from the far end of the distribution was also rural, well thats like two heads in a row. Nothing special, but how long a streak would I get? Well, you have to look at 32 stations ( sorted from coolest to warmest) before you get to an urban station. hmm. And the other thing that was striking was this. That station ALSO happens to be the first NON US station. go figure: On one hand the US stations tend to have longer records so I can expect a lot of US stations. Still the number of us stations in the “cool” distribution seemed worthy of investigation ( later work if somebody wants to play) 61111518000                PRAHA RUZYNE  50.10   14.25  View Larger Map A couple points that I have discussed about UHI. UHI  results from a few things.  Chief among them is the disruption of the boundary layer that results from building tall buildings. And of course changes in the surface properties and lastly waste heat, probably the least important. Population is not a precise measure of any of these. So here we have a site probably at the airport with a clear fetch all around. We do not have an urban environment with tall buildings. We also see that airports are not categorically bad.  There is another factor as well that Oke mentions that few have picked up on. That is the difference in wetness between the urban landscape and the surrounding rural environment. More on that later. lets hit the next “Urban site” 42572216003                  ALBANY 3SE  31.53  -84.13 Map And the next: 12567083000                ANTANANARIVO -18.80   47.48  Map So in the top 50 cooling sites, here is what we have: 3  ”urban” sites. 48 sites from the US.  At those “urban” sites we have two airports with what appear to be long  fetches. If you have a long fetch, your UHI is going to be minimized. If you have buildings destroying that fetch, you have some of the preconditions to generate UHI. That’s why, for example, I think some of the concerns about waste heat at airports are potentially flawed. And one final note. Note the lake. More on that at a future date. So sum up the little exploration of the data, I’ll leave you with something to chew on: recall that the 1500 or so stations with long records were split  50:25:25:  Rural:small town: urban. When we  segregate the data by trend and look at cooling stations, is this structure preserved: > table(cold90Inv$Rural) R   S   U 308 143  62 Nope:  Is it significant? Does it actually indicate anything? but it is interesting that when we look at the long records, and the cooling stations within those long records that urban sites are very few in number. Are they Actually urban? Nobody seems to ask questions like that. Partially that’s because people don’t understand everything that goes into UHI. Also because they tend to be mesmirized by the close up shot which focuses on waste heat or surface material. They often forget the bigger meso scale picture. That tarmac, sitting on the coast of an ocean has a long clean fetch and most students of Oke or Parker know what wind does to Tmin. Next up, I will need to re integrate some KML code and we can take a tour of these 62 ‘urban’ stations and all the ‘cooling’ stations. On the ground what do they have in common. Not in the “metadata”. on the ground. 	 0 Comments
Mike’s and my book is coming out	https://www.r-bloggers.com/2010/09/mikes-and-my-book-is-coming-out/	September 29, 2010	Shravan Vasishth		 0 Comments
Some Oddities with cooling stations	https://www.r-bloggers.com/2010/09/some-oddities-with-cooling-stations/	September 29, 2010	Steven Mosher	Now, that  the whole analysis has been moved to raster, I took some time to play around with a question that has interested  a couple of people. Cool stations. A while back when I was looking at ways of bounding uncertainties in the record I went on a hunt for the station that cooled the most and the station that warmed the most. A few weeks later Verity and Tonyb did a post on cooling stations. So, I started down the path again basically to understand how prevalent the ‘cooling stations” are and if there is anything special or unique about them. Now, the simplistic way to think about this problem is that in a warming world every place has to get warmer. Well, that’s  just plain common sense. or is it? We can put this question differently. if the average  goes up by say .8C in 110 year period how can ANY site see a negative trend? And if we find them, what does that mean? That’s an open question. So I started out down that path, and what I found makes me scratch my head. One way of looking at what I found is this: UHI may play a role.  I’m just going to point out the issue or oddity I found and ponder on it. Its definitely not conclusive, but I did scratch my head and wonder about the significance of this.  So this is notebook scriblings. We start by simply looking at the distribution of all trends. For this exercise I’m not correcting for any autocorrelation, I’m basically on a fishing expeditions for the highest and lowest trends. Looks like this  That’s a monthly slope. And without testing that distribution you should be able to see a couple things: it’s peaky with long tails and the mean is going to be positive. Also note that you have some very large warming trends and cooling trends. Since the ends can be revealing, I sorted all the stations by trend and looked through them all. some 5000 charts. The coolest: 40678349000  SANCTI SPIRIT  21.93 -79.45 Not much of a mystery there. In fact, going through the extremes you will find that nearly all of the extremes are these short records. one of the warmest: 40371881001  ROBB RSAL 53.23 -116.97  Now, there is a whole separate issue with these short records, leave that aside. So Instead, I went looking at long records. Record that have over 1080 months of data from 1900 to 2009. Why 90 years? No real justification, I’m just exploring. Anyway, as we can expect the distribution shifts. Below see a distribution of the decadal trends for all long stations. While eliminating short runs changes the shape of the distributions and shifts it positive we STILL see stations that cool. random chance? or is there something different about them. Think on that, More tommorrow  	 0 Comments
A texteditor for R	https://www.r-bloggers.com/2010/09/a-texteditor-for-r/	September 29, 2010	Rob J Hyndman	I’ve been using RWinEdt for the last few years for all my R coding. But it no longer works with WinEdt 6.0 and no update has been forthcoming. Consequently, I’ve been looking around for something similar to take its place. This question has been asked before on StackOverflow and many suggestions were made including popular unix editors such as Vim and Emacs, and the programmers IDE Eclipse with the StatET plugin. However, these all have a relatively steep learning curve and I’d prefer something simpler and more light-weight. Others preferred the Windows IDE Tinn-R, but again, I really just want a text editor not some program that is going to take over my R sessions for me. Instead, I’ve decided to switch to Notepad++ with NppToR. Notepad++ is a general text editor with all the functionality you would expect from a good text editor. It handles almost every species of text file and has colour syntax coding for all the common file types. With the NppToR plugin, there is syntax highlighting, code folding, and auto-completion for R code also. It also integrates nicely with R: just hit F8 to send the current line or selection to R, or Ctrl-F8 to send the whole file. The code-folding is a nice addition that RWinEdt did not provide. It makes it easier to work on larger files as I can hide away functions, loops and conditionally executed code in order to see only the parts of the code that I am currently working on. There is a nice introduction to NppToR in the R Journal for June 2010 (starting page 62), written by the developer, Andrew Redd. I’m not sure if Uwe Ligges is going to produce an updated RWinEdt for use with WinEdt 6.0, but I don’t think I will use it in any case. NppToR is just what I need.   	 0 Comments
Forecasting with long seasonal periods	https://www.r-bloggers.com/2010/09/forecasting-with-long-seasonal-periods/	September 28, 2010	Rob J Hyndman	I am often asked how to fit an ARIMA or ETS model with data having a long seasonal period such as 365 for daily data or 48 for half-hourly data. Generally, seasonal versions of ARIMA and ETS models are designed for shorter periods such as 12 for monthly data or 4 for quarterly data. The ets() function in the forecast package restricts seasonality to be a maximum period of 24 to allow hourly data but not data with a larger seasonal frequency. The problem is that there are  parameters to be estimated for the initial seasonal states where  is the seasonal period. So for large , the estimation becomes almost impossible. The arima() function will allow a seasonal period up to  but in practice will usually run out of memory whenever the seasonal period is more than about 200. I haven’t dug into the code to find out why this is the case — theoretically it would be possible to have any length of seasonality as the number of parameters to be estimated does not depend on the seasonal order. However, seasonal differencing of very high order does not make a lot of sense — for daily data it involves comparing what happened today with what happened exactly a year ago and there is no constraint that the seasonal pattern is smooth. For such data I prefer a Fourier series approach where the seasonal pattern is modelled using Fourier terms with short-term time series dynamics allowed in the error. For example, consider the following model:  where  is an ARIMA process. The value of  can be chosen by minimizing the AIC. This is easily fitted using R. Here is an example with ,  and  an ARIMA(2,0,1) process: The advantages of this approach are: The only real disadvantage (compared to a seasonal ARIMA model) that I can think of is that the seasonality is assumed to be fixed — the pattern is not allowed to change over time. But in practice, seasonality is usually remarkably constant so this is not a big disadvantage except for very long time series. The order of  can also be chosen automatically: Note that the seasonal parts of the ARIMA model should all be set to zero as we do not want  to be a seasonal ARIMA model. I’ll add the fourier() function to the forecast package on the next release. It is much harder to do something like this for ETS models. One of our students has been working on this and our paper on complex seasonality describes the procedure. We plan to add this functionality to the forecast package soon, but the code is not quite ready yet.   	 0 Comments
Google Summer of Code advances R	https://www.r-bloggers.com/2010/09/google-summer-of-code-advances-r/	September 28, 2010	David Smith	For the third year running, the Google Summer of Code program has sponsored a number of students working with R, and has again resulted in several new contributions expanding R in various fields. Dirk Eddelbuettel, who coordinated the R-related projects for GSoC in 2010, summarized the results, with details about the new packages now available for R thanks to the students and their mentors:  For more information about these projects, and links to download the new packages, check Dirk's post linked below. Thinking Inside the Box: R Project and Google Summer of Code: Wrapping up    	 0 Comments
Example 8.7: Hosmer and Lemeshow goodness-of-fit	https://www.r-bloggers.com/2010/09/example-8-7-hosmer-and-lemeshow-goodness-of-fit/	September 28, 2010	Ken Kleinman		 0 Comments
A Slider to Control Two Plotting Windows	https://www.r-bloggers.com/2010/09/a-slider-to-control-two-plotting-windows/	September 28, 2010	Yihui Xie	One of my readers asked two weeks ago how to control two graphics windows with the slider in gWidgets. Here is a simple example: The basic idea is to use dev.set() function to set focus on different graphics devices. After one plot has been drawn, go to the next device to draw the next plot. That’s it. 	 0 Comments
What is wrong with this graph?	https://www.r-bloggers.com/2010/09/what-is-wrong-with-this-graph/	September 28, 2010	Social data blog	"
 
 This is a graph from our surveys with the University of Munich in B&H about ten years ago.(Click on it to see a large version.) In previous presentations (but not in peer-reviewed journals) I have talked about the peak on the subscale of Paranoid Ideation for the B&H samples. But looking at it again I see that this is partly an illusion and it is due to the fact that I broke the rule of never implying a continuous scale (the x-axis) where there isn’t one. Paranoid ideation only looks so high because it *happens* to be squeezed in between two unrelated scales which happen to have lower scores. Permalink 

	| Leave a comment  »
 "	 0 Comments
"Some hints for the R Beginner: Avoiding ""blank screen syndrome"""	https://www.r-bloggers.com/2010/09/some-hints-for-the-r-beginner-avoiding-blank-screen-syndrome/	September 27, 2010	David Smith	For new users, starting R for the first time can be a little daunting. Luckily, Patrick Burns of Burns Statistics has put together a handy tutorial to help the first-time R user get beyond “blank screen syndrome“:  So you have successfully started R on your machine. Here's where the trouble sometimes starts — there's a big, huge prompt daring you to do something.  Some Hints for the R Beginner is a tutorial for the beginning R user, starting from the basics of interacting with that prompt for the first time. Here, you'll learn about the key object types and actions you can do with them (assignment, reading data, inspecting objects), and how to recover from (and learn from) errors. You'll also get introductions into graphics and object-oriented programming. If you're coming to R from another language like SAS or SPSS, there are hints to help you transition, and there are handy pointers to other helpful resources as well. It's all available as an on-line tutorial.  (By the way, if you're already confortable with the R prompt, check out Patrick's most famous R document, The R Inferno.) Burns Statistics: Some Hints for the R Beginner 	 0 Comments
Galton & simulation	https://www.r-bloggers.com/2010/09/galton-simulation/	September 27, 2010	xi'an	Stephen Stigler has written a paper in the Journal of the Royal Statistical Society Series A on Francis Galton’s analysis of (his cousin) Charles Darwin’ Origin of Species, leading to nothing less than Bayesian analysis and accept-reject algorithms! “On September 10th, 1885, Francis Galton ushered in a new era of  Statistical Enlightenment with an address to the British Association for  the Advancement of Science in Aberdeen. In the process of solving a  puzzle that had lain dormant in Darwin’s Origin of Species,  Galton introduced multivariate analysis and paved the way towards modern  Bayesian statistics. The background to this work is recounted,  including the recognition of a failed attempt by Galton in 1877 as  providing the first use of a rejection sampling algorithm for the  simulation of a posterior distribution, and the first appearance of a  proper Bayesian analysis for the normal distribution.” The point of interest is that Galton proposes through his (multi-stage) quincunx apparatus a way to simulate from the posterior of a normal mean (here is an R link to the original quincunx). This quincunx has a vertical screen at the second level that acts as a way to physically incorporate the likelihood (it also translates the fact that the likelihood is in another “orthogonal” space, compared  with the prior!): “Take another look at Galton’s discarded 1877 model for natural selection (Fig. 6). It is nothing less that a workable simulation algorithm for taking a normal prior (the top level) and a normal likelihood (the natural selection vertical screen) and finding a normal posterior (the lower level, including the rescaling as a probability density with the thin front compartment of uniform thickness).” Besides a simulation machinery (steampunk Monte Carlo?!), it also offers the enormous appeal of proposing the derivation of the normal-normal posterior for the very first time: “Galton was not thinking in explicit Bayesian terms, of course, but mathematically he has posterior . This may be the earliest appearance of this calculation; the now standard derivation of a posterior distribution in a normal setting with a proper normal prior. Galton gave the general version of this result as part of his 1885 development, but the 1877 version can be seen as an algorithm employing rejection sampling that could be used for the generation of values from a posterior distribution. If we replace  above by the density , his algorithm would generate the posterior distribution of Y given X=a, namely . The assumption of normality is of course needed for the particular formulae here, but as an algorithm the normality is not essential; posterior values for any prior and any location parameter likelihood could in principle be generated by extending this algorithm.” This historical entry is furthermore interesting at another (anecdotal) level in that it is reminding me of a visit I made as a teenager to the Birmingham museum of Sciences where I saw a quincunx for the first time and got fairly intrigued by the stable law exhibited therein… (In a completely unrelated manner, let me point out the excellent pastiche of Dickensian dark novels called The Quincunx by Charles Palliser.) 	 0 Comments
Mean reverting strategies and volatility	https://www.r-bloggers.com/2010/09/mean-reverting-strategies-and-volatility/	September 27, 2010	kafka	"Mean reverting strategies are beating on mean reversion of the prices. There are various flavors of mean reverting strategies, but as a proxy I chose RSI(2). You can find many entries on blogosphere about this strategy, but nowadays its popularity dried up. What made me wondering is that there was an idea about correlation between return of such strategy and market volatility. That means, that during high volatility periods this strategy produces higher return and during low volatility -lower. To test this idea, I built two strategies. RSI Simple goes long, if RSI(2) indicator is below 10 or it goes short, if RSi(2) is above 90. It closes the open position, then indicator is above/below 70/30 respectively.
RSI Garch follows the same rules as RSI Simple, except one additional rule. By using Garch model I forecast volatility for next day. If  volatility value is greater that 65 % of values during the year (252 days), then order is executed. By adding this filter I can catch most volatile days of the last year. The result of Garch filter are slightly better, but the question remains – is it worth of adding it? Next thing I tried to look at correlation between volatility of the market and return. Actually, it was not so trivial to implement that. The problem is the duration of the investment, which is not fix in days (it can take between 1 to 19 days to close a position). So, I had two approaches- either measure volatility at the beginning of the investment or to fix max investment time (for example 5 days) and then measure volatility on the last day of investment. Despite the differences the results are similar and I will present former.
As you can see from the graph below there’s correlation between return of RSI(2) strategy and volatility of the last 20 days. However, I would attribute this correlation to the fact, that return (profit or loss) tend to be higher, then volatility is higher. This thought is supported by R^2, which was only 0.1.  You can see return ranges against volatility ranges on the next graph. As in the first example above, I can’t see hard evidence of the correlation.  Summary
Volatility filter can slightly improve return of RSI(2) strategy, but it is not significant. In the future I will run the same test on pairs trading strategy (another flavor of mean reverting strategies). "	 0 Comments
How to send an HTTP PUT request from R	https://www.r-bloggers.com/2010/09/how-to-send-an-http-put-request-from-r/	September 27, 2010	Christopher Bare	I wanted to get R talking to CouchDB. CouchDB is a NoSQL database that stores JSON documents and exposes a ReSTful API over HTTP. So, I needed to issue the basic HTTP requests: GET, POST, PUT, and DELETE from within R. Specifically, to get started, I wanted to add documents to the database using PUT. There’s CRAN package called httpRequest, which I thought would do the trick. This wound up being a dead end. There’s a better way. Skip to the RCurl section unless you want to snicker at my hapless flailing. The httpRequest package is very incomplete, which is fair enough for a package at version 0.0.8. They implement only basic get and post and multipart post. Both post methods seem to expect name/value pairs in the body of the POST, whereas accessing web services typically requires XML or JSON in the request body. And, if I’m interpreting the HTTP spec right, these methods mishandle termination of response bodies. Given this shaky foundation to start with, I implemented my own PUT function. While I eventually got it working for my specific purpose, I don’t recommend going that route. HTTP, especially 1.1, is a complex protocol and implementing it is tricky. As I said, I believe the httpRequest methods, which send HTTP/1.1 in their request headers, get it wrong. Specifically, they read the HTTP response with a loop like one of the following: Notice that they’re counting on a blank line, a zero followed by a blank line or the server closing the connection to signal the end of the response body. I dunno where the zero thing comes from or why we should count on it not being broken up during reading. Looking through RFC2616 we find this description of an HTTP message: While the headers section ends with a blank line, the message body is not required to end in anything in particular. The part of the spec that refers to message length lists 5 ways that a message may be terminated, 4 of which are not “server closes connection”. None of them are “a blank line”. HTTP 1.1 was specifically designed this way so web browsers could download a page and all its images using the same open connection. For my PUT implementation, I fell back to HTTP 1.0, where I could at least count on the connection closing at the end of the response. Even then, socket operations in R are confusing, at least for the clueless newbie such as myself. One set of socket operations consists of: make.socket, read.socket/write.socket and close.socket. Of these functions, the R Data Import/Export guide states, “For new projects it is suggested that socket connections are used instead.” OK, socket connections, then. Now we’re looking at: socketConnection, readLines, and writeLines. Actually, tons of IO methods in R can accept connections: readBin/writeBin, readChar/writeChar, cat, scan and the read.table methods among others. At one point, I was trying to use the Content-Length header to properly determine the length of the response body. I would read the header lines using readLines, parse those to find Content-Length, then I tried reading the response body with readChar. By the name, I got the impression that readChar was like readLines but one character at a time. According to some helpful tips I got on the r-help mailing list this is not the case. Apparently, readChars is for binary mode connections, which seems odd to me. I didn’t chase this down any further, so I still don’t know how you would properly use Content-Length with the R socket functions. Falling back to HTTP 1.0, we can just call readLines ’til the server closes the connection. In an amazing, but not recommended, feat of beating a dead horse until you actually get somewhere, I finally came up with the following code, with a couple variations commented out: After all that suffering, which was undoubtedly good for my character, I found an easier way. Duncan Temple Lang's RCurl is an R wrapper for libcurl, which provides robust support for HTTP 1.1. The paper R as a Web Client - the RCurl package lays out a strong case that wrapping an existing C library is a better way to get good HTTP support into R. RCurl works well and seems capable of everything needed to communicate with web services of all kinds. The API, mostly inherited from libcurl, is dense and a little confusing. Even given the docs and paper for RCurl and the docs for libcurl, I don't think I would have figured out PUT. Luckily, at that point I found R4CouchDB, an R package built on RCurl and RJSONIO. R4CouchDB is part of a Google Summer of Code effort, NoSQL interface for R, through which high-level APIs were developed for several NoSQL DBs. Finally, I had stumbled across the answer to my problem. I'm mainly documenting my misadventures here. In the next installment, CouchDB and R we'll see what actually worked. In the meantime, is there a conclusion from all this fumbling? HTTP is so universal that a high quality implementation should be a given for any language. HTTP-based APIs are being used by databases, message queues, and cloud computing services. And let's not forget plain old-fashioned web services. Mining and analyzing these data sources is something lots of people are going to want to do in R. Others have stumbled over similar issues. There are threads on r-help about hanging socket reads, R with CouchDB, and getting R to talk over Stomp. RCurl gets us pretty close. It could use high-level methods for PUT and DELETE and a high-level POST amenable to web-service use cases. More importantly, this stuff needs to be easier to find without sending the clueless noob running down blind alleys. RCurl is greatly superior to httpRequest, but that's not obvious without trying it or looking at the source. At minimum, it would be great to add a section on HTTP and web-services with RCurl to the R Data Import/Output guide. And finally, take it from the fool: trying to role your own HTTP (1.1 especially) is a fool's errand. 	 0 Comments
Riemann, Langevin & Hamilton [reply]	https://www.r-bloggers.com/2010/09/riemann-langevin-hamilton-reply/	September 27, 2010	xi'an	Here is a (prompt!) reply from Mark Girolami corresponding to the earlier post: In preparation for the Read Paper session next month at the RSS, our research group at CREST has collectively read the Girolami and Calderhead paper on Riemann manifold Langevin and Hamiltonian Monte Carlo methods and I hope we will again produce a joint arXiv preprint out of our comments. (The above picture is reproduced from Radford Neal’s talk at JSM 1999 in Baltimore, talk that I remember attending!) Although this only represents my preliminary/early impression on the paper, I have trouble with the Physics connection. Because it involves continuous time events that are not transcribed directly into the simulation process. It might be easier to look at the two attached figures rather than the isocontours in the complete phase space (q, p) of the 1-d Gaussian that is in the pic you include in your post. Both figures relate to sampling from a zero mean bivariate Gaussian with covariance matrix  — marginal variance 1 and cross-covariance  (example taken from Neal 2010). In the first figure there are three panes showing the 20 integration steps obtained by standard HMC where the metric tensor for the space of bivariate Gaussians is NOT employed and so the leftmost plot shows the proposal effectively moving from -1.5, -1 to  1.5, 1.5 basically a large traversal over the 2d sampling space. The middle plot shows the corresponding momentum variable steps and the right plot shows the total Hamiltonian value (energy or negative joint likelihood) where the difference at the start and end of the integration is very small indicating a high probability of acceptance. That is all fine – large proposal step accepted with high probability. Now lets look at the second figure. Lay aside the Physics interpretation and let’s try to adopt a geometric one in this case to see if this helps the understanding. So our task is to simulate from this bivariate Gaussian we know that the metric tensor  defines a flat manifold of bivariate densities. Now if we wanted to move from one point to another we would wish to take the most direct route – the shortest path in the space — the geodesic in other words. The geodesics on a manifold are defined by the second order differential equation in terms of the coordinates (our sample space) and the Christofell symbols of the manifold — so to make a proposed move along the geodesic we need to solve the differential equations describing the geodesic. Now by rewriting the geodesic equation in a different form we end up with Hamiltons equations -— in other words the solution flow of the geodesic equations coincides with the Hamiltonian flows — so solving Hamiltons equations is just another way to solve the geodesic equation. So the variable p just emerges from the rewriting of the geodesic equation and nothing more. In this case as the manifold corresponding to bivariate Gaussians is flat and the metric tensor is the inverse of the covariance matrix — the geodesics will be elliptical paths defined by  — in other words the isocontours of the bivariate Gaussian. So the first panel in this figure shows 15 integration steps and as can be observed the path follows the elliptical isocontour of the target density — as the p variable is the dual in the geodesic equations then it also maps out an isocontour defined by the metric tensor. Does  that alternative explanation of the deterministic proposal mechanism  help?? If we accept the geometric view and wish to make proposals that  follow the geodesics then integrating Hamiltons equations is just another  way to solve the geodesic equations nothing more — so there is no need  to appeal to the Physics interpretation at all — I am considering  actually presenting the method from that perspective at the RSS meeting —  thoughts on that? Overall, trying to take advantage of second order properties of the target, just like the Langevin improvement takes advantage of the first order?is a natural idea which, when implementable, can obviously speed up convergence. This is the Langevin part, which may use a fixed metric M or a local metric defining a Riemann manifold, G(.). So far, so good, assuming the derivation of an observed or expected information G(.) is feasible up to some approximation level. The Hamiltonian part that confuses me introduces a dynamic on level sets of where p is an auxiliary vector of dimension D. Namely. while I understand the purpose of the auxiliary vector, namely to speed up the exploration of the posterior surface by taking advantage of the additional energy provided by p, I fail to understand why the fact that the discretised (Euler) approximation to Hamilton?s equations is not available in closed form is such an issue?. The fact that the (deterministic?) leapfrog integrator is not exact should not matter since this can be corrected by a Metropolis-Hastings step. I think that I have attempted to answer your question about the auxilliary vector p. So the leapfrog integrator is NOT exact — it does not EXACTLY preserve the Hamiltonian — it is second order accurate — and the MH step corrects for the bias in the invariant measure introduced by this error. The integrator does exactly preserve volume elements — so the determinant is not required to appear in the MH ratio — of course this requirement could be relaxed and then we would have to include the Jacobian in the MH ratio — this might be ok to compute. It is also symmetric/reversible so the effective proposal densities do not need to appear in the MH ratio – just the ratio of the joint likelihoods (q and p). In this paper I wanted to make sure everything was exact and formally correct and let further work relax these restrictions and develop more efficient numerics and / or approximations. While the logistic example is mostly a toy problem (where importance sampling works extremely well, as shown in our survey with Jean-Michel Marin), the stochastic volatility is more challenging and the fact that the Hamiltonian scheme applies to the missing data (volatility) as well as to the three parameters of the model is quite interesting. I however wonder at the appeal of this involved scheme when considering that the full conditional of the volatility can be simulated exactly? The LR example is indeed a toy one and it was used just to calibrate and illustrate the various methods with other well known methods — with the SV as with the log-Cox model we chose to sample from the conditionals of latent volatilities given parameters and then parameters given volatilities -—in a Gibbs style scheme — we coud also have sampled jointly or used the exact sampling from the conditional (which I must confess I had not noticed at the time) — the point being that these are illustrative of the methods proposed — I made no claim to optimality of the schemes used — they were used to illustrate the potential of the methodology. Reversibility is a “second-order” property for MCMC algorithms. E.g., the Gibbs sampler is not reversible but does work without that property? In addition, the standard Metropolis-Hastings scheme is reversible by construction (courtesy of the detailed balance property). My point in that post (and in the incoming discussion) is that, similar to Langevin, the continuous time “analogy” may be unnecessary costly in trying to carry this analogy in discrete (algorithmic) time. We are not working in continuous time, the invariance/stability properties of the continuous time process are not carried on to the discretised version of the process, we therefore should not care about reproducing exactly the continuous time process in discrete time. For instance, when considering the Langevin diffusion, the corresponding Langevin algorithm could use another scale for the gradient than the one used for the noise, i.e.rather than the Euler discretisation. A few experiments at the time of the first edition of MCSM (Chapter 6, Section 6.5) showed that a different scale could lead to improvements. Correct — again I wanted to be ‘purer than pure’ in the way this work was presented and whilst indeed different scaling for the drift and the diffusion terms in the Langevin scheme can work and imporve things — I felt that this is follow on work from the ideas presented — for example there are other discretisations of the Langevin SDE that may be more efficient than a simple first order Euler – which I do discuss in the final section.  	 0 Comments
Mumbai 2010: R Course for Finance	https://www.r-bloggers.com/2010/09/mumbai-2010-r-course-for-finance/	September 27, 2010	wuertz		 0 Comments
Maps with ggplot2	https://www.r-bloggers.com/2010/09/maps-with-ggplot2/	September 27, 2010	James	 The ggplot2 package offers powerful tools to plot data in R. The plots are designed to comply with the “grammar of graphics” philosophy and can be produced to a publishable level relatively easily. For users wishing to create a good map without too much thought I would recommend this worksheet. For those without their own shapefiles who rely on the “maps” package they may wish to consult Hadley Wickham‘s ggplot2 book.  Data Requirements: London Sport Participation Shapefile. Download (requires unzipping) poly_coords function. Download Install the following packages (if you haven’t already done so): maptools, RColorBrewer, ggplot2 ﻿ 	 0 Comments
RcppArmadillo 0.2.7	https://www.r-bloggers.com/2010/09/rcpparmadillo-0-2-7/	September 26, 2010	Thinking inside the box	"
The short NEWS file extract follows, also containing Conrad’s entry for 0.9.80::

 
More information is on the 
RcppArmadillo page.
Questions, comments etc should go to the
rcpp-devel mailing list
off the R-Forge page.

 "	 0 Comments
Animation	https://www.r-bloggers.com/2010/09/animation/	September 25, 2010	Steven Mosher	A couple hours work and we now have animations of the global anomalies: Created with the animation package in R.  The code examples were a bit terse about some of the details but after fiddling about I was able to get  the program to output an Html animation complete with java based playback controls. Write if you wanna know how it works and I’ll integrate this code into the  system  	 0 Comments
GotoBLAS2	https://www.r-bloggers.com/2010/09/gotoblas2/	September 25, 2010	Shige		 0 Comments
Visualising questionnaires	https://www.r-bloggers.com/2010/09/visualising-questionnaires/	September 25, 2010	richierocks	Last week I was shown the results of a workplace happiness questionnaire.  There’s a cut down version of the dataset here, with numbers and wording changed to protect the not-so-innocent. The plots were ripe for a makeover.  The ones I saw were 2nd hand photocopies, but I’ve tried to recreate their full glory as closely as possible.  To the creator’s credit, they have at least picked the correct plot-type: a stacked bar chart is infinitely preferable to a pie chart.  That said, there’s a lot of work to be done.  Most obviously, the pointless 3D effect needs removing, and the colour scheme is badly chosen.  Rainbow style colour schemes that change hues are best suited to unordered categorical variables.  If you have some sense of ordering to the variable then a sequential scale is more appropriate.  That means keeping the hue fixed and either scaling from light to dark, or from grey to a saturated colour.  In this case, we have ordering and also a midpoint – the “neutral” response.  That means that we should use a diverging scale (where saturation or brightness increases as you move farther from the mid-point). More problematic than these style issues is that it isn’t easy to answer any useful question about the dataset.  To me, the obvious questions are On balance, are people happy? Which questions indicate the biggest problems? Which sections indicate the biggest problems? All these questions  require us to condense the seven points of data for each question down to a single score, so that we can order the questions from most negative to most positive.  The simplest, most obvious scoring system is a linear one.  We give -3 points for “strongly disagree”, -2 for “disagree”, through to +3 for “strongly agree”.  (In this case, all the questions are phrased so that agreeing is a good thing.  A well designed questionnaire should contain a balance of positively and negatively phrased questions to avoid yes ladder (link is NSFW) type effects.  If you have negatively phrased questions, you’ll need to reverse the scores.  Also notice that each question uses the same multiple choice scale.  If your questions have different numbers of responses, or more than one answer is allowed then it may be inappropriate to compare the questions.) Since the scoring system is slightly arbitrary, it is best practise to check your results under a different scoring system.  Perhaps you think that the “strongly” responses should be more heavily weighted, in which case a quadratic scoring system would be appropriate.  (Replace the weights 1:3 with (1:3)^2/2.)  Assume the data is stored in the data frame dfr. For the rest of this post, I’ll just present results and code for the linear scoring system.  Switch the word “linear” with “quad” to see the alternative results.  To get an ordering from “worst” to “best”, we order by -score.linear. To make the data frame suitable for plotting with ggplot, we reshape it from wide to long format. To answer the first question, we simply take a histogram of the score, and see if they are mostly above or below zero.  Hmm, not good.  Most of the questions had a negative score, implying that the workforce seems unhappy.  Now we want to know why they are unhappy.  Here are the cleaned up versions of those stacked bar charts again.  As well as the style improvements mentioned above, we plot all the questions together, and in order of increasing score (so the problem questions are the first things you read).  So deaf managers are the biggest issue.  Finally, it can be useful to know if which of the sections scored badly, to find more general problem areas.  First we find the mean score by section. Now we visualise these scores as a dotplot.  Here you can see that communication the biggest problem area. 	 0 Comments
Two New R Packages: log4r and SortableHTMLTables	https://www.r-bloggers.com/2010/09/two-new-r-packages-log4r-and-sortablehtmltables/	September 25, 2010	John Myles White	I’ve just released two new packages for R: log4r and SortableHTMLTables. log4r is a minimal logging utility for R that’s inspired by the log4j family of logging tools. It has substantially fewer features than other logging tools for R, but it’s hopefully easier to use. SortableHTMLTables uses brew and the jQuery Tablesorter plugin to provide a function that dumps a data frame out to an HTML document containing a sortable table. You can see an example of the output here. Both packages were submitted to CRAN today and should reach the mirrors over the next few days. 	 0 Comments
Damn Close 5.0	https://www.r-bloggers.com/2010/09/damn-close-5-0/	September 24, 2010	Steven Mosher	Code will be in the drop box in a bit, once I shower:  [DONE] This is a wholesale replacement of previous versions, completely rewritten in raster. It will be the base going forward. All of the analysis routines will be rewritten using raster.  For time series functionality I will continue to use zoo as that package rocks and Gabor is a great help. Also, Thanks to the following: Robert H for a great effort on raster, Ron Broberg for his careful eye and GIS inspiration, Zeke for verifying my early versions so long ago, SteveMc for encouragement to learn R and for code here and there, RomanM for his patience and R help, JeffId for pieces of code here and there and some nice R code that helped me understand, Ryan O for nice bits of code, Peter Oneill for KML code, ( needs to get re integrated), Nick Stokes for a good deal of R code that  helped me early on, and too many people to mention on the R help list> If I left you off, ping me and I’ll edit you in. oh and Nicholas for fixing the bug in “uncompress” Download the Zip: unzip. Make the folder your working directory. Run Downloadall.R, Run SetUpData.R Run LandOcean. Report any issues. When I put Moshtemp to bed last night there was a lingering problem. I was significantly warmer than CRU. About .2C warmer. Since we have different data as Ron Broberg pointed out I took some comfort in that. But our methods are nearly identical. Our SST matched. My Land while noisier than theirs matched when the mean of monthly values was taken. The noise in my data cancelled out. So why was my global figure higher.  I spent the day cleaning up code and improving the style, basically getting a consistent coding style with respect to naming. I lean toward this guide and suggest it for others https://docs.google.com/View?id=dddzqd53_2646dcw759cb henrik write very readable code and I use his packages, so I’m moving toward his convention. I don’t follow it entirely, but this version is much improved. The amount of code I was able to remove by moving to raster was substantial and many of my utility programs went into the trash can. I’ve added some new general purpose code for IO that is part of the ICOADS project ( what a bear ) Let me dwell a minute on the bug. Here is the wrong code: Land Weights  Ocean  Coastal  Global  The problem with that is in the calculation of weights. The weights cannot be computed separately and then added. Instead we must proceed like this: Land  Coastal  Global   Global   Land     SST      Very simply. To create a Coastal cell I want to add the Anomaly in the land cell with the anomaly in the SST cell. But I want to weight them ONLY by the land percentage/ocean percentage When I create a Global brick I want to “cover” this coastal brick, with an Land brick that is weighted by percent of land and by a SST brick that is weighted by percent of ocean of ocean in each cell. Then and only then do I want to weight each cell by the area. Why? because we care about the total area sampled and we want these weights to equal one. In the “bugged” version the weights for SST and Land were computed seperately and thats wrong. Finally I take the land brick and the SST brick and I weight them separately for the individual comparisons. This has to be done after a global brick is constructed. the land weights have to equal 1 and the SST weights have to equal one. Time for charts: Land with Moshtemp  in BLUE  As we’ve noted before we run a bit warmer than CRU.  next is the SST  And Finally Global:  And  a sample of global maps ( yes the colors and scales need work )  	 0 Comments
Find Duplicate Records in a File	https://www.r-bloggers.com/2010/09/find-duplicate-records-in-a-file/	September 24, 2010	C		 0 Comments
Because it’s Friday: Statistician vs Scientist	https://www.r-bloggers.com/2010/09/because-its-friday-statistician-vs-scientist/	September 24, 2010	David Smith	This video brings flashbacks for my days as a statistics consultant to a medical school. Some days we just weren't speaking the same language…   (With thanks to reader MK for the tip: “It's funny because it is true…”.)   	 0 Comments
Making sense of MapReduce	https://www.r-bloggers.com/2010/09/making-sense-of-mapreduce/	September 24, 2010	Joseph Rickert	From guest blogger Joseph Rickert. Last night I went to hear Ken Krugler of Bixolabs talk about Hadoop at the monthly meeting of the Software Developers Forum. Maybe because Ken is an unusually lucid speaker, or maybe because I just reached some sort of cumulative tipping point through the prep work of all those patient people who have tried to help me in the past; but finally, I think I get Hadoop and MapReduce. First of all, it’s MapReduce and then Hadoop: not Hadoop with MapReduce. The way to think about Hadoop is that it is the open source choice for implementing MapReduce like algorithms. Hadoop may provide technical advantages over competitors (Twister, Greenplum and others) about which I am in no position to comment, but it is nevertheless only one of several MapReduce frameworks. The second big “revelation” was the nature of MapReduce. When dealing with massive data, it is very likely that the size of the data to be processed is many orders of magnitude larger than size of the software required to process it. The revelation is: not only does it make sense to move the code to the data, but one ought also to do as much parallel processing as possible. Looked at this way, MapReduce is a variation of the flow based programming techniques developed by mainframe programmers in the 60’s and 70’s which were part of a tremendous effort to understanding parallel programming during that time period. In fact, the tuples of Linda spaces, the precursor of Network Spaces which forms the conceptual foundation Revolution’s parallel R programming package, ParallelR, is probably a direct ancestor of MapReduce’s key – value pairs. Furthermore, the buzz in the R community about Hadoop makes sense if one thinks about Hadoop as the open source way to implement sophisticated R based MapReduce algorithms. R should be an ideal choice for implementing parallel algorithms that work on independent chunks of data. 	 0 Comments
Connecting to a MongoDB database from R using the C API for MongoDB	https://www.r-bloggers.com/2010/09/connecting-to-a-mongodb-database-from-r-using-the-c-api-for-mongodb/	September 24, 2010	Pierre Lindenbaum		 0 Comments
R / Finance 2011 Call for Papers	https://www.r-bloggers.com/2010/09/r-finance-2011-call-for-papers/	September 24, 2010	Thinking inside the box	"
 
 
R/Finance 2011: Applied Finance with R
April 29 and 30, 2011
Chicago, IL, USA
 
The third annual R/Finance conference for applied finance using R will be
held this spring in Chicago, IL, USA on April 29 and 30, 2011. The two-day
conference will cover topics including portfolio management, time series
analysis, advanced risk tools, high-performance computing, market
microstructure and econometrics. All will be discussed within the context of
using R as a primary tool for financial risk management, portfolio
construction, and trading.
 
One-page abstracts or complete papers (in txt or pdf format) are invited to
be submitted for consideration. Academic and practitioner proposals related
to R are encouraged. We welcome submissions for full talks, abbreviated
“lightning talks”, and for a limited number of pre-conference (longer)
seminar sessions.
 
Presenters are strongly encouraged to provide working R code to accompany the
presentation/paper. Data sets should also be made public for the purposes of
reproducibility (though we realize this may be limited due to contracts with
data vendors). Preference may be given to presenters who have released R
packages.
 
Please send submissions to: committee at RinFinance.com.
 
The submission deadline is February 15th, 2011. Early submissions may receive
early acceptance and scheduling.
 
Submissions will be evaluated and submitters notified via email on a rolling
basis. Determination of whether a presentation will be a long presentation or
a lightning talk will be made once the full list of presenters is known.
 
R/Finance
2009 and
2010
included attendees from around the world and featured
keynote presentations from prominent academics and practitioners. 2009-2010
presenters names and presentations are online at the conference website. We
anticipate another exciting line-up for 2011 including keynote presentations
from John Bollinger, Mebane Faber, Stefano Iacus, and Louis Kates. Additional
details will be announced via the conference website
as they become available.
 
For the program committee:
 
So see you in Chicago in April! 

 "	 0 Comments
Connecting to a MongoDB database from R using Java	https://www.r-bloggers.com/2010/09/connecting-to-a-mongodb-database-from-r-using-java/	September 24, 2010	nsaunders	"It would be nice if there were an R package, along the lines of RMySQL, for MongoDB.  For now there is not – so, how best to get data from a MongoDB database into R? One option is to retrieve JSON via  the MongoDB REST interface and parse it using the rjson package.  Assuming, for example, that you have retrieved your CiteULike collection in JSON format from this URL: – and saved it to a database named citeulike in a collection named articles, you can fetch the first 5 articles into R like so: That works, but you may not want to use the MongoDB REST interface:  for example, it may be slow for large queries or there might be security concerns. MongoDB has both C and Java drivers.  R has packages that interface with these languages:  .C/.Call and rJava, respectively.  My only problem is that I can write what I know about C and Java on the back of a postage stamp. Not to be deterred, I took the approach that has served me well my whole professional life:  wing it, using what I could glean from Google searches and the Web.  In the end, using Java in R to connect with MongoDB was surprisingly easy.  Here’s a basic how-to.

I’ll assume that MongoDB is installed and running on your machine.  Packages for Ubuntu/Debian can be obtained here. 1. Install R packages
You’ll need rJava and rjson.  The latter was a simple install.packages(“rjson”) from the R console.  The former gave me some problems so as I use Ubuntu, I went with sudo apt-get install r-cran-rjava.  That should also install the necessary dependencies, including a JDK if you don’t already have one. 2. Install the MongoDB Java driver
Create a directory, e.g. ~/mongodb/java, change into it and grab the latest driver from GitHub.  I renamed the file to mongo.jar.  Having no idea what to do with it, I searched and discovered this guide.  I ran: The Java class files are located in com/mongodb. 3. Experiment with rJava
Still in ~/mongodb/java, I started an R console and loaded the libraries: Next, I added the MongoDB classes to the classpath: The next step was to consult the MongoDB Java tutorial and try to figure out how to convert “normal” Java syntax to rJava.  First, rJava has no import, so you create a new Mongo object like this: OK – that seems to have worked; we have a Java object of class Mongo, connected to the server on localhost.
You can see the available methods like this: As a non-Java programmer, that means very little to me.  Instead, I typed m$, hit the tab key a couple of times and saw this: That’s much more useful – I recognise those methods.  Let’s try connecting with the citeulike database: Progress, no errors, it’s all good.  Using the same approach – type db$ and hit tab, I saw this: Which led me to believe that I could access the articles collection like this: You get the idea.  The Java methods follow the names of the MongoDB shell commands.  Let’s fetch the first article: Success!  The toString() method converts the article to a JSON string.  Now all that’s left is to get that into an R data structure: Let the statistical analysis of your CiteULike library (or any other data from MongoDB) begin. "	 0 Comments
More AQP (algorithms for quantitative pedology) Examples	https://www.r-bloggers.com/2010/09/more-aqp-algorithms-for-quantitative-pedology-examples/	September 23, 2010	dylan	The aqp package can be downloaded from R-Forge. read more 	 0 Comments
Higher Order Functions in R	https://www.r-bloggers.com/2010/09/higher-order-functions-in-r/	September 23, 2010	John Myles White	Because R is, in part, a functional programming language, the ‘base’ package contains several higher order functions. By higher order functions, I mean functions that take another function as an argument and then do something with that function. If you want to know more about the usefulness of writing higher order functions in general, I’d recommend the classic Structure and Interpretation of Computer Programs and the more recent Higher Order Perl, both of which are freely available online. The six primary higher order functions built into R are I’ll go through these in order, giving some examples using basic number theoretic calculations in something approximating the style of SICP. Hopefully the use of mathematical examples isn’t as unnerving for stats-savvy readers as it can be for many readers of SICP. Reduce loops over pairs of elements of a vector, performing the same binary operation on all of the pairs along the way. That’s so abstract that it’s probably unclear. For that reason, I think it’s best to work up from a very specific example to the general notion of Reduce. Let’s start by assuming that you’ve been writing some horrible piece of code like the following: This approach is clearly so specific that it’s virtually unusable. The most obvious fault is that our code is unable to deal with arbitrarily long vectors. That can be easily fixed, though, with a simple loop: Reduce takes another abstraction step beyond the one we just made. Instead of making specific functions that repeatedly perform some set of pre-specified binary operations, Reduce provides one higher order generalization that takes an arbitrary binary operation as an argument. This lets us rewrite my.sum in one line: If you’re confused by that line, there are two potential sources of magic. First, the name of the addition operation in R is `+` with backquotes. Second, Reduce and its relatives assume that the first argument is a function that they will use on the elements of x. Once you realize that Reduce makes this sort of code trivial to write, it’s easy to continue exploiting this approach: Better yet, if you know that Reduce has other options like accumulate, which allows you to keep track of the interim results of the Reduce operations, then you can also make replacements for cumsum and cumprod easily: Filter is even easier to understand than Reduce: it simply selects the elements of a vector that meet a predicate you pass in as a function: We can use Filter along with some more interesting predicates to write much more complex functions quickly: And we can even combine Reduce and Filter to perform otherwise tedious tasks like enumerating the perfect numbers: Map is essentially the same abstraction as the apply family of functions provides. Try the following to see why I say that: Find is really a truncated form of Filter: it locates the first item in a vector that satisfies a predicate. For example, we can find the first prime greater than 1000 like so: Position is a variant of Find that provides the index of the element that would be returned by Find instead of its value: Negate simply flips the Boolean sense of an existing predicate. So we can do the following: The following example uses higher order functions to compute the value of a finite continued fraction. The code is lifted straight from the help docs for Reduce. If you’re not familiar with continued fractions, I’d recommend starting with this Wikipedia article and then reading The Higher Arithmetic (Incidentally, The Higher Arithmetic is the most enjoyable book on mathematics I have ever read.) This example also shows that you can have Reduce work right to left through a vector when you might need to. In this example, you do need to work in that specific order, because the binary operation is not associative, so moving left to right would produce different results. Given their flexibility, one might wonder if these higher order functions lose something in efficiency. Some basic profiling suggests they do, though I think their elegance makes up for small efficiency losses in many contexts. For example, filtering and selecting elements of a vector are essentially identical, so we can compare these two approaches: Similarly, we can compare Map and lapply: And finally we can compare our Reduce implementation of sum with the hardcoded one: 	 0 Comments
Effective sample size	https://www.r-bloggers.com/2010/09/effective-sample-size/	September 23, 2010	xi'an	In the previous days I have received several emails asking for clarification of the effective sample size derivation in “Introducing Monte Carlo Methods with R” (Section 4.4, pp. 98-100). Formula (4.3) gives the Monte Carlo estimate of the variance of a self-normalised importance sampling estimator (note the change from the original version in Introducing Monte Carlo Methods with R ! The weight W is unnormalised and hence the normalising constant  appears in the denominator.)  as  Now, the front term is somehow obvious so let us concentrate on the bracketed part. The empirical variance of the ‘s is  the coefficient  is thus estimated by  which leads to the definition of the effective sample size  The confusing part in the current version is whether or not we use normalised W’s and ‘s. I hope this clarifies the issue! 	 0 Comments
Seeing the Big Picture	https://www.r-bloggers.com/2010/09/seeing-the-big-picture/	September 23, 2010	John Myles White	"Here’s a nice snippet from a 2009 article by Kass that I read yesterday: 
According to my understanding, laid out above, statistical pragmatism has two main features: it is eclectic and it emphasizes the assumptions that connect statistical models with observed data. The pragmatic view acknowledges that both sides of the frequentist-Bayesian debate made important points. Bayesians scoffed at the artificiality in using sampling from a finite population to motivate all of inference, and in using long-run behavior to define characteristics of procedures. Within the theoretical world, posterior probabilities are more direct, and therefore seemed to offer much stronger inferences. Frequentists bristled, pointing to the subjectivity of prior distributions, to which Bayesians responded by treating subjectivity as a virtue on the grounds that all inferences are subjective. While there is a kernel of truth in this observation — we are all human beings, making our own judgments — subjectivism was never satisfying as a logical framework: an important purpose of the scientific enterprise is to go beyond personal decision-making. In fact, the dance around prior distributions has been a bit of a distraction and, it seems to me, the really troubling point for frequentists has been the Bayesian claim to a philosophical high ground, where compelling inferences could be delivered at negligible logical cost. Frequentists have always felt that no such thing should be possible. I believe this feeling has its origins in the gap between models and data, which is neither frequentist nor Bayesian. Statistical pragmatism avoids this irritation by acknowledging explicitly the tenuous connection between the real and theoretical worlds. As a result, its inferences are necessarily subjunctive. We speak of what would be inferred if our assumptions were to hold. The inferential bridge is traversed, by both frequentist and Bayesian methods, when we act as if the data were generated by random variables.
 "	 0 Comments
Global done!	https://www.r-bloggers.com/2010/09/global-done/	September 23, 2010	Steven Mosher	Over the past few weeks I’ve been working at getting Moshtemp to work entirely in the raster package. I’ve been aided greatly by the author of that package, Robert, who has been turning out improvements to the package with regularity. For a while I was a bit stymied by some irregularities in getting source from R Forge, so I turned to other work. More on that later. But now I’m happy to report that with version 1.5.9 I am able to do the whole analysis in raster. First some caveats. In what follows I benchmark against the analysis done by Hadley/CRU. That benchmark is complicated by the fact that we use different data and slightly different methods. I’ll point out those differences as I go through things. Generally my goal is just to get this kind of analysis grounded in open source tools and not quibble about the differences. So, there is no grand take away message, other than “you can use raster to do this kind of work.” Also, I’ll be going through the code a couple more times to throw out more code that I wrote and to make the whole project building process cleaner, more flexible, and better documented: To the code: source(“Global.R”)  First  we load all the scripts used by the program. That contains file names, constants, utility functions. It’s a lot of infrastructure that is important to understand. Start by getting all the data sources. These data sources are all downloaded and set up in the first two scripts “download” and  ”set up”. Read a raster of percent of land in each grid. The grid is read in in 1/4 degree cells. The getMask function uses your default cell size of 5 degrees to aggregate the mask to 5 degree figures. That can be changed, by passing in a different cell size. This grid has inland water. landMask  Next we calculate the “inverse” of that grid and get ourselves a mask of “water percent” in each grid. I do this just for clarity. there is no need to actually have this in memory since we use and disguard it OceanPercentMask   Next we  get a list of all stations. The Inventory of stations is a dataframe containing all the metadata for a station. Lat/lon, population, etc. It has been previously downloaded and clean up. Inv  Next we load temperature normals. This is a zoo object (an extension of time series) written out in the preprocessing steps. It has stations in columns and time in rows. That’s vital to know. As noted before you can just read this object in and plot station temperatures. like so plot(Anom[ ,1]) there are thousands of stations. Anom  In a OOP version I would probably turn this into an object type.  With the following properties: 1. rownames are a time class from zoo. 2.colnames are  station ids from an inventory. Measures are anomalies. Units are C, etc.  Next we load the SST anomalies. These have been processed from netCDF to a rasterbrick, with data from 1900 to 2009 in one month increments. SST  Now, we reconcile the inventory with the stations available in the normals file. The inventory has over 7000 stations, but when we processed normals we dropped many of those. So, Anom has about 5000 stations and Inv has 7000. if we want to we can subset Inv further. For example we can select only  ”urban” stations. There might be only 3000 of those. to get Anom and Inv on the same footing, we have to “intersect” the station Ids in Inv with the column names in Anom. to do that we call: Data  Intersect  wraps a ‘set intersect’ call and returns a list where the stations in the inventory match those in the anomaly structure Anom. They should be ordered correctly and Anom has been transposed so that rows are stations and columns are time. That’s a design choice I need to think about. Doing that prevents downstream issues, but if you try to plot(Anom[ ,1]) after the transpose, you’ll need to flip x and y again. Next we access the list output by the function Anom  Inv  If you want to preserve Anom in its untransposed order, then just do AnomTransposed That would leave Anom available in its untransposed state. Next comes the function that does all the heavy lifting for you. LandGrid Lets walk through this call. In Anom we have stations in rows. Every station has a Lon/Lat. We want to “map” those points into a grid structure. What kind of grid do we want to “map” them to?  landMask. That tell the routine the kind of grid that the points get mapped into. What points?  xy=getInvLonLat(Inv).these are the points associated with the stations in Inv.  xy takes in a structure, like a matrix, two columns wide, and rows equal to number of points (stations). The points MUST be in lonlat order. the utility function getInvLonLat() does that for you. Next, what are the values for those points by time (layer) values=Anom.  This assigns the time series (each row) to each layer in the brick. layer1 = month 1… The number of rows in xy MUST MATCH the number of rows in values. That is why we had to transpose Anom, since it originally has points in columns.  Now we could overcomplicate pointsToRaster and give it a flag to handle the transpose for us, but as it stands you have to do some work outside the call to prep your data. When multiple points “map” into the same 5 degree bin we want to apply a function. Do we add the temps? subtract them? or take the average? fun=mean,na.rm=T. We take the mean, and we remove NA from that calculation.  Now, temperatures have been gridded. Assigned to a cell and averaged on a monthly basis with all other stations in that grid. Weights  We calculate the weights for each layer in the brick. The weight is simply this. Every cell has an area. For any given month there will be some fraction of the total cells with measurements. Say 1000 cells of the 2592 cells in the entire grid. every month we total the entire area of the cells with measures and we calculate areacell/totalarea. The weights per layer sum to 1. The areas are area on the sphere. Next we multiply the weights by the temperatures by the percent of land in each grid. This gives us the area weighted temperature over land. Land  layerNames(Land)  landMonthly  Then I assign the global variable “timeline” which is 1900 to 2009 in months to the layer names. Then I collect the stats per layer into a vector named landMonthly. NOTE, you sum to get this value since each grid has been weighted. the land is done. next comes the Ocean: The code is self explantory: Weights  Ocean  layerNames(Ocean)  oceanMonthly  then the final bits which I’ve explained before: Coastal  Global = cover(Coastal, Ocean, Temps) layerNames(Global)  globalMonthly  recall that when we Add the Ocean to the Land, we ONLY add those cells they SHARE. which gives us the coast. Area weighted land fraction and ocean fraction, summed gives us the weighted value for the coast. Finally, we combine the three with “cover” Coast is coast cells and all other cells NA. You cover that with Ocean and the cells that are ocean get copied to the NA. Thats ocean and coast, cover that with land, and the land gets added. Lastly you sum the entire lot and you are done. Normally, when people plot the maps they plot the unweighted anomaly. I’ll do that later but for now we have this, which is a map of the weighted anomaly.  	 0 Comments
New housedata release 20100923	https://www.r-bloggers.com/2010/09/new-housedata-release-20100923/	September 23, 2010	jjh	I’ve just released an updated version of the HouseData file. There are 49,914 filings from 7897 committees going back to early 2001. Several important fixes to the data collection methodology have been applied to pull in earlier and earlier data. The utility library for reading FEC filings ,FECHell, still does not support version 3 filings so electronic files from the late 90′s and early 2000s are still touch and go. The Housedata page has a data description and the latest download links. Feedback is encouraged and greatly appreciated- jjh 	 0 Comments
IIATMS Guest Contribution	https://www.r-bloggers.com/2010/09/iiatms-guest-contribution/	September 23, 2010	Millsy		 0 Comments
R Project and Google Summer of Code: Wrapping up	https://www.r-bloggers.com/2010/09/r-project-and-google-summer-of-code-wrapping-up/	September 23, 2010	Thinking inside the box	"
 
 

      As in 2008 and
      2009,
      the R Project has again participated in the
      Google Summer of Code during 2010. 
     
      Based on 
      ideas collected and disussed on the R Wiki, 
      the projects and students listed below (and sorted alphabetically by student) were selected for participation and have been 
      sponsored by Google during the summer 2010.  

     
      The finished projects are available via the 
      R / GSoC 2010 repository at Google Code, and in
      several cases also via their individual repos (see below).
      Informal updates and final summaries on the work was also provided via the 
      GSoC 2010 R group blog. 
     
Proposal:
      radx is a package to compute derivatives (of any order) of native R code for multivariate functions with vector outputs,
      f:R^m -> R^n, through Automatic Differentiation (AD). Numerical evaluation of derivatives has widespread uses in many
      fields. rdx will implement two modes for the computation of derivatives, the Forward and Reverse modes of AD, combining
      which we can efficiently compute Jacobians and Hessians. Higher order derivatives will be evaluated through Univariate
      Taylor Propagation.
     
Delivered:
      Two packages 
      radx: forward automatic differentiation in R and
      tada: templated automatic differentiation in C++ were created; see 
      this blog post for details.
     
Proposal:
      R puts the latest statistical techniques at one’s fingertips through thousands of add-on packages available on the CRAN download
      servers. The price for all of this power is complexity. Deducer is a cross-platform cross-console graphical user interface built
      on top of R designed to reduce this complexity.

      This project proposes to extend the scope of Deducer by creating an innovative yet intuitive system for generating statistical
      graphics based on the ggplot2 package.
     
Delivered:

      All of the major features have been implemented, and are outlined in the video links in this 
      blog post.
     
Proposal:
      At present there does not exist a robust geometry engine available to R, the tools that are available tend to be limited in
      scope and do not easily integrate with existing spatial analysis tools. GEOS is a powerful open source geometry engine
      written in C++ that implements spatial functions and operators from the OpenGIS Simple Features for SQL specification. rgeos
      will make these tools available within R and will integrate with existing spatial data tools through the sp package.
     
Delivered:
      The rgeos project on R-Forge; see the
      final update blog post. 
     
Proposal:
      Social Relations Analyses (SRAs; Kenny, 1994) are a hot topic both in personality and in social psychology. While more and
      more research groups adopt the methodology, software solutions are lacking far behind – the main software for calculating
      SRAs are two DOS programs from 1995, which have a lot of restrictions. My GSOC project will extend the functionality of
      these existing programs and bring the power of SRAs into the R Environment for Statistical Computing as a state-of-the-art
      package.
     
Delivered:
      The TripleR package is now on CRAN and hosted on RForge.Net; see this
      blog post for updates.
     
Proposal:
      So-called NoSQL databases are becoming increasingly popular. They generally provide very efficient lookup of key/value pairs. I’ll
      provide several implementation of NoSQL interface for R. Beyond a sample interface package, I’ll try to support generic
      interface similar to what the DBI package does for SQL backends
     
Status:
      An initial prototype is available via RTokyoCabinet on Github.
      No updates were made since June; no communication occurred with anybody 
      related to the GSoC project since June and the project earned a fail.
     
Last modified: Wed Sep 22 19:39:43 CDT 2010 
 "	 0 Comments
Monte Carlo Statistical Methods third edition	https://www.r-bloggers.com/2010/09/monte-carlo-statistical-methods-third-edition/	September 23, 2010	xi'an	Last week, George Casella and I worked around the clock on starting the third edition of Monte Carlo Statistical Methods by detailing the changes to make and designing the new table of contents. The new edition will not see a revolution in the presentation of the material but rather a more mature perspective on what matters most in statistical simulation: Chapter 1 (Introduction) will include real datasets with more complex motivating models (e.g., Bayesian lasso). Chapter 2 (Random generation) will update the section on uniform generators and include a section on ABC. (I will also include a note on the irrelevance of hardware random generators.) Chapter 3 (Monte Carlo integration) will include a reference to INLA, the integrated Laplace approximation of Rue, Martinez and Chopin, as well as to our recent vanilla Rao-Blackwellisation paper. Chapter 4 (Control of convergence) will remove the multivariate normal approximation of the beginning, replace it with the Brownian bound solution already presented in Introducing Monte Carlo Methods with R, and include connections with the Read Paper of Kong et al. and the multiple mixture paper of Owen and Zhou. Chapter 4 (Stochastic optimisation) will include some example from Introducing Monte Carlo Methods with R and add recent results on EM standard error by Cappé and Moulines. Chapter 6 (Markov chains) will now be split in two chapter. The first chapter will deal with the basics of Markov chains, independent of MCMC algorithms. The second Markov Chain chapter will be devoted to the theory specific to MCMC use, including regeneration, Peskun ordering, batch means, spectral analysis… Chapter 7 (Metropolis-Hastings algorithms) will include a very basic example at the beginning and cover algorithms beyond the random walk. Adaptive MCMC will also be processed in this chapter. Chapter 8 (Slice sampling) and Chapter 9 (2-stage Gibbs sampling) will be reunited, as in the first edition of the book (!). The new chapter will also compare mixture models with product partition models. And hopefully do a better job at covering Liu, Wong and Kong (1994). Chapter 10 (general Gibbs sampling) will cover mixed linear models and hierarchical models in more details. It will include entries on parameter expansion, Dirichlet processes, JAGS, Bayesian lasso. Chapter 11 (Reversible jump algorithms) will face an in-depth change to become a chapter on computational techniques for model choice. This means covering intra-model as well as inter-model computational tools like bridge, path, umbrella, nested sampling, harmonic means, Chib’s representation, &tc. We will reduce considerably the entry on reversible jump and cover other stochastic search methods, like the shotgun gun stochastic search of Hans, Dobra and West. Chapter 12 (Diagnosing convergence) will focus more precisely on the methods that survived the test of time, removing some parts and illustrating remaining methods with coda output. Batch means and effective sample size will be part of the diagnostics. Chapter 13 (Perfect sampling) will disappear into a section of Chapter 6 (Markov chains), as we fear perfect sampling remained more of an elegant theoretical construct than a genuinely implementable technique, despite the fascination it inspired in the community, us included. Chapter 14 (Iterated and Sequential Importance Sampling) will become a chapter on Iterated and Sequential Monte Carlo, with an extensive rewriting in order to include some of the most recent advances in particle systems, including the 2009 Read Paper of Andrieu, Doucet and Holenstein. Overall, the goal is to make the book more focussed on well-established techniques, reinforcing the theoretical backup whenever possible, as well as to cover recent developments of importance in the field. Given the availability of the companion Introducing Monte Carlo Methods with R , we will not cover the practicals of R implementation, even though we will make all R codes available once the revision is completed. We hope to be done by next summer, even though the simultaneous handling of three other books will certainly be a liability for me… 	 0 Comments
New World Bank Data Available	https://www.r-bloggers.com/2010/09/new-world-bank-data-available/	September 22, 2010	C		 0 Comments
Guidelines for efficient R programming	https://www.r-bloggers.com/2010/09/guidelines-for-efficient-r-programming/	September 22, 2010	David Smith	R is designed to make it easy to clearly express statistical ideas in code, but when it come to writing code that runs as fast as possible, there are a few tips, tricks and caveats to be aware of. As part of the BioConductor conference this past summer, Martin Morgan prepared a tutorial on efficient R programming. (Patrick Abouyen presented the tutorial on the day.) The slides (PDF) include lots of handy guidelines, including:  There's also a collection of exercises (PDF) you can use to test out your efficient programming skills. Many of the examples come from the Genomics domain (as befits the BioConductor conference), but the advice is relevant to any R user. BioConductor.org: BioC 2010 Course Materials (see section Efficient R Programming)   	 0 Comments
Visualizations of US neighborhoods by race and ethnicity	https://www.r-bloggers.com/2010/09/visualizations-of-us-neighborhoods-by-race-and-ethnicity/	September 22, 2010	dan	HOMOPHILY + MAPS WITHOUT MAPPING SOFTWARE  In the past, Decision Science News has posted about homophily (“birds of a feather shop together“) and cool, lightweight visualizations (“maps without map packages in R“). Today, both topics come together in Eric Fischer’s fascinating set of images on Flickr called “Race and Ethnicity”(*).  According to Eric: Red is White, Blue is Black, Green is Asian, Orange is Hispanic, Gray is Other, and each dot is 25 people. Data from Census 2000. Your Editor had just thought this was just an interesting visualization, but a few seconds after emailing it to Jake and Sharad, the latter called out “homophily!” from a few desks away, making a sensible tie-in to our paper. To refresh the collective memory: Homophily is the idea that people who are in contact with one another tend to be similar in a number of dimensions. Here we guess a less friendly term for it would be segregation, though that word is often used with some sense of causality (e.g. a dictionary speaks of “enforced” or “voluntary” segregation), whereas homophily is plainly correlational. One funny thing about this representation is that it can make things look exaggerated when population density is high. For instance, on the Upper East Side of Manhattan (just to the right of the sharp rectangle that is Central Park, a third of the way down from the top) it looks like everybody is white. However, there are places that are just as white, but seem less so because of lower population density (e.g., the bit of New Jersey on the upper left). The white space makes things seem less, well, white. Also, if you zoom in on the Upper East Side, as below, one can see it is not pure red:  Now for another former home of Decision Science News: Chicago. The little red blob on the coast about two-thirds of the way down is the University of Chicago / Hyde Park. Your editor remembers being a student and making regular 20-minute drives to the orange blob due West of campus to get burritos at the original Maravillas.  (*) Well, it is not exactly without mapping software, but the background image adds little. H/T Eric Fischer. I found out about these pics from Mike Arauz. I also just learned that Andrew Gelman has blogged about this, too http://www.stat.columbia.edu/~cook/movabletype/archives/2010/09/how_segregated.html 	 0 Comments
Connecting to SQL Server from R using RJDBC	https://www.r-bloggers.com/2010/09/connecting-to-sql-server-from-r-using-rjdbc/	September 22, 2010	JD Long	A few months ago I switched my laptop from Windows to Ubuntu Linux. I had been connecting to my corporate SQL Server database using RODBC on Windows so I attempted to get ODBC connectivity up and running on Ubuntu. ODBC on Ubuntu turned into an exercise in futility. I spent many hours over many days and never was able to connect from R on Ubuntu to my corp SQL Server. Joshua Ulrich was kind enough to help me out by pointing me to RJDBC which scared me a little (I’m easily spooked) because it involves Java. The only thing I know about Java is every time I touch it I spend days trying to get environment variables loaded just exactly the way it wants them. But Josh assured me that it was really not that hard. Here’s the short version: Download the RJDBC driver from Microsoft. There’s Win and *nix versions, so grab which ever you need. Unpack the driver in a known location (I used /etc/sqljdbc_2.0/). Then access the driver from R like so: I have a few scripts that I want to run on both my Ubuntu laptop and my Windows Server. To accommodate that I made my scripts compatible with both by doing the following to my drv line: Obviously if you unpacked your drivers in different locations you’ll need to molest the code to fit your life situation. EDIT: A MUCH better place to put the JDBC drivers in Ubuntu would be the /opt/ path as opposed to /etc/ which I used above. In Ubuntu the /opt/ directory is where one should put user executables and /etc/ should be reserved for packages installed by apt. I’m not familiar with all the conventions in Ubuntu (or even Linux in general) so I didn’t realize this until I got some reader feedback.  Be forewarned, RJDBC is pretty damn slow and it appears to no longer be in active development. For my use case, RODBC was clearly faster. But RJDBC works for me in Ubuntu and that was my biggest need. 	 0 Comments
My Crappy Fantasy Football Draft	https://www.r-bloggers.com/2010/09/my-crappy-fantasy-football-draft/	September 22, 2010	Ryan	I compared the results of my fantasy football draft with the results of more than 1500 mock drafts at the Fantasy Football Calculator (FFC).  I looked at where player X was drafted in our league, subtracted off the average draft position on FFC, and divided by the standard deviation of the draft positon of that player on FFC.  In other words, I’ve computed a ‘standardized’ draft position for the given player. How do we interpret this standardized draft position?  Obviously if we have a positive score, then a player was drafted later in our draft than the average position on FFC.  This would mean that a team owner in our league got a pretty good deal on that player.  Understand?  Divided by the standard deviation just places all of the draft positions in a standardized unit for comparison purposes.  Here are the results of our draft.  What do we see from this?  Well, my draft sucked.  Most of my boxes in the heat map are negative!  So I drafted my players a little higher than the average draft position on FFC.  In particular, it looks like I picked Pierre Thomas way earlier. Some positives:  Yurcy picked Randy Moss with the 18th pick and his average draft position on this website was 8.8.  Possibly the biggest winner was Rob’s 6th round pick of Wes Welker…good value there. I’ll do the same for my league with the boys in Vermont.  Hopefully the results are a little better than what I did with the Princeton gang. The code is published at github under ffdraft. 	 0 Comments
R Function of the Day: foodweb	https://www.r-bloggers.com/2010/09/r-function-of-the-day-foodweb-2/	September 21, 2010	erik	"The R Function of the Day series will focus on describing in plain language how certain R functions work, focusing on simple examples
that you can apply to gain insight into your own data.
 
Today, I will discuss the foodweb function, found in the mvbutils
package.
 
In biology, a foodweb is a group of food chains, showing the complex
relationships that exist in nature. Similarly, the R foodweb
function contained in the mvbutils package on CRAN displays a
flowchart of function calls.  What functions does my function call?
What funtions in my package call the lm function? These are the
types of questions that can be answered in a graphical way using
foodweb. This information can be useful for documenting your own
code, and for learning how a package that you’re not familiar with
works. At the end of this post, you’ll see an example with a diagram
of the survexp function from the survival package.
 
First, you have to install and load the mvbutils package, so let’s
do that first.
 Let’s define a couple simple functions to see how foodweb works. We
simply define a function called outer that calls two functions,
inner1 and inner2.
 
Now let's use the foodweb function to diagram the relationship between
the outer and inner functions. If we don't give foodweb any
arguments, it will scour our global workspace for functions, and make
the diagram.  Since the only functions in my global workspace are the
ones we've just defined, we get the following plot.
 
We see a simple graph showing that the outer function calls both the
inner1 and inner2 functions, jus as we expect. We can make this look a
bit nicer by adjusting a few of the arguments.
 
You can see that you can control many of the graphical parameters of
the resulting plot. See the help page for foodweb to see the
complete list of graphical parameters you can specify.
 
As we saw above, by default foodweb will look in your global
workspace for functions to construct the web from.  However, you can
pass foodweb a group of functions to operate on.  There are several
ways of doing this, see the help page for examples.  The code below
shows one possiblity, when we want to limit our results to functions
appearing in a specific package.  The prune argument is very useful.
It takes a string (or regular expression) to prune the resulting graph
to.
 I have used the foodweb function to help understand other user's
code that I have inherited. It has proved very valuable in aiding the
comprehension of hard to read or complicated functions. I have also
used it in the development of my own packages, as it sometimes can
suggest ways to reorganize the code to be more logical.
 "	 0 Comments
Oil – Equities correlation – trading opportunity or new normal?	https://www.r-bloggers.com/2010/09/oil-equities-correlation-trading-opportunity-or-new-normal/	September 21, 2010	Lloyd Spencer		 0 Comments
The R-Files: Hadley Wickham	https://www.r-bloggers.com/2010/09/the-r-files-hadley-wickham/	September 21, 2010	David Smith	“The R-Files” is an occasional series from Revolution Analytics, where we profile prominent members of the R Community.     Name: Hadley Wickham Profession: Assistant Professor of Statistics, Rice University Nationality: New Zealand Years Using R: 10 Known for: Developing popular R packages including ggplot2, plyr, reshape; creator of crantastic.org; author of ggplot2: Elegant Graphics for Data Analysis  An Assistant Professor of Statistics at Rice University, Hadley Wickham has been using R for over 10 years. In that time, he has emerged as one of the most prominent members of the R community. Wickham’s research focuses primarily on data analysis and developing innovative tools that facilitate the understanding of complex statistical models through visualization. He is known for developing some of today’s most popular R packages, including ggplot2, plyr and reshape. Wickham’s R packages are used by a wide range of high-profile institutions for complex statistical analyses, including the U.S. Air Force Research Laboratories, Mozilla Labs and the Vanderbilt University Center for Human Genetics Research. ggplot2 was also used earlier this year by NYU doctoral student Drew Conway to visualize the Wikileaks data, offering far greater insight into troop movements and conflict hot zones in Afghanistan than was previously available to the public.   He first encountered R in an undergraduate statistics course at the University of Auckland—the birthplace of R. Admittedly, he found the language a bit overwhelming at first. Of his early experiences with the language, he says, “I remember when I first used R, I found it to be the most enormously frustrating—yet magical—computer language I had ever encountered.” For Wickham, the frustration came from R’s organic growth, which—he’s quick to add—is also its greatest strength. Thanks to its open source roots and community of users and contributors, R has grown without a roadmap or goal. While you can do just about anything with R, Wickham notes that it can be an intimidating language for new users to learn, and does not have the most user-friendly interface.   As a doctoral student at Iowa State University, Wickham became increasingly familiar with—and fond of—R. Working under the tutelage of Di Cook and Heike Hoffman, he began to master the language’s wide suite of functional capabilities and eventually started to develop R packages to contribute back to the community. To date, Wickham has developed 20 packages on top of R, with an emphasis on visualization and manipulation of data.   In his own classes, Wickham uses R to teach a new generation of statisticians. Among some of his favorite subjects for teach with R are tracking the recent American sub-prime mortgage crisis and using data released by the Social Security Administration to study the evolution of baby names. “R is the most powerful statistical computing language on the planet,” Wickham says. “Students today have such an advantage over previous generations; R has nearly limitless statistical capability and allows them to perform a far greater range of analyses than was previously possible.”  While critics argue that R is not as “pretty” a programming language as Java and C++, Wickham sees that as one of its greatest strengths. “One of the things I like about R is that it is so pragmatic,” he says. “It doesn’t try to hold to theories that might be lovely and elegant, but don’t work in practice. It’s the result of a brilliant community coming together to make it work, and it’s constantly evolving.” Wickham cites the community behind R, which is over 2-million users strong, and their willingness to support one another as the primary driver for its great success. “When you ask a question, you’re getting free help from the people who wrote R. Not only are they great programmers, they’re internationally recognized statisticians who have contributed and developed leading theories of modern statistics.”     	 0 Comments
Classification Trees using the rpart function	https://www.r-bloggers.com/2010/09/classification-trees-using-the-rpart-function/	September 21, 2010	Ralph	In a previous post on classification trees we considered using the tree package to fit a classification tree to data divided into known classes. In this post we will look at the alternative function rpart that is available within the base R distribution. Fast Tube by Casper A classification tree can be fitted using the rpart function using a similar syntax to the tree function. For the ecoli data set discussed in the previous post we would use: followed by We would then consider whether the tree could be simplified by pruning and make use of the plotcp function: Once the amount of pruning has been determined from this graph or by looking at the output from the printcp function: The prune function is used to simplify the tree based on a cp identified from the graph or printed output threshold. The classification tree can be visualised with the plot function and then the text function adds labels to the graph: Other useful resources are provided on the Supplementary Material page. 	 0 Comments
Example 8.6: Changing the reference category for categorical variables	https://www.r-bloggers.com/2010/09/example-8-6-changing-the-reference-category-for-categorical-variables/	September 21, 2010	Ken Kleinman		 0 Comments
"Install and load R package ""Rcmdr"" to quickly install lots of other packages"	https://www.r-bloggers.com/2010/09/install-and-load-r-package-rcmdr-to-quickly-install-lots-of-other-packages/	September 21, 2010	Stephen Turner		 0 Comments
R tee-shirt	https://www.r-bloggers.com/2010/09/r-tee-shirt/	September 21, 2010	xi'an	I gave my introduction to the R course in a crammed amphitheatre of about 200 students today. Had to wear my collectoR teeshirt from Revolution Analytics, even though it only made the kids pay attention for about 30 seconds… The other few “lines” that worked were using the Proctor & Gamble “car 54″ poster and calling bootstrap “Statistics for dummies”, but I have trouble every year in getting the students interested in the topic (simulation) until…I introduced a (dummy) finance example of computing option prices. Sad!  	 0 Comments
AQP Examples: Profile Visualization	https://www.r-bloggers.com/2010/09/aqp-examples-profile-visualization/	September 20, 2010	dylan	Two examples of the output from the profile_plot() function, using data from the Sierra Foothill Region, CA. read more 	 0 Comments
Auto-complete for R in EmEditor	https://www.r-bloggers.com/2010/09/auto-complete-for-r-in-emeditor/	September 20, 2010	datadebrief		 0 Comments
Saptarshi Guha on Hadoop, R	https://www.r-bloggers.com/2010/09/saptarshi-guha-on-hadoop-r/	September 20, 2010	David Smith	Saptarshi Guha (author of the Rhipe package) joins the likes of Ebay, Yahoo, Twitter and Facebook and as one of just 37 presenters at the Hadoop World conference. (Revolution Analytics is proud to sponsor Saptarshi's presence at this event, which take place in New York on October 12.) He'll be talking about using R and Hadoop to analyze Voice-over-IP network data for Quality of Service, as explained in this abstract: RHIPE is an R package that integrates the R environment for statistics and data analysis with the Hadoop distributed computing framework. With RHIPE, the user can store and compute with large and complex data sets using R functions and programming idioms. In this talk, I will demonstrate the use of RHIPE to analyze 190Gb of VoIP network data for QoS. The jitter between two consecutive packets is the deviation of the real inter-arrival time from theoretical. We show jitter follows desired properties and is negligible, which supports the assumption of the measured traffic being close to the offered traffic. Saptarshi explains more about his talk, and his work with Hadoop and R, in this interview at the Cloudera blog. Cloudera blog: Purdue University’s Saptarshi Guha Interviewed Regarding Hadoop, R and Hadoop World  	 0 Comments
Problems with R	https://www.r-bloggers.com/2010/09/problems-with-r/	September 20, 2010	Social data blog	"
	Struggling for two whole days to do some not-terribly-advanced repeated-measures analysis of variance in R. Probably my worst experience with R so far and really an example of where R could do with some centralisation of effort. I have explored six or seven ways to do more or less the same thing, but each approach was too lacking either in documentation or in the statistics department. I don’t blame the R devs, I blame the tens of thousands of institutions like Universities which are prepared to spend a large fortune on licenses for SPSS and the like. They could switch just some seats to open source solutions like R and spend a bit of the money they saved on bounties to the R community. Then even these holes could be pretty easily plugged.
	
 Permalink 

	| Leave a comment  »
 "	 0 Comments
Cricket – opinions and facts	https://www.r-bloggers.com/2010/09/cricket-opinions-and-facts/	September 20, 2010	prasoonsharma		 0 Comments
R syntax highlighting for bloggers on WordPress.com	https://www.r-bloggers.com/2010/09/r-syntax-highlighting-for-bloggers-on-wordpress-com/	September 20, 2010	Tal Galili	Good news for R bloggers who are using WordPress.com to host their blog. This week, the good people running WordPress.com (special thanks goes to Yoav Farhi), have added the ability for all the users of the WordPress.com platform to be able to highlight their R code inside posts. Basically you’ll need to wrap the code in your post like this: (Which will then look like this:  ) Further details (and other supported languages) can be read about on this WordPress.com support page. This new feature was possible thanks to the work of Yihui Xie (who create the famous cool animation package for R), who created a R syntax brush for the syntaxhighlighter WordPress plugin (the plugin used by WordPress.com for sytnax highlighting) .  And thanks should also go to Andrew Redd, the creator of NppToR (which connects between notepad++ to R).  He both made some good suggestions, and was game to take on the brush creation in case there would be problems, which thankfully so far there aren’t any) p.s: If you are a WordPress.org users (e.g: have a self hosted WordPress blog) and want to enable R syntax highlighting for your blog, I would recommend the use of the WP-Syntax plugin (enhanced with GeSHi version 1.0.8.6) which can be downloaded here. 	 0 Comments
Data Mining in A Nutshell	https://www.r-bloggers.com/2010/09/data-mining-in-a-nutshell/	September 20, 2010	Matt Bogard		 0 Comments
R Code Examples	https://www.r-bloggers.com/2010/09/r-code-examples/	September 19, 2010	Matt Bogard		 0 Comments
R Tutorial Series: Labeling Data Points on a Plot	https://www.r-bloggers.com/2010/09/r-tutorial-series-labeling-data-points-on-a-plot/	September 19, 2010	John Quick		 0 Comments
Why Use ProjectTemplate or Any Other Framework?	https://www.r-bloggers.com/2010/09/why-use-projecttemplate-or-any-other-framework/	September 19, 2010	John Myles White	We use frameworks like Ruby on Rails or ProjectTemplate to minimize the time we spend on irrelevant details. By definition, an irrelevant detail isn’t of interest to us. But how can we tell which details are irrelevant? This isn’t a trivial task and it seems to be, on the surface, a profoundly subjective matter. Thankfully, it’s much simpler to point to examples of irrelevant details than to provide a general theory, though I do have a general theory that I’ll describe in a follow up post. For now, let’s look at a source code file for a theoretical data analysis project written in R. The file is called load_data.R: My claim is that most of the code in this file is concerned with irrelevant details. How can you tell that these details are irrelevant? In this example, there are two flaws that are obvious to me: Solving these three problems was the first step that led me to design ProjectTemplate. Rather than rewrite load_data.R each time I start a new analysis project, it’s much easier to create a single generic script that can be used in all of my projects, even if I will only use the generic script as a base for something customized to the project on hand. By using a script, rather than a function, I can make it easy to change anything that needs to be customized for each specific project, such as skipping one enormous file that I don’t need to load each time or loading some data set from SQL databases. This default script approach is how I get around having to exploit R’s inheritance system, which seems sufficiently complex that I’d rather not emulate Rails’ approach directly. So let’s solve the data loading problem by making three assumptions: With these assumptions, coding a generic script is simple enough, if we use some of the more abstract functions in R like assign: In practice, the new version of ProjectTemplate I’ll release soon does a little more than the snippet of code above: it handles a small number of different file formats based on naming conventions and it will also decompress files on the fly. If you’re feeling entrepid, I’d love to add additional code that can automatically access databases, but that’s a good deal more work than I’m prepared to do myself. This example leads to a more general lesson that helps to explain why you should use some sort of framework for many of your projects: if you don’t absolutely have to, don’t repeat yourself. This is so important that the acronym DRY has become popular in the programming community as a reminder that you should build abstractions that are flexible, reusable and appropriate to the task at hand. Frameworks, in my mind, are like domain specific languages, in that they make a bunch of assumptions about what you’re trying to achieve and thereby allow you to articulate only what makes your project different from all other similar projects. If you don’t do this, you end having to restate the details that all projects have in common. In general, keep in mind that code that avoids repetition is easier to understand, easier to change and and more likely to be valuable to the whole world if you distribute it. There are a variety of ways to avoid repeating yourself. First off, you can avoid repeating yourself by moving inline code that varies only in the operands into functions and specialized scripts. This is arguably the crux of programming well: knowing which pieces of code can be rewritten as functions. The functions in Hadley’s ‘plyr’ package are a wonderful example: the ddply() function performs a single operation that many of us previously rewrote ad hoc every time we analyzed a new data set because we hadn’t realized that there was an appropriate abstraction that encompassed all of our requirements. Rewriting this sort of code that contains an unrecognized potential abstraction, often called a design pattern, was a waste of our time, introduced new bugs and inefficiencies haphazardly into our work, and prevented the fruits of our efforts from being shared in a way that could benefit others. Hadley’s generic function, in contrast, solves all of the problems in this class once and for all. It’s a time saver for Hadley himself, but it’s also a major time saver for the human race. If you save 10,000 people five minutes of their time, you’ve done a great deed. I think this is what makes programming so enjoyable for many people. It’s certainly what I find so fulfilling about programming. It’s important to note, though, that we did more than write a very generic function in the code I listed above. We also made a decision that many programmers have been uncomfortable with historically: we allowed part of our process to become completely implicit for end users who won’t actually read our new version of load_data.R, but will simply call it at the start of their own scripts. We made it possible for users to skip the step of specifying the location and type of their data files by making assumptions about how the user has set up their directory structure and how they’ve named their files. By exploiting a general strategy of preferring convention over configuration, we can often get a lot of results with very little work. And we can do it in a way that allows us total flexibility to override the default behavior when it’s necessary. This is a egoistic reason for standardizing on a set of conventions: it saves you time when you write code. But there are altruistic reasons as well, including how you relate to your future self, who Derek Parfit will tell you is a separate person from your current self in some important senses. If you employ the same conventions in all of your work, it’s easier for you to return to an old project or for outsiders to come into a new one. Standard conventions mean that everyone knows that the data files are in a specific location. Similarly, standardizing preprocessing makes it easy to figure out what happens between loading data and analyzing it. In general, following some set of conventions, even slightly inane ones, saves time by allowing you to focus only on the parts of a program that are different from all other programs. To summarize what I’ve said so far, Living up to these ideals is the inspiration for ProjectTemplate, but there a few other ways in which I hope ProjectTemplate helps the R programming community. Most of these revolve around good programming practices that are boring and tedious, but make your life much better in the long term: 	 0 Comments
Response Times, The Exponential Distribution and Poisson Processes	https://www.r-bloggers.com/2010/09/response-times-the-exponential-distribution-and-poisson-processes/	September 18, 2010	John Myles White	"I’m currently reading Luce’s “Response Times”. If you don’t know anything about response times, they are very easily defined: a response time is the length of time it takes a person to respond to a simple request, measured from the moment when the request is made to the moment when the person’s response is recorded. In principle, you can measure response times when asking people to indicate that they’ve heard a tone as easily as you can measure them when you’ve asked people to solve a problem in calculus. In practice, response times are most easily analyzed when the task a person is performing is so short that the person completing it can usually be assumed not to be performing other tasks simultaneously. To convince non-psychologists that response times are a quantity worth measuring, I’ll give an example from my own work that I suspect is widely generalizable. I consistently find that the quality of data I can extract from Mechanical Turk HIT’s grows considerably if I simply weight each person’s responses by a function of their time to complete the HIT. For one recent task, the histogram of task completion times looked like the following: As you can obviously see, there are two noticeable clusters of participants for this HIT: a substantial chunk who spent shockingly little time on the task and a large majority of people who spent a longer and more variable period of time performing the task. In this case, completely excluding those who completed this specific task within ten seconds improved the results of my later analyses considerably at virtually no cost to myself. Other approaches, like using k-means clustering, mixture modeling or simple quantile cut-offs, can be equally useful. In general, response times are a useful proxy for the amount of effort a person puts into a task. Hopefully that example convinces you that reaction times are worth studying carefully. If you’re interested, I strongly recommend Luce’s book as an introduction to the use of response times to infer information about the mental state of specific human beings and about the global architecture of the human mind in general. It’s quite long, so many people would probably gain the most from only reading the first few chapters. In the rest of this post, I’m going to discuss a detail about the measurement of response times that I found particularly beautiful and insightful. Specifically, I was struck by a point that Luce raises in the second chapter of his book: the proper measurement of response times requires the use of an experimental design that employs the exponential distribution to produce a Poisson process of stimuli. That’s probably opaque to most readers, so let me unpack it: For me, this was a revelatory example that finally made the exponential distribution’s relevance clear. Understanding this point made me understood intuitively, rather than simply algebraically, why Luce says this in the first chapter: 
[We] see that the answer is […] the exponential distribution. This may come as a bit of a surprise since, after all, the largest value of the exponential value is at 0 — instantaneous repetition — which must tend to produce clustering if the process is repeated in time. This is, indeed, the case, as can either be seen, as in Figure 1.3, or heard if such a process is made audible in time — it seems to sputter. To neither the eye nor the ear does it seem uniform in time, but that is an illusion, confusing what one sees or hears with the underlying rate of these events. The tendency for the event to occur is constant. Such a process is called Poisson — more of it in Chapters 4 and 9. It is the basic concept of constant randomness in time — that is, the unbounded, ordered analogue of a uniform density on a finite, unordered interval.
 "	 0 Comments
Elder Research Two Day Course	https://www.r-bloggers.com/2010/09/elder-research-two-day-course/	September 18, 2010	C		 0 Comments
Using R	https://www.r-bloggers.com/2010/09/using-r/	September 18, 2010	Matt Bogard		 0 Comments
Classification Trees	https://www.r-bloggers.com/2010/09/classification-trees/	September 18, 2010	Ralph	Decision trees are applied to situation where data is divided into groups rather than investigating a numerical response and its relationship to a set of descriptor variables. There are various implementations of classification trees in R and the some commonly used functions are rpart and tree. Fast Tube by Casper To illustrate the use of the tree function we will use a set of data from the UCI Machine Learning Repository where the objective of the study using this data was to predict the cellular localization sites of proteins. The data provided on the website is shown here: We can use the xtabs function to summarise the number of cases in each class. As noted in the comments the package that I used was the tree package: The complete classification tree using all variables is fitted to the data initially and then we will try to prune the tree to make it smaller. The tree function is used in a similar way to other modelling functions in R. The misclassification rate is shown as part of the summary of the tree. This tree can be plotted and annotated with these commands: To prune the tree we use cross-validation to identify the point to prune. This suggests a tree size of 6 and we can re-fit the tree: The misclassification rate has increased but not substantially with the pruning of the tree. Other useful resources are provided on the Supplementary Material page. Data used in this post: Ecoli Data Set. 	 0 Comments
What should we call the stats Q&A site?	https://www.r-bloggers.com/2010/09/what-should-we-call-the-stats-qa-site/	September 18, 2010	Rob J Hyndman	The Q&A site at stats.stackexchange.com has been steaming ahead with loads of good questions and answers. We are currently trying to select a name and not many people have voted. Although we have more than 1300 users of the site, less than 40 people have currently voted on the name. There are four days left. So if you have signed up to use the site, please go to site name selection and vote. If you haven’t signed up to use the site, please try it out now. Anyone doing data analysis will find it useful.   	 0 Comments
A simple Metropolis-Hastings MCMC in R	https://www.r-bloggers.com/2010/09/a-simple-metropolis-hastings-mcmc-in-r/	September 17, 2010	Florian Hartig	"While there are certainly good software packages out there to do the job for you, notably BUGS or JAGS, it is instructive to program a simple MCMC yourself. In this post, I give an educational example of the Bayesian equivalent of a linear regression, sampled by an MCMC with Metropolis-Hastings steps, based on an earlier version which I did to together with  Tamara Münkemüller. Since first publishing this post, I have made a few small modifications to improve clarity. A similar post on Metropolis-Hastings MCMC algorithms by Darren Wilkinson is also worth looking at. Also, we give an introduction on this algorithm and alternatives in our review on statistical inference for stochastic simulation models. [Update]: More on analyzing the results of this algorithm can be found in a recent post. Figure: A visualization of the Metropolis-Hastings MCMC, from Hartig et al., 2011. (copyright see publisher)  Creating test data As a first step, we create some test data that will be used to fit our model. Let’s assume a linear relationship between the predictor and the response variable, so we take a linear model and add some noise. I balanced x values around zero to “de-correlate” slope and intercept. The result should look something like the figure to the right Defining the statistical model The next step is to specify the statistical model. We already know that the data was created with a linear relationship  y = a*x + b between x and y and a normal error model N(0,sd) with standard deviation sd, so let’s use the same model for the fit and see if we can retrieve our original parameter values.  Derive the likelihood function from the model For estimating parameters in a Bayesian analysis, we need to derive the likelihood function for the model that we want to fit. The likelihood is the probability (density) with which we would expect the observed data to occur conditional on the parameters of the model that we look at. So, given that our linear model y = b + a*x + N(0,sd) takes the parameters (a, b, sd) as an input, we have to return the probability of obtaining the test data above under this model (this sounds more complicated as it is, as you see in the code, we simply calculate the difference between predictions y = b + a*x and the observed y, and then we have to look up the probability densities (using dnorm) for such deviations to occur.   Log likelihood profile of the slope parameter As an illustration, the last lines of the code plot the Likelihood for a range of parameter values of the slope parameter a. The result should look something like the plot to the right. Why we work with logarithms You might have noticed that I return the logarithm of the probabilities in the likelihood function, which is also the reason why I sum the probabilities of all our datapoints (the logarithm of a product equals the sum of the logarithms). Why do we do this? You don’t have to, but it’s strongly advisable because likelihoods, where a lot of small probabilities are multiplied, can get ridiculously small pretty fast (something like 10^-34). At some stage, computer programs are getting into numerical rounding or underflow problems then. So, bottom-line: when you program something with likelihoods, always use logarithms!!! Defining the prior As a second step, as always in Bayesian statistics, we have to specify a prior distribution for each parameter. To make it easy, I used uniform distributions and normal distributions for all three parameters. [Some additional information for the ""professionals"", skip this when you don't understand what I'm talking about: while this choice can be considered pretty ""uninformative"" for the slope and intercept parameters, it is not really uninformative for the standard deviations. An uninformative prior for the latter would usually be scale with 1/sigma (if you want to understand the reason, see here). This stuff is important when you seriously dive into Bayesian statistics, but I didn't want to make the code more confusing here.]  The posterior The product of prior and likelihood is the actual quantity the MCMC will be working on. This function is called the posterior (or to be exact, it’s called the posterior after it’s normalized, which the MCMC will do for us, but let’s not be picky for the moment). Again, here we work with the sum because we work with logarithms. The MCMC Now, here comes the actual Metropolis-Hastings algorithm. One of the most frequent applications of this algorithm (as in this example) is sampling from the posterior density in Bayesian statistics. In principle, however, the algorithm may be used to sample from any integrable function. So, the aim of this algorithm is to jump around in parameter space, but in a way that the probability to be at a point is proportional to the function we sample from (this is usually called the target function). In our case this is the posterior defined above.  This is achieved by It’s fun to think about why that works, but for the moment I can assure you it does – when we run this algorithm, distribution of the parameters it visits converges to the target distribution p. So, let’s get this in R: Again, working with the logarithms of the posterior might be a bit confusing at first, in particular when you look at the line where the acceptance probability is calculated (probab = exp(posterior(proposal) – posterior(chain[i,]))). To understand why we do this, note that p1/p2 = exp[log(p1)-log(p2)].  The first steps of the algorithm may be biased by the initial value, and are therefore usually discarded for the further analysis (burn-in time). An interesting output to look at is the acceptance rate: how often was a proposal rejected by the metropolis-hastings acceptance criterion? The acceptance rate can be influenced by the proposal function: generally, the closer the proposals are, the larger the acceptance rate. Very high acceptance rates, however, are usually not beneficial: this means that the algorithms is “staying” at the same point, which results in a suboptimal probing of the parameter space (mixing). It can be shown that acceptance rates between 20% and 30% are optimal for typical applications (more on that here).  Finally, we can plot the results. There are more elegant ways of plotting this which I discuss in another recent post, so check this out, but for the moment I didn’t want to use any packages, so we do it the hard way: The resulting plots should look something like the plot below. You see that we retrieve more or less the original parameters that were used to create our data, and you also see that we get a certain area around the highest posterior values that also have some support by the data, which is the Bayesian equivalent of confidence intervals.   Figure: The upper row shows posterior estimates for slope (a), intercept (b) and standard deviation of the error (sd). The lower row shows the Markov Chain of parameter values. Gelman, A.; Carlin, J. B.; Stern, H. S. & Rubin, D. B. (2003) Bayesian Data Analysis  Andrieu, C.; de Freitas, N.; Doucet, A. & Jordan, M. I. (2003) An introduction to MCMC for machine learning Mach. Learning, Springer, 50, 5-43 Hartig, F.; Calabrese, J. M.; Reineking, B.; Wiegand, T. & Huth, A. (2011) Statistical inference for stochastic simulation models – theory and application Ecol. Lett., 14, 816–827.   "	 0 Comments
Running R on the iPad	https://www.r-bloggers.com/2010/09/running-r-on-the-ipad/	September 17, 2010	David Smith	I'm a hardcore Mac user, so it's annoying to me that we don't yet support Revolution R Enterprise on MacOS X. Believe me, I've argued the point. But MacOS is still a relatively uncommon platform in business, and there's just not the demand yet from customers to justify porting the Revolution R extensions to MacOS X. (Open-source Revolution R Community is available for MacOS X, but doesn't include big data or the IDE or other extras.) But Jan de Leeuw points out there's another option: If you want your Mac to run Revolution R Enterprise, or some of the CUDA/GPU packages that run best under Linux, then this can of course be done by running the appropriate OS in an emulator such as Parallels Desktop. Better yet, you can make it run on the iPad too, using this technique: Parallels now also has Parallels Mobile for the iPad, which is basically a screen sharing app. You have a Parellels running on a remote desktop that you login to from the iPad, and then you start (for example) Revolution R from your iPad, and you can enter commands directly into the R interpreter from the iPad keyboard. Before the end of the year you'll also be able to print remotely. It's still clunky, but it can be done. And it can be done over 3G as well. OK, that's not quite running R on the iPad — it's more like using the iPad as a remote terminal — but still very cool. If you want to run R natively on the iPad, things aren't quite so simple: the rules of Apple's App Store forbid implementing language interpreters like R, so the chances of an R app being approved seem slim. So you're stuck with jailbreaking iOS to install non-approved apps (a practice the US courts recently ruled legal over Apple's objections). There are detailed instructions for running R on an iPhone running iOS4 here, and there have been reports of it working with some limitations: the base and recommended packages appear to work fine, but CRAN packages that rely on a Fortran compiler are proving problematic to build. In any case iOS4 won't be available for the iPad until November, and even then there's no guarantee it will be jailbroken. Guess we'll just have to wait and see. Computational Mathematics: R on the iPhone 	 0 Comments
Installing ‘intergraph’ package	https://www.r-bloggers.com/2010/09/installing-%e2%80%98intergraph%e2%80%99-package/	September 17, 2010	Michal	After some e-mails I decided to put together a bit more detailed instruction how to install the ‘intergraph’ package together with the namespaced version of the ‘network’ package. See here. There seemed to be some problems with downloading the namespaced ‘network’ from my webstite (thanks Neil!). File permissions were set incorrectly. I fixed that and everything should be OK now. Because it’s Friday, another example of using ‘intergraph’. This time plot a network using both ‘igraph’ and ‘network’ functionality on the same plotting device. Here is the result:  And here is the code to produce it. lx in line 12  is “Deus Ex Machina”, or rather “Deus Ex locator()”…   	 0 Comments
Physical economy v social economy	https://www.r-bloggers.com/2010/09/physical-economy-v-social-economy/	September 17, 2010	Pat	There’s a hole in the bucket of traditional economics. Homo socialus seems to be on the rise, and homo economicus is getting harder to find. Vulcanism has become evident in the R community over the last several days.  One of the visible eruptions has been over money.  This resulted in a blog post by Tal Galili Why R developers shouldn’t be paid that includes three great videos. This matters to fund management.  If your theory maintains that Wikipedia can’t even exist, then your predictions for Microsoft’s encyclopedia might be slightly off. I must admit that I’m quite confused by the whole issue.  I don’t think the world is going to go back to “normal” though just because I don’t understand. 	 0 Comments
Typo in Example 3.6	https://www.r-bloggers.com/2010/09/typo-in-example-3-6/	September 16, 2010	xi'an	Edward Kao pointed out the following difficulty about Example 3.6 in Chapter 3 of “Introducing Monte Carlo Methods with R”: I  have two questions that have puzzled me for a while.  I hope you can shed some lights.  They are all about Example 3.6 of your book. 1.  On page 74, there is a term x(1-x) for m(x).  This is fine. But the term disappeared from (3.5) on p.75.  My impression is that this is not a typo.  There must be a reason for its disappearance.  Can you elaborate? I am alas afraid this is a plain typo, where I did not report the x(1-x) from one page to the next. 2.  On page 75, you have the term “den=dt(normx,3)”.  My impression is that you are using univariate t with 3 degrees of freedom to approximate.  I thought formally you need to use a bivariatet with 3 degrees of freedom to do the importance sampling.  Why would normx=sqrt(x[,1]^2+x[,2]^2) along with a univariate t  work? This is a shortcut that would require more explanation. While the two-dimensional t sample is y, a linear transform of the isotonic x, it is possible to express the density of y via the one-dimensional t density, hence the apparent confusion between univariate and bivariate t densities… 	 0 Comments
Open source and money – why paying R developers might not always help the project	https://www.r-bloggers.com/2010/09/open-source-and-money-%e2%80%93-why-paying-r-developers-might-not-always-help-the-project/	September 16, 2010	Tal Galili	This post can be summed up by one  two sentences: “We can’t buy love.” “Starting to pay for love could make it disappear” while at the same time “We need money to live and love”.  These two conflicting forces, with relation to open source, are the topic of this post. This post is directed to the community or R users but is relevant to people of all open source projects.  it deals with the question of open source projects and funding.   Specifically, should a community of open source developers and users, once it exists, want to start raising/donating money to the main code contributers? The conflict stems since, on the one side,  we intuitively want to repay the people who have helped us, some studies in behavioral science suggests that doing so might destroy the motivation of the developers to continue working without contently getting payed, and that making the shift from doing something for one reason (whatever it is) to doing it for money, might not easily be turned back. On the other side, developers needs to make a (good) living, and we (as a community) should strive for them to be well payed. How can these two be reconciled? This article won’t offer a decisive conclusions – and my hope is to invite discussion on the matter (from both amatures and professionals in the field of open source and behavioral economics) so to give more ideas for people to base their opinions on. Update: this post was substantially updated from it’s original version, thanks to responses both in the comments, and especially in the e-mails.  I apologies for writing a post that had needed so many corrections, and at the same time I am grateful for all the people who took the time to shed light in places where I was wrong. * * * * In the past two weeks there has been a raging debate regarding the future of R (hint: “what is R“).  Without going deeper into the topic (I already wrote about it here, where you too can go and respond), I’ll sum up the issue with a quote from Ross Ihaka (one of the two founders of R) who recently wrote: I’ve been worried for some time that R isn’t going to provide the base that we’re going to need for statistical computation in the future. (It may well be that the future is already upon us.) There are certainly efficiency problems (speed and memory use), but there are more fundamental issues too. Some of these were inherited from S and some are peculiar to R. After this, several discussion threads where started around the web (for example: 0, 1, 2, 3, 4 ,5, 6 ), but then a comment was made in the R-help mailing list by Jaroslaw Piskorski who wrote: A few days ago Tal Galili posted a message about some controversies concerning the future of R. Having read the discussions, especially those following Ross Ihaka’s post, I have come to the conclusion, that, as usual, the problem is money. I doubt there would be discussions about dropping R in its present form if the R-Foundation were properly funded and could hire computer scientists, programmers and statisticians. If a commercial company is able to provide big-database and multicore solutions, then so would a properly founded R-Foundation. To which my response is that: I strongly disagree with this statement.. That is, I do agree that money could help with things.  It could be that money could be a part of the solution.  But I doubt that the core of this problem is money.  Nor that it would be solved if we could only now hire “computer scientists, programmers and statisticians” (although that could be part of the solution). And the reason I am doubtful stems from two sources:  The first reason is presented in the following short (~10 minutes) video titled “Drive: The surprising truth about what motivates us”, adapted from Dan Pink’s talk at the RSA. This talk dicusses what motivates us, and also about what (surprisingly) doesn’t motivate us: money.  What does?  watch the talk (it’s fun):  The second concern I have comes after reading Dan Ariely’s great book “Predictably Irrational”.  In chapter 4, Ariely makes the distinction between (what he terms) “Social Norms” and “Market Norms”.  You can listen to him talk about it in the following (~4 minutes) video:  Another example of what is said in this video is an experiment Dan made where he took three groups for a 5-minute task on a computer, dragging circles into a square.  One group did it as a favor to the experimenter, one group was paid $5 and the third group was paid 50 cents.  The 50-cent group was less productive than the $5 group.  But the people who did the task as a favor were the most productive of all!  In case this artificial experiment is not convincing, Dan also gave real-life examples, including military service and the pro bono work of lawyers. In our case, we can often teach R in the University or use it to solve real world problems while getting paid (and being expected to be paid), but at the same time we ask and give help (answering questions and programming) online (and offline with friends) for free.  We are walking a thin line of keeping a psychological balance here.  Ariely wrote that: When a social norm collides with a market norm, the social norm goes away for a long time. In other words, social relationships are not easy to reestablish. And as Stormy Peters (executive director of the GNOME Foundation) wrote: One of the things about the open source community that continues to baffle those non-open source people is, “why do you do it?” Open source developers work on open source software for a number of reasons from scratching an itch to gaining a reputation to building a resume to contributing to a good cause. The interesting problem comes when money enters into the equation. Research shows that when someone works on something for free (for internal rewards) if you start paying them you replace those internal rewards. Then if you stop paying them, they will stop working on it. Does that hold true for open source software?  Are commercial companies killing open source by paying people to work on it? Or said differently (and elsewhere): Once you are paid to work on open source software, it would be hard to go back to doing it for free.  Hence my question (and fear): Would starting to pay people in the R community (who by now where working for free) to work on R (a free open source project) – will end up killing it? Let’s be clear here – we can pay open source developers, but this includes the risk of later loosing them from the community.  Would this always be the case – I don’t know (hence the room for discussion at the end of the post). The answer: Love One thing I do believe we should do is to be grateful:  Send developers kind e-mails, buy their books, link to their blogs/home-pages – show them we love their work.  Why am I using the “L” word? Because open source is (as Clay Shirky convinced me in his ~9 minutes video) is about Love:  And in the context of R: The quality of R would not have been made possible even if all the people who have worked for it where to pay the developers from day 1.  I am saying this again: If we where forced to pay to have R developed for us, it would have been more expansive then we could have been able to ever afford (under market norms).  Since some of the best statisticians in the world (please correct me if I am wrong), have spent countless hours (days/months/years) to make this software work and support it’s user base,  can you imagine how much money that would have cost? Answer (IMHO): no! Money, like a knife, is a tool – not good nor evil. On the one hand, switching from community appreciation to rewarding with money might turn out to be a dangerous path for the future of R. At the same time I believe (and I am willing to be proven wrong) that we SHOULD get money collected to our community, and that we should use it to pay people outside our community: graphic designers, UI people, CS students (who are not very deep into the R world), grants for young students and maybe even paying the R developers core team.  My current rule of thumb will be that money should be used only  mostly to get skills that are outside of our current community base. The questions this post reflects upon are relatively new to us (e.g: how to scale up social relationships in collaboration projects).  One of our best sources for solutions is probably the experience of other open source projects.  While history may not predict our future, it can still give us ideas and even inspire us of what can be done. I hope to link here to posts on the topic, if you have any, please link to them in the comments. (Originally I wrote about WordPress, but after getting responses from some friends closer to the core team – I decided to remove that section so to not misinform people) I don’t think R developers should be paid. (this is not exact and I erased this sentence) I believe money should be collected by the community for the community. Paying R developers is tricky, I don’t know how it can be done in a healthy and stable way (although I believe it can be done). A good strategy for spending community’s money could be to pay for services that are outside our community knowledge/skill base. Your opinions are welcomed. 	 0 Comments
Complex-valued linear models	https://www.r-bloggers.com/2010/09/complex-valued-linear-models/	September 16, 2010	ellbur	Someone has probably already written code to do this. But I couldn’t find it in CRAN, so here goes. Oh no lm() won’t take complex numbers! (or rather, it’ll take them, but it’ll discard the imaginary part.) Easy enough fix. Split into real. So, if we’re looking to discover the coefficients a1 and a2, we can split the vectors y and x like So if we have a function to do a complex fit, the first thing is to make new variables based on the inputs. Then put those into a data.frame and make an appropriate formula. It’s important to put the “-1″ in the formula so that lm() doesn’t include a constant term. (We might want a constant term, but it would have to look more like c(1, 0, 1, 0, …) because it is complex). Then do the fit and extract the coefficients. The whole function looks like Now test it Excellent. 	 0 Comments
R’s time is now	https://www.r-bloggers.com/2010/09/rs-time-is-now/	September 16, 2010	David Smith	Jeff Kelly of Tech Target has just published a feature article about R. While R has been around for almost 20 years now (R&R first started the project in 1993), “its time may have finally come”, he says. One thing I really like about the article is how well it highlights R's flexibility and “top-notch” data visualizations. It includes an in-depth profile of the creator of the  ggplot2 graphics system, Hadley Wickham, who talked about the R community and the flexibility of R: “One of the real strengths of R is the community,” Wickham said. “You get access to that absolute cutting-edge stuff.” R’s biggest differentiator, however, is its flexibility. Because of its code-based interface, statisticians and programmers have the ability to customize predictive analytics models and visualizations down to the tiniest detail, Wickham said. Most proprietary analytics tools simply don’t offer that level of granular control, instead offering users a pre-populated library of visualization templates. Also quoted is Marick Sinay, a quantitative financial analyst with Bank of America, who also lauded R's flexibility but also it's data visualization capabilities. Speaking for himself, he said: “R produces publication-quality [visualizations],” Sinay said. “SAS’s graphics are years behind,” he said, adding that they “look kind of tacky” in comparison. The article also talks about how the team here at Revolution is “bringing R out of the shadows” and promoting its use in commercial environments with the add-ons to R included in Revolution R Enterprise: the big-data tools of RevoScaleR and the web services integration of RevoDeployR (announced just this week and profiled separately today in The Register and IT Unmasked). It also previews the graphical user interface we're building for R, to be released next year. Read the entire article by Jeff Kelly at SearchBusinessAnalytics.com. SearchBusinessAnalytics.com: New GUIs aim to make R open source analytics language more accessible  	 0 Comments
Did what you write drive what I read?	https://www.r-bloggers.com/2010/09/did-what-you-write-drive-what-i-read/	September 16, 2010	Timothée	GoogleReader allows you to track your activity, by representing the number of news items read and published by day and by hour. I use it quite a lot to stay up to date with the scientific literature (I subscribed to probably over 30 journals) and a bunch of other feeds. Stuff tend to accumulate faster in my unread folder, so I often have a lot of material to read. I was wondering if the two patterns (what is published vs. what I read) share a common temporal dynamic. Using the digitize package and a screenshot, I was able to get my own data for the last 30 days, and an overview of the pattern by hours (for the same period). My big question was: is there some days of the weeks, or hours of the day, at which I read more than the other publish? Using R and ggplot2, it was really easy to visualize the results. Below are the results sorted by day of the week (where 1 is monday).  It tend to confirm what I already observed. I tend to let stuff accumulate over the week, and go through it from friday night to sunday. The fact that the number of items published and read are the same on the week-end can indicate that I am more prone to read news « as they happen ». Perhaps the fact that I spent all my week-ends for the last month in the lab, and that my RSS feeds were a good source of entertainment while waiting for bacteria to grow can explain it. Now that it is established that I read stuff « as it happens » on the week-end, and less so during the week, let’s focus on the hourly pattern.  As I read it, this graph is a summation of all the events that occurred at a given time for a 30 days period, so there could be a potential bias if my daily pattern changes between week and week-end. Anyway. I am usually asleep between 1am and 7 to 8am, where there is a strong decrease in my reading activity. As a matter of fact, there are less published items in the same time (probably because european feeds are quite numerous in my google reader). My strongest period of activity is between 5pm and 9pm, and overall my reading pattern is similar to the publishing pattern. But are the residuals evenly distributed? As shown below, no, they are not.  I tend to read more around noon (usually a much more casual time in the lab) and after 8pm, i.e. when I’m back home. So, all in all, I guess the message is that the volume of published news somehow impacts my reading pattern (but otherwise I would spend hours going through stuff), but the divergence between what I am actually reading and what I « should » be reading based on the publication volume are not evenly distributed through time. 	 0 Comments
Principal Component Analysis (PCA) vs Ordinary Least Squares (OLS): A Visual Explanation	https://www.r-bloggers.com/2010/09/principal-component-analysis-pca-vs-ordinary-least-squares-ols-a-visual-explanation/	September 16, 2010	JD Long	"Over at stats.stackexchange.com recently, a really interesting question was raised about principal component analysis (PCA). The gist was “Thanks to my college class I can do the math, but what does it MEAN?” I felt like this a number of times in my life. Many of my classes were focused on the technical implementations they kinda missed the section titled “Why I give a shit.” A perfect example was my Mathematics Principles of Economics class which taught me how to manually calculate a bordered Hessian but, for the life of me, I have no idea why I would ever want to calculate such a monster.  OK, that’s a lie. Later in life I learned that bordered Hessian matrices are a second derivative test used in some optimizations. Not that I would EVER do that shit by hand. I’d use some R package and blindly trust that it was coded properly. So back to PCA: as I was reading the aforementioned stats question I was reminded of a recent presentation that Paul Teetor gave at a August Chicago R User Group. In his presentation on spread trading with R he showed a graphic that illustrated the difference between OLS and PCA. I took some notes and went home and made sure I could recreate the same thing. If you have wondered what makes OLS and PCA different, open up an R session and play along. Your Independent Variable Matters: The first observation to make is that regressing x ~ y is not the same as y ~ x even in a simple univariate regression. You can illustrate this by doing the following: set.seed(2)
x 
 y 
e 
y 
 plot(x,y)
yx.lm 
lines(x, predict(yx.lm), col=”red”) xy.lm 
lines(predict(xy.lm), y, col=”blue”) You should get something that looks like this:  So it’s obvious they give different lines. But why? Well, OLS minimizes the error between the dependent and the model. Two of these errors are illustrated for the y ~ x case in the following picture:  But when we flip the model around and regress x ~ y then OLS minimizes these errors:  Ok, so what about PCA? Well let’s draw the first principal component the old school way: #normalize means and cbind together
xyNorm 
plot(xyNorm) #covariance
xyCov 
eigenValues 
eigenVectors 
 plot(xyNorm, ylim=c(-200,200), xlim=c(-200,200))
lines(xyNorm[x], eigenVectors[2,1]/eigenVectors[1,1] * xyNorm[x])
lines(xyNorm[x], eigenVectors[2,2]/eigenVectors[1,2] * xyNorm[x]) # the largest eigenValue is the first one
# so that’s our principal component.
# but the principal component is in normalized terms (mean=0)
# and we want it back in real terms like our starting data
# so let’s denormalize it
plot(xy)
lines(x, (eigenVectors[2,1]/eigenVectors[1,1] * xyNorm[x]) + mean(y))
# that looks right. line through the middle as expected # what if we bring back our other two regressions?
lines(x, predict(yx.lm), col=”red”)
lines(predict(xy.lm), y, col=”blue”) PCA minimizes the error orthogonal (perpendicular) to the model line. So first principal component  looks like this:  The two yellow lines, as in the previous images, examples of two of the errors which the routine minimizes. So if we plot all three lines on the same scatter plot we can see the differences:  The x ~ y OLS and the first principal component are pretty close, but click on the image to get a better view and you will see they are not exactly the same. All the code from the above examples can be found in a gist over at GitHub.com. It’s best to copy and past from the github as sometimes WordPress molests my quotes and breaks the codez. The best introduction to PCA which I have read is the one I link to on Stats.StackExchange.com. It’s titled “A Tutorial on Principal Components Analysis” by Lindsay I Smith. "	 0 Comments
BLAS, BLASter, BLAStest: Some benchmark results, and a benchmarking framework	https://www.r-bloggers.com/2010/09/blas-blaster-blastest-some-benchmark-results-and-a-benchmarking-framework/	September 15, 2010	Thinking inside the box	"

Another issue that I felt needed addressing was a
comparison between the different alternatives available, quite possibly
including GPU computing.  So a few weeks ago I sat down and wrote a small package to
run, collect, analyse and visualize some benchmarks. That package, called
gcbd (more about the
name below) is now on CRAN as of this morning.
The package both facilitates the data collection for 
the paper
it also contains (in the vignette form common among
R packages) and provides code to
analyse the data—which is also included as a SQLite database.  All this is done in the
Debian and
Ubuntu context by transparently
installing and removing suitable packages providing BLAS implementations:
that we can fully automate data collection over several competing implementations via
a single script (which is also included). Contributions of benchmark results
is encouraged—that is the idea of the package.

 
The paper itself
describes the background and technical details before presenting the
results.  The benchmark compares the basic reference BLAS, Atlas (both single- and
multithreaded), Goto, Intel MKL and a GPU-based approach. 
This blog post is not the place to recap all results, so please do see the 
paper for more
details. But one summary chart regrouping the main results fits well here:

 
 


This chart, in a log/log form, shows how reference BLAS lags everything, how
multithreaded newer Atlas improves over the standard Atlas package currently
still the default in both distros, how the Intel MKL (available via Ubuntu)
is fairly good but how Goto wins almost everything.  GPU computing is
compelling for really large sizes (at double precision) and too costly at
small ones.  It also
illustrates variability and different computational cost across the methods
tested: svd is more expensive than level-3 matrix multiplication, and the
different implementations are less spread apart. More details are in the
paper; code, data etc
are in the package gcbd.

 

The larger context is to do something like this benchmarking
exercise, but across distributions, operating systems and possibly also GPU
cards. Mark and I started to talk about this during and after R/Finance earlier this year and have some ideas.
Time permitting, that work should be happening in the
GPU/CPU Benchmarks (gdb)
project, and that’s why this got called gcbd as a simpler GPU/CPU Benchmarks on
Debian Systems study.

 "	 0 Comments
Dumping functions from the global environment into an R script file	https://www.r-bloggers.com/2010/09/dumping-functions-from-the-global-environment-into-an-r-script-file/	September 15, 2010	Tal Galili	Looking at a project you didn’t touch for years poses many challenges.  The less documentation and organization you had in your files, the more time you’ll have to spend tracing back what you did back when the code was written. I just opened up such a project, that was before I ever knew to split my .r files to “data.r”, “functions.r”, “do.r”.  All I have are several versions of an old .RData file and many .r files with a mix of functions and commands (oh the shame!) One idea I had for the tracing back was to take the latest version of .RData I had, and see what functions I had in it’s environment.  simply typing ls() wouldn’t work.  Also, I wanted to have a list of all the functions that where defined in my .RData environment.  Thanks to the code recently published by Richie Cotton, I was able to create the “save.functions.from.env”.  This function will go through all your defined functions and write them into “d:\\temp.r”. I hope this might be useful to one of you in the future, here is the code to do it: Update: Joshua Ulrich gave on stackoverflow another solution for this challenge: And also suggested to look into ?prompt (which creates documentation files for objects) and / or ?package.skeleton. 	 0 Comments
Three dimensional plots using rgl package	https://www.r-bloggers.com/2010/09/three-dimensional-plots-using-rgl-package/	September 15, 2010	tanvir	" We all know that R can do amazing things including 3 dimensional plots. scatterplot3d is probably the most popular package for doing this. But a few days ago I got introduced with rgl package which can do 3 dimensional graphs with some added advantages like we can rotate the plot using mouse, zoom in or out using the mouse scroll wheel and even can play beautiful animations.
The data I have used for all the examples here is-   Here, X,Y and Z are monthly import,export and first differenced net foreign asset of Bangladesh from July 1998 to July 2010  respectively. That means there are a total of 133 observations in each vector. Here is the command for doing a scatter plot in rgl package-
 And the plot is (we can rotate the plot as our will, here just two different views are given)-


We can save the screen shot of the plot as png file by the command-  The plot can be saved as other format like pdf and eps by rgl.postscript command.
In either cases the saved file will no longer be rotatable.
Now, we can make the plot a bit more beautiful by putting type=”s” in plot3d function. That means the points will be represented by spheres. The code is-   If we leave the radius argument as default and put col=rainbow(n) the plot becomes very interesting to see. The command will be-  And the output of this command is the main figure of this article, have a look again.
This plot gives us no added advantage, in fact it is very difficult to understand, but I can not resist myself (because the plot is so beautiful now!). Now, let’s do some fancy animations. Let’s rotate the plot automatically so that we can relax and see every aspect of our three dimensional plot.
Here are the commands-  This command will rotate the plot for 10 seconds in the X axis at 180 degrees.
If you want to rotate it in Y axis, just put x=0 and y=1. And by changing the values of angle, x, y and z we can make the plot rotate in every possible way. Now let’s do something more interesting- let’s write a simple function that would make the plot rotate automatically in several ways for a given number of times, so that we can have very well and details look of it.  The function is-  Now write-  Watch and enjoy. There are so many other interesting things in rgl package. Those who are interested in statistical programming (especially in R) please try it and share your knowledge and experience and help me to enhance mine. "	 0 Comments
New R User Group in Brisbane	https://www.r-bloggers.com/2010/09/new-r-user-group-in-brisbane/	September 15, 2010	David Smith	There are now four — count 'em, four — local R user groups in Australia, with the addition of the latest group in Brisbane, Queensland. GRUB (Group for R Users in Brisbane) is just getting started and doesn't have a meeting scheduled yet, but if you're in the vicinity why not join up and make a suggestion! meetup.com: Group for R Users in Brisbane (GRUB) 	 0 Comments
R Function Reference	https://www.r-bloggers.com/2010/09/r-function-reference/	September 15, 2010	Thomas Hopper	Updated below The R Function Reference is a mind map that I created as a guide for novice and intermediate users of the R statistics language. When you first open it, I suggest that you collapse all the nodes by clicking on the “Expand/Collapse all nodes” button in the bottom left of the screen to make the map easier to navigate. You can also adjust the zoom level with the slider next to that button. The top-level nodes of the R Function Reference The mind map is arranged in eight sections, or main branches, arranged by task. What do you want to do? Each branch covers a general set tasks, such as learning to use R, running R, working with data, statistical analysis or plotting data. The end of each string of nodes is generally a function and example. The Reference provides code fragments, rather than details of the function or complete reproducible code blocks. Once you’ve followed the Reference and have an idea of how to accomplish something, you can look up the details in R’s help system (e.g. “?read.csv” to learn more about using the read.csv() function), or search Google or the online R-Help mailing list archives for answers using the function name. There are a lot of useful nodes and examples, especially in the “Graphs” section, but the mind map is not complete; some trails end before you get to a useful function reference. I am sorry for that, but it’s a work in progress, and will be slowly updated over time. Comments and suggestions are welcome. In comments, several users reported problems opening the mind map. With a little investigation, it appears that the size of the mind map is the problem. To try to fix the problems , I have split the mind map out into several small mind maps, all linked together. The new main mind map is the R Function Reference, Main. The larger branches on this main map no longer expand to their own content, but contain a link to a “child” mind map. The link looks like a sheet of paper with an arrow pointing to the right, click on it and little cartoon speech bubble will pop up with a link that you have to click on to go to the child mind map. Likewise, the central nodes on the child mind maps contain a link back to the main mind map. Due to load times and the required extra clicks, this may slightly reduce usability for users who didn’t have a problem with the all-in-one version, but will hopefully make the mind map accessible to a broader audience. I have to offer praise to the developers of Mind42. Though I couldn’t directly split branches off into their own mind maps or duplicate the mind map, it was very easy to export the mind map as a native Mind42 file and then import it multiple times, editing the copies without any loss of data or links. The ability to link directly between mind maps within Mind42 was also a key enabling feature. Considering that this is a free web app, its capabilities are most impressive. They were also quick to respond when I posted a call for help on the Mind42 forum. Please let me know how the new, “improved” version works. The old mind map, containing everything, is still available, but I will not update it. 	 0 Comments
Who’s the best all rounder in World Cup Cricket history?	https://www.r-bloggers.com/2010/09/whos-the-best-all-rounder-in-world-cup-cricket-history/	September 14, 2010	prasoonsharma		 0 Comments
Who’s the best bowler in World Cup Cricket history?	https://www.r-bloggers.com/2010/09/whos-the-best-bowler-in-world-cup-cricket-history/	September 14, 2010	prasoonsharma		 0 Comments
Who’s the best batsman in World Cup Cricket history?	https://www.r-bloggers.com/2010/09/whos-the-best-batsman-in-world-cup-cricket-history/	September 14, 2010	prasoonsharma		 0 Comments
News on MCMSki III	https://www.r-bloggers.com/2010/09/news-on-mcmski-iii/	September 14, 2010	xi'an	"Here is a message sent by the organisers of MCMSki III in Utah next early January. When registering, make sure to tick the free registration for Adap’skiii as well! The fourth joint international meeting of the IMS (Institute of Mathematical Statistics) and ISBA (International Society for Bayesian Analysis), nicknamed “MCMSki III“, will be held at The Canyons resort in Park City, Utah from Wednesday, January 5 to Friday, January 7, 2011. The central (but not sole) theme of the conference will be  Markov chain Monte Carlo (MCMC) and related methods and applications, and will feature 3 plenary speakers (Nicky Best, Mike Newton, Jeff Rosenthal) and six invited sessions from internationally known experts covering a broad array of current and developing statistical practice: * Environmental Health Statistics (organized by: Montse Fuentes)
* Modeling Dependence for High Throughput Data (organized by: Peter Mueller)
* Advances in MCMC for Genomics (organized by: Giovanni Parmigiani and Clelia DiSerio)
* MCMC for Computationally Intensive Inverse Problems (organized by: Dave Higdon)
* Bayes versus Frequentism in Observational Studies (organized by: Sander Greenland) In addition, a pre-conference “AdapSki” satellite meeting on adaptive and other advanced MCMC methods will take place on January 3-4,  2011, with Profs. Christian Robert and Christophe Andrieu again serving as lead organizers. From our registration page, you will find links to the IMS-sponsored conference registration page, the Canyons lodging page, and our abstract submission page.  Please note the “early bird” registration deadline of *November 1, 2010*. Finally (and most importantly for some), we are very pleased to announce that we have received funds from NSF, NSA, NIH, and ISBA sufficient to help support the travel expenses of perhaps 30 junior investigators (defined as current PhD student, or less than 5 years since PhD).  Some of these funds (NSA) are restricted to students from the U.S., while others (ISBA) are primarily intended for students from economically disadvantaged countries. At the time you register for this support, in addition to the three required items you send by email (copy of your CV, copy of your paper, and *brief* supporting note from your advisor), please also submit your abstract and indicate whether or not you would like to be considered for one of 4 oral presentation slots in our new Young Investigator Invited Session (10 am on Thursday Jan 6).  Please note that the deadline for applying for this support is *October 22, 2010*; again, the deadline to register for the conference itself without a late fee is November 1, 2010. We look forward to welcoming you in Park City this January! Best, Brad Carlin and Antonietta Mira, MCMSki III conference co-organizers
Shane Reese, MCMSki III local arrangements chair "	 0 Comments
Introducing RevoDeployR: Web Services for R	https://www.r-bloggers.com/2010/09/introducing-revodeployr-web-services-for-r/	September 14, 2010	David Smith	Today, Revolution Analytics announced another add-on to R as part of Revolution R Enterprise. RevoDeployR is a Web Services framework for R, designed to make it easy to scalably and securely integrate computations done in R into other applications like spreadsheets and web pages. The idea is simple: as an R programmer, you can create a script in R and then publish it to the RevoDeployR server. To make things easy for integration, you'll define the interface to the script in terms of well-specified inputs (akin to  function arguments) and outputs (data, files, graphics). You can use all of the features of R (and extensions like CRAN packages) in the script to implement any calculation you want: grab data from a database (or from one of the supplied inputs), build a statistical model, return forecasts, generate a chart. Then, it's over to an application developer to integrate your R code into an application. (Of course, if you're a crack API/web services programmer you can do this part, too, but we'll assume different people are doing the R programming and the integration development.) The application developer will use the extension language of the target application: .NET for Microsoft Excel, JavaScript for Jaspersoft, etc. to extend it to call out to RevoDeployR, using the developer libraries provided with RevoDeployR. How the interface looks and behaves is completely up to the developer (within the limits of the target application, anyway): there could be buttons, fields, and file choosers to control the inputs to the R script, and the outputs can be embedded in the application in a data grid, file download, or chart. From the point of view of the application user, all of this happens invisibly, behind the scenes. There might be a new control, or menu item in the application they've been using for years, and there's some new information (based on some fancy statistical analysis created by the Research folks down the hall) to make decisions with, but other than that everything's as before. The user is most likely unaware that their application is now making RESTful API calls to a remote RevoDeployR server via the web, or even that R is involved at all. And that's a good thing: now, more people can benefit from the power of R, even if they don't know R themselves. If you want to learn more about RevoDeployR, check out this white paper by Joseph Rickert. I'll also be giving a webinar about RevoDeployR in conjunction with Jaspersoft next Wednesday. Finally, you can see an overview of RevoDeployR and some demonstrations of applications using RevoDeployR in this YouTube video, embedded after the jump. Revolution Analytics: RevoDeployR — Application Integration, Deployment and Administration for R  	 0 Comments
Which functions in R base call internal code?	https://www.r-bloggers.com/2010/09/which-functions-in-r-base-call-internal-code/	September 14, 2010	richierocks	"In a recent question on Stack Overflow about speeding up group-by operations, Marek wondered which functions called .Internal code (and consequently were fast).  I was surprised to see that there appears to be no built-in way to check whether or not this is the case (though is.primitive is available for primitive functions). Writing such a function is quite straight forward.  We simply examine the contents of the function body, and serach for the string “.Internal” within it. callsInternalCode <- function(fn)

{

+++if(!require(stringr)) stop(""stringr package required"")

+++any(str_detect(capture.output(body(fn)), "".Internal""))

}

 You can retrieve all the function in the base package with this one-liner taken from an example on the Filter help page. funs <- Filter(is.function, sapply(ls(baseenv()), get, baseenv())) Now getting the list of functions that call internal code is easy. names(funs)[sapply(funs, callsInternalCode)]

[1] ""abbreviate""                 ""agrep""

[3] ""all.names""                  ""all.vars""

[5] ""anyDuplicated.default""      ""aperm""

...

 "	 0 Comments
Some useful bar plots using R	https://www.r-bloggers.com/2010/09/some-useful-bar-plots-using-r/	September 14, 2010	tanvir	" In this article I am trying to show how to produce bar plots using R.  Many of my friends think SPSS is the most useful software for producing plots and they keep using it (some of them even use Excel!).
My goal is to show that R can do every type of graphs that other commercial softwares can do. In fact it does much better than the simple point and click packages, as R gives us much better control over our analysis.  The data of my concern is –  This data is not a real data, completely created by me just to do experiments using R codes. Now, I want to produce a bar plot where ‘sex’ would be the category axis and the clusters will represent mean and median ‘income’, i.e. I want to produce a plot that we produce in SPSS by the command- graph
/bar=mean(income) median(income) by sex.
 So, at first I calculate mean and median ‘income’ for both male and female. m1
m2
r
b
legend(“topleft”,c(“mean”,”median”),col=c(“green”,”blue”),pch=15) Then if I want to put the numbers represented by the bars above them,
the code will be- text(x=b,y=c(r[1],r[2],r[3],r[4]),labels=c(round(r[1],2),
round(r[2],2),r[3],r[4]),pos=3,col=”black”,cex=1.25) And the plot is- Now, if I want to produce a more complex plot that is a bar plot that will show mean income for all the three districts separately for male and female, i.e. the plot we produce in SPSS by the command- graph
/bar=mean(income) by sex by district.
 For the required summary statistics I used a package ‘Epi’ and with the following command produced a very useful summary table- s=stat.table(list(district,sex),contents=list(mean(income))) And the produced table is-
 As we all know this statistics can also be found by a few lines of codes instead of ‘stat.table’, but I used it just to cut down some codes.
Then, I did the following commands- female
male
r
row.names(r)
b
legend(“topleft”,c(“barisal”,”chittagong”,”dhaka”),col=
c(“red”,”green”,”yellow”),pch=15,bty=”n”)
text(x=b,y=c(r[1:6]),labels=c(r[1:6]),cex=1.25,pos=3)
 And the plot is-
 Hope this codes will be useful to those who really want to do every type of statistical work(including producing graphs) in R.  "	 0 Comments
Example 8.5: bubble plots part 3	https://www.r-bloggers.com/2010/09/example-8-5-bubble-plots-part-3/	September 14, 2010	Ken Kleinman		 0 Comments
A Not Quite Random Number Generator (NQRNG)	https://www.r-bloggers.com/2010/09/a-not-quite-random-number-generator-nqrng/	September 13, 2010	Matt Shotwell	I connected the instrumentation amplifier described in an earlier post to a piezoelectric transducer (buzzer) and made recordings at 5000 gain. The plot below shows 1000 such measurements over 1.0 seconds. There is a 4.0 second (at 1000Hz) sample of the data here piezo.csv. There is a clear sinusoidal signal in these data of about 60Hz. These findings were baffling at first, but apparently result from capacitative coupling between the measurement device and 60Hz household AC power lines.  I fit simple periodic mean model to these data: , where  is the measured ADC (Analog to Digital Converter) value,  is time in seconds,  is the mean ADC value,  is the peak amplitude,  is the frequency in Hertz,  is the phase in radians, and  is a zero-mean error term. The blue curve in the plot represents the least-squares fit to these data.  The least-squares estimates are as follows:   The estimated frequency () is very close to the theoretical value (60). The ADC of the ATmega168 has 10 bit precision. That is, values can range from  to . The Atmel manual gives the following formula for back-calculating the voltage at the ADC pin: , where  is the ADC value and  is the reference voltage ( in my setup). In addition, the instrumentation amplifier provides  gain. Accounting for gain, the voltage measured between the instrumentation electrodes is . The estimated peak amplitude in ADC units was , so . That is, household AC coupling induced a  peak potential between the instrumentation electrodes. In reality, the gain of the amplifier is less than , and  is a lowball estimate. Assuming there are no other signals in these data (which is likely incorrect), the residuals in this regression ought to resemble independent random variates. The histogram for the residuals looks good (bell shaped, no noticeable bias or skew), however, there is obvious residual autocorrelation. The figure below is a plot of the autocorrelation within the residuals.   It’s clear that this method isn’t quite ready to replace the pseudo-RNGs. As time permits, I plan to try some signal processing with these data, in hopes to isolate the independent noise. If the only signal is that induced by capacitative coupling, then a high pass filter (at 60Hz) ought to do the trick.  A recent blog post at Xi’An’s mentioned some tests for randomness, including the Marsaglia Diehard Battery and the NIST Test Suite. Maybe if I get something that isn’t obviously nonrandom, I’ll try out some of these tests. 	 0 Comments
10w2170, Banff [2]	https://www.r-bloggers.com/2010/09/10w2170-banff-2/	September 13, 2010	xi'an	Over the two days of the Hierarchical Bayesian Methods in Ecology workshop, we managed to cover normal models, testing, regression, Gibbs sampling, generalised linear models, Metropolis-Hastings algorithms and of course a fair dose of hierarchical modelling. At the end of the Saturday marathon session, we spent one and half discussing some models studied by the participants, which were obviously too complex to be solved on the spot but well-defined so that we could work on MCMC implementation and analysis. And on Sunday morning, a good example of Poisson regression proposed by Devin Goodman led to an exciting on-line programming of a random effect generalised model, with the lucky occurrence of detectable identifiability issues that we could play with… I am impressed at the resilience of the audience given the gruesome pace I pursued over those two days, covering the five first chapters of Bayesian        Core, all the way to the mixtures! In retrospect, I think I need to improve my coverage of testing as the noninformative case presumably sounded messy. And unconvincing. I also fear the material on hierarchical models was not sufficiently developed. But, overall, the workshop provided a wonderful opportunity to exchange with bright PhD students from Ecology and Forestry about their models and (hierarchical) Bayesian modelling. As an aside, I also noticed that several of the participants from the University of Alberta were (considering) using data cloning, which is a MCMC method for computing maximum likelihood estimates that was locally developed by Subhash Lele (University of Alberta). The method is based on using MCMC data augmentation methods to optimise likelihood functions by considering a larger and larger number of replicates of the missing data. Data cloning is thus essentially a user-friendly simulated annealing technique for missing data models and one of the multiple replicas of what I called the prior feedback technique in 1991. The idea of prior feedback (also exposed in our 1996 JASA paper with Gene Hwang) is that when the likelihood is raised to a high power, any prior distribution with an unrestricted support leads to a “posterior” distribution concentrated around the MLE. (I actually got this idea from reading the 1991 Read Paper by Murray Aitkin on posterior Bayes factors. The following discussion was quite critical, including Dennis Lindley’s now famous “One hardly advances the respect with which statisticians are held in society by making such declarations”!, but instead of solving the problem with improper priors in Bayesian testing it suggested to me a computational technique to compute MLE’s…)  With Arnaud Doucet and Simon Godsill, we developed in 2002 an algorithm for missing data and state-space models called SAME (for state augmentation for marginal estimation) that got published in Statistics and Computing but resurfaces periodically in the literature, the latest avatar being data cloning… Judging from a web search, the data cloning approach is getting more popular in Ecology, including an R package named dcr and developed by Peter Solymos. 	 0 Comments
R 2.12.0 scheduled for October 15	https://www.r-bloggers.com/2010/09/r-2-12-0-scheduled-for-october-15/	September 13, 2010	David Smith	Just announced: the next release of R, version 2.12.0, will be released on October 15 (in source code form; binaries usually follow the same day or soon thereafter). Looking at the NEWS file, this release features several dozen tweaks and bug fixes to functions in the R language, but nothing so dramatic to prevent it being an easy upgrade from R 2.11.1. R-announce mailing list: R 2.12.0 scheduled for October 15 	 0 Comments
the spatial concentration of Green support	https://www.r-bloggers.com/2010/09/the-spatial-concentration-of-green-support/	September 13, 2010	jackman	I did some poking and prodding of the polling place data, looking at the way Green support is highly concentrated in the inner-capital cities (and in Melbourne and Sydney, in particular).   There are a few interesting exceptions that will make sense to Australian readers: Nimbin, Kuranda, Broome… A sample graph from the report:  	 0 Comments
Ross Ihaka to R:  Drop Dead	https://www.r-bloggers.com/2010/09/ross-ihaka-to-r-drop-dead/	September 13, 2010	Andrew Gelman		 0 Comments
RcppArmadillo 0.2.6	https://www.r-bloggers.com/2010/09/rcpparmadillo-0-2-6/	September 13, 2010	Thinking inside the box	"
The short NEWS file extract follows:

 
More information is on the 
RcppArmadillo page.
Questions, comments etc should go to the
rcpp-devel mailing list
off the R-Forge page.

 "	 0 Comments
Handling .Z files	https://www.r-bloggers.com/2010/09/handling-z-files/	September 13, 2010	Steven Mosher	"A while back Steve Mcintyre was looking for a way to handle .Z files in R Ron Broberg over at the whiteboard had an approach that steve adopted both for untar and for uncompressing .Z files.  While the approach is slick, its somewhat of a hack. Nothing wrong with that, but I wanted something a bit more elegant. Long ago a reader Nicholas created a package on R called “uncompress” to handle the .Z file issue, but steve was not able to get it to work and neither was I. Luckily Nicholas made his contact info available and I was able to get him a bug report with a file (ghcnv2.Z) and the code I used to download the file and unzip it. The error was relatively minor and related to end of file padding. Nicholas fixed the “bug”  and today I had sucess with downloading and unzipping .Z files. So now in Moshtemp when you download the ghcnv2.Z file I will automagically unzip it for you. next I decided to look at the untar problem. Steve Mc had “untared” files by copying a version of untar down to his system and then fed that exe a command from inside R. That’s un necessary as R has an “untar” command. So, below, we can see how to download “tar”  files  from NOAA, untar them, and then uncompress them.  Any questions on “uncompress” just write. Its on CRAN ftp   
 Imma  
 start 
 end   
 years 
 Tar_Dir 
 Zfile_Dir 
 Icoads_Dir 
 dir.create(Tar_Dir) dir.create(Zfile_Dir) dir.create(Icoads_Dir) fnames 
 # fnames is ALSO fetchable with RCurl.. when I learn it getIcoadsTar 
 for(i in 1:length(files)){ fullname 
 destinationfile=file.path(tDir,files[i],fsep=.Platform$file.sep) download.file(fullname,destfile=destinationfile) untar(destinationfile,exdir=file.path(getwd(),zDir,fsep=.Platform$file.sep))} } unZipIcoads 
 files 
 localnames
 destnames 
 for(i in 1:length(files)){ handle 
 data 
 close(handle) uncomp_data 
 handle 
 writeBin(uncomp_data, handle) close(handle) } } The first function will download and untar the files. When that completes, you unzip them all. Have a nice weekend UPDATE:  a cleaner version that cleans up .Z files as you progress: ftp   
 Imma  
 start 
 end   
 years 
 Tar_Dir 
 Zfile_Dir 
 Icoads_Dir 
 dir.create(Tar_Dir) dir.create(Zfile_Dir) dir.create(Icoads_Dir) fnames 
 download.unTarUnzipIcoads 
 for(i in 1:length(tars)){ fullname 
 destinationfile=file.path(tDir,tars[i],fsep=.Platform$file.sep) download.file(fullname,destfile=destinationfile) untar(destinationfile,exdir=file.path(getwd(),zDir,fsep=.Platform$file.sep)) files 
 localnames
 destnames 
 for(j in 1:length(destnames)){ handle 
 data 
 close(handle) uncomp_data 
 handle 
 writeBin(uncomp_data, handle) close(handle) } unlink(files) } } And if you just want a stand alone version to unzip .Z files unZipdotZ
 # this function is called for the side effect of uncompressing a .Z file # Zfile is a path to the Zfile # destfile is the uncompressed file to be written # no protection against overwriting # remove the Z file if(!file.exists(Zfile))stop( cat(Zfile,” does not exist”)) handle 
 data 
 close(handle) uncomp_data 
 handle 
 writeBin(uncomp_data, handle) close(handle) if(remove==TRUE)unlink(Zfile) } "	 0 Comments
One Day Cricket Champions by Decade	https://www.r-bloggers.com/2010/09/one-day-cricket-champions-by-decade/	September 12, 2010	prasoonsharma		 0 Comments
“simply start over and build something better”	https://www.r-bloggers.com/2010/09/%e2%80%9csimply-start-over-and-build-something-better%e2%80%9d/	September 12, 2010	xi'an	"The post on the shortcomings of R has attracted a huge number of readers and Ross Ihaka has now posted a detailed comment that is fairly pessimistic… Given the directions drafted in this comment from the father of R (along with Robert Gentleman), I once again re-post this comment as a main entry to advertise more broadly its contents. (Obviously, the whole debate is now far beyond my reach!)
 Since (something like) my name has been taken in vain here, let me chip in. I’ve been worried for some time that R isn’t going to provide the base that we’re going to need for statistical computation in the future. (It may well be that the future is already upon us.)  There are certainly efficiency problems (speed and memory use), but there are more fundamental issues too. Some of these were inherited from Sand some are peculiar to R. One of the worst problems is scoping. Consider the following little gem. f =function() {
if (runif(1) > .5)
x = 10
x
} The x being returned by this function is randomly local or global. There are other examples where variables alternate between local and non-local throughout the body of a function. No sensible language would allow this.  It’s ugly and it makes optimisation really difficult.  This isn’t the only problem, even weirder things happen  because of interactions between scoping and lazy evaluation. In light of this, I’ve come to the conclusion that rather than “fixing” R, it would be much more productive to simply start over and build something better.  I think the best you could hope for by fixing the efficiency problems in R would be to boost performance by a small multiple, or perhaps as much as an order of magnitude. This probably isn’t enough to justify the effort (Luke Tierney has been working on R compilation for over a decade now). To try to get an idea of how much speedup is possible, a number of us have been carrying out some experiments to see how much better we could do with something new.  Based on prototyping we’ve been doing at Auckland, it looks like it should be straightforward to get two orders of magnitude speedup over R, at least for those computations which are currently bottle-necked.  There are a couple of ways to make this happen. First, scalar computations in R are very slow. This in part because the R interpreter is very slow, but also because there are a no scalar types.  By introducing scalars and using compilation it looks like its possible to get a speedup by a factor of several hundred for scalar computations. This is important because it means that many ghastly uses of array operations and the apply functions could be replaced by simple loops.  The cost of these improvements is that scope declarations become mandatory and (optional) type declarations are necessary to help the compiler. As a side-effect of compilation and the use of type-hinting it should be possible to eliminate dispatch overhead for certain (sealed) classes (scalars and arrays in particular). This won’t bring huge benefits across the board, but it will mean that you won’t have to do foreign language calls to get efficiency. A second big problem is that computations on aggregates (data frames in particular) run at glacial rates.  This is entirely down to unnecessary copying because of the call-by-value semantics. Preserving call-by-value semantics while eliminating the extra copying is hard.  The best we can probably do is to take a conservative approach.  R already tries to avoid copying where it can, but fails in an epic fashion. The alternative is to abandon call-by-value and move to reference semantics.  Again, prototyping indicates that several hundredfold speedup is possible (for data frames in particular). The changes in semantics mentioned above mean that the new language will not be R. However, it won’t be all that far from R and it should be easy to port R code to the new system, perhaps using some form of automatic translation. If we’re smart about building the new system, it should be possible to make use of multi-cores and parallelism. Adding this to the mix might just make it possible to get a three order-of-magnitude performance boost with just a fraction of the memory that R uses. I think it’s something really worth putting some effort into. I also think one other change is necessary. The license will need to a better job of protecting work donated to the commons than GPL2 seems to have done. I’m not willing to have any more of my work purloined by the likes of Revolution Analytics, so I’ll be looking for better protection from the license (and being a lot more careful about who I work with). "	 0 Comments
Missing Data in R	https://www.r-bloggers.com/2010/09/missing-data-in-r/	September 12, 2010	Quantitative Finance Collector		 0 Comments
R on the iPhone	https://www.r-bloggers.com/2010/09/r-on-the-iphone-2/	September 12, 2010	Jon	"iPhone users who have jailbroken their phone have been able to compile R for a while now since they have access to the command line via MobileTerminal.  However, I just recently found out that a package is now up on Cydia to ease installation. So I fired up my iPhone 4 and installed R on it just for fun using the instructions here. I’ll write out the complete steps for installing R onto the iPhone here as they are actually quite simple: 1. Have a jailbroken iPhone and the Cydia package manager app installed
(instructions not provided here).  2. Install OpenSSH and MobileTerminal via Cydia
Note that if you are running iOS 4, the current version of MobileTerminal on Cydia does not work properly, you’ll need to get an updated version from: MobileTerminal 354.3-12 Debian package Copy the file to your iPhone via SFTP (or download it to your iPhone directly) and then install it via SSH using: dpkg -i mobileterminal_364.3-12_iphoneos-arm.deb 3. Add a new repository to Cydia
In Cydia, go to Manage->Sources and Add the following repository: http://leafmoon.users.sourceforge.net/cydia/ 4. Install R the Statistical Language
Use Search to find it, if you cannot, make sure your user level is either Hacker or Developer level under Manage->Settings because R is a command-line program so it will not show up if you are at the User level. You can change your level back to User immediately afterwards. 5. Run R from command line
Load up MobileTerminal and run R by simply typing: R  Other notes Thanks to ech0chrome for the Cydia repository and the reference instructions. "	 0 Comments
Chicago Half Marathon 2010	https://www.r-bloggers.com/2010/09/chicago-half-marathon-2010/	September 12, 2010	Thinking inside the box	"
Race conditions were fantastic. We had a rainy and gray day yesterday but
today is pure bliss. Temperatures around 60 degrees at the 7:00am start, no wind,
sunshine and not a cloud in the sky. 

 
The race itself went well. I had a pretty brutal running year suffering most
of the time from some archilles tendon inflammation. It has gotten better in
the last few weeks possibly thanks to some heel cups I now put in the
shoes. But I had exactly one run longer than ten miles since the  
Boston Marathon.
So I lost a lot of speed, as well as endurance and was a little nervous as to
how I’d do.  And considering all this, it went pretty well. I fininished in 1;41:50
or a 7:47 pace.  While is easily the slowest half in a number of years, at
least I got to run it evenly, pain free and with a negative split (== faster
second half) and some gas left for a fast last half mile or so.  So maybe I
don’t have to retire from running just yet. We’ll see if I get some speed back in 2011.

 "	 0 Comments
Preventing argument use in R	https://www.r-bloggers.com/2010/09/preventing-argument-use-in-r/	September 12, 2010	richierocks	"It sounds silly, but sometimes you don’t want to let people use some arguments of a function.  The canonical example is write.csv.  The function is effectively a wrapper to write.table, but using “,” as the separator and “.” as the decimal. We could implement the function in a really simple way, as simple.write.csv <- function(...) write.table(sep = "","", dec = ""."", ...) Under most circumstances, this implementation works as expected.  The problem occurs when we pass the sep or dec arguments into the function; in this case we get a rather unhelpful error message. Error in write.table(sep = "","", dec = ""."", ...) :

+++formal argument ""sep"" matched by multiple actual arguments

 To get round this, the real implementation of write.csv is a little more complex.  The basic idea behind it is this: “Take a look at the call to the function write.csv.  Warn the user if any forbidden arguments were passed in, then replace them with specified values.  Then pass these on to write.table.” Let’s take a look at the code. 

write.csv <- function (...)

{

+++Call <- match.call(expand.dots = TRUE)

+++for (argname in c(""append"", ""col.names"", ""sep"", ""dec"", ""qmethod""))

++++++if (!is.null(Call[[argname]]))

+++++++++warning(gettextf(""attempt to set '%s' ignored"", argname), domain = NA)

+++rn <- eval.parent(Call$row.names)

+++Call$append <- NULL

+++Call$col.names <- if (is.logical(rn) && !rn) TRUE else NA

+++Call$sep <- "",""

+++Call$dec <- "".""

+++Call$qmethod <- ""double""

+++Call[[1L]] <- as.name(""write.table"")

+++eval.parent(Call)

}

 Understanding this requires some technical knowledge of the R language.  When you type things at the R command prompt and hit Enter, two things happen.  Those characters are turned into a call, and then they are evaluated to give you your answer.  Effectively, what happens is eval(quote(the stuff you typed)) You can examine the contents of a call by converting it to be a list, for example 

> as.list(quote(2+3))[[1]]

`+`
[[2]]

[1] 2
[[3]]

[1] 3
 The first element of the list gives the function being called, and the rest of the elements are the arguments to pass in to that function.  Notice that even ‘+’ is a function (and so is ‘
 If you want to know more about this, then take a look at section 6.1 of the R Language definition.  Expressions, which are, more or less, lists of calls are discussed in section 6.4. match.call is like calling quote(the stuff you typed when you called this function) Returning to the example, let’s have a sample call to write.csv. 

dfr <- data.frame(x = 1:5, y = runif(5))

write.csv(dfr, file = ""test.csv"", sep = ""!"")

 The crux of understanding write.csv is in the variable Call, assigned on the first line of the function. 

> as.list(Call)[[1]]

write.csv
[[2]]

dfr
$file

[1] ""test.csv""
$sep

[1] ""!""
 The rest of the function involves manipulating this object.  The element sep is replaced with a comma (with a warning) and some additional arguments are set.  The function that is called is then swapped for write.table, and finally this call is evaluated. Deep understanding of this can get a bit mind-melting but if you want to use this principle in your own functions, you can more or less copy and paste from write.csv. "	 0 Comments
A (fast!) null model of bipartite networks	https://www.r-bloggers.com/2010/09/a-fast-null-model-of-bipartite-networks/	September 12, 2010	Timothée	One of the challenges for ecologists working with trophic/interaction networks is to understand their organization. One of the possible approaches is to compare them across a random model, with more or less constraints, in order to estimate the departure from randomness. To this effect, null models have been developed. The basic idea behind a null model is to generate a network with properties that are the same that the original network, but with a different distribution of the interactions. That allows to test wether your network could have been generated by chance, although Bascompte and colleagues showed that in almost all the cases, the distribution of some key measures are significantly different from their generated random distribution. There are various model of increasing complexity, but one that is particularly interesting is the one described in the aforementioned Bascompte paper. The probability of having an interaction at the position Pl,c (where l are the lines and c the columns) is given by which as explained in the image below (behold my drawing abilities!) is the mean of the probability to have an interaction in the line and an interaction in the column (which, assuming the network is made entirely of 0 and 1, are the means of resp. the lines and the columns).  In R, it is quite easy to do it, using the colMeans and consorts functions. The brute force approach it to use two for loops, to go through the lines and columns. This is horribly slow, however, and the very interest of using null models is the ability to simulate a truckload of data. The function I wrote starts by finding the minimal size of the network, on which it will iterate (if there are less columns than rows, we will use a for loop on the columns). That way, we can construct the probability matrix, on which it is easier to use apply (and we already know the good margin!) to get the new network. And it is way faster than the double-for-loop method, as showed in the below picture. The circles are the non optimized function, and the triangles are the new one. Just make sure to take a look at the axes: the new function is able to generate a network of size superior to 106 interactions (e.g. 1000 predators and 1000 preys) in roughly 0.25 seconds. This network size is highly unrealistic! Working with networks of roughly 400 interactions, I was able to generate 104 nulls in less than 6 seconds (on a unibody MacBook with 4Go RAM).  The function is right here: 	 0 Comments
India Pakistan Cricket over the years	https://www.r-bloggers.com/2010/09/india-pakistan-cricket-over-the-years/	September 11, 2010	prasoonsharma		 0 Comments
SyntaxHighlighter Brush for the R Language	https://www.r-bloggers.com/2010/09/syntaxhighlighter-brush-for-the-r-language/	September 11, 2010	Yihui Xie	"shBrushR.js (1Kb)
  Hopefully Tal can persuade the WordPress.com manager to add support for R syntax highlighting, so these users do not need to worry much about the installation and configuration. For WordPress users, there are a couple of choices to make use of SyntaxHighlighter, e.g. the SyntaxHighlighter Evolved plugin, but I find this plugin somehow out-dated because it is still using an old version of SyntaxHighlighter, and it looks not easy to add support for new languages. Therefore I decided not to use any WordPress plugins, but to manually add the necessary HTML code in my header and footer; this approach works for any HTML pages. You need to upload the latest version (3.0.83) of SyntaxHighlighter somewhere first (of course, with the R brush shBrushR.js added in the scripts directory), change the following paths accordingly and insert them before the <body> tag in your HTML page: Then add these lines in the footer area (right before the </body> tag): Above is only my configuration; you may refer to the manual of SyntaxHighlighter for more information. To add those HTML code in your pages, you may modify the theme files (typically the header.php and footer.php). To make use of highlighting, you need to assign a special CSS class to your <pre></code> tag, e.g.</p>
<pre><pre class=""brush: r"">
test.function = function(r) {
    return(pi * r^2)
}
test.function(1)
</pre>
</pre>
<p>The result:</p>
<pre>test.function = function(r) {
    return(pi * r^2)
}
test.function(1)
</pre>
<p>You may double-click on the code to select all of them, and copy them with normally (e.g. with Ctrl + C). The old version of SyntaxHighlighter uses a Flash file to copy the code to the clipboard and you have to select the line numbers as well when you only want to select the code; I do not like these features.</p>
<p>Finally, the JS brush file can be improved in a few aspects but in fact I’m not quite good at JS RegExp, so I leave these possible improvements to the readers:</p>
<ol>
<li>highlight the function arguments — loosely speaking, the pattern is: they are in parentheses “()” followed by “=”; the first argument follows “(” and others follow “,”.</li>
<li>highlight the variables — those strings followed by “<-” or “=” when “=” is not in paretheses.</li>
</ol>
<p>P.S. today I converted all my <code><pre></code> tags in this whole site to <code><pre class=""brush: r""></code> using a SQL command:</p>
<pre>UPDATE `wp_posts`
SET `post_content` = REPLACE(`post_content`, '<pre>', '<pre>')
WHERE `post_content` LIKE '%<pre>%';
</pre>
<p>You may also consider this batch processing instead of manually adding <code>class=""brush: r"" to your  "	 0 Comments
RProtoBuf 0.2,0	https://www.r-bloggers.com/2010/09/rprotobuf-0-20/	September 11, 2010	Thinking inside the box	"
This is only the second release after
0.1-0 
more than six months ago.  Given that
Rcpp is such a key
ingedrient for 
RProtoBuf, and
that 
Rcpp underwent so
many exciting changes itself, Romain and I never got around to releasing new versions of 
RProtoBuf.
This version is now much closer to the actual C++ API and fairly feature
rich.  We summarised a few of these new things in the
presentation
at
useR! 2010.

 
There is more information at the
RProtoBuf
page; there is a
draft package vignette,
a ‘quick’ overview vignette
and a
unit test summary vignette.

Questions, comments etc should go to the
rprotobuf mailing list
off the RProtoBuf page at R-Forge.

 "	 0 Comments
Raster “Cover” function	https://www.r-bloggers.com/2010/09/raster-%e2%80%9ccover%e2%80%9d-function/	September 11, 2010	Steven Mosher	"The last bits of raster are coming together and everything is working so we are getting really close here. Today i will cover the raster function ‘cover’ which robert has just updated to handle a brick function. Lets review the situation with a toy example: First, we read in the Land data into a Brick. A brick is object with layers of rasters. One layer per month. lat*lon*time. A single raster layer will look like this. A matrix of NA and temps. The NA can be missing reports OR cells that are not Land  3   3     3  NA NA  3 NA NA NA The SST layers in the ocean brick also follow the same rules. The SST Layer could look like this NA  NA  4 4       4     NA 4       4      4 So generally the SST layer will have values where the land does not and vice versa. EXCEPT for coastal cells. See the upper right hand cell? the land has 3 and the ocean has 4. So we are going to combine two operations. First, we add the ocean to the land. This will give the following: NA, NA, 7 NA NA  NA NA NA NA Then we will “cover” the Land with the ocean and the coast. That will copy filled cells to NA cells. If the land has a value and the ocean has NA, then the value is copied. In the end we have all three components in one layer. It look like this in code: Coast Globe The three bricks of coast, land and ocean are all combined with the cover function. In the end the code looks like this. Please note I have to integrate “pointsToRaster” yet. source(“Global.R”) Land
 OceanPercentMask 
 Inv
 Inv
 Anom
 Data
 Anom
 Inv
 CellMatrix
 m
 b
 clearValues(b) # clear the decks m[as.numeric(row.names(CellMatrix)),]
 b
 Weights 
 Temps
 SST 
 Weights 
 Ocean 
 Coastal 
 all = cover(Coastal, Ocean, Temps) # mask NAs and combine all three x
 Below find HadCrut in red, Moshtemp in black. Note that differences are due to slightly different data between HadCrut and moshtemp. We use GHCN raw. And yes, I show more warming in recent months      What about the differences? summary(err) Min.             1st Qu.   Median     Mean       3rd Qu.     Max. -1.26200 -0.25080 -0.04142 -0.04890  0.15190  1.56000 "	 0 Comments
mapping the Australian election (2010 edition)	https://www.r-bloggers.com/2010/09/mapping-the-australian-election-2010-edition/	September 11, 2010	jackman	The AEC makes this reasonably easy, as do the authors of some very helpful R packages, the good people at Google Maps etc. Full description here (PDF); entire collection here; a sample here, showing Green 1st preferences, by polling places across metropolitan Melbourne:  	 0 Comments
EmEditor Professional as an R script editor	https://www.r-bloggers.com/2010/09/emeditor-professional-as-an-r-script-editor/	September 11, 2010	datadebrief		 0 Comments
Using the {plyr} (1.2) package parallel processing backend with windows	https://www.r-bloggers.com/2010/09/using-the-plyr-1-2-package-parallel-processing-backend-with-windows/	September 11, 2010	Tal Galili	Hadley Wickham has just announced the release of a new R package “reshape2” which is (as Hadley wrote) “a reboot of the reshape package”. Alongside, Hadley announced the release of plyr 1.2.1 (now faster and with support to parallel computation!). Both releases are exciting due to a significant speed increase they have now gained. Yet in case of the new plyr package, an even more interesting new feature added is the introduction of the parallel processing backend. (as written in Hadley’s announcement) plyr is a set of tools for a common set of problems: you need to __split__ up a big data structure into homogeneous pieces, __apply__ a function to each piece and then __combine__ all the results back together. For example, you might want to: It’s already possible to do this with base R functions (like split and the apply family of functions), but plyr makes it all a bit easier with: Considerable effort has been put into making plyr fast and memory efficient, and in many cases plyr is as fast as, or faster than, the built-in functions. You can find out more at http://had.co.nz/plyr/, including a 20 page introductory guide, http://had.co.nz/plyr/plyr-intro.pdf.  You can ask questions about plyr (and data-manipulation in general) on the plyr mailing list. Sign up at http://groups.google.com/group/manipulatr The exiting news about the release of the new plyr version is the added support for parallel processing. l*ply, d*ply, a*ply and m*ply all gain a .parallel argument that when TRUE, applies functions in parallel using a parallel backend registered with the foreach package. The new package also has some minor changes and bug fixes, all can be read here. In the original announcement by Hadley, he gave an example of using the new parallel backend with the doMC package for unix/linux.  For windows (the OS I’m using) you should use the doSMP package (as David mentioned in his post earlier today). However, this package is currently only released for “REvolution R” and not released yet for R 2.11 (see more about it here).  But due to the kind help of Tao Shi there is a solution for windows users wanting to have parallel processing backend to plyr in windows OS. All you need is to install the doSMP package, according to the instructions in the post “Parallel Multicore Processing with R (on Windows)“, and then use it like this: 	 0 Comments
10w2170, Banff	https://www.r-bloggers.com/2010/09/10w2170-banff/	September 11, 2010	xi'an	Yesterday night, we started the  Hierarchical Bayesian Methods in Ecology workshop by trading stories. Everyone involved in the programme discussed his/her favourite dataset and corresponding expectations from the course. I found the exchange most interesting, like the one we had two years ago in Gran Paradiso, because of the diversity of approaches to Statistics reflected by the exposition. However, a constant theme is the desire to compare and rank models (this term having different meanings for different students) and the understanding that hierarchical models are a superior way to handle heterogeneity and to gather strength from the whole dataset. A two-day workshop is certainly too short to meet students’ expectations and I hope I will manage to focus on the concepts rather than on the maths and computations… As each time I come here, the efficiency of BIRS in handling the workshop and making everything smooth and running amazes me. Except for the library, I think it really compares with Oberwolfach in terms of environment and working facilities. (Oberwolfach offers the appeal of seclusion and the Black Forest, while BIRS is providing summits all around plus the range of facility of the Banff Centre and the occasional excitement of a bear crossing the campus or a cougar killing a deer on its outskirt…) 	 0 Comments
Billy Bragg	https://www.r-bloggers.com/2010/09/billy-bragg/	September 10, 2010	Thinking inside the box	"
Turned out I could, and it became a nice evening out.
Darren Hanlon started up the
evening as the opener for a good half hour, and was quite decent; somewhat
charming in a good natured way, not taking himself too too seriously. I’d
gladlt see him again.

 
After a longer-than-needed break
Billy Bragg came on stage and played for
two straight hours, alternating between an electric and acoustic guitar. And also
alternating between some newer material and (especially towards the end and
the encore) some old crowd-pleasure. I don’t know his material all that well
but have of course know of his career over these last 25 years and am quite
glad I went to see him.  Nice way to end the week.

 "	 0 Comments
Rcpp 0.8.6	https://www.r-bloggers.com/2010/09/rcpp-0-8-6-2/	September 10, 2010	romain francois	Dirk released Rcpp 0.8.6 to CRAN Most of the development of this release was trigerred by a question on the Rcpp-devel mailing list. After Richard’s question, we added d-p-q-r functions for most of the distributions available in R. The file runit.stats.R contains several examples of using them.  We have also started developing Rcpp 0.8.7, which will depend on the next version of R (R 2.12.0) since it will use some of the features it will introduce. More on this later… Dirk also blogged about the release, including the relevant NEWS extract.  	 0 Comments
Because it’s Friday: Religion and reading level	https://www.r-bloggers.com/2010/09/because-its-friday-religion-and-reading-level/	September 10, 2010	David Smith	"The dating site OK Cupid often publishes on their blog interesting analyses of based on the self-reported data from their users, and the latest post is no exception. Most commentary has focused on race/gender preference analysis: according to the “Likes” sections of OK Cupid profiles, white males like “sweaty guitar rock, bro-on-bro comedies, things with engines, and dystopias” whereas black women like soul food and the color purple. (Remember: this is based on self-reported race and “likes”. It's also notable that the post opens with the rhetorical question, “Is there any way to make fun of other races in public and get away with it?”.) I was most interested, though, in what appeared to be a throwaway analysis at the end of the post. If you use a standardized score to rank the writing proficiency of the users based on the essays they use to describe themselves in their profiles, and then break it down by self-described religion, this is what you find: 
 Offered with no further comment. OKTrends: The REAL 'Stuff White People Like' "	 0 Comments
R SQL-ish aggregation	https://www.r-bloggers.com/2010/09/r-sql-ish-aggregation/	September 10, 2010	ellbur	I came to R from SQL. I’ve heard that packages such as reshape and plyr are quite beautiful, but to me they are somewhat non-intuitive. I’m always looking for I struggled with this a bit. First, it’s tempting to use tapply(), except that’s more of a one-column-in, one-column-out sort of thing. There’s by(), which is nice, and with a short wrapper can be OK. Well here’s what I did. First, the pretty stuff. Because R, while it is generally considered a practical language, is, to me, more of a pretty language. A great way to make things pretty in R is to abuse dynamic binding. Say I have a table like this: I’d like to call select() like this: Where ‘Alice’ gives the column to group by, and a=, b= give the columns to create. So this could look like First, the as.list(substitute(list(…)))[-1L] trick to get the unevaluated expressions for the columns. Then substitute(Key) gets the unevaluated key. deparse(S.Key) gives it a name (better would let the caller specify the name, but I haven’t wanted to do that yet so I didn’t implement it). evaluates the grouping column, so now we have it as a vector, with (hopefully) the same number of rows as the table. Now the next step turned out to suffer some unusual efficiency problems. The obvious approach is to use by() to split Table into many subtables, and evaluate the output columns on all of those. This works, but when the number of output rows is very large, this is extraordinarily slow. An implementation using by() would look like this The following implementation, which, instead of using by(), deals with vectors directly (not making a data.frame until the end), is about 10 times faster. With a large table The difference becomes apparent Unfortunately, when the number of output rows is very, very large, even select2() becomes too slow (and much slower than tapply()) (plyr is even slower). The best solutions I have found are 	 0 Comments
plyr and reshape: better, faster, more productive	https://www.r-bloggers.com/2010/09/plyr-and-reshape-better-faster-more-productive/	September 10, 2010	David Smith	Hadley Wickham has just released updates to his data-manipulation packages for R, plyr and reshape (now called reshape2), that are much faster and more memory-efficient than the previous incarnations. The reshape2 package lets you flexibly restructure and aggregate data using just three functions (melt, acast and dcast), whereas the plyr package is kind of like a supercharged SQL “GROUP BY” statement for R data frames. One of the most interesting aspects of this update is that plyr can now parallelize its operations and make use of multiple processors simultaneously to speed up really big data-munging jobs. It makes use of Revolution's contributed foreach package, so whatever platform you're on (Windows, Linux, or Mac) you can specify a suitable parallel backend and take advantage of significant speedups on multiprocessor machines. For example, on a 2-core Windows box can use the doSMP package from Revolution R to speed up a plyr call as follows:  require(doSMP)workers registerDoSMP(workers) llply(my_data, aggr_function, .parallel=TRUE) On Unix-like platforms (including Linux and Mac) you can use the doMC package for similar ends. Find more information about plyr at Hadley's website, below. Had.co.nz: plyr  	 0 Comments
R vs. Stata, or, Different ways to estimate multilevel models	https://www.r-bloggers.com/2010/09/r-vs-stata-or-different-ways-to-estimate-multilevel-models/	September 10, 2010	Andrew Gelman		 0 Comments
Revolution R Enterprise, some thoughts	https://www.r-bloggers.com/2010/09/revolution-r-enterprise-some-thoughts/	September 9, 2010	Shige		 0 Comments
Off to Banff!!	https://www.r-bloggers.com/2010/09/off-to-banff/	September 9, 2010	xi'an	Today I am travelling from Paris to Banff, via Amsterdam and Calgary, to take part in the Hierarchical Bayesian Methods in Ecology two day workshop organised at BIRS by Devin Goodsman (University of Alberta),  François Teste (University of Alberta), and myself. I am very excited both by the opportunity to meet young researchers in ecology and forestry, and by the prospect in spending a few days in the Rockies, hopefully with an opportunity to go hiking, scrambling and even climbing. (Plus the purely random crossing of Julien‘s trip in this area!) The slides will be mostly following those of the course I gave in Aosta, while using Introducing  Monte Carlo Methods with R for R practicals:  	 0 Comments
What can other languages learn from R?	https://www.r-bloggers.com/2010/09/what-can-other-languages-learn-from-r/	September 9, 2010	David Smith	"At the ASA Statistical Computing Award ceremony in Vancouver last month, R's co-creator Ross Ihaka said that R began as a “system that was functional and well-designed, and made it easy and fun for other people to help”. R's other co-creator, Robert Gentleman, also described the ability of others to contribute as “the exit strategy” that made R so successful. And the success is undeniable: the rate at which contributors from around the world have created new add-on packages for R has grown, literally, exponentially: 
  The chart above shows the number of packages — third-party add-on libraries for R — added to the CRAN archives over the past decade, with data to the end of August, 2010. (And this chart doesn't even include packages released outside the CRAN system, such as those for the BioConductor project.) The CRAN system, the result of excellent design and untold hours of implementation by R-Core members Kurt Hornik and Fritz Leisch, is the key contributing factor in this growth. R's package system together with the CRAN infrastructure provides a standardized process for authoring, documenting, validating, building and distributing packages to millions of users worldwide. (If you're interested in the details of R's package system, be sure to check out the Writing R Extensions manual.) Can other programming languages learn from R's success in enabling a vast development community around a core language? That's the question that the ACM's Bay Area chapter will ask in a meeting on Wednesday, September 15 in a meeting titled Software Package Development Processes and R. Spencer Graves and Sundar Dorai-Raj and will use the R system as an illustration of a good software package development process, and see how those processes can be applied to other language systems. Bay Area ACM: Software Package Development Processes and R  "	 0 Comments
A quick ggplot2 hack (multiple dataframes)	https://www.r-bloggers.com/2010/09/a-quick-ggplot2-hack-multiple-dataframes/	September 9, 2010	Timothée	I’m starting to get familiar with ggplot2, and I really like it. I just found a very quick way to use several dataframes within the same plot, provided that the dataframes share columns names. One obvious application is the production of graphs with the mean (obtained by aggregate) superposed to the original raw data. You can do this in ggplot2 simply with something along the lines of By changing the data argument, you will recycle the aes settings. This is really handy. The result is  Now, of course, is someone knows a simpler way to do it, I’d like to know! 	 0 Comments
NHANES Data: Management with R	https://www.r-bloggers.com/2010/09/nhanes-data-management-with-r/	September 9, 2010	VCASMO - drewconway		 0 Comments
Harvesting & Analyzing Interaction Data in R:  The Case of MyLyn	https://www.r-bloggers.com/2010/09/harvesting-analyzing-interaction-data-in-r-the-case-of-mylyn/	September 9, 2010	VCASMO - drewconway		 0 Comments
Typo in Chapter 5	https://www.r-bloggers.com/2010/09/typo-in-chapter-5/	September 9, 2010	xi'an	"Gilles Guillot from Technical University of Denmark taught a course based on our R book and he pointed out to me several typos in Chapter 5 of “Introducing Monte Carlo Methods with R”:  should be  [right, another victim of cut-and-paste] I checked the last item with the new version of R and got the following confirmation that optimise does not accept (any longer) the abbreviation of its arguments… demo(Chapter.5)
———————— Type     to start : > # Section 5.1, Numerical approximation
>
> ref=rcauchy(5001) > f=function(y){-sum(log(1+(x-y)^2))} > mi=NULL > for (i in 1:400){
+   x=ref[1:i]
+   aut=optimise(f,interval=c(-10,10),max=T)
+   mi=c(mi,aut$max)
+   }
Error in f(arg, …) : unused argument(s) (max = TRUE) > optimise(f,interval=c(-10,10),maximum=T)
$maximum
[1] -2.571893 $objective
[1] -6.661338e-15 "	 0 Comments
The future of R (2010)	https://www.r-bloggers.com/2010/09/the-future-of-r/	September 8, 2010	Andrew Gelman		 0 Comments
Webinar September 22: Deploying R	https://www.r-bloggers.com/2010/09/webinar-september-22-deploying-r/	September 8, 2010	David Smith	In 2 weeks, on Wednesday September 22, I'll be hosting a webinar in conjunction with Andrew Lampitt at Jaspersoft. This new webinar is all about how to Deploy R: in other words, how to use the new server-based capabilities of Revolution R Enterprise 4 for Linux to embed the results of R scripts into applications and web pages. One very cool example of this is getting the output of R based models into Jaspersoft BI dashboards — Matt Dahlman will demonstrate.  Full details about the webinar are after the jump, and you can register at the link below. Revolution Analytics Webinars: Deploying R: Advanced Analytics On Demand in Applications, in Dashboards, and on the Web  R is a popular and powerful system for creating custom data analysis, statistical models, and data visualizations. But how can you make the results of these R-based computations easily accessible to others? A PhD statistician could use R directly to run the forecasting model on the latest sales data, and email a report on request, but then the process is just going to have to be repeated again next month, even if the model hasn't changed. Wouldn't it be better to empower the Sales manager to run the model on demand from within the BI application she already uses—daily, even!—and free up the statistician to build newer, better models for others? In this webinar, David Smith (VP of Marketing, Revolution Analytics) will introduce the new “RevoDeployR” Web Services framework for Revolution R Enterprise, which is designed to make it easy to integrate dynamic R-based computations into applications for business users. RevoDeployR empowers data analysts working in R to publish R scripts to a server-based installation of Revolution R Enterprise. Application developers can then use the RevoDeployR Web Services API to securely and scalably integrate the results of these scripts into any application, without needing to learn the R language. With RevoDeployR, authorized users of hosted or cloud-based interactive Web applications, desktop applications such as Microsoft Excel, and BI applications like Jaspersoft can all benefit from on-demand analytics and visualizations developed by expert R users. To demonstrate the power of deploying R-based computations to business users, Andrew Lampitt will introduce Jaspersoft commercial open source business intelligence, the world's most widely used BI software. In a live demonstration, Matt Dahlman will show how to supercharge the BI process by combining Jaspersoft and Revolution R Enterprise, giving business users on-demand access to advanced forecasts and visualizations developed by expert analysts. 	 0 Comments
ECG Project	https://www.r-bloggers.com/2010/09/ecg-project/	September 8, 2010	Matt Shotwell	In an earlier post, I described some ECG signal processing. In fact, these recordings were from my own heart, and I had collected them using a homemade ECG. The following is a repost from my old site describing the device. I assembled an instrumentation amplifier to measure cardiac potentials (i.e. an electrocardiograph (ECG)). The analog-to-digital converter of an ATmega168 microcontroller was used to convert the amplified potentials and transmit the values to a computer via a serial connection. The microcontroller can transmit up to 2.5k values per second, which is sufficient for cardiac potentials. The analog amplifier was a bit tricky to set up. I ended up purchasing the INA128 instrumentation amplifier after some unsuccessful trials with individual op-amps. The final amplifier circuit is given in the following diagram.  The diagram has the following components: The gain in this amplifier is set by the R_GAIN (47 Ohm) resistor, given by G = 1 + 50000 / R_GAIN. Hence, the gain of this circuit is about 1065V/V. Since cardiac potentials on the skin surface are in the region of 1mV-3mV, this gain should be sufficient. The inputs to the INA128 must be biased to within a small voltage (+-0.5V) of VCC/2, termed the common mode voltage (VCM). AC-coupling capacitors (C1 and C2) prevent the need to apply VCM to the limb electrodes, which could result in shock if the body is grounded to earth. The limb electrodes are instead biased to GND. The RA and LA leads in this amplifier might also be connected to a piezoelectric transducer (electric buzzer) to record audio/vibrations. However, the frequency of recorded audio is limited by the A/D conversion and the serial transmission rates. That is, audio frequencies higher than about 1200Hz (See the Sampling Theorem) cannot be recorded with accuracy. It might be possible to connect VOUT directly to the ‘line in’ jack on an audio recording device, such as a computer sound card. Disclaimer: I do NOT recommend the circuit above for ANY specific use, or guarantee its safety. Think carefully before attaching homemade electronics to your body. 	 0 Comments
Computer languages and Applied Math	https://www.r-bloggers.com/2010/09/computer-languages-and-applied-math/	September 8, 2010	Larry D'Agostino		 0 Comments
Clipping a Surface By a Polygon	https://www.r-bloggers.com/2010/09/clipping-a-surface-by-a-polygon/	September 8, 2010	James	" Background: A common function in standard GIS software enables users to create a raster surface and extract values or clip it based on a set of polygons. This may be used in cases where you want analysis to be constrained to within a town’s boundaries or a coastline. This tutorial will outline how to create a surface using kernel density estimation (KDE) and then clip the surface so that it is constrained within the City of London Boundary.

Data Requirements:
 City of London Boundary Shapefile: Download (requires unzipping). London Cycle Hire Locations: Download. Install the following packages (if you haven’t done so already): sm, maptools.﻿ "	 0 Comments
Canterbury Earthquakes part III	https://www.r-bloggers.com/2010/09/canterbury-earthquakes-part-iii/	September 8, 2010	Samuel Brown		 0 Comments
Julien on R shortcomings	https://www.r-bloggers.com/2010/09/julien-on-r-shortcomings/	September 8, 2010	xi'an	"Julien Cornebise posted a rather detailed set of comments (from Jasper!) that I thought was interesting and thought-provoking enough (!) to promote to a guest post. Here it is , then, to keep the debate rolling (with my only censoring being the removal of smileys!). (Please keep in mind that I do not endorse everything stated in this guest post! Especially the point on “Use R!“) On C vs R 
As a reply to Duncan: indeed C (at least for the bottlenecks) will probably always be faster for the final, mainstream use of an algorithm [e.g. as a distributed R library, or a standalone program]. Machine-level, smart compilers, etc etc. The same goes for Matlab, and even for Python: e.g. Pierre Jacob (Xian’s great PhD student) uses Weave to inline C in his Python code for the bottlenecks — simple, and fast. Some hedge funds even hire coders to recode the Matlab code of their consulting academic statisticians. Point taken. But, as Radford Neal points out, that doesn’t justify R to be much slower that it could be: On Future Language vs R
Thanks David and Martyn for the link to Ihaka’s great paper on R-like lisp-based. Says things better than I could, and with an expertise on R that I haven’t. I also didn’t know about Robert Gentleman and his success at Harvard (but he *invented* the thing, not merely tuned it up). Developing a whole new language and concept, as advocated in Ihaka’s paper and as suggested by gappy3000 would be a great leap forward, and a needed breakthrough to change the way we use computational stats. I would *love* to see that, as I personally think (as Ihaka advocates in the paper you link to) that R, as a language, is a hell of a pain [2] and I am saddened to see a lot of “Use R” books who will root its inadequate use for needs where the language hardly fits the bill — although the library does.
 But R is here and in everyday use, and the matter is more of making it worth using, to its full potential. I have no special attachment to R, but any breakthrough language that would not be entirely compatible with the massive library contributed over the years would be doomed to fail to pick-up the everyday statistician—and we’re talking here about far-fetched long-term moves. Sanitary breakthrough, but harder to make happen when such an anchor is here.
I would say that R has turned into the Fortran of statistics: here to stay, anchored by the inertia that stems from its intrinsic (and widely acknowledged) merits  (I’ve been nice, I didn’t say Cobol.). So until of the great leap forward comes (or until we make it happen as a community), I second Radford Neal‘s call for optimization of the existing core of R. Rejoinder
As a rejoinder to the comments here, I think we need to consider separately It seems to me from this topic that the community needs/should push for, in chronological order. From then on
Who knows the R community enough to relay this call, and make it happen ? I’m out of my league. Uninteresting footnotes:
[1] I have twitched several times when trying R, feeling the coding was somewhat unnatural from a CS point of view. [Mind, I twitch all the same, although on other points, with Matlab]
[2] again, I speak only out of the few tries I gave it, as I gave up using it for my everyday work, I am biased — and ignorant "	 0 Comments
Straight, curly, or compiled?	https://www.r-bloggers.com/2010/09/straight-curly-or-compiled/	September 7, 2010	Thinking inside the box	"
Now, let me prefix this by saying that I really enjoyed Radford’s posts. He obviously put a lot of time into finding a number of (all somewhat
small in isolation) inefficiencies in R which, when taken together, can make a difference in
performance. I already spotted one commit by Duncan in the SVN logs for R so this is being looked at.  

 
Yet Christian, on the other hand, goes a little overboard in bemoaning performance differences somewhere between ten and fifteen percent — the
difference between curly and straight braces (as noticed in Radford’s first post). Maybe he spent too much time waiting for his MCMC runs to
finish to realize the obvious:  compiled code is evidently much faster.

 
And before everybody goes and moans and groans that that is hard, allow me to just interject and note that it is not.  It really
doesn’t have to be. Here is a quick
cleaned up version of Christian’s example code, with proper assigment operators and a second variable x.  We then get to the
meat and potatoes and load our
Rcpp package as well as 
inline to define the same little test function in C++.  Throw in 
rbenchmark which I am becoming increasingly fond of for these little timing tests,
et voila, we have ourselves a horserace:

 
And how does it do?  Well, glad you asked. On my i7, which the other three cores standing around and watching, we get an
eighty-fold increase relative to the best interpreted version:

 "	 0 Comments
Ah Bach…	https://www.r-bloggers.com/2010/09/ah-bach/	September 7, 2010	C		 0 Comments
Eigenimages: The AT&T Cambridge Faces Database	https://www.r-bloggers.com/2010/09/eigenimages-the-att-cambridge-faces-database/	September 7, 2010	Matt Shotwell	 I picked up the AT&T Laboratories Cambridge database of faces for a clustering application. The database consists of images of 40 distinct subjects, each in 10 different facial positions and expressions. Typically, the goal of clustering in these data is to recover the ‘true’ partition, or that which isolates images of distinct subjects. Each image is is 92 x 112 pixels in dimension, taking black-and-white integer values in the 8-bit range (0 to 255). Such high-dimensional images (92 x 112 = 10304) are difficult to work with directly. We can look to data-squashing to help here. (Actually, I’m not sure the term ‘data-squashing’ was intended for methods like PCA, but it seems appropriate to me.) I used principal components analysis to identify a set of rotated pixels that were highly variable, and presumably most useful for discriminating between the images, resulting in this interesting image montage. The first 20 eigneimages (in reading order) each represent the rotation of a 92 x 112 black-and-white image onto a single pixel. Darker regions in the eigenimages load higher in the rotation. Consequently, darker regions are important for discriminating between images in the dataset. The dark pixels in the top-left image account for about 18% of the variability in the entire dataset. In other words, these regions of the face may be the most useful for facial recognition. I’ve put together an archive of the images, a function to read the PGM image pixels into R, do the PCA, and recreate the graphic above, in less than 60 lines (though I shouldn’t boast, else someone will cut it to 20 lines and shame me). You can download the archive here ATTfaces.tar.gz (please be patient, ~3.7MB). From a shell prompt, recreate the graphic as follows: Disclaimer: This image is a re-posting from my old website. However, the code and discussion were not given before. 	 0 Comments
Revolution R Enterprise 4.0 free download for academics	https://www.r-bloggers.com/2010/09/revolution-r-enterprise-4-0-free-download-for-academics/	September 7, 2010	David Smith	The Windows version of our latest enterprise distribution of R, Revolution R Enterprise 4.0, is now being delivered to subscribers and is also available for free download for members of the academic community. Revolution R Enterprise 4.0 is a major update, and includes many new and improved features:  If you're at a university anywhere in the world and want to use Revolution R Enterprise for research or teaching, you can download it free of charge. (If you're at a commercial or non-profit business, you'll need to purchase a subscription to get additional features included with Revolution R Enterprise.) At the moment only Windows versions are available for download; the Red Hat version will be available later in the year. (Unfortunately for Mac users like me, we don't plan to produce a MacOS X version of Revolution R Enterprise until there's more demand from businesses. But we do offer the open-source Revolution R Community distribution on Macs, but it doesn't include any of the proprietary Revolution features.) Revolution Analytics: Revolution R Enterprise Free Academic Software  	 0 Comments
Barchart or Dotchart?	https://www.r-bloggers.com/2010/09/barchart-or-dotchart/	September 7, 2010	David Smith	"Which of the following two charts (both created with R) to you prefer? This dotchart: 
 Or this bar chart? 
 Andrew Gelman (who, incidentally, is speaking at the October NYC UseR meeting) prefers the dotchart prefers a line plot (update: see Gelman's comment, below), but personally I think the bar chart is more easily interpreted. What do you think? You can join the discussion at Decision Science News at the link below. Update Sep 8: Prof. Gelman responds, with a detailed record leading to the correction above. Decision Science News: Which chart is better? "	 0 Comments
Example 8.4: Including subsetting conditions in output	https://www.r-bloggers.com/2010/09/example-8-4-including-subsetting-conditions-in-output/	September 7, 2010	Ken Kleinman		 0 Comments
Embed R Code with Syntax Highlighting on your Blog	https://www.r-bloggers.com/2010/09/embed-r-code-with-syntax-highlighting-on-your-blog/	September 7, 2010	Stephen Turner		 0 Comments
Writing a Spatial Function: The Location Quotient	https://www.r-bloggers.com/2010/09/writing-a-spatial-function-the-location-quotient/	September 7, 2010	James	"Background:
In some cases it is necessary to conduct the same analysis multiple times on either the same or different data. In such circumstances it is worth writing a function to simplify the code. In this example the location quotient provides a simple calculation easily written in to a function. The location quotient (LQ) is an index for comparing a region’s share of a particular activity with the share of that same activity found at a more aggregate spatial level (a good book on this kind of thing is Burt et al.). In this example we take a shapefile of London Boroughs that contains information on the population of each borough and the percentage of sports participation in each borough. In this case there is little point in calculating the LQ as the percentage alone would be more meaningful. The focus here is how to undertake the methods, not their appropriate use, or the validity of the results. Data Requirements: London Sport Participation Shapefile: Download (requires unzipping) Install the following packages (if you haven’t already done so): maptools, RColorBrewer. "	 0 Comments
Canterbury Earthquakes part II	https://www.r-bloggers.com/2010/09/canterbury-earthquakes-part-ii/	September 7, 2010	Samuel Brown		 0 Comments
Confidence Bands for Universal Scalability Models	https://www.r-bloggers.com/2010/09/confidence-bands-for-universal-scalability-models/	September 7, 2010	Neil Gunther		 0 Comments
CCE	https://www.r-bloggers.com/2010/09/cce/	September 7, 2010	Steven Mosher	Wasnt that hard. green is land, blue is sea. new data awaits you. old data taken down For Zeke: Green is landcoast cells, blue is seacoast.  The cells are AREA weighted per cce request but not Fractiionally weighted. That is if a cell is 90% land and 10% water, this weighting is NOT applied. I think thats right for what he is after  	 0 Comments
Truly random?!	https://www.r-bloggers.com/2010/09/truly%c2%a0random/	September 6, 2010	xi'an	Having purchased the September edition of La Recherche because of its (disappointing!) coverage on black matter, I came by a short coverage on an Intel circuit producing “truly random” numbers… I already discussed this issue in an earlier post, namely that there is no reason physical generators are “more” random than congruential pseudo-random generators, but this short paper repeats the same misunderstanding on the role of “random” generators. The paper mentions dangers of pseudo-random generators for cryptography (but this is only when you know the deterministic function and the sequence of seeds used so far), while it misses the essential aspect of valid generators, namely that their distribution is exactly known (e.g., uniform) and, in the case of parallel generations, which seems to be the case for this circuit, that the generators are completely independent. La Recherche mentions that the entropy of the generator is really high, but this is more worrying than reassuring, as the Intel engineers do not have a more elaborate way to prove uniformity than a Monte Carlo experiment… There is actually a deeper entry found on Technology Review. (Which may have been the source for the paper in the technology tribune of La Recherche.) The article mentions that the generator satisfied all benchmarks of “randomness” maintained by NIST. Those statistical tests sound much more reassuring than the entropy check mentioned by La Recherche, as they essentially reproduce Marsaglia’s DieHard benchmark… I remain rather skeptical about physical devices, as compared with mathematical functions, because of (a) non-reproducibility which is a negative feature despite what the paper says and of (b) instability of the device, which means that proven uniformity at time t does not induce uniformity at time t+1. Nonetheless, if the gains in execution are gigantic, it may be worth the approximation for most applications. But please stop using “true” in conjunction with randomness!!! 	 0 Comments
R Maps	https://www.r-bloggers.com/2010/09/r-maps/	September 6, 2010	James	 This is an updated version of my Making Maps with R tutorial. I think  the code is lot simpler and it also includes some data for you to play  around with. Background:  Spatial data are becoming increasingly  common, as are the tools available in R to process it. Of course one of  the best ways of visualizing spatial data is through a map. Maps need to  be well thought out. Not least, the selected colours need to be  appropriate and sufficient context is provided through the use of a  legend, title, scale bar and north arrow. The worksheet will demonstrate  how to produce a map with R that includes all these elements.  Data Requirements: London Sport Participation Shapefile. Download (requires unzipping) Install the following packages (if you haven’t already done so): maptools, RColorBrewer, classInt 	 0 Comments
Less than negative?	https://www.r-bloggers.com/2010/09/less-than-negative/	September 6, 2010	Samuel Brown		 0 Comments
R Reshape Package	https://www.r-bloggers.com/2010/09/r-reshape-package/	September 6, 2010	Quantitative Finance Collector		 0 Comments
Canterbury Earthquake	https://www.r-bloggers.com/2010/09/canterbury-earthquake/	September 6, 2010	Samuel Brown		 0 Comments
Coastal	https://www.r-bloggers.com/2010/09/coastal/	September 6, 2010	Steven Mosher	"UPDATE: cce asked for a chart of coastal locations broken out by their SST component and their Land componement. That’s a little tricky, but once you figure out the mask logic its just addition and such.There are TWO masks: One mask contains ALL zeros, except for coast cells which are fraction of LAND.The other mask contains all zeros except the same coast cell are fraction of water. NOTE. the values here are the contribution of warming from coastal cells. Its SMALL. remember everything is area weighted. But you could look at the trends. Red is the “land” portion, blue is the sea portion. orange is the sum. index is months from 1900.  UPdate: if you want to follow along after this I suggest you get hooked up to raster on R Forge. I’ll be testing out new capability. The code to date uses the CRAN release 1.4.10, but I need to move up to 1.5.2 to test the new capability. However, baselining the old code probably needs to happen first. A few more bits to test and then we will be shifting to Forge releases of raster ( Cran requires formal testing, so be patient) A weird little benefit of doing things in raster. I can get a look at coastal if we do Coast 
 the result will be ONLY those areas where they overlap coast image Thats because SST has NA where there is 100% Land, and land has NA where there is 100% Ocean Add those togther and you get NA everywhere BUT the coast Land
 Inv
 # this line goes away Inv
 Anom
 Data
 Anom
 Inv
 ##################################################### # this code becomes a one liner with the new ‘pointstoRaster’ basically # we take the Anomalies which are station based, and aggregate them into a # cell structure taking the average of all the stations (points) that map to the raster cell. # see R Forge if you dont understand. stations are points(x,y) you aggregate those points # into a raster (cells of lat/lon) using a “mean” function. so the work of as.Grid goes away and most # of the lines that follow. We basically read a V2anomaly structure into a brick. slick. CellMatrix
 m
 b
 clearValues(b) m[as.numeric(row.names(CellMatrix)),]
 b
 ############################## Weights 
 Temps
 SST 
 OceanPercentMask 
 Weights 
 Ocean 
 Coastal 
        

 "	 0 Comments
In{s}a(ne)!!	https://www.r-bloggers.com/2010/09/insane/	September 5, 2010	xi'an	"Having missed the earliest entry by Radford last month, due to disconnection in Yosemite, I was stunned to read his three entries of the past month about R performances being significantly modify when changing brackets with curly brackets! I (obviously!) checked on my own machine and found indeed the changes in system.time uncovered by Radford… The worst is that I have a tendency to use redundant brackets to separate entities in long expressions and thus make them more readable (I know, this is debatable!), so each new parenthesis add to the wasted time… Note that it is the same with curly bracket: any extra curly bracket takes some additional computing time… > f=function(n) for (i in 1:n) x=1/(1+x)
> g=function(n) for (i in 1:n) x=(1/(1+x))
> h=function(n) for (i in 1:n) x=(1+x)^(-1)
> j=function(n) for (i in 1:n) x={1/{1+x}}
> k=function(n) for (i in 1:n) x=1/{1+x}
> x=1
> system.time(f(10^6))
user  system elapsed
1.684   0.020   1.705
> system.time(g(10^6))
user  system elapsed
1.964   0.012   1.976
> system.time(h(10^6))
user  system elapsed
2.452   0.016   2.470
> system.time(j(10^6))
user  system elapsed
1.716   0.008   1.725
> system.time(k(10^6))
user  system elapsed
1.532   0.016   1.548 In the latest of his posts, Radford lists a series of 14 patches that speed up R up to 25%… That R can face such absurdities is pretty annoying! Given that I am currently fighting testing an ABC algorithm with MCMC inner runs, which takes forever to run, this is even depressing!!! As Radford stresses, “slow speed is a significant impediment to greater use of R, much more so than lack of some wish list of new features.” "	 0 Comments
MoshTemp 4.1	https://www.r-bloggers.com/2010/09/moshtemp-4-1/	September 5, 2010	Steven Mosher	"Replaces 4.0, minor changes made here and there. totally replaces 4.0 as the testing move forward. see 4.1 Run: DownloadAll. Run: DataSetUp. SSTTest reruns the SST analsyis and compares at HADSST2 Land1900Present: reruns the old version of program with a bench against CRU land figures. Next version will integrate more of raster to the analysis, then we throw out a bunch of old code and use raster for everything. First raster bit is working Land
 Inv
 Inv
 Anom
 Data
 Anom
 Inv
 CellMatrix
 ##################################################### m
 b
 clearValues(b) m[as.numeric(row.names(CellMatrix)),]
 b
 plot(cellStats(b,mean,na.rm=T)) bricktest Adding area weighting and land masking m
 b
 clearValues(b) m[as.numeric(row.names(CellMatrix)),]
 b
 Weights 
 Temps
 monthly
 landBrick2 "	 0 Comments
Cricket World Cup 2011	https://www.r-bloggers.com/2010/09/cricket-world-cup-2011/	September 5, 2010	prasoonsharma		 0 Comments
1936.	https://www.r-bloggers.com/2010/09/1936/	September 4, 2010	Steven Mosher	UPDATE: Ron Broberg has a more definitive explanation of the difference which indicates that 5sig issue is not the main cause of the difference. See his exposition here CRU, it appears, trims out station data when it lies outside 5 sigma. Well, for certain years where there was actually record cold weather that leads to discrepancies between CRU and me. probably happens in warm years as well. Overall this trimming of data amounts to around .1C. ( mean of all differences) below, see what 1936 looked like. Average for every month, max anomaly, min anomaly, and 95% CI (orange) And note these are actual anomalies from 1961-90 baseline. So thats a -21C departure from the average.  With a sd around 2.5 that means CRU is trimming  departures greater than 13C or so.  A simple look at the data showed bitterly cold  weather in the US. Weather that gets snipped by a 5 sigma trim.  And  More interesting facts: If one throws out data because of outlier status one can expect outliers to be uniformly distributed over the months. In other words bad data has no season. So, I sorted the ‘error’ between CRU and Moshtemp. Where do we differ. Uniformly over the months? or does the dropping of 5sigma events happen in certain seasons. First lets look at when CRU is warmer than Moshtemp. I take the top 100 months in terms of positive error. Months here are expressed as fractions 0= jan  Next, we take the top 100 months in terms of negative error. Is that uniformly distributed? If this data holds up upon further examination it would appear that CRU processing has a seasonal Bias, really cold winters and really warm winters ( 5 sigma events) get tossed. Hmm. The “delta” between Moshtemp and CRU varies with the season. The worst months on average are dec/jan. The sd for the winter month delta is twice that of other months. Again, if these 5 sig events were just bad data we would not expect this. Over all Moshtemp is warmer that CRU, but  when we look at TRENDS it matters where these events happen  	 0 Comments
5 Sigma in CRU	https://www.r-bloggers.com/2010/09/5-sigma-in-cru/	September 4, 2010	Steven Mosher	UPDATE: Ron Broberg has a more definitive explanation of the difference which indicates that 5sig issue is not the main cause of the difference. See his exposition here A short update. I’m in the process of integration the Land Analysis and the SST analysis into one application. The principle task in front of me is integrating some new capability in the ‘raster’ package.  As that effort proceeds I continue to check against prior work and against the accepted ‘standards’. So, I reran the Land analysis and benchmarked against CRU. Using the same database, the same anomaly period, and the same CAM criteria. That produced the following  My approach shows a lot more noise. Something not seen in the SST analysis which matched nicely. Wondering if CRU had done anything else I reread the paper. ” Each grid-box value is the mean of all available station anomaly values, except that station outliers in excess of ﬁve standard deviations are omitted.” I dont do that!  Curious, I looked at the monthly data:  The Month were CRU and I differ THE MOST is  Feb, 1936. lets look at the whole year of 1936 First CRU 	 0 Comments
Thinking in R: vectors	https://www.r-bloggers.com/2010/09/thinking-in-r-vectors/	September 4, 2010	Derek-Jones	While I have been using the R language/environment for over five years or so, whenever I needed to do some statistical calculations, until recently I was only a casual user.  A few months ago I started to use R in more depth and made a conscious decision to try and do things the ‘R way’. While it is claimed that R is a functional language with object oriented features these are sufficiently well hidden that the casual user would be forgiven for thinking R was your usual kind of domain specific imperative language cooked up by a bunch of people who had not done this sort of thing before. The feature I most wanted to start using in an R-like way was vectors.  A literal value in R is not a scalar, it is a vector of length 1 and the built-in operators take vector operands and return a vector result, e.g., the result of the multiplication (c is the concatenation operator and in the following case returns a vector of containing two elements) The language does contain a for-loop and an if-statement, however the R-way is to use the various built-in functions operating on vectors rather than explicit loops. A problem that has got me scratching my head for a better solution than the one I have is the following.  I have a vector of numbers (e.g., 1 7 1 2 9 3 2) and I want to find out how many of each value are contained in the vector (i.e., 1 2, 2 2, 3 1, 7 1, 9 1). The unique function returns a vector containing one instance of each value contained in the argument passed to it, so that removes duplicates.  Now how do I get a count of each of these values? When two vectors of unequal length are compared the shorter operand is extended (or rather the temporary holding its value is extended) to contain the same number of elements as the longer operand by ‘reusing’ values from the shorter operand (i.e., when indexing reaches the end of a vector the index is reset to the beginning and then moves forward again).  This means that in the equality test X == 2 the right operand is extended to be a vector having the number of elements as X, all with the value 2.  Some of these comparisons will be true and others false, but importantly the resulting vector can be used to index X, i.e., X[X == 2] returns a vector of 2′s, one for each occurrence of 2 in X.  We can use length to obtain the number of elements in a vector. The following function returns the number of instances of n in the vector X  (now you will see why thinking the ‘R way’ needs practice): My best solution to the counting problem, so far, uses the function lapply, which applies its second argument to every element of its first argument, as follows: Using lapply feels like a bit of a cheat to me.  Ok, the loop is in compiled C code rather than interpreted R code, but an R function is being called for every unique value. I have been rereading Matter Computational which has gotten me looking for a solution like those in the first few chapters of this book (and which will probably be equally obscure). Any R experts out there with suggestions for a non-lapply solution?   	 0 Comments
The FourierDescriptors Package	https://www.r-bloggers.com/2010/09/the-fourierdescriptors-package/	September 4, 2010	John Myles White	I’ve just uploaded a new package to CRAN based on a stimulus generation algorithm that I use for my experiments on vision. The FourierDescriptors package provides methods for creating, manipulating and visualizing Fourier descriptors, which are a representational scheme used to describe closed planar contours. The canonical reference from the literature is Zahn and Roskies 1972. The images most easily described using Fourier descriptors are useful as stimuli for experiments in psychology and neuroscience. This package has been submitted to CRAN. When it propagates through the mirrors, you can install it using a simple call to install.packages(): For the time being, please install it using the included source package by downloading the GitHub repository and running: The following example showcases the central methods that currently exist in the FourierDescriptors package: It’s useful to see the images generated by very basic amplitude spectra to see how these determine the shape of the induced contour. Please note that only even-numbered frequencies can have non-zero amplitudes or the described curve will not be closed. 	 0 Comments
Weekend art in R (Part 4)	https://www.r-bloggers.com/2010/09/weekend-art-in-r-part-4/	September 4, 2010	Matt Asher	 Computer creations are perfect by design. We put in numbers, and if all goes well we get out an exact result. If we want a line, we want it perfectly straight. If we want a circle, it should conform to the platonic ideal of a circle. From a mathematical standpoint, these perfect shapes and precisely computed numbers are ideal. Someday, perhaps, we will have true fuzzy computation built right into our hardware. For now, it takes considerable effort to achieve just the right level of imperfection needed for simulating mistakes, or any organic processes. I sent each of the circles shown above on a random walk. That part was easy, getting each circle to end up where it started (and close the loop) took a bit more effort. To vary the “wigglyness” of the lines, adjust the “sd” parameter in “rnorm”. To change how quickly randomness tapers off, change the “4″ in “i/4″. Here is my code:  What do your circles look like? 	 0 Comments
Cricket data analysis	https://www.r-bloggers.com/2010/09/cricket-data-analysis/	September 4, 2010	prasoonsharma		 0 Comments
Creating a Presentation with LaTeX Beamer – Frame Transitions	https://www.r-bloggers.com/2010/09/creating-a-presentation-with-latex-beamer-%e2%80%93-frame-transitions/	September 4, 2010	Ralph	Transitions are often used in presentations to break up the presentation and to keep the audience awake, but often the outcome is irritation/distraction with text or other objects flying on or off the screen from different directions. As such they should be used sparingly if at all in a talk. LaTeX beamer has a simple mechanism for including transitions in a presentation Fast Tube by Casper To use this transitions pdflatex needs to be used to create the document as a pdf file and the presentation needs to be viewed in full screen mode to observe the transitions. Adding a transition on a slide is as simple as adding a command like: to the slide. This transition disolves the contents of the slide. The LaTeX beamer user guide has further information about the transitions that are available. Other useful resources are provided on the Supplementary Material page. 	 0 Comments
Oh (de)bugger! Part II	https://www.r-bloggers.com/2010/09/oh-debugger-part-ii/	September 3, 2010	richierocks	"It’s Friday night, and I recently discovered that adding a dash of Cointreau somewhat enhances a gin and tonic.  Consequently, I’m drunk-blogging.  Will try not to make too many tpyos. In the first part of this series, I discussed some standard debugging techniques, like the use of the browser function.  One of the limitations of browser is that it isn’t much help if the problem is somewhere higher up the call-stack.  That is, the case when you passed a dodgy argument to another function, which maybe passed it on to something else before your code fell over. If you have an error, but you don’t know where it is, then a helpful thing to see is your call stack, and the variables that you’ve put in it.  The function I’m going to show you uses sys.calls to retrieve the call stack, and sys.frames to retrieve the environment for each function you called.  Next it calls ls.str in each of those environments to tell you what variables you’ve created.  Finally, there are some methods to print your output more clearly.  After all, the whole point of this function is to give you quick insight into what you’ve just done. When I was trying to think of a suitable name for this function, I toyed with variations on the word “stack”.  Eventually, I figured that it should be named for the word most commonly used when things go wrong.  And so, I present to you, the bugger function. 

bugger <- function(...)

{

# ... contains arguments passed to ls.str.  See that function for a description.

+++stack_length <- sys.nframe()

+++call_stack <- sys.calls()[-stack_length] # -stack_length is so we don't include this function

+++frames <- sys.frames()[-stack_length]

+++ls_stack <- lapply(frames, function(e) ls.str(envir = e, ...))

+++call_names <- make.names(lapply(call_stack,

++++++function(cl) as.character(cl)[1]), unique = TRUE)

+++names(call_stack) <- call_names

+++names(ls_stack) <- call_names

+++class(call_stack) <- ""call_stack""

+++class(ls_stack) <- ""ls_stack""

+++list(call_stack = call_stack, ls_stack = ls_stack)

}

 The call to make.names is there to ensure that each items in call_stack and ls_stack have a unique name, and thus can be referenced more easily.   To finish, these two components get S3-style overloads of the print function. 

print.call_stack <- function(x)

{

+++n <- length(x)

+++if(n < 1) cat(""Call stack is emptyn"")

+++calls <- sapply(x, function(x) as.character(as.expression(x)))

+++cat(paste(seq_len(n), calls, sep = "": ""), sep = ""n"")

+++invisible(x)

}

 

print.ls_stack <- function(x, ...)

{

+++n <- length(x)

+++if(n < 1) cat(""Call stack is emptyn"")

+++titles <- paste(seq_len(n), names(x), sep = "": "")

+++for(i in seq_len(n))

+++{

++++++cat(titles[i], ""n"")

++++++noquote(print(x[[i]], ...))

++++++cat(""n"")

+++}

+++invisible(x)

}

 And finally, here’s an example to put the code to use 

bar <- function(xx, ...)

{

+++bugger()

}

 

foo <- function(x, y, z)

{

+++another_variable <- 999

+++bar(x, y, letters[1:5])

}

 

foo(1:10, ""monkey"", list(runif(3), zzz = ""zzz"")) This yields the result


$call_stack

1: foo(1:10, ""monkey"", list(runif(3), zzz = ""zzz""))

2: bar(x, y, letters[1:5])

 

$ls_stack

1: foo

another_variable :  num 999

x :  int [1:10] 1 2 3 4 5 6 7 8 9 10

y :  chr ""monkey""

z : List of 2

+$    : num [1:3] 0.154 0.974 0.189

+$ zzz: chr ""zzz""2: bar

xx :  int [1:10] 1 2 3 4 5 6 7 8 9 10
 One thing to note is that ls.str doesn’t pick up the ... arguments.  It is possible to retrieve the call to those dots from each environment, but it isn’t always possible to evaluate them.  Playing with R’s evaluation model in this way is deep magic, so accessing them involves excessive use of substitute and eval.  I’m going to postpone talking about them for an occasion that didn’t involve gin and cointreau. "	 0 Comments
Fourteen patches to speed up R	https://www.r-bloggers.com/2010/09/fourteen-patches-to-speed-up-r/	September 3, 2010	Radford Neal	Following my discovery of two surprising inefficiencies in R, I’ve been inspired to spend much of the last two weeks looking for ways to speed it up.  I’ve had quite a bit of success, both at finding ways to speed up particular functions, and at finding ways to reduce general interpretive overhead. You can get my fourteen patches to the R source code here.  I’d be interested in hearing how much it speeds up typical applications, on various machines.  Of course, you need to be comfortable with installing R from source code to use these patches.  For meaningful speed comparisons, you also need to be sure to compile the modified and unmodified versions of R with the same compiler, same options, etc. There look to be some more places in the R source code where speed improvements are possible, but for now, I had better switch to preparing for the coming teaching term… UPDATE: I discovered a bug in the vec-subset patch.  The version you can get from here now has this fixed.  I also split the vec-subset patch into patch-vec-subset and patch-subscript, since these two parts are really independent.  So there are now fifteen patches.  	 0 Comments
Moshtemp4.0	https://www.r-bloggers.com/2010/09/moshtemp4-0/	September 3, 2010	Steven Mosher	"Now that SST has been processed with raster we can return to the land processing and put that on the same footing. To get there I’ll rebuild from scratch making some minor adjustments along the way. As we go foward the amount of code we have to write should become less and less because we are letting raster do things for us. As much as I hate to trash old code I wrote putting everything into raster will be the right thing to do. In the end the vast majority of the code we have will be basic bookkeeping code and project organizing code. So, I spent sometime looking at new R packages ( project template) and spent time trying to bullet proof the installation and the file handling. As a result, the code will now check for installed packages and install them only of they are not installed. The other problem that remains is the “uncompress” issue with .Z files. Following Ron Broberg, SteveMc has a nifty little solution for this problem. First, what’s the problem: Many of the files we deal with are tar and/or .Z files. .Z is a old Unix compression standard. many folks have moved on from this. But there  are some who still use the format. Ron’s solution and Steve’s after him was to put the uncompress exe in on the system path and then invoke an R system() call, calling the uncompress with  the file as a parameter. Pretty slick!  I tried to use the R package “uncompress” but I get failures as did SteveMc. I prefer the R package approach. So, I’m trying to contact the maintainer to update his package. The source is available, so any c programmer should be able to figure it out. When Time permits I’ll look at his source and see if I can find the bug. It’s a unexpected EOF type error. I’m suspecting that he only tested on Linux since he’s a linux guy and that the tests Cran runs are not sufficient. SteveMc runs windows and I use MAC. Wonder if Ron Broberg had any luck on linux. We will start by rebaselining. Moshtemp4 Zip will go up at the drop and in the box on this page. Unzip it. Make it your working directory and run Downloadall.R That script will take a while. It will download all the assets you need. The files, masks, inventories, and CRU results which we will eventually calibrate against.  you will then have to manually uncompress v2.mean.Z. On the Mac thats just a click and on linux uncompressed should also be built it. next run SetUpData.R.  That script will create 1. V2Mean.Rdata: this is a processed version of v2mean with all duplicates averaged 2. V2Anomalies.Rdata: this is giant matrix of all stations. StationID is in the column. Every row is a month of anomaly in C ( not tenths) from 1900 to 2009. 3. SST.Rdata: a brick of SST anomalies from 1900 to 2009 After SetUpData completes, you can test it by doing this: plot(V2Anomalies[,150]) You could even loop thru all 5072 stations and plot every one [ see the routine, plotAnomalySeries() ] You can also check out the calibration data: Y
 "	 0 Comments
Competition: Data Visualization with ggplot2	https://www.r-bloggers.com/2010/09/competition-data-visualization-with-ggplot2/	September 3, 2010	David Smith	"The ggplot2 package for R is an amazing system for creating entirely new visualizations of data. It allows data analysts to tell a detailed, meaningful and yet easy-to-interpret story about complex and/or unusual data sets.   To promote more data stories being told, ggplot2 author Hadley Wickham has organized a ggplot2 case study competition. Simply create a new visualization of a data set, and post it to the ggplot2 wiki before October 1. The competition announcement is reproduced below, and to whet your appetite check out some of the amazing entries from last year's competition after the break. We've moved the ggplot2 wiki to a new location to make it more useful: http://github.com/hadley/ggplot2/wiki. To publicize the new location,
we’re organizing a second ggplot2 case study competition. Some great
prizes are at stake:   To enter, write up a small case study where you’ve found ggplot2
useful and add it to the wiki home page. The deadline for entry is
October 1, 2010. For more info, check out the competition details. Here are some examples from last year's competition for inspiration. Click the images for full-size versions, and follow the links for the R code. Flight activity in the US following the attacks of September 11 (Heike Hofmann, Iowa State University): 
   Swimming direction of coral reef fish larvae (Jean-Olivier Irisson, University of Miami): 
  Transmission of light through gold particles (Baptiste Auguié, University of Exeter): 
      "	 0 Comments
NIPS 2010: Monte Carlo workshop	https://www.r-bloggers.com/2010/09/nips-2010-monte-carlo-workshop/	September 3, 2010	xi'an	In the wake of the main machine learning NIPS 2010 meeting in Vancouver, Dec. 6-9 2010, there will be a very interesting workshop organised by Ryan Adams, Mark Girolami, and Iain Murray on Monte Carlo Methods for Bayesian Inference in Modern Day Applications, on Dec. 10. (And in Whistler, not Vancouver!) I wish I could attend, but going to a conference in honour of Larry Brown’s 70th birthday in Wharton the week after makes it impossible… Monte Carlo methods have been the dominant form of approximate inference  for Bayesian statistics over the last couple of decades. Monte Carlo  methods are interesting as a technical topic of research in themselves,  as well as enjoying widespread practical use. In a diverse number of  application areas Monte Carlo methods have enabled Bayesian inference  over classes of statistical models which previously would have been  infeasible. Despite this broad and sustained attention, it is often  still far from clear how best to set up a Monte Carlo method for a given  problem, how to diagnose if it is working well, and how to improve  under-performing methods. The impact of these issues is even more  pronounced with new emerging applications. What does the workshop address and accomplish? Identifying features of applications of Monte Carlo methods: This  workshop is aimed equally at practitioners and core Monte Carlo  researchers. For practitioners we hope to identify what properties of  applications are important for selecting, running and checking a Monte  Carlo algorithm. Monte Carlo methods are applied to a broad variety of  problems. The workshop aims to identify and explore what properties of  these disparate areas are important to think about when applying Monte  Carlo methods. 	 0 Comments
Bot Botany – K-Means and ggplot2	https://www.r-bloggers.com/2010/09/bot-botany-k-means-and-ggplot2/	September 2, 2010	C		 0 Comments
New R User Group in New Jersey	https://www.r-bloggers.com/2010/09/new-r-user-group-in-new-jersey/	September 2, 2010	David Smith	Folks in the New Jersey area no longer need to trek over to New York City to meet other R users. Now there's NewJerseyR, a new R user group put together by Mango Solutions. The first meeting will in Iselin on September 16, with speakers from Mango, Pfizer, and Bristol Myers Squibb. Full details at the NewJerseyR website, linked below. NewJerseyR: NewJerseyR home page 	 0 Comments
Statisfaction	https://www.r-bloggers.com/2010/09/statisfaction/	September 2, 2010	xi'an	A collective blog has been started by the statistics students and postdocs at CREST, in the wake of the Valencia meeting. It is called Statisfaction. (The Rolling Stones of Statistics?! Actually, Andrew Gelman also has a post with that title… And it is even part of the Urban Dictionnary!) Since I have no responsability nor even say in the contents of this independent blog, I cannot help but recommend following it! The latest posting is about the slides of Peter Müller’s slides of his Santa Cruz course in Bayesian nonparametrics being available on line. 	 0 Comments
Update	https://www.r-bloggers.com/2010/09/update-2/	September 2, 2010	Millsy		 0 Comments
How the expiration of the assault weapon ban affected Mexico	https://www.r-bloggers.com/2010/09/how-the-expiration-of-the-assault-weapon-ban-affected-mexico/	September 2, 2010	Diego Valle-Jones		 0 Comments
R-bloggers announcement – maintenance mode – site might be down	https://www.r-bloggers.com/2010/09/r-bloggers-maintenance/	September 2, 2010	Tal Galili		 0 Comments
Third, and Hopefully Final, Post on Correlated Random Normal Generation (Cholesky Edition)	https://www.r-bloggers.com/2010/09/third-and-hopefully-final-post-on-correlated-random-normal-generation-cholesky-edition/	September 2, 2010	JD Long	André-Louis Cholesky is my homeboy When I did a brief post three days ago I had no plans on writing two more posts on correlated random number generation. But I’ve gotten a couple of emails, a few comments, and some Twitter feedback. In response to my first post, Gappy, calls me out and says, “the way mensches do multivariate (log)normal variates is via Cholesky. It’s simple, instructive, and fast.”  And I think we’re all smart enough to read through Mr. Gappy’s comment and see that he’s saying I’m a complicated, opaque, and slow, גוי‎. My wife called and said his list would be more accurate if he added ‘emotionally detached.’ I have no idea what she means. At any rate, in response to Gappy’s comment, here is the third verse (same as the first). The crux of the change is the following lines: Edit: When I first publishes this example, I didn’t shift the means prior to taking the cov(). I’ve sense corrected that.  Also thanks to @fdaapproved on Twitter who pointed out that I can replace the loop above with myDraws  This method, which uses Cholesky decomposition, is how I initially learned to create correlated random draws. I think this method is comparable to the mvrnorm() method. mvrnorm() is handy because it wraps everything above in one single line of code. But the above method is reliant only on the Matrix package and that’s only for the nearPD() function. If you are familiar with the guts of the mvrnorm() function and the chol() function, I’d love for you to comment on any technical differences. I looked briefly at the code for both and quickly realized my matrix math was rusty enough that it was going to take a while for me to sort through the code. If you want the whole script you can find it embedded below and on Github. 	 0 Comments
Stochastic approximation in Bristol	https://www.r-bloggers.com/2010/09/stochastic-approximation-in-bristol/	September 2, 2010	xi'an	"This is very short notice, but for those in the vicinity and not at the RSS conference, there is a highly interesting workshop taking place in Bristol in ten days (I would certainly have gone, had I not been at the same time in Banff!): We would like to invite you to contribute to our 3 day workshop on “Stochastic approximation: methodology, theory and applications in statistics” that will take place in the Mathematics Department of the University of Bristol (UK) from 13-15 September 2010. The aim of this workshop is to gather world specialists on stochastic approximation and its applications, who might not have the opportunity to meet otherwise, in order to present and discuss recent methodological and theoretical developments in the area, as well as applications. The current list of invited speakers who have agreed to present their work is
Michel Benaïm
Han-Fu Chen
Jayanta Ghosh
Éric Moulines
Georg Pflug
Boris Polyak
Pierre Tarres
George Yin
The workshop is part of SuSTaIn: a programme funded by the EPSRC and the University of Bristol with the ambitious goal of strengthening the discipline of Statistics in the UK, by equipping it to face the challenges of future applications. We will offer limited funding to a restricted number of participants. Priority will be given to young researchers. If you wish to participate, please fill in the registration form "	 0 Comments
Rllvm	https://www.r-bloggers.com/2010/09/rllvm/	September 1, 2010	omegahat	"Over the past 10 years, I have been torn between building a new stat. computing environment
or trying to overhaul R.  There are many issues on both sides.  But the key thing is to
enable doing new and better things in stat. computing rather than just making the existing things
easier and more user-friendly. If we are to continue with R for the next few years, it is essential that it get faster.
There are many aspects to this. One is compiling interpreted R code into something faster.
LLVM is a toolkit that facilitates the compilation of machine code.   So in the past few days
I have looked into this and developed an R package that provides R-bindings to some of
the LLVM functionality. The package is available from http://www.omegahat.org/Rllvm, as are several examples
of its use.
I used the package to implement a compiled version of one of Luke Tierney’s compilation examples
which uses a loop in R to add 1 to each element of a vector.  The compiled version gives a speedup
of a factor of 100, i.e. 100 times faster than interpreted R code.  This is slower than x + 1
in R which is implemented in C and does more. But it is a promising start.  The compiled version is also faster than bytecode interpreter approaches.  So this is reasonably promising. Of course, it would be nicer to leverage an existing compiler! (Think SBCL and building on top of LISP). "	 0 Comments
Rffi	https://www.r-bloggers.com/2010/09/rffi/	September 1, 2010	omegahat	"A few weeks ago, I posted the Rffi package on the Omegahat repository.
It is an interface to libffi which is a portable mechanism for invoking native routines
without having to write and compile any wrapper routines in the native language.
In other words, we can use this in R to call C routines using only R code.
This enables us to call arbitrary routines and get back arbitrary values, including structures
arrays, unions, etc. One could use the RGCCTranslationUnit package to obtain descriptions of routines and data
structures and then generate the interfaces to those routines via functions in Rffi. Writing or generating C/C++ code for wrappers (see RGCCTranslationUnit) is still the way to
go in many ways, but Rffi is very convenient for dynamic invocations without any write and compile
setup costs. As usual, you can install this from source from the Omegahat repository install.packages(“Rffi”, repos = “http://www.omegahat.org/R&#8221;, type = “source”) but you will need to have installed libffi. "	 0 Comments
How to generate correlated random numbers	https://www.r-bloggers.com/2010/09/how-to-generate-correlated-random-numbers/	September 1, 2010	David Smith	We've covered how to generate random numbers in R before, but what if you want to go beyond generating one random number at a time? What if you want to generate two, or three or more random numbers, and what's more, you want them to be correlated? JD Long lays out the way in a couple of posts at his Cerebral Mastication blog. If you want to generate bivariate (or trivariate, or more) random multivariate Normal variates, it's pretty easy, as JD points out. Just use the mvrnorm function from the MASS package in R, specify the covariance matrix, and you're all set. But what if the random variables need to follow some other marginal distribution than Normal? This is a common task for simulations, where you may know the distributions you want for each variable, but need a way to specify the covariance structure. This is where copulas come in handy, as explained in JD's second post, complete with R code implementing a simulation.  Cerebral Mastication: Even Simpler Multivariate Correlated Simulations  	 0 Comments
Random dive MH	https://www.r-bloggers.com/2010/09/random-dive-mh/	September 1, 2010	xi'an	"A new Metropolis-Hastings algorithm that I would call “universal” was posted by Somak Dutta yesterday on arXiv. Multiplicative random walk Metropolis-Hastings on the real line contains a different Metropolis-Hastings algorithm called the random dive. The proposed new value x’ given the current value x is defined by  when  is a random variable on . Thus, at each iteration, the current value is either shrunk or expanded by a random amount. When I read the paper in the Paris metro, while appreciating that this algorithm could be geometrically ergodic as opposed to the classical random walk algorithm, I was not convinced by the practical impact of the method, as I thought that the special role of zero in the proposal was not always reflected in the target. Especially when considering that the proposal is parameter-free. However, after running the following R program on a target centred away from zero, I found the proposal quite efficient. f=function(x){.5*dnorm(x,mean=14)+.5*dnorm(x,mean=35)}
Nsim=10^5
x=rep(5,Nsim)
for (t in 2:Nsim){
coef=runif(1,min=-1)^sample(c(-1,1),1)
prop=x[t-1]*coef
prob=abs(coef)*f(prop)/f(x[t-1])
x[t]=x[t-1]
if (runif(1)
}
hist(x,pro=T,nclass=113,col=”wheat2″)
curve(f,add=T,n=1001,col=”sienna”,lwd=2)
 Obviously, it is difficult to believe that this extension will keep working similarly well when the dimension increases but this is an interesting way of creating a heavy tail proposal. "	 0 Comments
How to check if a file exists with HTTP and R	https://www.r-bloggers.com/2010/09/how-to-check-if-a-file-exists-with-http-and-r/	September 1, 2010	respiratoryclub	So, there’s probably an easier way to do this (please let me know if you know it)… Suppose you’re working with a system which creates (binary) files and posts them for download on a website.  You know the names of the files that will be created.  However, they may not have been made yet (they’re generated on the fly, and appear in a vaguely random order over time).  There are several of them and you want to know which ones are there yet, and when there are enough uploaded, run an analysis. I spent quite a bit of time trying to work this out, and eventually came up with the following solution: What this does is uses RCurl to download the file into a variable z.  Then your system will check to see if z now contains the file.   If the file doesn’t exist, getBinaryURL() returns an error, and your loop (if you are doing several files) will quit.  Wrapping the getBinaryURL() in try() means that the error won’t stop the loop from trying the next file (if you don’t trust me, try doing the above without the try wrapper).  You can see how wrapping this in a loop could quickly go through several files and download ones which exist. I’d really like to be able to do this, but not actually download the whole file (e.g. just the first 100 bytes) to see how many files of interest have been created, and if enough have, then download them all.  I just can’t work out how to yet – I tried the range option of getBinaryURL() but this just crashed R.  This would be useful if you are collecting data in real time, and you know you need at least (for example) 80% of the data to be available before you jump into a computationally expensive algorithm.   So, there must be an easier way to do all this, but can I find it? … 	 0 Comments
Is this good or bad programming?	https://www.r-bloggers.com/2010/09/is-this-good-or-bad-programming/	September 1, 2010	mikeksmith's posterous	"
     Permalink 

	| Leave a comment  »
 "	 0 Comments
apply() function and ABM in R	https://www.r-bloggers.com/2010/09/apply-function-and-abm-in-r/	September 1, 2010	E.Crema		 0 Comments
Monte Carlo testing of classification groups	https://www.r-bloggers.com/2010/09/monte-carlo-testing-of-classification-groups/	September 1, 2010	Michael Bedward		 0 Comments
ABC lectures [finale]	https://www.r-bloggers.com/2010/10/abc-lectures-finale/	October 31, 2010	xi'an	The latest version of my ABC slides is on slideshare. To conclude with a pun, I took advantage of the newspaper clipping generator once pointed out by Andrew. (Note that nothing written in the above should be taken seriously.) On the serious side, I managed to cover most of the 300 slides (!) over the four courses and, thanks to the active attendance of several Wharton faculty, detailed PMC and ABC algorithms in ways I hope were accessible to the students. This course preparation was in any case quite helpful in the composition of a survey on ABC now with my co-authors. 	 0 Comments
Scatter plot with ggplot2	https://www.r-bloggers.com/2010/10/scatter-plot-with-ggplot2/	October 31, 2010	kariert	I decided to go use ggplot2 more frequently and go through everything. For future reference I will start a series of blog posts on ggplot2. Scatter plot: In my next post, I will change the axis labels. 	 0 Comments
Presenting Immer’s barley data	https://www.r-bloggers.com/2010/10/presenting-immer%e2%80%99s-barley-data/	October 31, 2010	richierocks	Last time I talked about adapting graphs for presentations.  This time I’m putting some of the concepts I discussed there into action, with a presentation of Immer’s barley dataset.  This is a classic dataset, originally published in 1934; in 1993 Bill Cleveland mentioned it in his book Visualising Data on account of how it may contain an error.  Here’s the paper/screen version.  Here’s the presentation. For the record, the presentation was created with Impress and the audio recorded with Audacity.  Using these tools, it’s pretty straightforward to make and share an audio presentation. 	 0 Comments
How to buy a used car with R (part 1)	https://www.r-bloggers.com/2010/10/how-to-buy-a-used-car-with-r-part-1/	October 31, 2010	Dan Knoepfle's Blog	"I’m in the process of buying a used car.  Since I enjoy making these decisions as complicated as possible, I’ve written some R code to scrape relevant websites for informative data.  I’ve written this up as a blog entry because I think it’s a decent example of how one might use the XML package and Firebug to quickly and easily bring data from websites into R. In the past, the first resource a used car buyer looking for price information might have turned to was the Kelley Blue Book; now, this information is available for free at KBB.com:  For now, I’m going to skip ahead to the page containing the kind of information that we want; later, I’ll back up and go through the process of getting to that page and detail how I wrote some simple functions automating queries for different parameters. Here’s http://www.kbb.com/used-cars/honda/accord/2005/private-party-value/pricing-report?condition=excellent&id=846&mileage=10000, giving the KBB private party value for a 2005 Honda Accord DX Sedan with automatic transmission, standard options, and 10,000 miles:  To get at the data we want, we need to identify where it is located in the structure of the page.  While one can do this by simply reading the HTML source code, Firebug makes things much simpler.  Load up Firebug and go to the HTML tab.  Click the Inspect Element button (or go to the Firebug menu and choose Inspect Element); as you mouse-over elements on the page, you’ll notice that the corresponding tag in the HTML element tree is opened and highlighted.  In the screenshot below, I’ve clicked on the value for the Excellent condition:  Examining the HTML tree in the Firebug display, we can see that all of the information we’re interested in is contained in a table with id ‘priceCondition’.  Similarly, if you’re using Google Chrome, you can accomplish the same thing with the Developer Tools.  Below, Firefox is on the left and Chrome is on the right:   The XML package includes a convenient function called readHTMLTable to grab the data from the table we identified earlier.  We can simply give it the URL of the page and it returns a list containing each of the page’s tables as an R object (converting them to data.frame by default). With this minimal amount of effort, we’re most of the way to what we’re after: By explicitly specifying the header, skipping the first two rows, and extracting the ‘priceCondition’ data.frame itself, we’re left with the raw data we are interested in: Now, if we take a look at the URL we’re using, http://www.kbb.com/used-cars/honda/accord/2005/private-party-value/pricing-report?condition=excellent&id=846&mileage=10000, it should be apparent that fetching these values for any given mileage won’t be any trouble.  The following code gets the KBB values for 10,000 mile increments from 10,000 to 150,000 miles: Finally, we can convert the list into one big data.frame and augment it with the corresponding mileages and the model year.  This leaves us with a nice data.frame from which we can extract whatever information we desire. Our last trick for the day is a simple one:  take the data and make a pretty picture.  Having collected the KBB values for different conditions and mileages, it is straightforward to construct a plot of value versus mileage for each condition. First, however, we need to convert the kbbValues$Value column from its current human-readable state (a factor with levels like “$10,265”) into a more natural form for analysis.  A quick bit of regular expressions magic using gsub does the trick, and we’re left with a nice column of numbers: Use of ggplot is a subject best left for another time.  Here, it’s as simple as: This gives us the following beautiful plot:  So, where did that peak at 100,000 miles come from? Well, looking back, it’s clear that it’s present in the raw data in kbbValues.  If we check the original page (http://www.kbb.com/used-cars/honda/accord/2005/private-party-value/pricing-report?condition=excellent&id=846&mileage=100000), however, the values don’t match.  What happened? The culprit, and a correction, are in the code below: Using the corrected procedure, we are rewarded with a nice, smooth graph:
 "	 0 Comments
Errors in Ghcn Inventories	https://www.r-bloggers.com/2010/10/errors-in-ghcn-inventories/	October 30, 2010	Steven Mosher	" In the debate over the accuracy of the global temperature nothing is more evident than errors in the location data for stations in the GHCN inventory. That inventory is the primary source for all the temperature series. One question is “do these mistakes make a difference?” If one believes as I do that the record is largely correct, then it’s obvious that these mistakes cannot make a huge difference. If one believes, as some do, that the record is flawed, then it’s obvious that these mistakes could be part of the problem. Up until know that is where these two sides of the debate stand. Believers convinced that the small mistakes cannot make a difference; and dis-believers holding that these mistakes could in fact contribute to the bias in the record.  Before I get to the question of whether or not these mistakes make a difference, I need to establish the mistakes, show how some of them originate, correct them where I can and then do some simple evaluations of the impact of the mistakes. This is not a simple process. Throughout this process I think we can say two things that are unassailable: 1. the mistakes are real. 2. we simply don’t know if they make a difference. Some believe they cannot (but they haven’t demonstrated that) and some believe they will (but they haven’t demonstrated that). The demonstration of either position requires real work. Up to now no one has done this work. This matters primarily because to settle the matter of UHI stations must be categorized  as urban or rural. That entails collecing some information about the character of the station, say it’s population or the characteristics of the land surface. So, location matters. Consider Nightlights which Hansen2010 uses to categorize stations into urban and rural. That determination is made by looking up the value of a pixel in an image. If it bright, the site is urban. If its dark (mis-located in the ocean) the site is rural. In the GHCN metadata the station may be reported at location xyz.xyN yzx.yxE. In reality it can be many miles from this location. That means the nightlights lookup or ANY georeferenced data ( impervious surfaces, gridded population, land cover) may be wrong. One of my readers alerted me to a project to correct the data. That project can be found here. That resource led to other resources including a 2 year long project to correct the data for all weather stations. Its a huge repository. That led to the WMO documents one of the putative sources for GHCN. This source also has errors. Luckily the WMO has asked all member nations to report more accurate data back in 2009. That process has yet to be completed and when it is done we should have data that is reported down to the arc second. Until then we are stuck trying to reconcile various sources. The first problem to solve is the loss of precision problem. The WMO has reports that are down to the arc minute. It’s clear that when GHCN uses this data and transforms it into decimal degrees that they round and truncate. These truncations, on occasion, will move a station.  I’ve documented that by examining the original WMO documents and the GHCN documents. In other cases it hard to see the exact error in GHCN, but they clearly dont track with WMO. First the WMO coordinates for WMO 60355 and then the GHCN coordinates: WMO:   60355	SKIKDA	36 53N	06 54E  [36.8833333, 6.9000] GHCN: 10160355000 SKIKDA  36.93    6.95 GHCN places the station in the ocean. WMO places it on land as seen above. To start correcting these locations I started working through the various sources. In this post I will start the work by correcting the GHCN inventory using WMO information as the basis. Aware, of course that WMO may have it own issue. The task is complicated by the lack of any GHCN documents showing how they used WMO documents. In the first step I’ve done this. I compared the GHCN inventory with the WMO inventory and looked at those records where GHCN and WMO have the same  station number and station name. That is difficult in itself because of the way GHCN truncates names to fit a data field. It’s also complicated by the issue of re spelling, multiple names for each site and the issue of GHCN Imod flags and WMO station index sub numbers. Here is what we find. If we start with the 7200 stations in the GHCN inventory and use the WMO identifier to look up the same stations in the WMO official inventory we get roughly 2500 matches. Here are the matching rules I used. 1. the WMO number must be the same 2. The GHCN name must match the WMO name (or alternate names match). 3. The GHCNID must not have any Imod variants. (no multiple stations per WMO) 4. The WMO station must not have any sub index variants. (107 WMO numbers have subindexes) That’s a bit hard to explain but in short I try to match the stations that are unique in GHCN with those that are unique in the WMO records. Here is what a sample record looks like.WMO positions are translated from degrees and minutes to decimal degrees and the full precision is retained. You can check that against GHCN rounding. As we saw in previous posts slight movements in stations can move them from Bright to dark and from dark to bright pixels. 63401001000     JAN MAYEN 70.93 -8.67              1001    JAN MAYEN 70.93333 -8.666667 63401008000     SVALBARD LUFT 78.25 15.47    1008    SVALBARD AP 78.25000 15.466667 63401025000 TROMO/SKATTO      69.50 19.00    1025   TROMSO/LANGNES 69.68333 18.916667 63401028000 BJORNOYA                 74.52 19.02    1028    BJORNOYA 74.51667 19.016667 63401049000  ALTA LUFTHAVN 69.98 23.37    1049  ALTA LUFTHAVN 69.98333 23.366667 You also see some of the name matching difficulties where the two records have the same WMO and slightly different names. If we collate all differences on lat and lon in matching stations we get the following:  And when we check the worst record we find the following WMO:  60581  HASSI-MESSAOUD             31.66667      6.15 GHCN:  10160581000 HASSI-MESSOUD 31.7               2.9 GHCN has the station at latitude 2.9. According to GHCN the station is an airport:  The location in the WMO file  And the difference is roughly 300km.WMO is more correct than GHCN. GHCN is off by 300km  An old picture of the approach  And diagrams of the airfield Now, why does this matter.  Giss uses GHCN inventories to get Nightlights. Nightlights uses the location information to determine if the pixel is dark (rural) or bright (urban) NASA thinks this site is dark. They think it is pitch dark. Of course they are looking 300km away from the real site. From the inventory used in H2010. "	 0 Comments
How to Start Using (pgf)Sweave in LyX in One Minute	https://www.r-bloggers.com/2010/10/how-to-start-using-pgfsweave-in-lyx-in-one-minute/	October 30, 2010	Yihui Xie	"This works for MikTeX under Windows (Server 2003 / Win7), and TeXLive 2009 under Ubuntu 10.10, MacTeX 2010 under Mac OS; R 2.12.0 or 2.11.1; LyX 1.6.x. Although Sweave in LyX is convenient to use, it is not a trivial task for beginners to configure and understand how it works. The video below can give you an idea on what it looks like in LyX (Chinese visitors please go to Photobucket or 56.com to watch the video):  As we can see, R code can be easily embedded into LyX. If you are familiar with Sweave, you don’t even need time to learn anything. For those who do not know Sweave well, a good place to look at is the help page ?Sweave. A Sweave document is dynamic in the sense that everything in the document can be changed by the R code (nothing is hard-coded), so we don’t need to worry too much about the specific numbers and plots in the output. Instead, we focus on the code which produces these output. In the above video, I used a LaTeX macro \Sexpr{} to output the value of pi and I don’t need to write the specific number 3.1415926 there. While Sweave is a great invention for reproducible research, there are other packages which can improve R’s default Sweave functionality. A brilliant one is the pgfSweave package. It was built upon the cacheSweave package to support caching R objects (to avoid unnecessary repeated computations and save time), and it also provided a mechanism to cache graphics! Beside the speed issues, a remarkable feature is the quality of graphics — it is unbeatable. I’m not exaggerating. This packages uses the tikzDevice package to produce pgf/tikz graphics which are essentially LaTeX code, in other words, the R graphics are represented in the LaTeX language so that they are treated (compiled) in the same way as the body of a LaTeX document. This will make the style of graphics completely consistent with the body of a document, e.g. the fonts. By the way, I also like the nogin option for Sweave.sty to be the default in pgfSweave, because I really don’t like the idea of setting the size of graphics by a LaTeX macro \setkeys{Gin}{width=0.8\textwidth}. In pgfSweave, we just set the width and height naturally in the code chunk options like <<width = 5, height =4>>=. pgfSweave comes with a command line usage like Sweave: R CMD pgfSweave your-file.Rnw. I’m not using this approach in LyX, because this requires system admin privilege to install pgfSweave. Instead, I use this way: R can accept a string in its -e argument, e.g. Later I’ll explain how to connect LyX and R/pgfSweave in this way. To make LyX work with Sweave, we need to take these steps: Each step involves with several novel concepts for beginners. For example, you may ask “what’s a texmf tree?” “What’s a layout file? Where is it?” “Where is the preference file?” “What’s a converter?”… I spent several hours on writing an R script trying to cover all these gory details automatically, so the configuration becomes as easy as This is convenient, which is good. But I have to confess it is a really nasty script — (for Windows) it calls several system commands to help the configuration, such as initexmf or mpm, or setx to set the system PATH variable; this is a dangerous practice and some users may feel extremely uncomfortable with it. Under Mac and Linux, it tries to download and copy files to your home directory so that LaTeX and LyX will work properly. It will do no harm to your system, but I need to warn you first in the spirit of “open source”. You are still reading… So first you have to read Gregor Gorjanc’s paper in R News and that make most things clear in this blog entry. A few more things I need to add are: After you have got everything ready, here are two demos that you can play with: LyX and pgfSweave demo (7.6K)
 I hope this is useful to the community. Please feel free to report problems during your installation and configuration. "	 0 Comments
A question from the R list	https://www.r-bloggers.com/2010/10/a-question-from-the-r-list/	October 30, 2010	Steven Mosher	"I am currently working on rectifying the GHCN station list to improve the location information. Its the kind of database work that is mind numbingly tedious and a PITA in R. not because R lacks capabilities, its just tough and not very sexy to matching and fuzzy matching and greping and blah blah blah. Instead, I’ll try to work a problem that was posted on the R list. When you work with big data it’s sometimes hard to get help on the list because your problem requires actually loading the data. This note appearred on the help list: So, we will see if we can figure out how to help “I  am trying to use “clim.pact” package for my work, but since this is the  beginning for me to use gridded datasets in “R”, I am having some  trouble. I want to do seasonal analyses like  trends, anomalies, variograms, EOF and probably kriging too to  downscale my 1 degree gridded data to 0.5.  So, as a first step, I  compiled my entire dataset (with 25 yeears of daily dataset which were  present as 25 files) into a single netcdf file. Then, I downloaded clim.pact to do further analysis, which works but seems  to change dataset’s original dimensions’ order for  ”retrieve.nc”  function (i.e. original lon, lat, time order was changed  to time, lat,  lon after using this function to get a subset). I am not  sure as to why  this happened and not able to get any plots such as box  plot (showing  trend in “lon”, “lat”, “time”), variogram (or variance),  correlation  analysis done because of this conversion problem. Further, basic “R”  functions seem to work well with objects such as  dataframe, matrix ..etc  with time in a separate column, and the data  values (precipitation, or  temperature) in a separate coulmn with  corresponding station values  (lon/lat). So, now I have very little idea  about what I have to do. Can anyone suggest me a better (probably  more refined way) way than what I am currently doing to analyze these  data? The first thing we will do is question the whole need to put the data into a ncdf. R can read ncdf and so can raster. But i’ll suggest here that using ncdf as an intermediate data transfer tool is probably not necessary. In the end when we want to exchange data with others we can output ncdf or maybe HCF ( something I want to try for a particular project) So, I’ve invited the writer of this question here and we will back up to where the data stands before he output a ncdf. I’ll also try to get his ncdf working. And
 "	 0 Comments
Findings increasingly novel, scientists say…	https://www.r-bloggers.com/2010/10/findings-increasingly-novel-scientists-say%e2%80%a6/	October 29, 2010	nsaunders	"…was the tongue-in-cheek title of an image that I posted to Twitpic this week.  It shows the usage of the word “novel” in PubMed article titles over time.  As someone correctly pointed out at FriendFeed, it needs to be corrected for total publications per year. It was inspired by a couple of items that caught my attention.  First, a question at BioStar with the self-explanatory title Locations of plots of quantities of publicly available biological data.  Second, an item at FriendFeed musing on the (over?) use of the word “insight” in scientific publications. I’m sure that quite recently, I’ve read a letter to a journal which analysed the use of phrases such as “novel insights” in articles over time, but it’s currently eluding my search skills.  So here’s my simple roll-your-own approach, using a little Ruby and R.

Initially, I entered “novel[Title]” at the PubMed website, download all 143 031 results in Medline format and parsed the “DP” (publication date) field.  Useful, in that I learned the earliest title (1845); inefficient, in that the resulting download is ~ 397 MB. Fortunately, BioRuby comes with a nice set of methods for search and retrieval from the NCBI Entrez databases, including esearch_count() – as the name suggests, it simply counts returned results for a query. So, to search pubmed for (1) all articles published from 1845 – 2009 and (2) those articles with the word “novel” in title or abstract is as simple as this: Save and run that as pmnovel.rb > pmdata.txt.  Obviously, we’re having a bit of fun here.  You could search for any terms that you like and in a real script, you’d probably want to specify the terms and date range as command-line options. Next, load the tab-delimited output file into R for some simple plotting. Exciting times 😉
 PubMed novelty, 1845 - 2009 "	 0 Comments
Could someone please set this as the new R default in base graphics?	https://www.r-bloggers.com/2010/10/could-someone-please-set-this-as-the-new-r-default-in-base-graphics/	October 29, 2010	Andrew Gelman		 0 Comments
The stimulus, mapped	https://www.r-bloggers.com/2010/10/the-stimulus-mapped/	October 29, 2010	David Smith	Edward Tufte created this “Lights-On Map”, animating the grants under the American Recovery and Investment Act (more commonly known as the Stimulus):  Click the image to see the lights come on over time. It's a nice visualization of the distribution of the stimulus projects over the country and time, but I'm actually surprised that it doesn't do a great job of representing how the funds are distributed: it seems that each “light” is one project, regardless of the amount it was funded for. I wonder if that was proposed, but was deemed too politically risqué to demonstrate the concentration of funds in certain districts. Recovery.gov: Lights On Map 	 0 Comments
Because it’s Friday: Werner Heisenberg at a traffic stop	https://www.r-bloggers.com/2010/10/because-its-friday-werner-heisenberg-at-a-traffic-stop/	October 29, 2010	David Smith	I saw this on in a random image stream and it made me chuckle:  A little googling suggests MathematiciansPictures.com is the source, where you can also get Heisenberg on a mug. Whether your coffee is in or outside the mug is, unfortunately, indeterminable. 	 0 Comments
SAS vs Open Source	https://www.r-bloggers.com/2010/10/sas-vs-open-source/	October 29, 2010	David Smith	SAS CEO Jim Goodnight elicited guffaws from the audience during his keynote at JSM this year when he said “I have a problem with government using open source software”. Now it seems Goodnight is at the center of another controversy related to open source, having dismissed the growth of open-source software for business intelligence in an interview with CBR online: Asked whether open source BI and data integration software from the likes of Jaspersoft, Pentaho and Talend is a growing threat, [Goodnight] said: “We haven't noticed that a lot. Most of our companies need industrial strength software that has been tested, put through every possible scenario or failure to make sure everything works correctly.” Between the growth of R for data analysis, Talend for ETL and the likes of Jaspersoft and and Pentaho for BI — all areas where SAS has pricey commercial solutions — it seems hard to believe that SAS hasn't noticed the impact of open source software in commercial environments. And, as Pentaho's Vinay Joosery pointed out today in a stinging response, “Denying a market reality doesn't help your business in the long term.” Well said, Vinay. CNR Online: Pentaho fires back across SAS' bows over “limited” open source appeal 	 0 Comments
What I’ve been up to..	https://www.r-bloggers.com/2010/10/what-ive-been-up-to/	October 29, 2010	Murray Stokely		 0 Comments
World Economic Forum Data Visualization Challenge	https://www.r-bloggers.com/2010/10/world-economic-forum-data-visualization-challenge/	October 28, 2010	Andrew Gelman		 0 Comments
Adapting graphs for presentations	https://www.r-bloggers.com/2010/10/adapting-graphs-for-presentations/	October 28, 2010	richierocks	I’ve just finished reading slide:ology by Nancy Duarte.  It contains lots of advice about how to convey meaning through aesthetics.  The book has a general/business presentation focus, but it got me wondering about how to apply the ideas in a scientific context.  Since graphs from a big part of most scientific talks, and since that’s the bit I know best; that’s what I’m going to discuss here. We start with a basic example using the ggplot2 mtcars dataset.  Something I’ve been burned with recently is overestimating the size of the projector screen.  Although my graphs looked great when my face was next to my monitor, the axes were hard to read when projected.  A good rule of thumb for text is your font size should be half the age of the eldest member of your audience or 30 points, whichever is bigger.  For graphs axes we can perhaps get away with something a little smaller.  Don’t forget that by the time you’ve printed your graph to file and played with it in your presentation software, the font size you’ve picked may bear no relation to it’s final value.  That said, the point remains that you need to make the text bigger.  Reading a graph can be quite an in depth process which can overwhelm you audience.  Once you’ve shown them the graph in its basic form, you should emphasis interesting features, one at a time.  The one at a time thing is important – if you want your audience to be concentrating, then you shouldn’t distract them with many things at once.  Here I’ve increased the size of the outliers in the bottom right hand corner.  As well as emphasising values, a useful tool can be to de-emphasise other regions of the graph.  Overlaying a translucent white rectangle on such regions does the trick nicely.  For more deemphsising more complicated regions, I find it easier to use dedicated image manipulation software.  Paint.NET is my personal zero-cost favourite but any graphics software should suffice. Finally, it can be useful to annotate your graph.  By now, if your audience hasn’t figured out which bit of the graph you want them to look at you’re in trouble.  Let me know in the comments if you have any other ideas for how to draw graphs for presentations. 	 0 Comments
Integrating R with C++: Rcpp, RInside, and RProtobuf	https://www.r-bloggers.com/2010/10/integrating-r-with-c-rcpp-rinside-and-rprotobuf/	October 28, 2010	Ellen Ko		 0 Comments
Google tech talk / Rcpp, … presentation on youtube	https://www.r-bloggers.com/2010/10/google-tech-talk-rcpp-presentation-on-youtube/	October 28, 2010	romain francois	Following this post, the 90 minutes presentation is now available to watch on youtube:  	 0 Comments
Revolution Analytics’ Chief Scientist	https://www.r-bloggers.com/2010/10/revolution-analytics-chief-scientist/	October 28, 2010	David Smith	Lee Edlefsen, who has led the Revolution R development team in Seattle for more than three years, is now the Chief Scientist for Revolution Analytics. With more than 30 years experience in high-performance and statistical computing, this is a great position for Lee. There's more details in the press release linked below. Revolution Analytics: Revolution Analytics names Chief Scientist to accelerate success with R     	 0 Comments
R is Hot: Part 4	https://www.r-bloggers.com/2010/10/r-is-hot-part-4/	October 28, 2010	David Smith	This is Part 4 of a five-part article series, with new parts published each Thursday. You can download the complete article from the Revolution Analytics website.    R is especially useful for generating charts and graphics, quickly and easily. The ability to create visual plots of complex data is more than just a handy trick; it’s an incredibly important step in the analysis of data because it enables you to literally “see” the patterns and anomalies hidden within the data.  The New York Times has been a leader in the use of charts and graphics that make it easier for readers to get the gist of complicated stories. Amanda Cox, a graphics editor at the Times, says R is particularly valuable in deadline situations when data is scant and time is precious. “If you can picture it in your head, chances are good that you can make it work in R,” says Cox. “R makes it easy to read data, generate lines and points, and place them where you want them. It’s very flexible and super quick. When you’ve only got two or three hours until deadline, R can be brilliant.” When Michael Jackson died in 2009, the Times quickly prepared a graphic timeline showing how the artist’s songs had performed on the Billboard Hot 100 chart from 1971 to the present. It would have been difficult or impossible to prepare a similar chart on deadline using other analytic techniques. Peter Aldhous, the San Francisco bureau chief of New Scientist magazine, has used R to generate information that is subsequently used by graphic designers to create some of the charts that illustrate his articles. But he also uses R to generate simple plots that allow him to perceive quickly what’s really going on underneath the data he collects. For a journalist, the ability to draw quick insights from data is absolutely invaluable. “I’ve got a Ph.D. in animal behavior, so I have some statistical training,” says Aldhous. “R is great for doing exploratory work that gives me an idea of what the distributions look like. I’ve found it incredibly useful for processing data quickly.” Recently, Aldhous investigated complaints about certain academic papers on stem cell research being subjected to “obstructive” reviews, resulting in delays or spurious rejections by peer-reviewed journals. Using an R package to generate a quick series of box plots and scatter plots, he saw that papers from scientists outside the US seemed to take longer to get accepted and published. He was then able to follow up and analyze the data in R, using the most appropriate statistical and graphical methods: Cox proportional hazards regression and Kaplan-Meier curves. In an article headlined “Hey, Green Spender,” Aldhous and colleague Phil McKenna examined the gap between consumer perception and environmental realities across multiple industries such as retail, media, travel and leisure, food and beverages, technology, construction and chemicals. When the data was plotted, the differences between the perceptions and the realities were immediately visible – and the reporters knew they were on the right track.  “It’s not just about producing graphics for publication,” Aldhous explains. “It’s about playing around and making a bunch of graphics that help you explore your data. This kind of graphical analysis is a really useful way to help you understand what you’re dealing with, because if you can’t see it, you can’t really understand it. But when you start graphing it out, you can really see what you’ve got.” R makes it possible for people who aren’t professional analysts to create high quality charts and graphics such as maps, 3-D surfaces, image plots, scatter plots, histograms, bar plots and pie charts.  “R is far from easy when you first encounter it, especially if you’re not a programmer who is used to working in the command line. It has a very steep initial learning curve,” says Aldhous. “But once you get to grips with its conventions and quirks, and if you study the documentation, then it becomes easy to plug different variables into the same code to create a series of related graphics.” When Aldhous hit a snag or got in over his head, he reached out to the R community for help.  “The community is delightful and incredibly helpful,” he says. “I could not have done all of this without expert help.”  Continued Thursdays 	 0 Comments
Lattice vs. ggplot2	https://www.r-bloggers.com/2010/10/lattice-vs-ggplot2/	October 28, 2010	Shige		 0 Comments
Random generators for parallel processing	https://www.r-bloggers.com/2010/10/random-generators-for-parallel-processing/	October 28, 2010	xi'an	Given the growing interest in parallel processing through GPUs or multiple processors, there is a clear need for a proper use of (uniform) random number generators in this environment. We were discussing the issue yesterday with Jean-Michel Marin and briefly looked at a few solutions: Obviously, we did not do any serious search in the recent literature and there are many other approaches to be found in the computer literature, including the scalable parallel random number generation (SPRNG) packages of Michael Mascagni (including an R version) and Matsumoto’s dynamic creator, a C program that provides starting values for independent streams when using the Mersenne twister. 	 0 Comments
Google Tech Talk on Integrating R and C++: video and slides	https://www.r-bloggers.com/2010/10/google-tech-talk-on-integrating-r-and-c-video-and-slides/	October 27, 2010	Thinking inside the box	"
A video recording of
our ninety-minute talk is already available via
the YouTube channel for Google Tech Talks.
The (large) pdf with slides (which
Romain had
already posted on slideshare) is also available from my
presentations page.
 "	 0 Comments
R Cookbook	https://www.r-bloggers.com/2010/10/r-cookbook/	October 27, 2010	David Smith	Following up on the successful “R in a Nutshell”, O'Reilly has just published a new book on R, The R Cookbook, by Paul Teetor. Here's the description: Perform data analysis with R quickly and efficiently using the task-oriented recipes in this cookbook. The R language and environment include everything necessary to perform statistical work right out of the box, but its structure can often be difficult for beginners and experienced computer geeks alike. R Cookbook offers a collection of concise recipes that will help you be productive with R immediately. I haven't taken a look at it yet, but it's an interesting concept, and the table of contents looks like it contains useful, practical recipes for common R tasks. I'll report back when I've taken a more detailed look, but in the meantime, I'd love to hear comments from anyone that's found this book useful. Safari Books Online: R Cookbook     	 0 Comments
"InfoWorld: R a programming language ""on the rise"""	https://www.r-bloggers.com/2010/10/infoworld-r-a-programming-language-on-the-rise/	October 27, 2010	David Smith	In an article looking at once-niche programming languages that are now being deployed in businesses, R is named as one of 7 programming languages on the rise: R is another Swiss Army Knife of numerical and statistical routines for hacking through the big data sets — collections big enough that it might be better called a Swiss Army Machete. Lou Bajuk-Yorgan, senior director of product management for Tibco's Spotfire S-Plus, says its software is used by a number of clients who are studying how business or engineering projects might work or why they fail to work. Analyzing weather patterns to find the best places to build wind-powered generators is one example. The other languages names are Python, Ruby on Rails, Matlab, JavaScript, Erlang, Cobol(!) and CUDA. InfoWorld: 7 Programming Languages on the Rise 	 0 Comments
A million ? what are the odds…	https://www.r-bloggers.com/2010/10/a-million-what-are-the-odds/	October 27, 2010	arthur charpentier	50 days ago, I published a post, here, on forecasting techniques. I was wondering what could be the probability to have, by the end of this year, one million pages viewed (from Google Analytics) on this blog. Well, initially, it was on my blog at the Université de Rennes 1 (http://blogperso.univ-rennes1.fr/arthur.charpentier/), but since I transfered the blog, I had to revise my code. Initially, I had that kind of graphs,       	 0 Comments
R references for handling Big data	https://www.r-bloggers.com/2010/10/r-references-for-handling-big-data/	October 27, 2010	Larry D'Agostino		 0 Comments
Where People Share Links About NYC	https://www.r-bloggers.com/2010/10/where-people-share-links-about-nyc/	October 27, 2010	Drew Conway	"Last week I participated in bit.ly’s fourth hackabit hack-a-thon, which is a wonderful opportunity for NYC area hackers to get together, eat pizza, drink energy drinks, and stay up late hacking with some of the best data geeks around.  I was lucky enough to saddle up next to Hilary Mason, bit.ly’s lead scientist, recently named one of New York’s 100 Coolest Tech People by Business Insider and all-around awesome hacker.  We thought it would be fun to plot the locations from which people were sharing links about New York City. With Hilary’s exclusive access to bit.ly’s massive data set, she scraped a random sample of shared links about NYC and we parsed the geo-location data; she in Python while me in R—just for fun.  With the latitude and longitude data in hand, I set off to generate some visualizations of the where the shared links were coming from.  Below are three of those visualizations with increasing focus on New York City. 
  As should be no surprise, this small sample shows that New Yorkers share the most links about New York, and I suspect that a larger sample would only further reinforce this.  Plotted points are sized by the relative number of links coming from that location; with a modal frequency of one shared link per lat/long. Interestingly, it appears that that people in the San Francisco Bay Area and the Washington, DC metropolitan area are the next most frequent sharers.  All that talk about how the New York tech scene will never be as good as the Bay Area’s, but yet it seems many people in that area find our city interesting enough to share links about it.  Here I have zoomed in further on the Northeastern/Tri-state area of the U.S. to bring New York City further into focus.  It was fun playing with different map projections to literally bend the world around NYC to make it the center of this map’s universe.  Using ggplot2 and the coord_map grammar set options to projections=""lambert"", lat0=-75, lat1=-72 for this New York-centric perspective.  At further magnification we see that it appears no links are being shared from the island of Manhattan.  While this is quite possible, it is also likely due to a lack of accuracy in the geo-location. A quick and dirty data visualization project is the perfect thing to dig into at these hack-a-thons, and I encourage others to come to future events.  I am sure that bit.ly will host a hackabit five, but in the meantime check out the Great Urban Hack, happening on November 6th.  Hope to see you there! "	 0 Comments
A use for tables (really)	https://www.r-bloggers.com/2010/10/a-use-for-tables-really/	October 27, 2010	Andrew Gelman		 0 Comments
Bayesian Model Averaging (BMA) with uncertain Spatial Effects	https://www.r-bloggers.com/2010/10/bayesian-model-averaging-bma-with-uncertain-spatial-effects/	October 27, 2010	BMS Add-ons » BMS Add-ons	" This file illustrates the computer code to use spatial filtering in the context of Bayesian Model Averaging (BMA). For more details and in case you use the code please cite Crespo Cuaresma and Feldkircher (2010).
In addition, this tutorial exists as well in PDF form: web_tutorial_spatfilt.pdf To get the code started you need to install the R-packages spdep and BMS (Version ≥ 0.2.5). Then install the add-on package spatBMS by downloading one of the following binaries according to your system: For further information on installing spatBMS manually, consult the corresponding page on installing BMS.
The following text has been tested with R.2.11. Consider a cross-sectional  regression of the following form: where y is an N-dimensional column vector of the dependent variable,  α is the intercept term, ιN is an N-dimensional
 column vector of ones, Xk = (x1,…,xk) is a matrix whose columns are stacked data for k 
 explanatory variables and   χk = (χ1,…,χk)’  is the k-dimensional parameter vector corresponding to
  the variables in Xk. The spatial autocorrelation structure is specified via a spatial weight matrix W. The coefficient ρ attached to W reflects the degree of spatial autocorrelation. 

 Equation (1) constitutes a parametric spatial model where the spatial parameter ρ is often interpreted as a spillover parameter.  In this setting, on top of the uncertainty regarding the choice of explanatory  an extra degree of uncertainty arises:
 we do not know the actual nature of the spatial interactions which we model through the spatial autoregressive term in equation (1),
 that is, if we conduct inference conditional on W.  Spatial autocorrelation will be observable whenever the phenomenon under study is 
a spatial process or omitted variables cause spatial variation in the residuals (Tiefelsdorf and Griffith, 2007). Note that both
arguments typically apply to economic cross-section data, where economic units interact with each other 
and omitted variables decrease the level of confidence in econometric analysis. Since inference from the SAR model is conditional on the weight matrix W, which has to be exogenously specified, explicitly accounting for this source of model uncertainty is a natural generalization to uncertainty in the nature of Xk in the framework of BMA. In most applications there is little theoretical guidance on which structure to put on the weight matrix rendering its specification a serious challenge. The spatial filtering literature seeks to remove residual spatial autocorrelation patterns prior to estimation and is in principle
  not interested in directly estimating ρ in equation (1). The approach put forward by Getis and Griffith (2002) and
  Tiefelsdorf and Griffith (2007), is based on an eigenvector decomposition of a transformed W matrix, where the transformation
  depends on the underlying spatial model. The eigenvectors ei are included as additional explanatory variables and the regression equation
  (1) becomes: where each eigenvector ei spans one of the spatial dimensions. By introducing the eigenvectors into the regression,
  we explicitly take care of (remaining) spatial patterns in the residuals. Furthermore spatial commonalities among
  the covariates in Xk are conditioned out. This reduces the degree of multicollinearity and further separates spatial effects
  from the ‘intrinsic’ impact the employed regressors exert on the dependent variable. From a Bayesian perspective, the problem of obtaining estimates of the parameter associated with a covariate under uncertainty in both the nature of W
 and Xk
  can be handled in a straightforward manner using spatial filtering techniques. Let us assume that we are interested in a particular regression coefficient, β. 
  Denote the set of potential models    by   M={M11, M12,…, M12^K, … M21,
…, M22^K,…, MZ1,…, MZ2^K },
  where K stands for the number of potential explanatory variables and Z the number of candidate spatial weighting matrices
  Wz, z=1,…, Z each with associated set of eigenvectors Ez. The cardinality of M is therefore 2K*Z.
  A particular model, say Mzk, is characterized by its parameter vector θzk=(α,  χk, ηz)
  corresponding to the intercept term included in all models, the coefficients on the regressors entering the model and the coefficients on the set of
   eigenvectors Ez related to Wz.
  In the BMA framework, the posterior distribution  of β takes now the form of with y denoting the data  and β  the coefficient of interest.  Inference on  β is  based on single inferences under models j=1,…,2(Z*K) weighted by their respective posterior model probabilities, p(Mzj| y), which in turn depend on the corresponding matrix of spatial weights. For more technical details please see Crespo Cuaresma and Feldkircher (2010). In R we will do spatial filtering with the ‘Boston housing data’ which has been originally published in Harrison and Rubinfeld (1978). The dependent variable (CMEDV) contains the
	corrected median value of owner-occupied homes in USD 1000’s for 506 observations. Among the explanatory variables we have per capita crime (CRIM), the pupil-teacher ratio by town 
(PTRATIO),  the proportion of owner-occupied units built prior to 1940 (AGE), the proportion of non-retail business acres per town (INDUS), a variable that is proportional to
the share of Afro Americans per town (B) and a variable reflecting the nitric oxides concentration (NOX) among others. For more details please see ?dataBoston. We start
with loading the data in R : As in the  earlier analyses of these data, we take logarithms of the variables CMEDV, DIS, RAD and LSTAT and squares of the regressors RM (RM#RM) 
and  NOX (NOX#NOX) to model potential non-linearities. These transformations have been already carried out and the transformed variables stored in dataBoston We proceed with the construction of several weight matrices. To keep the example simple, we limit the space of candidate W matrices to five: The first matrix is included in the spdep package and constitutes probably a first order contiguity matrix. This class of matrices assigns positive (identical) weights to observations 
 that share a common border. See Anselin (1988) and Crespo Cuaresma and Feldkircher (2010) for an interpretation of different coding schemes. 
 For the sake of illustration we will randomly perturb the matrix using the function jitterW_binary which is provided in the corresponding Sweave file to this pdf. The perturbation randomly adds / drops N=5 neighborhood relationships of the boston.soi weight matrix.           
The function takes as argument a matrix object, thus we have to transform boston.soi with the function nb2mat into a matrix. It is necessary to  re-transform the perturbed matrices into the nb class since the SpatialFiltering function we will use later on
only allows for objects of this class as inputs. This can be done with 
the function mat2listw and extracting the neighborhood object by typing object$neighbours (in the example W2$neighbours). Finally we set up two k-nearest neighbor matrices. As stated above we assume the model follows a SAR process. To free the residuals from spatial correlation  we now filter the dependent variable. The SpatialFiltering function provided in the package  spdep will  extract the eigenvectors. A linear combination of these eigenvectors will allow us to separate spatial correlation from the dependent variable by using the identified eigenvectors as additional regressors in our econometric model. Depending on the size of your W matrix,  spatial filtering can take some while. The function takes  the neighborhood objects we have defined above and the data to be filtered as main arguments.
For more details see ?SpatialFiltering . Note also that we have set ExactEV to FALSE (quicker) which provides an approximation
for our illustration example. Finally we collect the eigenvectors in a list. Note that you can also access the extracted eigenvectors by typing e.g. yFilt.colGal0$dataset instead of 
e.g. fitted(yFilt.colGal0). Now we can start with the BMA part. All functions and input arguments of the BMS library are applicable. 
There are two important exceptions though: First, the empirical Bayes estimation as well as the hyper-g priors are so far not implemented. Thus you can specify the g-prior either by using the benchmark priors (Fernandez et al. 2001) or by any numerical number g>0. See Feldkircher and Zeugner (2009) for more details on the influence of g on posterior results. Secondly, the enumeration algorithm (mcmc=enum) is in its current version not available for spatFilt.bms.  Finally, to speed up
calculations BMS provides the option force.full.ols=FALSE, which is not available in spatFilt.bms. The additional argument WList of the function spatFilt.bms  must be a list  object with length corresponding to the number of weight matrices you use 
length(WList) must be greater than 1, that is you have to submit at least two sets of eigenvectors in order to use spatial filtering in the context of BMA.
Each element of the list contains a matrix with the extracted eigenvectors, where the matrices do not have to have the same column dimension.  In the example 
we have collected the eigenvectors  in the object WL.boston. To have a quick look at the boston data set we run a short BMA chain with 1 million posterior draws
(iter=1e06) after discarding the first 100,000 draws (burn=1e05). For more information regarding the other function arguments type ?spatFilt.bms. The object model1 is a standard BMS object. It is important, to note that aall the reported statistics (Posterior Inclusion Probabilities, Posterior Means, Posterior
Standard deviations, etc.) are  after having integrated out uncertainty with respect to W.  To fix ideas, we will look at the disaggregated results to - for example - assess whether avariable receives only posterior support under a particular weight matrix or to look at theposterior inclusion probabilites of the spatial weight matrices, first: In the example we show posterior results for the first 5 models. The line W-Index tells you which W has been included in the particular model. In our example
the W-Index indicates that the first weight matrix in WL.boston (i.e. W0='boston.soi') has been used in the first 5 regression models. On the contrary: 
shows the aggregated results: a matrix of containing the best models along with the according posterior model probabilities (exact and frequencies) after integrating out uncertainty with respect to the weight matrices. That is, if the first two models would be the same in terms of explanatory variables but differ regarding the employed weight matrix, the posterior mass of the two models is aggregated. 
In the same vein, the posterior inclusion probability (PIP) for the variable RM for example corresponds to the sum of the posterior model probabilities of all regression models including that variable and the posterior mean is calculated as the weighted (by posterior model probabilities) average of posterior means over all weight matrices. Also note that the prior over the W space is uniform. 
Coming back to the disaggregated (W-specific) results. To calculate the posterior inclusion
probabilities of the W matrices, you can look a the frequency with which each W matrix
has been visited by the sampler and express this in percentages: In the example, the original 'boston.soi' W matrix along with its perturbations receives
overwhelming posterior support, whereas the k-nn matrices cannot explain the spatial patterns present in the data. If you prefer statistics based on the best 
models (in the example we have set nmodel=100 meaning that results are based on a maximum of 100 models receiving highest posterior support in terms of posterior
model probabilities), you can use the function pmpW.bma: Usually model1$Wcount gives you a reasonable approximation and is directly accessible from  the model1 object. We finally want to check whether there is remaining spatial autocorrelation present in the residuals. For that purpose we can use the 'Moran's I' test calling 
lm.morantest for the best models.  Although - in order to save time - we could have a look at the 10 best models only (in the example the first 10 models
 already account for more than 80% of posterior mass) we carry out the residual test for all  models in order to get more accurate results.
 The function mTest is wrapper for the  function lm.morantest provided in the package spdep. The  loop re-computes the nmodel best - 
 in terms of posterior model probabilities - regressions  in order to get an object of class lm and finally applies the lm.morantest. We do this once for the eigenvector augmented regressions (the spatial filtering approach) and once in a pure OLS fashion by setting the 
option variants=""double"": This allows us for a direct comparison of a non-spatial regression approach and the spatial filtering BMA approach pursued in this article.
If the non-spatial  linear regression models do not show any patterns of spatial residual autocorrelation a standard BMA regression
 - as with the function bms - dealing solely with uncertainty with respect to the explanatory variables might be preferable. We now  extract the corresponding p-values (remember the null hypothesis corresponds to no spatial autocorrelation): Figure 1 shows the distribution of the p-values of the 'Moran's I' test, once for pure OLS regressions (without any spatial correction, left panel) and once augmented with the eigenvectors identified
by the spatial filtering algorithm (right panel). To sum up  by incorporating the eigenvectors we successfully removed spatial residual autocorrelation from the regressions. The BMA framework helped us to explicitely deal
with uncertainty stemming from the construction of the weight matrices and allowed us to get posterior inclusion probabilities of weighting matrices as well as  the usual
 BMA statistics dealing with spatial correlation.
Finally, once again please note that all functions of the BMA package BMS are fully applicable to its spatial filtering
variant. The remainder of this illustration shows posterior plots to assess convergence of the MCMC sampler, the posterior distribution of coefficients of interest, 
an image plot as well as a plot of the posterior model size. You can also type
spatBMS.demo to get a short demonstration of spatFilt.bms's main functions.

 "	 0 Comments
The class of Donald Bradman	https://www.r-bloggers.com/2010/10/the-class-of-donald-bradman/	October 26, 2010	prasoonsharma		 0 Comments
Handling Large Datasets in R	https://www.r-bloggers.com/2010/10/handling-large-datasets-in-r/	October 26, 2010	Quantitative Finance Collector		 0 Comments
"DEADLINE EXTENDED — CFP: Special Issue in JSS for ""Magnetic Resonance Imaging in R"""	https://www.r-bloggers.com/2010/10/deadline-extended-cfp-special-issue-in-jss-for-magnetic-resonance-imaging-in-r/	October 26, 2010	Brandon Whitcher		 0 Comments
R nominated for best open-source project in New Zealand	https://www.r-bloggers.com/2010/10/r-nominated-for-best-open-source-project-in-new-zealand/	October 26, 2010	David Smith	The R project, born in New Zealand in 1993, has been nominated as the best open-source project in the New Zealand Open-Source Awards 2010. R's co-creator Ross Ihaka talks about the project in this article by the New Zealand Herald: Ross Ihaka from the University of Auckland started developing R 20 years ago, but it took off about a decade ago as the internet picked up speed. He said the university wanted to commercialise it. “We got as far as buying a book on business and we could have gone that way and there would have been half a dozen people in Auckland using the software. Now there are thousands,” Ihaka says. He says open sourcing it means some of the best brains on the planet help to maintain and develop it. “These guys are at a level where money is not a motivation.” Read more at the link below and cast your vote for R (if you're in NZ, anyway – the voting page is blocked for me here in the US). New Zealand Herald: Annual awards source of pride 	 0 Comments
Example 8.11: violin plots	https://www.r-bloggers.com/2010/10/example-8-11-violin-plots/	October 26, 2010	Nick Horton		 0 Comments
Upcoming R courses from Statistics.com	https://www.r-bloggers.com/2010/10/upcoming-r-courses-from-statistics-com/	October 26, 2010	David Smith	The online training provider Statistics.com has three great courses based on R coming up in the next few months: Nov. 5 – Dec. 3: “Graphics in R,” with Paul MurrellNov. 20 – Dec. 18:  Support Vector Machines in R” with Dr. Lutz HamelDec. 17 – Jan. 22: “Geostatistics in R” with Prof. David Unwin The courses take place online at statistics.com in a series of 4 weekly lessons and assignments, and requires about 15 hours/week. You participate at your own convenience; there are no set times when you are required to be online. More details about these courses appears after the jump, or click the links above to register. Nov. 5 – Dec. 3: “Graphics in R,” with Paul Murrell“Graphics in R,” teaches you how to produce publication-quality statistical plots of data using R. It will cover plots such as scatterplots, bar plots, histograms, boxplots and Trellis plots.  It will review the underlying model used to produce plots in R so that you can extensively customize these plots.  Finally, the course will introduce the grid graphics system and look at producing unique plots from the ground up using basic components.Dr. Paul Murrell, instructor for this course, is a Senior Lecturer in the Department of Statistics at the University of Auckland, New Zealand. Paul has been a member of the core development team for R since 1999, with a focus on the graphics system in R. He is the past Chair of the Section for Statistical Graphics of the American Statistical Association.  He has recently served as Editor-in-Chief of “R News”, the newsletter of the R project, and is an Associate Editor for “Computational Statistics” and “The Journal of Statistical Software”.  Participants can ask questions and exchange comments with Dr. Murrell via a private discussion board throughout the period.Details and registration: http://www.statistics.com/ourcourses/graphicsR/   Nov. 20 – Dec. 18:  Support Vector Machines in R” with Dr. Lutz HamelSupport vector machines (SVMs) have established themselves as one of the preeminent machine learning models for classification and regression over the past decade or so, frequently outperforming artificial neural networks in task such as text mining and bioinformatics.  Dr. Lutz Hamel, author of “Knowledge Discovery with Support Vector Machines” from Wiley will present his online course “Introduction to Support Vector Machines In R” Nov. 20 – Dec. 18 at statistics.com.“Support Vector Machines in R” will give you an understanding on what is going on “under the hood” when using SVMs. After completing this course, you will be able to interpret the performance of SVM models and make appropriate choices for model parameters during the model evaluation and selection cycle. You will understand the difference between linear, polynomial, and gaussian kernels and know how to tune their parameters. In addition, you will have a deep understanding on how the cost constant “C” affects the quality of your models.Dr. Lutz Hamel teaches at the University of Rhode Island and was the founder of the machine learning and data mining group there. He is the author of Knowledge Discovery with Support Vector Machines(the course text). Before becoming an academic, Dr. Hamel was Director of Software Development at Thinking Machine Corporation, and Vice President of R&D for Bluestreak, where he oversaw the development of advanced technologies for online ad delivery and optimization, and directed the building of a next generation data warehouse-driven system for campaign analysis and design tools.Participants can ask questions and exchange comments with Dr. Hamel via a private discussion board throughout the course.Details and registration: http://www.statistics.com/ourcourses/SVM/   Dec. 17 – Jan. 22: “Geostatistics in R” with Prof. David Unwin This course will teach users how to implement spatial statistical analysis procedures using R software. Topics covered include point pattern analysis, identifying clusters, measures of spatial association, geographically weighted regression and surface procession. Dr. David Unwin is Emeritus Chair of Geography at Birkbeck College, University of London, and also a Visiting Professor in the Department of Geomatic Engineering at University College, also in the University of London. His work using and developing spatial statistics in research stretches back some 40 years, and he has authored over a hundred academic papers in the field, together with a series of texts, of which the most recent are his Geographic Information Analysis, 2nd edition (with D. O'Sullivan, 2010) and a series of edited collections at the interface between geography and computer science in “Visualization in GIS” (Hearnshaw and Unwin, 1994), “Spatial Analytical Perspectives on GIS” (Fischer, Scholten and Unwin, 1996), Virtual Reality in Geography (Fisher and Unwin, 2002) and, most recently representation issues in “Re-presenting GIS” (Fisher and Unwin, 2005). Having developed the world's first wholly internet-delivered Master's program in GIS in 1998, David Unwin has considerable experience of teaching and tutoring online.Details and registration: http://www.statistics.com/ourcourses/GeostatsinR/ 	 0 Comments
Different results from different software	https://www.r-bloggers.com/2010/10/different-results-from-different-software/	October 26, 2010	Rob J Hyndman	I’ve had a few questions on this topic lately. Here is an email received today: I use Eviews to estimate time series, but I have been checking out R recently, and your Forecast package. I cannot understand why 2 similar equations in Eviews and R are giving different estimated output. Your insights will be invaluable for my work. The equations are: Even with include.drift=FALSE in R or without c in Eviews they give different output. Much appreciated if you could share your view. There are several issues here. The seasonal order needs to be specified in the sar term. I’ve always thought this was a bizarre choice of syntax because it is so easy to make mistakes. Here the differencing is explicit. Provided the model is correctly specified, none of this should make too much difference to the forecasts obtained, but be prepared for some variations from different packages, and even from different versions of the same package. 	 0 Comments
R User Groups 2010-10-25 21:14:50	https://www.r-bloggers.com/2010/10/r-user-groups-2010-10-25-211450/	October 25, 2010	Szilard	Videos from the October meeting “Text Mining with R” of the Los Angeles R users group: Rob Zinkov, “Text Mining with R”:  Ryan Rosario, “Accessing R from Python using RPy2″:  	 0 Comments
Algorithmic Trading with IBrokers	https://www.r-bloggers.com/2010/10/algorithmic-trading-with-ibrokers/	October 25, 2010	Joshua Ulrich	"
 "	 0 Comments
The language of Statistics	https://www.r-bloggers.com/2010/10/the-language-of-statistics/	October 25, 2010	David Smith	R is the lingua franca of Statistics: R code and R packages is the means by which statisticians communicate ideas and methods for statistical analysis. The reasons why are discussed in this article, but it also begs the question: what's wrong with the spoken or written word? How Statistics and Probability relate to the English language is the subject of a great op-ed article in last week's New York Times, Stories vs Statistics. In particular, it talks about how some of the famous thought experiments and puzzles in statistics and probability (such as the Monty Hall problem, the Birthday paradox, and the Two Boys problem) are heavily dependent on language: the solution depends not only on how the problem is described, but on how the reader interprets the story around the puzzle. It's a great read: check out the link below. New York Times: Stories vs Statistics 	 0 Comments
R API to Interactive Brokers Trader Workstation	https://www.r-bloggers.com/2010/10/r-api-to-interactive-brokers-trader-workstation/	October 25, 2010	Quantitative Finance Collector		 0 Comments
Parametric Bootstrap Power Analysis of GISS Temp Data	https://www.r-bloggers.com/2010/10/parametric-bootstrap-power-analysis-of-giss-temp-data/	October 24, 2010	apeescape	Previosly, I calculated a bunch of ad-hoc power curves from GISTEMP data. Power is essentially a reframing of the p-value, to see the significance of the trend lines in the global temps. However, power calculations are inherently very noisy, hence, my ad-hoc way of aggregating the data. Another method is to bootstrap through the responses from the linear model to arrive at a distribution of power at each start year (data is from start year to end year, which is 2009). The simulate function in R essentially creates data that is randomized according to the initial linear model fit to the data. I fitted the simulated data with another simple linear model, and I’ve plotted it below. The yellow dots are the original power calculations (w/ assumptions same as previous article), and the red histograms come from the simulations.  As we can see, there is a lot of uncertainty in the power calculations, unsurprisingly for low power values. Power is bounded below by the Type I error rate (0.05), and even when the data shows this low power, the simulations leaped over the satisfactory mark (power of 0.8) on rare occasion. Just adding one data point changes the conclusions significantly (say from 97 to 96). We can sort of understand why power calculations are uncertain. Given the alternative hypothesis is true, power is the probability the data is showing an extreme result for the parameter. Especially for moderate power, small changes in the data (hence, estimated parameter) covers the “heaviest” part of the alternative distribution (see this picture). This mean that the area under the curve is sensitive for moderate power. Arguably, the amount of data in which you get moderate power is most important in science. Alternatively, we can simply look at the confidence intervals of the interested parameter.  For the question, “is global warming happening now? (2009),” the latter graph gives a more robust result. One might answer, “yes, since 1999″ when looking at confidence intervals, but “yes, since 1995″ when looking at power — a significant difference. Gerald et al. (1998) argues that retrospective power calculations (like I did here) are not useful because of its inherent instability. OTOH, prospective power calculations are useful to determine proper sample size for a study. Refs: 	 0 Comments
Accessing R from Python using RPy2	https://www.r-bloggers.com/2010/10/accessing-r-from-python-using-rpy2/	October 24, 2010	Ryan	"This past Tuesday I had the opportunity to present a short talk (a bit long) related to text mining at the Los Angeles R Users’ Group. Since I do most of my text mining in Python, I took this opportunity to discuss RPy2, an interface to R from Python.    My slides are below:
   Code for demonstration is here:
 Video:
   To see the main talk of the evening, click here.  Some Recommended Books

Natural Language Processing
 Text Mining Data Mining   Web Mining "	 0 Comments
Programming with R – Checking Function Arguments	https://www.r-bloggers.com/2010/10/programming-with-r-%e2%80%93-checking-function-arguments/	October 24, 2010	Ralph	In a previous post we considered writing a simple function to calculate the volume of a cylinder by specifying the height and radius of the cylinder. The function did not have any checking of the validity of the function arguments which we will consider in this post. R has various functions that we can use to test certain conditions in our function. These include the functions stop, warning and conditional statements such as if statements combined with stop or warning. As an example consider extending the function to calculate volumes to test whether either the height or radius has not been submitted when the function is called. We will make use of the missing function that tests whether a specific argument has been provided with the function call. The new function is: We use the if statement to test whether each of the arguments is missing and when they are the function is stopped and an error message is written to the console. Here are a couple of examples of the function being halted when insufficient information is provided: So this handles one particular type of problem with the function but there are other checks that we might want to make. For example, negative values for the height or radius are not sensible and should also lead to an error. We can check this condition using an if statement in the function: An example of the function in action: These are a couple of basic examples of validation that we can include in our function that will hopefully allow us to catch erratic behaviour in software which is more of an issue as programs get larger and more complicated. 	 0 Comments
Generate your own Risk Characterization Theatre	https://www.r-bloggers.com/2010/10/generate-your-own-risk-characterization-theatre/	October 24, 2010	Stubborn Mule	 In the recent posts Visualizing Smoking Risk and Shades of grey I wrote about the use of “Risk Characterization Theatres” (RCTs) to communicate probabilities. I found the idea in the book The Illusion of Certainty, by Eric Rifkin and Edward Bouwer. Here is how they explain the RCTs: Most of us are familiar with the crowd in a typical theater as a graphic illustration of a population grouping. It occurred to us that a theater seating chart would be useful for illustrating health benefit and risk information. With a seating capacity of 1,000, our Risk Characterization Theater (RCT) makes it easy to illustrate a number of important values: the number of individuals who would benefit from screening tests, the number of individuals contracting a disease due to a specific cause (e.g., HIV and AIDS), and the merits of published risk factors (e.g., elevated cholesterol, exposure to low levels of environmental contaminants). As regular readers would know, most of the charts here on the blog are produced using the statistics and graphics tool called R. The RCT graphics were no exception. Writing the code involved painstakingly reproducing Rifkin and Bouwer’s theatre floor plan (as well as a few of my own design, including the stadium). For the benefit of anyone who would like to try generating their own RCTs, I have published the code on github.  Using the code is straightforward (once you have installed R). Copy the two files plans.Rdata and RCT.R onto your computer. Fire up R and switch to the directory containing the downloaded files. Load the code using the following command: You will then have a function available called rct which will generate the RCTs. Try the following examples: The rct function has quite a few optional parameters to tweak the appearance of the theatre: rct(cases, type=”square”, border=”grey”, fill=NULL, xlab=NULL, ylab=””, lab.cex=1, seed=NULL, label=FALSE, lab.col=”grey”, draw.plot=TRUE)  	 0 Comments
Grabbing Tables in Webpages Using the XML Package	https://www.r-bloggers.com/2010/10/grabbing-tables-in-webpages-using-the-xml-package/	October 24, 2010	Yihui Xie	Tables are pretty common in web pages as data sources, and the most direct way to get these data is probably to copy and paste. This is OK if there are only two or three tables, and when we need to grab 5000 tables in 1000 web pages, we may not really wish to fulfill the task by hand. That is one of the reasons for why we need programming — we want to be as lazy as possible. Who is willing to spend 2 hours copying and pasting? Just let the computers do the tedious job and we can watch movies. The R package XML is a handy tool to deal with web pages (both XML or HTML). I’m actually a big fan of its author, Duncan Temple Lang, who did a lot of work on the infrastructure of statistical computing (see the Omegahat project). Next I use the Stat579 homework of week 8 as an example to show how to read tables from web pages directly using R, i.e. no Excel, no Word, no copy & paste. The task is to grab 3 data tables from the web pages for 3 states, clean the data and do some graphics. Specifically, I’ll take the page for Iowa as an example. See the R code below: The most important function is readHTMLTable(), which is a convenient wrapper to parse an HTML page and retrieve the table elements. The rest of work is simply to figure out which table we need. Then we have to remove some characters which are not numbers. This is done by a regular expression [^0-9] in which ^ means matching any characters other than the following ones (in this case, they are digits from 0 to 9). It is easy to extend this script to reading other web pages too — just change the URL (e.g. using a loop). To assist understanding the code above, I put some intermediate results below: 	 0 Comments
how to speak ggplot2 like a native, and Predictive Analytics World	https://www.r-bloggers.com/2010/10/how-to-speak-ggplot2-like-a-native-and-predictive-analytics-world/	October 24, 2010	Harlan	I was recently given the opportunity to re-present my ggplot2 talk, which I originally gave to the NYC R Meetup, to the DC R Meetup group. The Meetup was held co-located with the Predictive Analytics World conference in Alexandria, VA. (More on my thoughts on PAW below…) Contentwise, I made only small changes, changing a bit of patter and adding more examples at the end. I still love ggplot, with some frustration at the way it is typically introduced. Some of the audience had no R experience at all, while others were experts. One person, a grad student at U. of Maryland, had had very similar difficulty as I had when originally learning ggplot2, and his enthusiastic nods during my presentation were very validating! For reference, the Meetup page is here, and I stuck the current version of the slides in a public Dropbox, located here. And a few thoughts about PAW. The conference was well-run (although I have my gripes with the hotel and its location!) and there were an interesting and eclectic lineup of speakers, from a variety of industries. Compared to academic conferences I’ve attended, I missed having all the grad students around. At PAW, I felt rather young, which had not been true at academic conferences in quite a long time! The content of the conference focused on people using predictive methods (statistics, data mining, machine learning) at the individual-customer level, for marketing or retainment or other purposes. That’s not my primary interest right now — my work is focused at a slightly higher operations-research-y level, trying to make sure that customers in the aggregate have good options. But I enjoyed learning about what other people are doing using somewhat similar methods. Next year, though, I think I’ll try to go to a different conference, perhaps UseR! in the UK, or INFORMS’ applied conference… 	 0 Comments
Le Monde puzzle [42]	https://www.r-bloggers.com/2010/10/le-monde-puzzle-42/	October 24, 2010	xi'an	"An interesting suduko-like puzzle for this week puzzle in Le Monde thi A 10×10 grid is filled by a random permutation of {0,…,99}. The 4 largest figures in each row are coloured in yellow and the 4 largest values in each column are coloured in red. What is the range of the number of yellow-and-red figures? Here is a random grid > lascases=matrix(sample(0:99),10,10)
> lascases
 [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10]
 [1,]   14   40   33   80   51   55   32   52   43    49
 [2,]   56    8   19   87   13   99   98   70   29    61
 [3,]   63   11    3   35   90   25   50    7   74    39
 [4,]    4   67   18   68   53   22   96   84   81    65
 [5,]   48   91    1   17   71   89   38   73    6    64
 [6,]   69   78   23   47   72   45   10   42   83    36
 [7,]    2   82   54   46   59   20   30   85   41    16
 [8,]   97   28   15   60    5    0   24   62   86    88
 [9,]   31   27   93   77   75   95   92   76   34    12
[0,]   21   94   26   58    9   57   79   37   44    66 whose yellow figures are given by > apply(lascases,1,rank)>6
 [,1]  [,2]  [,3]  [,4]  [,5]  [,6]  [,7]  [,8]  [,9] [,10]
 [1,] FALSE FALSE  TRUE FALSE FALSE  TRUE FALSE  TRUE FALSE FALSE
 [2,] FALSE FALSE FALSE FALSE  TRUE  TRUE  TRUE FALSE FALSE  TRUE
 [3,] FALSE FALSE FALSE FALSE FALSE FALSE  TRUE FALSE  TRUE FALSE
 [4,]  TRUE  TRUE FALSE  TRUE FALSE FALSE FALSE FALSE  TRUE  TRUE
 [5,]  TRUE FALSE  TRUE FALSE  TRUE  TRUE  TRUE FALSE FALSE FALSE
 [6,]  TRUE  TRUE FALSE FALSE  TRUE FALSE FALSE FALSE  TRUE FALSE
 [7,] FALSE  TRUE  TRUE  TRUE FALSE FALSE FALSE FALSE  TRUE  TRUE
 [8,]  TRUE  TRUE FALSE  TRUE  TRUE FALSE  TRUE  TRUE FALSE FALSE
 [9,] FALSE FALSE  TRUE  TRUE FALSE  TRUE FALSE  TRUE FALSE FALSE
[0,] FALSE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE FALSE  TRUE and red figures by > apply(lascases,2,rank)>6
 [,1]  [,2]  [,3]  [,4]  [,5]  [,6]  [,7]  [,8]  [,9] [,10]
 [1,] FALSE FALSE  TRUE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE
 [2,]  TRUE FALSE FALSE  TRUE FALSE  TRUE  TRUE FALSE FALSE FALSE
 [3,]  TRUE FALSE FALSE FALSE  TRUE FALSE FALSE FALSE  TRUE FALSE
 [4,] FALSE FALSE FALSE  TRUE FALSE FALSE  TRUE  TRUE  TRUE  TRUE
 [5,] FALSE  TRUE FALSE FALSE  TRUE  TRUE FALSE  TRUE FALSE  TRUE
 [6,]  TRUE  TRUE FALSE FALSE  TRUE FALSE FALSE FALSE  TRUE FALSE
 [7,] FALSE  TRUE  TRUE FALSE FALSE FALSE FALSE  TRUE FALSE FALSE
 [8,]  TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE  TRUE
 [9,] FALSE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE FALSE
[0,] FALSE  TRUE  TRUE FALSE FALSE  TRUE  TRUE FALSE FALSE  TRUE hence resulting in the yellow-and-red figures > (apply(lascases,1,rank)>6)*(apply(lascases,2,rank)>6)
 [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10]
 [1,]    0    0    1    0    0    0    0    0    0     0
 [2,]    0    0    0    0    0    1    1    0    0     0
 [3,]    0    0    0    0    0    0    0    0    1     0
 [4,]    0    0    0    1    0    0    0    0    1     1
 [5,]    0    0    0    0    1    1    0    0    0     0
 [6,]    1    1    0    0    1    0    0    0    1     0
 [7,]    0    1    1    0    0    0    0    0    0     0
 [8,]    1    0    0    0    0    0    0    0    0     0
 [9,]    0    0    1    1    0    1    0    1    0     0
[0,]    0    0    0    0    0    0    0    0    0     1 Therefore computing the number of yellow-and-red figures is simply done by > sum((apply(lascases,1,rank)>6)*(apply(lascases,2,rank)>6))
 [1] 21 So it is enought to cycle through random allocations to monitor the range of this sum.
miny=99
maxy=0 for (t in 1:10^5){
 lascases=matrix(sample(0:99),10,10)
 res=sum((apply(lascases,1,rank)>6)*(apply(lascases,2,rank)>6))  if (res>maxy) maxy=res
 if (res
 }  With 105 simulations, I find a range of (5,30)… However, if I apply the computation to the matrix filled row-wise by 0:99, the number of yellow-and-red figures is >   lascases=matrix(0:99,10,10)
>   sum((apply(lascases,1,rank)>6)*(apply(lascases,2,rank)>6))
[1] 40 while, if > lascases
 [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10]
 [1,]   90   80   70   60   50   40   30   20   10     0
 [2,]   91   81   71   61   51   41   31   21   11     1
 [3,]   92   82   72   62   52   42   32   22   12     2
 [4,]   93   83   73   63   53   43   33   23   13     3
 [5,]   94   84   74   64   54   44   34   24   14     4
 [6,]   95   85   75   65   55   45   35   25   15     5
 [7,]   96   86   76   66   56   46   36   26   16     6
 [8,]   97   87   77   67   57   47   37   27   17     7
 [9,]   98   88   78   68   58   48   38   28   18     8
[0,]   99   89   79   69   59   49   39   29   19     9
>   sum((apply(lascases,1,rank)>6)*(apply(lascases,2,rank)>6))
[1] 0 so the range is (0,40) since there cannot be more than 4 yellow-and-red figures on each row… Simulation (as done above) is simply too costly to reach the whole range. "	 0 Comments
Reader suggestions on alternative ways to create combination dotplot/boxplot	https://www.r-bloggers.com/2010/10/reader-suggestions-on-alternative-ways-to-create-combination-dotplotboxplot/	October 24, 2010	Nick Horton		 0 Comments
R GUI now offers interactive graphics – Deducer 0.4-2 connects with iplots	https://www.r-bloggers.com/2010/10/r-gui-now-offers-interactive-graphics-%e2%80%93-deducer-0-4-2-connects-with-iplots/	October 24, 2010	Tal Galili	"Earlier today, Ian Fwllows has announced the release of Deducer 0.4-2 and DeducerExtras 1.2 to CRAN (I copy his announcement here):

Deducer 0.4-2 contains a few bug fixes, and an interface to the iplots package. With the new iplots interface it is now possible to do interactive plots with Deducer. An introductory example screen cast (by Ian) is available on the tube:  DeducerExtras 1.2 contains a few new dialogs including ‘load data from package’, and ‘t-test power’. Additionally, a new Windows R/JGR/Deducer installer is available which installs R-2.12.0, JGR with it’s launcher, Deducer, DeducerExtras, and DeducerPlugInScaling. It is available on the Deducer website: http://www.deducer.org/pmwiki/pmwiki.php?n=Main.WindowsInstallation "	 0 Comments
Aquamacs customizations (auctex, ESS)	https://www.r-bloggers.com/2010/10/aquamacs-customizations-auctex-ess/	October 23, 2010	jackman	I gave an informal talk on my Mac based “workflow” at Stanford on Friday.  I talked a lot about Aquamacs as a tool for editing LaTeX (I use MacTeX) and for working with R (thanks auctex and ess, respectively).  Skim also got a mention; I emphasized TeX-PDF synchronization. Some of the students were asking for my Aquamacs customizations.  Here is the current version of ~/Library/Preferences/Aquamacs Emacs/customizations.el from my laptop machine.  There are some definitions that are specific to my machine, but otherwise it is reasonably straightforward. I’d appreciate any discussion on this —things I’m missing, things I’m doing in a very old fashioned way etc. 	 0 Comments
R & Rapidminer tutorial	https://www.r-bloggers.com/2010/10/r-rapidminer-tutorial/	October 23, 2010	a Physicist	  You can see in the following video a simple tutorial of Rapidminer R plugin   via: neuralmarkettrends. 	 0 Comments
Google slides	https://www.r-bloggers.com/2010/10/google-slides/	October 22, 2010	romain francois	Last stop on my World tour was Google headquarters in Mountain View, California, where Dirk and I presented Rcpp, RInside, RProtoBuf, etc … for 90 minutes today. The talk was recorded, and will be broadcasted on youtube at some point. In the meantime, the slides are available here:  	 0 Comments
Bayesian Diabetes Projections by CDC	https://www.r-bloggers.com/2010/10/bayesian-diabetes-projections-by-cdc/	October 22, 2010	Matt Shotwell	"Bayesian methods are supporting decisions and news at the national level! The Centers for Disease Control and Prevention summarizes a report published in the journal Population Health Metrics. The news also made it to the national media. The report (JP Boyle, TJ Thompson, EW Gregg, LE Barker, and DF Williamson (2010) “Projection of the year 2050 burden of diabetes in the US adult population: dynamic modeling of incidence, mortality, and prediabetes prevalence.”Population Health Metrics. 8:29) projects a two fold increase in the annual incidence of diabetes among American adults. The authors project the prevalence of diabetes to increase from 14% to between 25% and 28% by 2050. However, the authors claim that “these projected increases are largely attributable to the aging of the US population, increasing numbers of members of higher-risk minority groups in the population, and people with diabetes living longer.” The authors model the incidence of diabetes  at year  according to the Bayesian nonlinear model:
 "	 0 Comments
Help! My model fits too well!	https://www.r-bloggers.com/2010/10/help-my-model-fits-too-well/	October 22, 2010	Tony	This is sort-of related to my sidelined study of graph algebra.  I was thinking about data I could apply a first-order linear difference model to, and the stock market came to mind.  After all, despite some black swan sized shocks, what better predicts a day’s closing than the previous day’s closing?  So, I hunted down the data and graphed exactly that: Isn’t that just lovely?  The tight clustering around the line indicates that we have found a very good linear fit.  How good?  Well, lets take a peek at our summary(model) Whoa!  An R-squared of .9998.  In other words, my very simple model describes 99.98% of all the variation seen in the Dow Jones industrial Index days-end prices.  Show this to any statistician and they’d say that’s nearly impossible.  You’ve got to have some tautology in the model, some independent variable that is basically the same as the dependent variable.  And they’d be right.  However, the linear model is not my goal.  I don’t want to predict the progress of the Dow over a day.  I want to do it over a much longer term.  For that reason, I can look past their complaints and build the first-order linear difference model. If we plot the function y(x) = 1.0000351(y(x-1)) + 0.4299070, the output is a little less than satisfying.  Here is that function over a scatterplot of Dow scores: That looks pretty underwhelming.  In fact, it almost looks…linear.  Gross.  What happened? First off, I assure you it is not the problem the aforementioned statisticians pointed out.  The real problem was that, though our slope was really convincing, it was also really close to 1.  Which means that it basically fell out of our equation, leaving y(x) = y(x-1) + .423.  If all we’re doing is adding .423 every iteration, we have in fact generated the linear equation y = .423x + .423.  That doesn’t tell me anything about the stock market! Take home points: If you’re interested in running this yourself, the R code is here: 	 0 Comments
Because it’s Friday: Arthur C Clarke predicts the present	https://www.r-bloggers.com/2010/10/because-its-friday-arthur-c-clarke-predicts-the-present/	October 22, 2010	David Smith	"On the BBC Horizon programme in 1964, Arthur C Clarke made some predictions about the future. He prefaced his predictions with the following caveat: If, by some miracle, a prophet could describe the future exactly as it was going to take place, his predictions would sound so absurd, so farfetched, that everybody would laugh him to scorn. So what absurd, farfetched, then-unbelievable technologies did Clarke predict in 1964 to be in existence in 2000? 





  Clarke predicted in 1964, when the internet barely existed, that these breakthroughs would be made possible by developments in communication. We still have cities and work travel, but it's fascinating how prophetic these predictions were, and how fantastic (in the literal sense of the word) they must have seemed 50 years ago. "	 0 Comments
Incremental improvements to Nightlights mapping thanks to R-Bloggers	https://www.r-bloggers.com/2010/10/incremental-improvements-to-nightlights-mapping-thanks-to-r-bloggers/	October 22, 2010	Jeffrey Breen	My recent post Nightlights: cool data, bad geocoding highlighted some of the geocoding challenges Steve Mosher has been finding as he works with this interesting “light pollution” data set. It was also my first article reposted on Tal Galili’s fantastic R-Bloggers site which I have been following for a while. But even better than the surge of new visitors were the great comments and suggestions posted by members of the community. In this post, I’m going to walk through each suggestion to illustrate just how generous and helpful this community can be. Our starting point is where we ended up in my first post, using ggplot2 to display the raster nightlights data and map overlay: Note the mismatch between the data and map overlay and the weirdness in the map where points are missing on the North Shore: original data, positioning, and borders in ggplot2 Ben Bolker suggested a way to eliminate the artifacts which led me to this discussion on R-sig-Geo between Hadley Wickham and Paul Hiemstra which tipped me off to the existence of geom_path layer in addition to the geom_polygon layer which borders() usually produces. Polygons are closed but paths need not be, so that helps.  And ggplot2′s map_data() function seems to grab the same data as borders(): Bonus: geom_path() obeys the “alpha=0.5” directive to set the transparency: Worst artifacts solved by switching to map_data() and geom_path() But Robert Hijmans really hit it out the park with two great suggestions.  First, he pointed me towards a much, much better source of coastline data by using raster’s getData() function to grab data from the GADM database of Global Administrative Areas: Level 0 will get you country boundaries, Level 1 for state/province, and so on.  So we’ll lose state boundaries, but these files are pretty big to start with and can take a lot longer to plot. Also, be warned: apparently somebody sinned against The Church of GNU, so you may need to run gpclibPermit() manually before running fortify() on the SpatialPolygonsDataFrame: With that hoop cleared, we can fortify() and plot this new layer: The coordinates are still shifted, but—wow—what a beautiful coast line.  Cape Ann on the North Shore is really there now: beautiful coastline data from GADM Robert also points out an important mismatch in that GDAL returns the top left corner and resolution, so we could be off by a pixel or so. A quick call to xmax() and ymax() will fix this in our original raster: Hey, that’s not bad at all: final try, after adjusting GDAL pixel shift Looking at the final version leads me to wonder how much of the geocoding problem is position, and how much is resolution/blurring/smearing. The lights of Provincetown, for instance, look pretty good. Maybe the blob is too north by a few pixels, but at least it’s well contained by land now. On Nantucket, the blur is half in the harbor.  Then again, on Nantucket, most of the lights are right on the harbor, from the ferry terminal and running east to main street.  So the lights are just about where they should be. Perhaps they’re just blurred and therefore spill into the harbor? But the real point of the post is to highlight the generosity of this community.  For that, thanks. And again: welcome R-Bloggers readers! 	 0 Comments
A workflow for R	https://www.r-bloggers.com/2010/10/a-workflow-for-r/	October 22, 2010	David Smith	Writing an R script is one thing. Organizing your process: where to put the data, how to refer to files in scripts, how to run the scripts, and how to produce and collect and report the results; that's quite another. Every R user has their own workflow for doing data analysis with R, but the best workflows achieve the following goals: Other than the package system (which is great, but can be overkill for many projects), R doesn't have any formal standards for designing a workflow. But here are a couple of suggestions from the R community: If you have other suggestions for organizing an R workflow, let us know in the comments. 	 0 Comments
Creating even NICER, publishable, embeddable plots using tikzDevice in R for use with LaTeX	https://www.r-bloggers.com/2010/10/creating-even-nicer-publishable-embeddable-plots-using-tikzdevice-in-r-for-use-with-latex/	October 22, 2010	Vinh Nguyen	It’s true. I like to do my work in R and write using LaTeX (well, I prefer to use org-mode for less formal writing and/or if I don’t have to typeset a lot of math). I haven’t done a lot of LaTeX’ing or Sweaving in the last year since 1) I’ve been collaborating with scientists (stuck using Word) and 2) my simulations in R have been a little overwhelming to keep in one file a la literate programming. I have a feeling I’ll be going back to LaTeX soon since I have to write up my dissertation (and lectures if I end up at an academic institution, **crosses finger**).   Subconsciously I’ve always wanted a tighter integration between R and LaTeX. Sweave did a fantastic job bringing R to LaTeX, greatly improving my workflow and jogging my memory when I revisit projects (just look at the one file consisting of documentation/writing and code). Despite R’s outstanding capabilities in creating publishable plots, I always felt it needed work in the realm of typesetting math. Sure it supported mathematical expressions. I used them a few times, but whenever I included the generated plot in a LaTeX document, the figure appeared out of place. I’ve explored Paul Murrell’s solution by embedding Computer Modern Font into the R-generated plot; UPDATE 10/23/2010 I also explored the psfrag in this post. The required effort probably outweighs the cost in most situation in my opinion (I haven’t done it in a real life scenario). I also tried to create a simplistic plot and overlay LaTeX code afterwards; again, haven’t done much with this, although I expect this will come in useful when I have to write over a pdf file that I do not have access to the source code.   I’ve also explored how to draw in LaTeX using the Picture package and XY package. I didn’t do much with it after the exploration because I didn’t know the syntax well and because drawing in R, Google Docs, or OpenOffice suffices 99.9% of the time. I prefer to draw in R or LaTeX to have reproducible code.   I was recently introduced to tikzDevice by this post via the R blogosphere. What it does is translate an R plot to TikZ. That is, instead of creating the plot device via pdf(), you do it with tikz(). This creates a .tex file via three modes (well four but I don’t think I’ll use the barebones mode):   Read the vignette; it’s fairly complete. The authors claim that the software is still in Beta stage (they’re still testing certain interface features), but my initial testing shows that it is ready for prime time, at least for my usage.   If you want the results in jpeg/png for use with the internet or Word documents, you can always convert the pdf to another format via the convert command.   Here is my example for the standalone (2) case:   Note I make use of the rubber command. Feel free to replace it with pdflatex.   UPDATE 10/23/2010: Make use of pgfSweave with this!  	 0 Comments
For a wider use of R	https://www.r-bloggers.com/2010/10/for-a-wider-use-of-r/	October 22, 2010	Shige		 0 Comments
How to avoid annoying a referee	https://www.r-bloggers.com/2010/10/how-to-avoid-annoying-a-referee/	October 22, 2010	Rob J Hyndman	It’s not a good idea to annoy the referees of your paper. They make recommendations to the editor about your work and it is best to keep them happy. There is an interesting discussion on stats.stackexchange.com on this subject. This inspired my own list below. For some applied papers, there are specific statistical issues that need attention: More tongue-in-cheek advice is provided by Stratton and Neil (2005), “How to ensure your paper is rejected by the statistical reviewer”. Diabetic Medicine, 22(4), 371-373. Feel free to add your own suggestions over at stats.stackexchange.com. 	 0 Comments
abc	https://www.r-bloggers.com/2010/10/abc/	October 21, 2010	xi'an	Michael Blum and Olivier François, along with Katalin Csillery, just released an R package entitled abc. (I am surprised the name was not already registered!) Its aim is obviously to implement ABC approximations for Bayesian inference: Description The ’abc’ package provides various functions for parameter estimation and model selection in an ABC framework. Three main functions are available: (i) ’abc’ implements several ABC inference algorithms, (ii) ’cv4abc’ is a cross-validation tool to evaluate the quality of the estimation and help the choice of tolerance rate, and (iii) ’postpr’ implements model selection in an ABC setting. All these functions are accompanied by appropriate summary and plotting functions. The core abc function starts from simulated samples (from the prior and from the sampling distribution) and elaborates on the standard hard-thresholding found in the basic ABC algorithm. The extensions use nonparametric perspectives defended by Blum and Francois that I think are appropriate in this setting. Other major functions include a cross-validation procedure for selecting the threshold and an application that computes posterior probabilities of models under competition, using the conglomerate of summary statistics across models. (As in our paper with Jean-Marie Cornuet, Aude Grelaud, and Jean-Michel Marin.) I have not had time yet to experiment with the package, however I can testify the manual is well-written! 	 0 Comments
Promising R Packages	https://www.r-bloggers.com/2010/10/promising-r-packages/	October 21, 2010	John Myles White	As a quick note, here are two R packages that were mentioned to me recently and that look promising: reldist and mixtools. 	 0 Comments
vecLib: Why Mac users are better off with Open Source R	https://www.r-bloggers.com/2010/10/veclib-why-mac-users-are-better-off-with-open-source-r/	October 21, 2010	Jeffrey Breen	The July and August meetings of the New England R Users group focused on two different aspects of R performance: parallel processing techniques and the effects of compiler & library selection when compiling the R executable itself. It was during Amy Szczepanski’s excellent introduction to multicore, Rmpi, and foreach (slides here) that she mentioned that the nice people who compile R for the Mac use optimized libraries to improve its performance. Amy works at the University of Tennessee’s Center for Remote Data Analysis and Visualization where they build and run machines with tens of thousands of cores, so her endorsement carries a lot of weight. I think Amy mentioned that she had benchmarked the open source vs. Revolutions distributions on her Mac, but I can’t find it in her slides and, well, in one ear and out the other…. It was the comprehensive presentation byIBM’s Vipin Sachdeva (slides here) showing 15-20X speedups through compiler and library selection that made me want to try a couple of benchmarks myself.  And my recent it’s-about-time upgrade to 2.11 seemed like the perfect opportunity. Performance is one of the advantages claimed by Revolution Analytics for its distributions, with their product page promising “optimized libraries and compiler techniques run most computation-intensive programs significantly faster than Base R” even with their free, Community edition. I have heard good things about its performance on Windows, so I was curious to see if it provides an improvement over the already-optimized Mac binary. First, some disclaimers: I am not a serious benchmarker and have made no special effort for statistical rigor.  I am just looking for order-of-magnitudes here, so I kept a normal number of programs running in the background, like Firefox and OpenOffice, though nothing was doing anything substantial and I avoided any user input while each test ran. My machine is the short-lived, late-2008, aluminum unibody 13″ MacBook (MacBook5,1) with 4GB RAM and Mac OS X Leopard 10.5.8 running the 32-bit kernel. It has a 2.4GHz Core 2 Duo — nothing special. For my tests, I ran the standard R Benchmark 2.5 available from AT&T ‘s benchmarking page which performs various matrix and vector calculations — perfect for discerning the effects of such optimized libraries.  I kept the defaults, such as running each test 3 times, and installed the required “SuppDists” package.  I tested the open source 2.10.1 32-bit version I already had on my machine and then installed Revolution’s 2.10.1-based 64-bit community edition. I should have repeated the test with the open source 64-bit edition, but I didn’t think of it at the time (I told you I wasn’t serious about this), so instead I later re-ran the benchmark with the 32- and 64-bit versions of the open source 2.11.1 to check if there are any significant 32-vs-64 differences. It didn’t take long to realize that the Revolutions community edition was not going to fare well.  During just the fourth benchmark, 2800×2800 cross-product matrix (b = a’ * a), there was a pregnant pause in the output while my laptop’s fans kicked in and soon spun up to full force. It took nearly 25 seconds to complete each turn of that one test where the open source 2.10.1 had finished in less than one tenth the time. (The complete output for each test is at the end of this post.) Figure 1: Summary-level benchmark results. (Smaller bars are better.) Figure 1 shows the geometric means of the elapsed times for each benchmark section as reported by R Benchmark 2.5. Clearly the Revolutions distribution did significantly worse on the matrix benchmarks. Figure 2 drills into the individual benchmarks to show the roughly 2-8X difference on the five slowest matrix benchmarks.  Only on the sixth,  Grand common divisors of 400,000 pairs (recursion), was the slowdown matched by the base 64-bit distribution. Only on Revolution’s fastest benchmark,2400×2400 normal distributed random matrix ^1000 all the way at the bottom of Figure 2, did the 64-bit versions hold a distinct (and roughly equal) advantage over their 32-bit brethren. Figure 2: Individual benchmark results. (Smaller bars are better.) So, no surprise — Amy was right. The off-the-shelf open source distributions of R for the Mac are already optimized.  But how?  Vipin walked us through all the different choices for BLAS and LAPACK libraries, not to mention the different C and FORTRAN compilers and their optimization flags. How can we know what’s being used by a given distribution?  Well, it turns out that R makes it easy to find out with the config options to “R CMD”: Following the symbolic link, “libRblas.vecLib.dylib” is the library being used for  BLAS. A quick consultation with Google reveals that “vecLib” is Apple’s vector library from their “Acceleration” framework in Mac OS. Here’s what Apple’s Developer site says about vecLib’s BLAS and LAPACK components:: Basic Linear Algebra Subprograms (BLAS) VecLib also contains Basic Linear Algebra Subprograms (BLAS)  that use AltiVec technology for their implementations. The functions are grouped into three categories (called levels), as follows: A Readme file is included that contains the following sections: LAPACK LAPACK provides routines for solving systems of simultaneous  linear equations, least-squares solutions of linear systems of  equations, eigenvalue problems, and singular value problems. The  associated matrix factorizations (LU, Cholesky, QR, SVD, Schur, generalized Schur) are also provided, as are related computations such  as reordering of the Schur factorizations and estimating condition  numbers. Dense and banded matrices are handled, but not general sparse  matrices. In all areas, similar functionality is provided for real and  complex matrices, in both single and double precision. Also, see <http://netlib.org/lapack/index.html. As Vipin had demonstrated, using a fast BLAS and LAPACK libraries can make all the difference in the world (well, 20X or so). And since Apple controls the horizontal and vertical on their platform, it shouldn’t be a surprise that vecLib is fast on their hardware and OS.  The real question is why doesn’t Revolutions simply link to vecLib too? It can’t be because their libraries are better (they clearly aren’t). Nor could they be afraid of competing with their Enterprise edition because, according to this edition comparison chart, they don’t offer an enterprise edition for the Mac. Perhaps they’re simply not that familiar with the platform and don’t know about vecLib.  I know I didn’t know anything about it until these tests prompted me to consult The Google. Google also pointed me to this recent discussion on the R-SIG-MAC mailing list in which Simon Urbanek refers to a serious bug in vecLib which prevents it from spawning threads and shows timings from a “tcrossprod” benchmark on a new 2.66GHz Mac Pro: So there still seems to be plenty of opportunity to beat vecLib if you’re willing to compile R and mix and match BLAS libraries.  For the rest of us, the open source distribution offers the best bang for the buck. Maybe I need to ask Vipin to take a look at my Mac at the next meeting…. 	 0 Comments
R is Hot: Part 3	https://www.r-bloggers.com/2010/10/r-is-hot-part-3/	October 21, 2010	David Smith	This is Part 3 of a five-part article series, with new parts published each Thursday. You can download the complete article from the Revolution Analytics website.    If the R movement has a genuine rock star, it’s probably Hadley Wickham. He’s an assistant professor and the Dobelman Family Junior Chair in Statistics at Rice University. He’s written and contributed to more than 20 R packages, and he’s won the John Chambers Award for Statistical Computing. Most of Wickham’s research focuses on making data analysis better, faster and easier. He is especially interested in using visualization techniques to improve how data and models are understood. In other words, he’s all about making it easy to use R. “R was designed from the ground up to deal with common data problems,” says Wickham. “Compared to other programming languages, it’s designed to help you do the kinds of things that you do most often when you’re performing data analysis. For example, R has data frames built into the core language. It’s such a natural structure, and it makes working with data much easier. But very few other languages have data frames built in.” Because R was created by statisticians for statisticians, it’s already loaded with many of the crucial features required to accomplish the everyday tasks of statistical analysis. The very design of the R language is often described as “elegant” – in other words, R is in tune with the way statisticians think and work. For example, says Wickham, “In statistics, it’s really critical to keep track of missing values. That’s when you don’t know what a value is, but you need some way of indicating it. R keeps track of that for you, so that if you add a number to a missing number, you still don’t know what that number is and R will keep track of it. That’s important.”   Precisely because R is a programming language – as opposed to being a pre-fabricated piece of software – new analytic techniques that are written in R can be saved and re-used. So when R users discover something fresh and exciting, they have two options that are not generally available to users of pre-fab software:   These are not trivial or minor advantages – they represent enormous potential value. The ability to save and re-use improvised functions means that you’re not forced to reinvent the wheel each time that you run an analytic operation. Try doing that in SAS or SPSS and you’re in for a long haul. The ability to share new R code through forums hosted by CRAN (Comprehensive R Archive Network) and other groups ensures a state of continuous evolution. Bluntly put, the world of R never sits still. “New methods show up in R before they show up in other packages,” says Michael Elashoff of CardioDX, a molecular diagnostics company that collects data from multiple sources and builds predictive models in R that help physicians detect cardiovascular diseases in their patients. “We do a lot of predictive model development on complex data sets, so the ability use and evaluate new statistical methods is important to us. Especially in the last couple of years, many of these newer methods have been showing up as R packages first. R is definitely on the cutting edge,” says Elashoff. Zubin Dowlaty, VP / Head of Innovation & Development at Mu Sigma, has a similar take on the value of R. Headquartered in Chicago, Mu Sigma is a global analytics services company providing business decision support services to clients in data-intensive industries such as pharma, insurance, financial services, CPG/retail, healthcare and technology. All of that means that Mu Sigma is in the business of analyzing data – big time. “The large ecosystem of statisticians all over the world adding new functions and packages to the R system is a huge benefit,” says Dowlaty. “State-of-the-art algorithms are available quickly through the R platform.” The R platform has become so comprehensive that it now represents a “one-stop shop” for analytical techniques, says Dowlaty. “Most of the techniques you need to drive analytics into the business are available through R – everything from statistical to machine learning and optimization techniques. Unlike other vendors, like SAS or SPSS, R provides everything in one go-round.”    Continued Thursdays 	 0 Comments
Chicago R Meetup slides	https://www.r-bloggers.com/2010/10/chicago-r-meetup-slides/	October 21, 2010	romain francois	Second stop of my world tour was chicago yesterday night, where I presented a quick light review of various ways to represent objects in R: lexical scoping, S3, S4, the new reference classes and also with C++ using Rcpp modules or RProtoBuf My slides are on my slideshare account:  	 0 Comments
Ricky Ponting and Sachin Tendulkar	https://www.r-bloggers.com/2010/10/ricky-ponting-and-sachin-tendulkar/	October 21, 2010	prasoonsharma		 0 Comments
What’s that 5km from the station “location”	https://www.r-bloggers.com/2010/10/what%e2%80%99s-that-5km-from-the-station-%e2%80%9clocation%e2%80%9d/	October 21, 2010	Steven Mosher	In our last installment we looked at stations which were pitch black. The case I examined, Middlesboro Kentucky illustrated 1. The station location data used by Hansen2010 has inaccuracies. 2. While the purported station location was pitch dark, nearby within a couple 1/100ths of a degree there were urban lights. What this example illustrated was that you have no assurance that a station which has Zero lights is in fact in a rural location. the principle reason? station mislocation. The first screen I looked at was a 3km screen. That is I looked for pitch black stations with urban lights within 3km. There are only a handful In this post I push the boundary out and examine stations that fit two criteria: 1. Nasanightlights =0 2 DMSP nightlights > 35 ( urban class 1) within 5km. What we are essentially looking for are larger errors in the station location data.  The proceedure is exactly the same. We screen for these stations. We plot the google earth map and the light contour on top of each other ( to show the algorithm works) and then we investigate the details of the station by using a google earth tour that is loaded with all the station of this class. here are some samples    >              There are several things that become clear looking at these examples. 1. Airport locations have the wrong coordinates in the ROW. 2. US location data has better quality, in fact, using the 4 digit accuracy that is available from NCDC we can see that some sites, when properly placed are STILL pitch dark. 3. Coastal locations are particularly sensitive to this type of error. Those observations may afford us some remedies. In the USA the remedies are fairly easy. Since the location data is good ( if you use 4 digit accuracies we can see that the probability that location errors result in miscoding rural as urban is small. for the ROW we have to be concerned with Coastal and airport stations. I will quickly survey the coastal issue and the airport in  subsequent posts. Oh, and ISA has been added to metadata. more on that later   	 0 Comments
Finding presence data for species distribution modelling (SDM)	https://www.r-bloggers.com/2010/10/finding-presence-data-for-species-distribution-modelling-sdm/	October 20, 2010	kariert	Getting presence data of species is often not easy and can be a major obstacle when attempting to model the distribution of species. One way is using the GBIF data base. Here I show one way how to obtain presence data for the wolf (Canis lupus) for Poland with R. 	 0 Comments
Installing rJava on Ubuntu	https://www.r-bloggers.com/2010/10/installing-rjava%c2%a0on%c2%a0ubuntu/	October 20, 2010	kariert	I had some troubles installing rJava on Ubuntu. The easiest solution I found was installing from the command line using:  Then it worked without any problems. 	 0 Comments
"The ""tikzDevice"" package"	https://www.r-bloggers.com/2010/10/the-tikzdevice-package/	October 20, 2010	Shige		 0 Comments
The 2009 homicide data for Chihuahua has been updated	https://www.r-bloggers.com/2010/10/the-2009-homicide-data-for-chihuahua-has-been-updated/	October 20, 2010	Diego Valle-Jones		 0 Comments
R Links for the Beginner on World Statistics Day	https://www.r-bloggers.com/2010/10/r-links-for-the-beginner-on-world-statistics-day/	October 20, 2010	Larry D'Agostino		 0 Comments
Hold on to your hats: it’s World Statistics Day!	https://www.r-bloggers.com/2010/10/hold-on-to-your-hats-its-world-statistics-day/	October 20, 2010	David Smith	"Apparently today is the first ever World Statistics Day. I only knew about it because I'd seen a couple of passing references to it from the stats folks I follow on Twitter. But I guess this UN-sponsored event is a big deal, judging from the official website: The celebration of the World Statistics Day will acknowledge the service provided by the global statistical system at national and international level, and hope to help strengthen the awareness and trust of the public in official statistics. It serves as an advocacy tool to further support the work of statisticians across different settings, cultures, and domains. So let's look at some of the thrilling celebrations planned by a few of the 100+ countries participating in World Statistics Day:  United States associations and federal statistical agencies will conduct a breakfast briefing and open house on Capitol Hill to celebrate the contributions of statistics toward informing public policy and improving human welfare. Hong Kong's Census and Statistics Department (C&SD) has organized various activities, including a sub-regional course on “measuring and improving survey quality” and a one-month mini-exhibition on official statistics. Italy will cover the front of the Istat building with a curtain/video to witness the day (subject to a feasibility study and budget limitations). The Statistical Society of Canada has produced this uplifting YouTube video to commemorate the event. (Don't miss the eulogy at the end … in fact I suspect this is actually a parody!) [Update: this video is not from Statistics Canada as previously reported — thanks to reader GC for the correction.] Australia will celebrate with the launch of a new publication promoting the use of statistics for evidence based policy. It's not really fair for me to poke fun, and we statisticians aren't exactly renowned for our wild and extreme behaviour. But c'mon, there are better ways to celebrate World Statistics Day. Statistics is fun, and even official statistics can be made exciting with the right presentation, as Hans Gosling has ably demonstrated: 








 So how will you be celebrating World Statistics Day? World Statistics Day 2010: Official Site "	 0 Comments
Trading secrets	https://www.r-bloggers.com/2010/10/trading-secrets/	October 20, 2010	richierocks	Recently I had the opportunity to do a job swap with one of the guys in the laboratory here at HSL.  I helped out with the mass-spectrometry and James helped me with the data analysis.  Two very useful things came out of this. Firstly, it’s been very informative to see how the data I get is created.  I tend to assume that the numbers that are given to me are either correct or mistakes.  The reality though is more subtle.  One thing at surprised me was the length that the chemists have to go to to make sure that their instruments give sensible answers.  As well as testing urine samples, you need to test blank samples (to clean out the spectrometer’s tubes), standard samples (to calibrate the machine) and quality control samples (to check that the calibration is correct).  Even then, it wasn’t entirely clear that you would get the same answer if you ran the samples twice. The project was based around testing Thallium levels in the general population.  To give an idea of how much we could trust the data, I re-analysed 50 of the samples that James had run.  The tricky bit was the pipetting; there’s a surprising art to avoiding air bubbles.  As you can see, my results were consistently lower than James’s.  Taking James as the gold standard in mass-spectrometry skill and myself as the worst-case scenario, you can see that we should only trust the results to the nearest order of magnitude.  This is not a trivial exercise – it demonstrates what would happen if James is replaced by an idiot.  (All too possible, depending on what George Osbourne says later today.) The second really good thing to come out of this was that I managed to drill into James the importance of manipulating data with code instead of manually editing spreadsheets.  He in turn passed on this message when we presented our findings to the lab.  (Main finding: no-one is about to die of thalium poisoning.)  After the presentation, one of our toxicologists came up to me and said “I finally get it.  I understand why mathematicians keep saying that you shouldn’t use Excel.  It’s because in order to for your work to be reproducible and auditable, you need the trail of code to see what you’ve done.” Major win. 	 0 Comments
Programming with R – Function Basics	https://www.r-bloggers.com/2010/10/programming-with-r-%e2%80%93-function-basics/	October 20, 2010	Ralph	One of the benefits of using R for statistical analysis is the programming language which allows users to define their own functions, which is particularly useful for analysis that needs to be repeated. For example, a monthly output from a database may be provided in a pre-determined format and we might be interested in running the same initial analysis on the data. The function keyword is used to define a function and there is an optional list of function arguments that can be specified. Unlike some programming languages R provides a certain degree of flexibility with setting defaults for particular arguments and the way that the arguments are matched can sometimes cause unexpected behaviour. As such it is sensible to explicitly match a value to a particular argument, e.g. data = mydata, so that the matching is done as expected. Consider a simple example of a function that we could write to calculate the volume of a cylinder. The cylinder itself has a radius and height, which will be the two arguments to our function. The basic definition of our function is as follows: The volume of a cylinder is pi * raidus * height which we add to our function and save as an object that is returned at the end of the function calculations. The last line of code in a function, by default, is assumed to be the return value. This is a very simple example of function and if we call the function with a radius of 5 units and height of 10 units then the answer that is returned is: There are a number of things that we can do to the function to improve it. For example, how should the function react if the user does not specify a height and/or radius? Also what happens if a negative value is submitted to either argument? 	 0 Comments
"Central Limit Theorem
A nice illustration of the Central Limit…"	https://www.r-bloggers.com/2010/10/central-limit-theorema-nice-illustration-of-the-central-limit/	October 20, 2010	Isomorphismes	"A nice illustration of the Central Limit Theorem by convolution.in R: Heaviside <- function(x) {      ifelse(x>0,1,0) }
HH <- convolve( Heaviside(x), rev(Heaviside(x)),        type = ""open""   )
HHHH <- convolve(HH, rev(HH),   type = ""open""   )
HHHHHHHH <- convolve(HHHH, rev(HHHH),   type = ""open""   )
etc.


 What I really like about this dimostrazione is that it’s not a proof, rather an experiment carried out on a computer. This empiricism is especially cool since the Bell Curve, 80/20 Rule, etc, have become such a religion.NERD NOTE:  Which weapon is better, a 1d10 longsword, or a 2d4 oaken staff? Sometimes the damage is written as 1-10 longsword and 2-8 quarterstaff. However, these ranges disregard the greater likelihood of the quarterstaff scoring 4,5,6 damage than 1,2,7,8. The longsword’s distribution 1d10 ~Uniform[1,10], while 2d4 looks like a Λ. (To see this another way, think of the combinatorics.) "	 0 Comments
upgrade R – F77 cause compilation error	https://www.r-bloggers.com/2010/10/upgrade-r-%e2%80%93-f77-cause-compilation-error/	October 20, 2010	R on Guangchuang Yu	"I try to compile the source code of R 2.12 on CentOS, but it throw an error when trying to install *cluster*.  
*cluster* was one of the recommended packages. The first idea came into my mind was not to install that package. This can be done by:   But this did not solve the problem, and many recommended packages was not updated.
R/etc/Makeconf was very informative for figuring out what the cause is. I switched the fortran compiler to gfortran instead of g77, and it’s very weird to find that gfortran was not install on CentOS by default. As I know, gfortran usually coexists with GCC4, which also cause some problems when compiling some old version of R packages.  After switch the fortran compiler to gfortran, the installation works. After install the newest version of R, we can use the following command to upgrade all installed packages hosted in CRAN and Bioconductor to the newest version as mention in a previous post. "	 0 Comments
Transactions, and Pondering their Use in Casinos	https://www.r-bloggers.com/2010/10/transactions-and-pondering-their-use-in-casinos/	October 20, 2010	Ryan	"A couple of weeks ago, Bradford Cross of FlightCaster posted in Measuring Measures that transactions are the next big data category. I argue that they already are, and from reading his blog post, he seems to suggest this as well but I will admit that I think I missed his point. There are some clear examples of transactions and their importance: Transactions can be analyzed using several different statistical techniques. In the past month, I have visited Nevada casinos twice: Las Vegas and Laughlin. We would like to believe that “slot” machines are random (albeit biased towards house advantage obviously) and anonymous. The fact of the matter is that your visit to a casino is simply one large transaction. The data generated from such transactions can allow casinos to skew machines even more in their favor. Note that this is just my opinion, and there could very well be Nevada state laws barring usage of such transactional data for skewing games in a casino’s favor.
Casinos provide perks for their frequent visitors and players via their “rewards program” or some other promotional program such as free meals, free rooms, free show tickets etc.. These programs issue a card with a magnetic stripe. By shopping at the resort, or dining in their restaurants, the resort keeps track of purchases and rewards points based on their value. These cards are also inserted into slot machines, and swiped at card tables, as another major way to earn points. Each time this card is inserted, the magstripe is decoded and presumably information about the player is transmitted over an internal secure network. A server listening on this network can know exactly when money is inserted into the machine by the user, how often, and how much.  Casinos could use the amount of money inserted into the machine to adjust probabilities of winning each pot to rig towards higher bets. Casinos can also use win history to globally adjust the conditional probability of winning certain pots on machines across the casino.  Next, starting some time in the early 2000s (maybe even the late 1990s) machines no longer accepted or awarded hard money; instead, they reward and accept printed cash vouchers. Each voucher has a bar code and serial number printed on it. To the layman, this may simply represent the amount of money to be awarded when cashed in, and when it expires. I suspect these tickets are used for the same purpose as the casino card. Although each voucher has a different serial number, there may be a “cookie” encoded in the bar code with some unique transaction ID number. Even without such a cookie, tracing the transaction is simple as the serial number on the inserted voucher can easily be matched to the serial number on the printed voucher at the end of the game in a database on network server. With cash vouchers, transactions begin with the insertion of hard cash, and ends when the gamer has either spent all of his money on the game and leaves, when some time period has passed, or when the user leaves the game and never uses the voucher in another machine. Some people may be correct in their habit to “cash out” and then deposit fresh money into the machine.
By using cash vouchers and casino reward cards, casino owners can track gamers throughout the casino, throughout a small time window. This information can be visualized as a graph, or a map, and they can then optimize placement of machines to maximize bets and loss, and minimize payouts.
This suggests an experiment that may require several people in the same casino: Just some food for thought. "	 0 Comments
Monitoring Productivity Experiment	https://www.r-bloggers.com/2010/10/monitoring-productivity-experiment/	October 20, 2010	al3xandr3	"

 
For over a year now, i’ve been collecting how much time i spend in computer and how much of it is actually used in creative/productive activities.
 
By productive activity i mean that the time spent in text editor(emacs), terminal, excel or a database client is likely to be more creative/productive than the time spent in youtube, twitter, reading rss feeds, IM Chatting or replying Email. In average.
 
This is overly simplified, but  the tools i’m using work specially well for this, including automatic data collection without the need for manual data entry.
 I’m using the RescueTime application that tracks when there’s user activity on a particular computer application. And then i copy the data onto a google doc spreadsheet, keeping only a summary per week.
RescueTime like any other app, can have its hiccups, and i’ve noticed a couple of rare occasions when it was not tracking well, but overall works well.
 Per week i collect the hours of total, productive and distracting time.
 
Besides productive and distracting, there’s also the neutral time, that is something in between, for example, things like moving files around(in finder the osx equivalent of windows explorer), a google search or even a data gap that i am not able to classify they all go into the neutral time bucket.
 
thus, total = productive + distracting + neutral
 
I’ll look here at a full year(52 weeks worth of data).
 

 

 

 
For the exception of a couple loose ends, we see that the data follows the normal distribution quite well.
Which allows for a few assumptions when analyzing it.
And we could even cut off those loose ends(by excluding data), for even a more perfect match to the normal distribution.
 
Almost 1/4(~25%) of the whole year in front of computer. Wow!
 
Values are between [37.33372, 43.24320] for 95% confidence. Which means that ~40 is a very good estimation of the average time.
 
So thats close to 40 hours per week, almost 6 hours per day in computer. And this is in average for the whole year, that is, it includes weekends, vacations, holidays, etc…
 
Note: during the (8h)work day we are not 100% of the time active in computer, from my own data, RescueTime says that for a full hour in front of computer without interruptions, it captures in average 45min of activity. So, from a 8h working day you get already only 6h of active computer time, if you then add in the meetings, breaks, ocasional discussions, etc… that value goes lower.
 

 
Total and Productive time seem to be strongly correlated, what it means? there's 2 ways to look at it:
 
So, 1. is obvious and not interesting, but could 2. be true? 
 
Well, if we compare productive vs distracting, we see that productive(0.872) has a stronger correlation to total time than distracting(0.688). And because increasing distracting time will always increase the total(in exactly the same way as productivity will, as 1.) then it means that increasing the total is more likely to increase productivity time then the distracting time.
 

 
 The big drop towards the end is a 2 week vacation, where i barely used computer.
 
In the first half of the plot there is a drop in productivity, accompanied by an increase on distracting time.
 
It also shows that close to the end(last couple of months) there's a tendency for increase in all categories.
 This post was also made to try out the OrgMode Babel mode that i've discovered recently, that allows for literate programming(mixing in same document live/executable code and text).
 
This doc was written in (Aqua)Emacs using Orgmode. R as the statistics toolbox, loaded with the nice ggplot2 graphics package.
This allows for a very smooth work flow for creating this type of documents and it works very well 🙂
 
See here how it looks in raw format
 "	 0 Comments
Coincidence in lotteries	https://www.r-bloggers.com/2010/10/coincidence-in%c2%a0lotteries/	October 19, 2010	xi'an	Last weekend, my friend and coauthor Jean-Michel Marin was interviewed (as Jean-Claude Marin, sic!) by a national radio about the probability of the replication of a draw on the Israeli Lottery. Twice the same series of numbers appeared within a month. This lotery operates on a principle of 6/37  + 1/8: 6 numbers are drawn out of a pool of numbers from 1 to 37 and then an 7th number is drawn between 1 and 8. The number of possibilities is therefore  and the probability of replicating, on a given day, the draws from another given day is 1/18,598,272. Now, the event picked up by the radio does not have this probability, because the news selected this occurrence out of all the lottery draws across all countries, etc. If we only consider the Israeli Lottery, there are two draws per week, meaning that over a year the probability of no coincidence is  namely that a coincidence occurs within one year for this particular lotery with probability 3/10,000. If we start from the early 2009 when this formula of the lotery was started, there are about 188 draws and the probability of no coincidence goes down to , which means there is more than a 1‰ chance of seeing twice the same outcome. Not that unlikely despite some contradictory computations! It further appears that only the six digits were duplicated, which reduces the number of possibilities to  Over a month and eight draws, the probability of no coincidence is  which is indeed very small. However, if we start from the early 2009, the  probability of no coincidence goes down to 0.992, which means there  is close to an 8‰ chance of seeing twice the same outcome since the creation of this lottery… If we further consider that there are hundreds of similar lotteries across the World, the probability that this coincidence [of two identical draws over 188 draws] occurred in at least one out of 100 lotteries is 53%! Last weekend, my friend and coauthor Jean-Michel  Marin was interviewed (as Jean-Claude Marin, sic!) by a national radio  about the probability of the replication of a draw on the Israeli Lotery. Twice the same series of numbers appeared within a month. This lotery operates on a principle of 6/37  + 1/8: 6 numbers are drawn out of a pool of numbers  from 1 to 37 and then an 7th number is drawn between 1 and 8. The number  of possibilities is therefore  namely  that a coincidence occurs within one year for this particular lotery  with probability 3/1000. If we start from the early 2009 when this  formula of the lotery was started, there are 655 days and the 	 0 Comments
Happy World Statistics Day!	https://www.r-bloggers.com/2010/10/happy-world-statistics-day/	October 19, 2010	Rob J Hyndman	The United Nations has declared today “World Statistics Day”. I’ve no idea what that means, or why we need a WSD. Perhaps it is because the date is 20.10.2010 (except in North America where it is 10.20.2010). But then, what happens from 2013 to 2099? And do we just forget the whole idea after 3112? In any case, if we are going to have a WSD, let’s use it to do something useful. Patrick Burns has some ideas over at Portfolio Probe. Here are some of my own: Feel free to add your own ideas in the comments … unless you’re too busy celebrating this auspicious occasion. 	 0 Comments
Example 8.10: Combination dotplot/boxplot (teaching graphic in honor of World Statistics Day)	https://www.r-bloggers.com/2010/10/example-8-10-combination-dotplotboxplot-teaching-graphic-in-honor-of-world-statistics-day/	October 19, 2010	Nick Horton		 0 Comments
EM and Regression Mixture Modeling	https://www.r-bloggers.com/2010/10/em-and-regression-mixture-modeling/	October 19, 2010	John Myles White	Last night, Drew Conway showed me a fascinating graph that he made from the R package data we’ve recently collected from CRAN. That graph will be posted and described in the near future, because it has some really interesting implications for the structure of the R package world. But for the moment I want to talk about the use of mixture modeling when you have a complex regression problem. I think it’s easiest to see some example data to motivate my interest in this topic, so here we go: If you’ve never seen data like this, let’s just make sure it’s clear how you could have ended up with a plot that looks this way. We could end up with data like this if we had two classes of data points that each separately obey a standard linear regression model, but the models have different slopes for points from each of the two classes of data. In other words, this is the sort of data set you might fit using a varying-slope regression model — if you knew about the classes coming in to the problem. To make this idea really clear, here’s the simulation code that generated the plot I’ve just shown you. But what do you do when you don’t know anything about the classes because you’ve only discovered them after visualizing your data? It should be obvious that no amount of regression trickery is going to give us the class information we’re missing. And we also can’t fit a varying slope regression without some sort of class information. It would seem that we can’t get started at all given standard regression techniques, because we have a chicken-and-egg problem where we need either the class labels or the regression parameters to infer the other missing piece of the puzzle. The solution to this problem may amaze readers who don’t already know the EM algorithm, because it’s so shockingly simple and seemingly cavalier in its approach: we make up for the missing data by just making new data up out of thin air. Seriously. The approach I’ll describe reliably works and it works for two reasons that are obvious in retrospect once someone’s told them to you: With that said, let’s go through the details for this problem with example code. First, we have to make up imaginary class labels. Then we’ll plot this assignment of classes to see how well it matches the structure we see visually: This assignment doesn’t look good at all. That’s not surprisingly given that we made it up without any reference to the rest of our data. But it’s actually quite easy to go from this made up set of labels to a better set. How? By fitting a varying-slope regression, calculating the errors at each data point for both possible class labels, and then re-assigning data points to the class that makes the errors smallest. We can do that with the following very simple code: Here we fit a linear regression with two slopes, depending on the class being 0 or 1, and we’ve thrown out any intercept for simplicity. Then we determine which of the two classes would make the data more likely given the slopes we inferred using our imaginary classes. This actually makes a huge improvement in just one step: Luckily for us, there’s only data point that’s not been assigned properly, so we can just loop over the steps we took one more time to clean up our model to near perfection: And that’s it. Each step of this process is a step of the EM algorithm, because we first fit the best model given our hypothetical class labels (an M step) and then we improve the labels given the fitted models (an E step). Just to be clear, before we started the loop, we had already done a step of E, which is why this is the EM algorithm and not the ME algorithm. (Or maybe there’s just no “me” in machine learning.) I hope that’s clear to people. This is a really powerful technique, so I’d really appreciate feedback that would help me write up a clear introduction to EM for R users that haven’t been exposed to it before. 	 0 Comments
Introduction to statistical finance with R	https://www.r-bloggers.com/2010/10/introduction-to-statistical-finance-with-r/	October 19, 2010	Ryan	During the first part of our meeting, Nicolas Christou gave an introduction of statistical finance in R, and presented a package he co-authored with previous PhD student David Diez (2010). Video of the talk is below:  During the second part, we accommodated shorter talks outlining R users’ experiences with statistical finance in R. Kyle Matoba, a Finance PhD student from UCLA Anderson School of Management, presented on Algorithmic Trading with R.  Bryce Little, UCLA alum, presented on Constructing Minimum Variance Portfolios with R.  	 0 Comments
Adap’skiii program	https://www.r-bloggers.com/2010/10/adap%e2%80%99skiii-program/	October 19, 2010	xi'an	We have just posted the (mostly definitive) program for Adap’skii, January 3-4, The Canyons, Utah. This is taking place just before and as a satellite of the larger MCMSki III conference, January 4-7, same location. The registration for the conference and for lodging is available through the  MCMCSki III registration page, Remember also that this Friday is the deadline for applying for MCMSki III Young Investigator travel funding! 	 0 Comments
An Old Wives Tale from the 2000 Census	https://www.r-bloggers.com/2010/10/an-old-wives-tale-from-the-2000-census/	October 19, 2010	David Smith	"With the data from the 2010 US Census to be published early next year, here's a cautionary tale from the 2000 Census. If you take a look at the ratio of numbers of men to women in the 5-Percent “PUMS” sample from the 2000 census over various ages, you'll see an odd spike near age 65:  What causes this strange anomaly in the data? In the video below, Sue Ranney (VP of Product Management at Revolution Analytics) visualizes facets of the data and explores how this anomaly came to be. (Watch on YouTube for a larger, high-def version.) 





  You can download a report on this analysis, including the R script files, from the link below. The R script makes use of the RevoScaleR package for big-data analysis in R, available to Revolution R Enterprise subscribers and free to academics. Update: Reader Charlie in the comments points out this Freakonomics post, with more details about the problems with over-65s in the 2000 census data. Revolution Analytics White Papers: Visualizing Huge Data Sets with R: An Old Wives Tale from the U.S. Census  "	 0 Comments
Fast matrix inversion	https://www.r-bloggers.com/2010/10/fast-matrix-inversion/	October 19, 2010	Todos Logos		 0 Comments
Middlesboro Kentucky: Pitch Black?	https://www.r-bloggers.com/2010/10/middlesboro-kentucky-pitch-black/	October 19, 2010	Steven Mosher	In his august draft of Hansen2010, Dr. Hansen makes the following claim: “We present evidence here that the urban warming has little effect on our standard global temperature analysis.  However, in the Appendix we carry out an even more rigorous test.   We show there that there are a sufficient number of stations located in “pitch black” regions,  i.e., regions with brightness below the satellite’s detectability limit (~1 µW/m2/sr/µm),  o allow global analysis with only the stations in pitch black regions defining long-term  trends.  The effect of this more stringent definition of rural areas on analyzed global  temperature change is immeasurably small (<0.01°C  per century).  The finding of a negligible  effect in this test (using only stations in pitch black areas) also addresses, to a substantial  degree, the question of whether movement of weather stations to airports has an important  effect on analyzed global temperature change.  The pitch black requirement eliminates not  only urban and peri-urban stations but also three-quarters of the stations in the more than  500 GHCN records that are identified as airports in the station name.  (The fact that  one-quarter of the airports are pitch black suggests that they are in extreme rural areas  and are shut down during the night.) Station location in the meteorological data records  is provided with a resolution of 0.01 degrees of latitude and longitude, corresponding to  a distance of about 1 km.  This resolution is useful for investigating urban effects on  regional atmospheric temperature.” The are several claims here but I will only narrowly examine a few of them. I do not asses the claim about the role of UHI in the global record. That claim, in my mind cannot be assessed until the categorization of rural/urban is settled. So, my observations here have nothing to do with the effect of the issues that the case of Middlesboro will raise. In short, I still believe the world is warming and that man is the principle cause.  Instead on will focus on Dr. Hansen’s methodology. In particular, the assumption that the station locations are accurate to .01 degrees or 1 km ( at the equator) and his assumption that selecting “pitchblack stations” gives you a rural sample. Very simply, the station locations are not accurate to .01 degrees as we have seen repeatedly in this series. To understand this problem in detail requires focusing on individual stations. That focus should neither convince people that the problem is widespread nor should it convince them that it is rare. What it should do is motivate those concerned to be more comprehensive and diligent in their work and their criticism. The conclusions I draw then are most narrow. First, station location data is too inaccurate to use with a simple look up into nightlights, second, a pitch black requirement does not eliminate the issue, and third nightlights is not a reliable indicator of the actual physical processes that cause UHI. We will start with the GISS inventory data for this station: found here 42572326006 MIDDLESBORO  36.60  -83.73  358  469S   11HIxxno-9x-9COOL FOR./FIELD C2   0    decoding: the latitude is 36.60, longitude is -83.73. The “S” indicates it is a small town, 11 indicates a population of 11,000  and finally the last value  0, indicates that the station is pitch black by  nightlights. In H2010 this last value is apparently the one used to determine if a station is dark.  Lets look  what our replicated inventory shows. it shows that Nightlights is 0, but it also indicates that there is a light with value 54 within 55km of the site. More importantly, the expanded inventory shows that within 3km of the station location there is a light with a value more than 35 DN. Simply, there are urban lights very close to the proported station location. Because I process all the pixels within a radius of every station I can locate these cases automatically. I merely sort for all the pitch dark stations and then sort for those with urban pixels within 3, 5, 10  20 km  all the way out to the 1degree cell boundary. Having identified this station as a possible issue the program then outputs the relevant google map with an overlay of nightlights contours. Like so: look at the pale blue cross. So, my algorithm works.     The program also outputs a kml file which then I can bring  up in Google earth and tour all the stations.      Not seeing anything that looks like a weather station at the location, perhaps at the airport?  Well, if  we check source data at NCDC we find the actual location(s)  And we can map all four which are all north of 36.60. In the bright zone  Checking close to the airport  36.61 -83.74 E cellFromXY(hiResLights,c(-83.74,36.61)) [1] 276750752 The Nightlights value  value at that location? not zero. its 33. cellValues(hiResLights,cell=276750752)   33 To repeat. GISS have the station at  36.60,-83.73. The “lights at that location are Zero. But the actual station location is north of that in the bright zone .The lights at the airport are 33, which qualifies as Periurban, periurban type2. There are lights as high as 56 within the region. That qualifies as urban, urban type2 by Imhoff’s criteria. The lights in the area near to the station suggest something btween periurban type2 or urban type 2. Urban type 1  is roughly 680  people per square km. The town in fact has 20 square km which translates into roughly 13K people. Checking back with the GISS inventory: 11K  people . You can check wikipedia. So Imhoffs nightlights did a good job of guessing the population, but if the station location is wrong you look up a dark pixel as opposed to the bright picels right next to them. Hansen’s screen of pitch black stations is not adequate. A tighter screen, such as no dark pixels within the area of station location uncertainty would be better. We will work our way through that as we improve the tools here. And in case you wondered about the temps?  Now there is one last thing I had to check. Hansen speaks of stations in pitch black “areas” Looking at his charts however it appears he picked stations at pitch black pixels. To check this the only think I can do is compare his view of USA stations  with my view. They match fairly well ( he shows fewer which may mean the stations drop for other reasons like short records), so I’ll assume  that he picked stations at pitch black “pixels”. As we have seen the value at the “pixel” of a station can be misleading because of very very minor location errors. hansens graphic and then mine   For one final check, I produce a graphic of stations with periurban pixels within 3km ( marked by a cross) and those with periurban pixels within 5km of the site. Confirming the supposition that hansen has picked stations at pitch black pixels. he does not consider potential station location errors  	 0 Comments
Listing gene IDs from hyperGTest	https://www.r-bloggers.com/2010/10/listing-gene-ids-from-hypergtest/	October 19, 2010	R on Guangchuang Yu	hyperGTest compute Hypergeomtric p-values for over or under-representation of each GO term in the specified category among the specified gene set. *geneSample* was used as an example. After using hyperGTest to test GO terms for over-representation, I get the result which were shown below: I want to know which subset of the input genes, which does not reported, represented in the significant GO term. This can be done by using the genome wide annotation data, for human at this example, org.Hs.eg.db, for mapping Entrez gene IDs to GO IDs. Since GO ontology is a directed acyclic graph, all genes that are annotated with a child GO term are also annotated with their parent terms. So, org.Hs.egGO2ALLEGS is using for mapping rather than org.Hs.egGO. In the example above, we can get the corresponding gene set by: The gene set can further map to other identifiers or annotation data by biomaRt package. 	 0 Comments
Sunbelt workshop on SNA in R	https://www.r-bloggers.com/2010/10/sunbelt-workshop-on-sna-in-r/	October 19, 2010	Michał	The next Sunbelt Social Networks Conference (XXXI) will take place in February 2011 in St Pete Beach, Florida, USA. The preliminary program is available here. There is still time to submit abstracts (deadline is on October 29, 2010). At the conference I will be teaching the workshop “Introduction to Social Network Analysis with R”. It will take place on February 8 during morning and afternoon sessions. See below for a summary information. I will be posting additional information soon. I will be tagging all the posts and announcements related to this workshop with the “sunbelt xxxi workshop” tag. Use the tag cloud on the right or this link directly. Introduction to Social Network Analysis with R The workshop is an introduction to network data and classical SNA methods using R. It is planned to be a good prerequisite for other R-related workshops: Carter Butts’ workshop on classical statistical SNA, ERGM workshop, RSiena workshop, tnet and perhaps others. The first part of the workshop will introduce R itself. Participants will learn how to work with R interface, script files, data objects, and perform basic data manipulation, modeling and visualization tasks. The second part will focus on working with network data using R packages: network, igraph, and intergraph as well as preparing network data for analysis with RSiena. The material will cover Target audience: The workshop is targeted especially towards people who have limited or no experience with R. Requirements: One of the features of R is its reliance on script files and a lack of graphical user interface. For some users this may be intimidating. The main requirement of this workshop is being ready to follow a discussion which necessarily involves learning R syntax and completing all the tasks by writing R commands and not by point-and-clicking. From that perspective working with R resembles very much interacting with SPSS using syntax files or using Stata only through the command-line. Last but not least, some familiarity with basics of SNA is expected, including representations of social network data (approx. level of section 3 of Wasserman & Faust). Other requirements: The workshop will include some R exercises to be performed by the participants. Although it would be possible to follow the workshop without your own computer, bringing own laptop is encouraged. Participants are expected to install R on their laptops prior the start of the workshop. Some short installation instructions will be posted on-line soon and publicized through SOCNET mailing list. 	 0 Comments
Ideas for World Statistics Day	https://www.r-bloggers.com/2010/10/ideas-for-world-statistics-day/	October 19, 2010	Pat	World Statistics Day is 2010 October 20.  If you work with data (or you should), then you are a statistician and this is a day for you. More ideas are welcome. 	 0 Comments
NppToR 2.5.0	https://www.r-bloggers.com/2010/10/npptor-2-5-0/	October 18, 2010	andrew		 0 Comments
Hadley on a Postage Stamp?	https://www.r-bloggers.com/2010/10/hadley-on-a-postage-stamp/	October 18, 2010	C		 0 Comments
K-Means Redistricting	https://www.r-bloggers.com/2010/10/k-means-redistricting/	October 18, 2010	d sparks	"U.S. Congressional districts are today drawn with the aim of maximizing the electoral advantage of the state’s majority party, subject to some constraints, including compactness (which can be measured in numerous ways) and a “one person, one vote” standard. What if, instead of minimizing population variance across districts, we aimed to minimize the mean distance between each resident and their district center? To do so would be to employ something very much like k-means clustering, and produces some interesting results. Using the population and latitude and longitude coordinates of the centroid of each (2000) census tract (a block-level reproduction was deemed too computationally intensive for the present purposes), I produced a geospatial k-means clustering for several states. Each tract was represented by its centroid as a point, weighted by population (which required a custom function, as the default kmeans() function in R does not appear to permit weighted points. Since each run of the k-means algorithm begins with a random set of points, I replicated the function several thousand times, attempting to find a maximum inverse Herfindahl-Hirschman index of district population — the “effective number of districts,” as it were. For North Carolina, as shown below, I was able to find a maximum END of 12.17 for thirteen districts, which is a fairly even distribution of population.  Click to enlarge Interestingly, there is still substantially wider variation in population than would be permitted under the current system. The least populous district houses fewer than 400,000 individuals, and the most populous, nearly a million. These figures are much more extreme than the extant least- (Wyoming) and most- (Montana) populous districts. Population by district: However, the district boundaries (here hastily drawn by use of chull()) are not characterized by the ragged edges and elongated shapes often seen in the existing plans. I was interested in what the k-means-based plan would do to district partisanship, and decided to use population density as a rough proxy for local party affiliation. The distribution of population per square mile for each North Carolina census tract is shown below, with a vertical line indicating the median.  I decided to characterize any tract with greater-than-median population density as Democratic, and less-dense tracts as Republican. This resulted in the following proportion of Democrats residing in each district as plotted above: As the table indicates, full turnout under such a plan would result in the election of 6 Republicans and 7 Democrats. Below, I plot “Democratic” tracts in blue and “Republican” tracts in red, scaled according to their population. Urban centers are easily identifiable. Note the difference between this plan and the current actual plan, which draws a single elongated district (the twelfth) parallel to Interstate 85.  Click to enlarge Below, I replicate the same process for the state of Texas, generating 32 districts. One problem with the k-means algorithm is that larger states, or those with greater variance in population density, tend to generate districts with wide variations in population and inequalities of representation. The Texas plan below depicts a district with fewer than 200,000 residents and one with over 2 million. The Effective Number of Districts (maximum after 100 attempts) is a mere 21.58. Interestingly, the the district “partisanship” split is 22/10 majority Republican/Democrat — not far from the current 20/12 split. In this simulated redistricting, there are 10 districts in which the majority of residents live in higher-than-the-state-median density areas: four each in Houston and Dallas-Fort Worth, one each around San Antonio and Austin.  Click to enlarge The slideshow below depicts the incremental steps of the weighted k-means algorithm toward convergence around alternate districts for Ohio, beginning with set of random centers, and eventually minimizing collective distances from local centroids. 
 Finally, I used the same algorithm to investigate what a the continental United States would look like if states were partitioned according to the k-means rule. Clicking on the image below will bring you to an interactive, scalable map of the U.S. with 48 alternate states and inferred partisanship. Instead of initializing with random centers, I started the k-means algorithm with the population centroids of the actual states, and allowed the algorithm to converge to a minimizing partition. Many of these alternative states are more compact but familiar versions of the originals, although this new plan does realize Plunkitt’s Fondest Dream.  Click to enlarge "	 0 Comments
RProtoBuf 0.2.1	https://www.r-bloggers.com/2010/10/rprotobuf-0-2-1/	October 18, 2010	Thinking inside the box	"
This releases extends the recent 
0.2.0
release of 
RProtoBuf
with a patch for raw bytes serialization which Koert Kuipers kindly contributed. This
helps 
RProtoBuf
for RPC communication where raw bytes are often a preferred form.

 
As always, there is more information at the
RProtoBuf
page which has a
draft package vignette,
a ‘quick’ overview vignette
and a
unit test summary vignette.

Questions, comments etc should go to the
rprotobuf mailing list
off the RProtoBuf page at R-Forge.

 "	 0 Comments
More Graphics with Google earth	https://www.r-bloggers.com/2010/10/more-graphics-with-google-earth/	October 18, 2010	Steven Mosher	"Dr. Paul, R graphics guru, blessed us with his rendition of transparent contour maps drawn on a google earth image: Cool stuff. I’ll be taking his code and turning it into a function and sharing it back: here is the picture his code creates: That is just plain slick.  While I was looking over his code a couple more examples hit my inbox. One using ggplot which I havent got to ( see the comments on the previous post) and the other example came from the raster guru, Robert. First, lets take a look at his code: First we load the package dismo. Now ordinarily you would not go looking for resources to do graphics in a package that dealt with species distribution. In addition to working on raster Robert also works on dismo. It has a couple of tools we need. The first is g 
    "	 0 Comments
Cramer’s Stock Pick Recommendations Analyzed (Part II)	https://www.r-bloggers.com/2010/10/cramers-stock-pick-recommendations-analyzed-part-ii/	October 18, 2010	C		 0 Comments
The curious case of Oct-Jan NG spreads	https://www.r-bloggers.com/2010/10/the-curious-case-of-oct-jan-ng-spreads/	October 18, 2010	Lloyd Spencer		 0 Comments
Winners of 2010 ggplot2 case study competition	https://www.r-bloggers.com/2010/10/winners-of-2010-ggplot2-case-study-competition/	October 18, 2010	David Smith	The winners of this year's ggplot2 case study competition have been announced. I was honoured to be asked to be a judge of the competition this year, but it was a difficult job with so many excellent entries. In the end, the judging panel (which included Heike Hoffman and Hadley Wickham and me) selected three entries which each demonstrated the ability to convey meaning in complex data through elegant visualizations, while using some of the more powerful features of the ggplot2 library in R. First prize goes to David Kahle of Rice University, who integrated public crime statistics and Google Maps to create this “violent crime weather map” for the city of Houston, Texas:  While the underlying data are quite complex, this is an immediately understandable and relatable representation of the parts of Houston where violent crime is most prevalent. David wins a 32 Gb iPod Touch, donated by Revolution Analytics. There were also two runners-up in the competition, appearing after the jump. Finalist Claudia Beleites (Technische Universität Dresden / Università degli Studi di Trieste) created a visualization to aid with classification of brain tumors using Raman spectroscopy. After decomposing the spectra using linear discriminant analysis, the three tumor types can be identified (except in the overlapping areas) using 3-d histograms.  Claudia wins a Use R! book of her choice, donated by Springer. Finalist Michael Lavine (UMass Amherst) also looked at data from brains, but this time to identify regions that are “hyperexcitable”, where excessing electrical activity can lead to epileptic seizures. Working from a series of photographs of the living brain surface subjected to electrical probes (and eliminating effects due to heartbeat and respiration), he created these phase plots of electrical intensity versis rate of change over time (the color) for four regions of the brain under two different kinds of EEG stimulus.  Michael also wins a Use R! book from Springer. Congratulations to all of the entrants in the competition for an impressively high standard of data visualization. Check out all the entries (each with associated R code) at the link below. ggplot2 wiki: ggplot2 case studies 2010 	 0 Comments
New Housedata release with October 2010 filings.	https://www.r-bloggers.com/2010/10/new-housedata-release-with-october-2010-filings/	October 18, 2010	jjh	"I’d like to announce the availability of the latest Offensive Politics Housedata file. This file covers 51,463 individual electronic FEC filings for US House committees from 2001 to 10/18/2010. The latest file can be downloaded here:
HouseData-20101018 A data description and example usage of the file can be found at the HouseData page. Starting with this version a new automated cleaning procedure has been implemented for every release. The cleaning roughly looks like:  Comments and questions are always appreciated.  "	 0 Comments
Welcome Rumpel!	https://www.r-bloggers.com/2010/10/welcome-rumpel/	October 18, 2010	Martin Scharm	"Ladies and gentlemen, waiting is finally over. I’m proud to introduce Rumpel! 
After more than one year of high frequently power of persuasion he finally set up his own blog named RforRocks. A few minutes after release there are even lots of myths surrounding the origin of that name. May it stand for: Who knows? Me not! So try to grill Rumpel about this issue. And by the way, he’s in any case worthy to follow 😉
(so Rumpel, time for my payoff) "	 0 Comments
The BMS Add-ons Section	https://www.r-bloggers.com/2010/10/the-bms-add-ons-section/	October 18, 2010	BMS Add-ons » BMS Add-ons	This section will be updated irregularly with pages on applications of BMS. It intends to concentrate on two things: 	 0 Comments
Fast matrix multiplication in R: Strassen’s algorithm	https://www.r-bloggers.com/2010/10/fast-matrix-multiplication-in-r-strassens-algorithm/	October 18, 2010	Todos Logos		 0 Comments
American TV does cointegration	https://www.r-bloggers.com/2010/10/american-tv-does-cointegration/	October 18, 2010	Pat	Fringe provides an excellent example of cointegration.  This is a television show in which there are two adjacent universes.  The universes are almost alike but not exactly. Now, everyone knows that history is chaotic.  If a butterfly does an extra flap of its wings, then that difference spreads out to change subsequent events everywhere.  But the Fringe universes stay largely in step.  Decades ago a building was built in one universe, but not the other; a company was founded in one, but not the other.  Yet the universes are not diverging. That’s cointegration. I’m doubtful that Clive Granger and colleagues invented cointegration to help out the science fiction industry.  More likely they saw either something in the economic world that the mathematics couldn’t handle, or a hole in the mathematics.  They may or may not have had thoughts of a Nobel prize (achieved, 2003).  Hitting mainstream culture would have been just a pipe dream. Take the case of a company that has two classes of shares — common and preferred, say.  The price histories of those share classes are going to be cointegrated. The shares are essentially the same except for voting rights and perhaps a few other details.  Their prices are going to move together.  Not exactly together: there will be differences due to large trades and other random events.  But if the prices drift too far apart, then someone is going to take advantage of that discrepancy. More technical explanation: Two time series are cointegrated if each is not stationary (and hence “integrated”), but there is a linear combination of them that is stationary. In our example, the difference in prices will be a stationary series. Packages that include functions related to cointegration include: There are additional packages that contain tests for cointegration.  I’m unaware of any additional packages that are more useful in terms of cointegration than that.  Please correct me if I’ve missed anything. Bernhard Pfaff has a book: Analysis of Integrated and Cointegrated Time Series with R. My wife is quietly amused that she has got me to watch a science fiction show.  She hooked me with Walter, the scientist.  The science that he does — as far as I can tell — is complete nonsense.  (I think that’s why they call it science fiction.)  However, Walter is the picture of a real scientist. 	 0 Comments
Giss Nightlights Replicated	https://www.r-bloggers.com/2010/10/giss-nightlights-replicated/	October 18, 2010	Steven Mosher	UPDATE: holy open source to the rescue. I posted a question yesterday on a idea peter had. Transaprency for overlaying light maps onto google maps. reminds me of the old Quake days. Well, I know John Carmack, John is an old friend of mine, but I’m no John Carmack. Neither am I Dr Paul Murrell. He whipped out some code, took my sample files and produced this beauty. Expect a post once I figure out how the hell he did that! light map, (blacker is brighter) dang i feel stupid.  And you see that little patch of light at around 40.908, -4.1138 here’s what is there  Pretty good huh. Except, the station is in NAVACERRADA. which is just a little south of the “station location” Not far, just a couple decimal points away. In a peri urban/urban location that happens to be close to that 40 nightlights area, urban. Errors in station location matter. At least to the categorization…   A while back Ron Broberg and Peter O’Neill started down the path of understanding and replicating NASA’s use of nightlights. Ron’s latest effort is here. Peter’s work can be found here. My work has relied heavily on their insights and some of their trials and tribulations. I’ve had my own share of  great insights smashed upon looking at the code and data one last time and finally I’ve been able to replicate what GISS have done. This post will be long with a lot of code and charts. It assumes a familiarity with the Nightlights file and how NASA uses it. In Ron’s last post he was trying to use “raster” to read in the “nightlights” file and he remarked that he could not match GISS results. Peter has also noted some irregularities. Ron notes that his results seemed shifted a cell in one direction or another. To me that seemed like a boundary problem so I spent time chasing down the boundary conditions ( when a station lands on a boundary) That was wrong. After a visitor noted some ‘registration’ problems and after Robert had a look at the file the source of the problem was clear: when the nightlights file was turned into a GeoTiff the metadata was not written correctly. the extents and pixel size were off by fractions. This does not impact GISS. Looking at the file they posted it was clear they were not using a *tif file, but rather a *.dat file prepared for them by Imhoff. I’ve written Imhoff to alert him to the issue with the *tif. In the meantime , with robert’s help I’ve hacked my way around the bad metadata and the results are a very close match with GISS.  To the code: First we load the “world_ave.tif” into a raster. This is a 30 arcsecond file with nightlights every 1km. Recall these are derived from 2.7km pixels so the 1km resolution is really false. Next, we force an extent command onto the raster. Raster reads it extent from the file’s metadata. In this case the files metadata indicated the wrong extent. It left off a tiny sliver of the world. The resolution was consequently figured as .00833 degrees. That little imprecision was enough to cause big differences between GISS results and raster results. With that fixed, we proceed to load the Giss inventory. In previous programs I have modified this to hold information about the entire cell surrounding the station. The reason for that is the station location uncertainty. Next we will create some variables to categorize nightlights according to Imhoffs schema. We use ‘cut’ to recode the DN numbers into his categories, and we use “cut” to turn DN into GISS categories of “urban” “periurban” and “rural.” The we create a bar plot to compare NASA’s lights value from their inventory to our independent raster version. Finally we append the new variables to out inventory data frame. As the chart indicates there is a very small difference between NASA’s categorization and our categorization. Still, there is a small difference, a handful of stations. And so we investigate the ‘spread’ between NASA nightlight values and the one we generate:  The differences are minor and range from -67 to 31. This indicates that when each program does its lookup there are still some areas where we look up adjacent cells, but that difference does not result in changes to categorization. A brief check of the extrema in this case. First we look at the biggest positive difference and then the biggest negative difference. On second though I should probably turn this into an absolute difference.  Looking directly in the center you can see the ‘S’. And now for the biggest negative difference This is harder to make out but at the center of this city we have, apparently, two cells close together that differ by 67. Lets look at the entire inventory for that location Id:  22223074000 ;  Name: DUDINKA ;  Lat  69.4 ;Lon 86.17 ; Altitude 19 ; GridEl 50 Rural S ;  Population 20 (20,000) Topography FL  ;Vegetation NA ; Coastal: no ; DistanceToCoast NA ; Airport  FALSE ;  DistanceToTown  NA NDVI: WOODED TUNDRA Light_Code C ( this is the deprecated GHCN light code) US_LightCode NA ( this is the Giss 1,2,3 code: deprecated ) Nightlights  65 ( this is the value NASA sees when they look up the station location) CountryIndex  86  CountryName   RUSSIAN FEDERATION (ASIAN SECTOR)   Cell 106779141 ( the cell id in raster) DSMP  153 ( the value raster sees ) LonMin  84.75; LonMax 87.58333 ;LatMin 68.9; LatMax 69.9  ( the extent of the contour plot) MinLights  0 ;MaxLights 166  ;MeanLights 0.7348775;  AreaLights 12353.27 (sqkm); SumLights 29983 the minimum, max, and mean lights within the extent. also the area of the extent and sum of all lights PeriUrban 3km  Urban 3km:  how close  is the nearest periurban and urban light MoshLights:Urban  NasaLights: Urban  UrbanCode: Urban4 The codes for urbanity, including the more detailed Imhoff code PopDensity 3031.  the population density implied by the nightlights 3031 people sq/km And then the google earth:  And wikipedia Which indicates that the population figure in the  GISS inventory is not accurate. Next we look at the frequency of different types of lights we get at the stations: And then we will look through all the stations and create two categories. Stations where the surrounding area ( a 55km radius) is “rural” and those cells that have Periurban or urban lights in the vincinity And  then we select rural stations that have some peri/urban lights in the vicinity and we see how bight those nearby Lights are Then we pull out the following. We look for the rural station that has the brightest nighlight within a 55km radius. And we plot that stations lightmap. I also write the small raster to disk. This is for the next project which is to test putting overlays onto google map! ( R list is helping so they need code and data!)  And the google Map.. The bright lights are madrid.  And the code for that is below Next we will select the Nasa stations that qualify under GISS criteria for Rural. And here we will look at the distance for the closest Periurban pixel. That is, the station has a value of 01-10. And we ask the question  How many rural stations have a peri urban pixel within 3km, within 5km, within 10km, 20km, 30km, 40km. Essentially if the station location is in error, even by a little, will changing the location put the station in a peri urban place. In short, I searched around every station and looked for bright pixels. Simply, if you look at Rural stations (around 3800)  1100 of those stations will have a peri urban pixel within 3km. hence, the accuracy of the station lat and lon will matter. 1400 of the stations have periurban within 5km and half are with 5km of a peri urban pixel. 5km is roughly .05 degrees. hence, accuracy in lat/lon matters. And in our survey we found stations that were mislocated by more than a full degree. Next, we look for nearby Urban pixels Again, we take the rural stations and count how many stations have an urban pixel within 3km. That would be 10. How many have an urban pixel within 5km? 41, within 10km? 154. Location accuracy matters. Next, I noted that NASA have added a test looking at “pitch black” stations. That is, stations with light =0. What do these look like. First a count The issue, however, is not with the actual value at the station (0-10) the issue is station location accuracy. What is needed is accurate location data. Sorting for pitch dark does no good if the location is wrong. To understand that we will start to look at the pixel surrounding a pitch black station.  Because of the location errors we start by screening at the CELL. Here we know this: we know that for these stations there are no lit pixels within 55km. So, inthese cases location error is not a factor. The station could be anywhere in this 1 degree cell and it would still be pitch dark. This is a conservative screen which would minimize misidentification of peri urban or urban sites as rural. Again, better location data and this screen becaomes less stringent So, there are roughly 1000 stations where the station is pitch dark and all the surrounding area ( 55km) is dark. And next we look at “dim” cells. This approach counts the stations that are in areas where the entire cell has DN numbers less than 11. All rural cells: As one might expect the number increases a bit. There are roughly 1200 stations where the  station and the surrounding area ( radius 55km) are all rural by Imhoff’s designation. So location error doesnt play here as well. Next we look, at pitch dark stations. And here we asses the pixels in the vicinity The appraoch here is to look at pitch dark stations and then see what is within 3km, 5km etc.  Of all the pitch dark stations that NASA could select in its pitch dark test, 477 of them had periurban pixels within 3km, 224 stations had peri urban pixels within 5km. if the station location is off, then these stations would not be pitch dark. Next we look at the occurance of urban pixels within these boundaries  So, if you look at all pitch dark stations, 6 of them have an urban pixel within 3km, 19 have an urban pixel within 5km. 68 have an urban pixel within 10km, 113 within 20km. and so forth. Finally, we look at 3 different kinds of sites. we count “pitch black cells, pitch black stations with rural lights for 55km, and finally rural cells. We’ve done these before but here we just put them on one chart Any of these screens would diminish the location  error problems. Finally, I take the data from Imhoff and I contstruct a field for population density in sq km. This gets added to the inventory. Theres more work to do here. Documenting pitch black stations with urban pixels nearby, fusion tables, google tours, overlays on google earth. Fun times ahead 	 0 Comments
On the Gory Loops in R	https://www.r-bloggers.com/2010/10/on-the-gory-loops-in-r/	October 17, 2010	Yihui Xie	This blog post is mainly for Stat 579 students on the homework for week 7, since I received too many “gory” loops in the homework submissions and I think it would help a bit to write my thoughts on R loops for beginners. The immortal motto for newbies in programming is: If you want to make an apple pie from scratch, you must first create the universe. Carl Sagan There have been endless wars on which programming language is better than others, but my view point is, that is nothing but the balance between the code performance and the amount of work for programmers. In an extreme sense, almost all languages give you the ability to create the universe, but you do not really have to if you just want to make an apple pie. R was born after S, a language which was invented “to turn ideas into software, quickly and faithfully” and received the ACM Software System Award in 1998. Before the S language, statisticians often had to write “gory” low-level computing routines to do data analysis and statistical computation, including those “gory” loops, of course. For example, imagine what you have to do to compute the correlation coefficients in C. R has wrapped a lot of common tasks in lower-level programming languages (mainly C and Fortran) to make it easier to call and faster to compute (R’s (explicit) loops are generally slower than low-level languages), which frees statisticians from paying too much attention to the gory details in computation. However, the consequence is we have got too many tools in our hands, of which we are often unaware. I have no quick solution on this problem — we have to learn more about the capability of R through many ways, e.g. reading the R-help mailing list, asking experts, doing daily work with R, reading the source code of R functions and playing with the examples in help pages, etc. Being specific on this homework, I saw most submissions were using long loops, which is absolutely OK, since that was what we learned in class, and it is important to know how to write loops. Some loops are inevitable, but some are not. The rule of thumb is, functions do exist in R if you natural reaction to a problem is “why does not R have this common functionality?”. For example, several students used this function to concatenate all elements of a vector into a single string: But in fact you will get a neat solution if you take a closer look at the help page of paste(): This is one of the thousands of stories in which we created the universe to make an apple pie without knowing there was a perfect apple pie machine. Sometimes the feeling that we have to power to create the universe is so strong that we do so even we know the existence of the apple pie machine, e.g. here is a function to count the number of 0′s and 1′s in a vector: The loop is pretty much like low-level languages like C/Fortran: we assign initial values to a recording variable, do the loop and collect the result. But frequency tables are so common in statistics that it is hard to exclude such a functionality in R, table(), as we see in the last but one line of the code above. Now I give my solutions as promised: There are no explicit loops above. Instead, all loops are implicit, i.e. let the more efficient low-level languages do the loops for R. This is called vectorization. We can benefit a lot from vectorization — it is not only a matter of less heavier coding jobs, but also a huge improvement in terms of efficiency (speed) in general. If we write the above functions with loops, it will look like this (for Q2 only): A simple timing test shows that it is much slower than my first version: A few output examples: So remember the pain of struggling with this homework — the same pain of the statisticians before the S language was invented. And begin to breathe the fresh air in the R empire with vectorization! 	 0 Comments
Getting arm and lme4 running on the Mac	https://www.r-bloggers.com/2010/10/getting-arm-and-lme4-running-on-the-mac/	October 17, 2010	Andrew Gelman		 0 Comments
Generating graphs of retweets and @-messages on Twitter using R and Gephi	https://www.r-bloggers.com/2010/10/generating-graphs-of-retweets-and-messages-on-twitter-using-r-and-gephi/	October 17, 2010	cornelius	"After recently discovering the excellent methods section on mappingonlinepublics.net, I decided it was time to document my own approach to Twitter data. I’ve been messing around with R and igraph for a while, but it wasn’t until I discovered Gephi that things really moved forward. R/igraph are great for preprocessing the data (not sure how they compare with Awk), but rather cumbersome to work with when it comes to visualization. Last week, I posted a first Gephi visualization of retweeting at the Free Culture Research Conference and since then I’ve experimented some more (see here and here). #FCRC was a test case for a larger study that examines how academics use Twitter at conferences, which is part of what we’re doing at the junior researchers group Science and the Internet at the University of Düsseldorf (sorry, website is currently in German only).  Here’s a step-by-step description of how those graphs were created. Step #1: Get tweets from Twapperkeeper
Like Axel, I use Twapperkeeper to retrieve tweets tagged with the hashtag I’m investigating. This has several advantages: The sole disadvatage of Twapperkeeper is that we have to rely on the integrity of their archive — if for some reason not all tweets with our hastag have been retrieved, we won’t know. Also, certain information is not retained in Twapperkeepers’ CSV files that is present in Twitter’s XML (e.g. geolocation) that we might be interested in. Instructions: Step #2: Turn CSV data into a graph file with R and igraph
R is an open source statistics package that is primarily used via the command line. It’s absolutely fantastic at slicing and dicing data, although the syntax is a bit quirky and the documentation is somewhat geared towards experts (=statisticians). igraph is an R package for constructing and visualizing graphs. It’s great for a variety of purposes, but due to the command line approach of R, actually drawing graphs with igraph was somewhat difficult for me. But, as outlined below, Gephi took care of that. Running the code below in R will transform the CSV data into a GraphML file which can then be visualized with Gephi. While R and igraph rock at translating the data into another format, Gephi is the better tool for the actual visualization. Instructions: Code for extracting RTs and @s from a Twapperkeeper CSV file and saving the result in the GraphML format: Step #3: Visualize graph with Gephi
Once you’ve completed steps 1 and 2, simply open your GraphML file(s) with Gephi. You should see a visualization of the graph. I won’t give an in-depth description of how Gephi works, but the users section of gephi.org has great tutorials which explain both Gephi and graph visualization in general really well. I’ll post more on the topic as I make further progress, for example with stuff like dynamic graphs which show change in the network over time. "	 0 Comments
Le Monde puzzle [41]	https://www.r-bloggers.com/2010/10/le-monde-puzzle-41/	October 17, 2010	xi'an	"The current puzzle in Le Monde this week is again about prime numbers: The control key on a credit card is an integer η(a) associated with the card number a such that, if the card number is c=ab, its key η(c) satisfies η(c)=η(a)+η(b)-1. There is only one number with a key equal to 1 and the keys of 160 and 1809 are 10 and 7, respectively. What is the key of 2010? The key of 1 is necessarily 1 since η(a1)=η(a)+η(1)-1=η(a). So this eliminates 1. Now, the prime number decompositions of 160, 1809, and 2010 are given by > prime.factor(160)
 [1] 2 2 2 2 2 5
 > prime.factor(1809)
 [1]  3  3  3 67
 > prime.factor(2010)
 [1]  2  3  5 67  using a function of the (still bugged!) schoolmath package. We thus have the decompositions 5η(2)+η(5)=11 3η(3)+η(67)=8 Since η(2) cannot be 1 and is an integer, we necessarily have η(2)=2 but then this implies η(5)=1 (!), unless 0 is also a valid key (?), which would imply that η(5)=11. With the same constraint on 1, the second sum also leads to the unique solution η(3)=2 and η(67)=2. I thus wonder if there is a bug in the key of 160… "	 0 Comments
R-bloggers for other languages – wanna join?	https://www.r-bloggers.com/2010/10/r-bloggers-for-other-languages-wanna-join/	October 17, 2010	Tal Galili		 0 Comments
RPostgreSQL 0.1-7	https://www.r-bloggers.com/2010/10/rpostgresql-0-1-7/	October 17, 2010	Thinking inside the box	"
This version fixes a number of issues that had been compiled in the
issue tracker
on the project
site at Google Code.
Tomoaki Nishiyama, who joined our small development group for his package a few weeks ago, was instrumental in a number
of these fixes, with assistance from Joe Conway.

 
The relevant NEWS file entry follows below:
 
More information is on the 
my RPostgreSQL page, and on
project site at Google Code.

 "	 0 Comments
Boundary conditions Dominate	https://www.r-bloggers.com/2010/10/boundary-conditions-dominate/	October 16, 2010	Steven Mosher	In part 1 and part 2 we went over the background of nightlights and the fundamental problem: Station inventory data had errors in it: In Hansen 2010, Hansen writes: “ Station location in the meteorological data records is provided with a resolution of 0.01 degrees of latitude and longitude, corresponding to a distance of about 1 km.  This resolution is useful for investigating urban effects on regional atmospheric temperature.  Much higher resolution would be needed to check for local problems with the placement of thermometers relative to possible building obstructions, for example.  In many cases such local problems are handled via site inspections and reported in the “metadata” that accompanies station records, as discussed by Karl and Williams [1987], Karl et al. [1989], and Peterson et al. [1998a].” As we have shown in part 1 and part 2 this is appears to be wrong. Simply, when Hansen takes the station lat/lon at face value, he is introducing location error. That error is further confounded by the process of look up. Simply, when one takes a fractional value such as 73.54, 46.85 and then uses that value to “look up” the radiance value in a matrix of values that is defined by discrete values you get  a few problems. The first problem is related to rounding, the second problem is related to stations that fall on lat/lon boundaries. We can assess the latter problem simply by proccessing the lat/lon values to see how many fall on boundaries. Lets review how this works. The Nightlights file is defined thusly class       : RasterLayer filename    : /Users/mosher/Moshtemp5.1/world_avg.tif nrow       : 21600 ncol        : 43200 ncell       : 933120000 xres        : 0.00833 yres        : 0.00833 that is 30 arc second data.  every box is  1/120th of a degree on a side. So, when you take a Lat/lon of say  73.95, 65.36, you will want to calculate which “box” this falls into. That’s merely a matter of finding the multiple of  1/120 that station maps into. That division process will result in some lat/lons falling onto the lines. For example, if you had .25 degree boxes at 90,89.75, ect, then a station that was at  75.75 would fall on a line. A station at 86.63 would not fall on a line.Simple, the problem is what do you do with boundary cases? do they go up a box? or down a box? to the right? or to the left? there is no right answer, But it should be clear that the smaller the resolution the greater this problem is. How big is this problem with Nighlights. Big.   As you can see the vast vast majority of stations will fall onto corners. which means there are 4 possible boxes they could be put into depending upon your rules. Now, to repeat, one can argue that this should not make a huge difference IF the nightlight values are uniform. And we expect them to be somewhat uniform since the  1km data is derived from 2.7km data. But even here there will be points where the exact box matters. To prove that we compare Giss Lights values with values looked up using “raster” Simply, we take nasa nightlights and subtract raster nightlights. We get the following: The largest difference is 149. That means when Giss looks up the value they pick a cell that has a value of say “32″ and when raster looks up the same cell ( and picks the neighbor) it finds a value of “181″. Also, its obvious that many times they do get the same value.  What’s this all mean in terms of the determination of which sites are rural? That is shown here. Again, using the same station data as NASA, the same Nighlights data, but a look up method that just looks one cell left or right, up or down, you get dramatically different results for categgorizing stations: Nasa rule is that Nightlights less than 11 means rural and greater than 35 means urban. When we apply apply that rule we dont come close to the same outcome. Why, because merely by looking one box to the left/right or up/down we see a different nightlights value. A value that CHANGES our categorization of the station. This method of categorizing is not robust. In the posts that follow I’ll explore other methods ( including Hansen’s Pitch black criteria) and show how different methods lead to different results.  The issues here, however, also run deep into the nightlights file itself. Upon closer examination there appear to be some irregularities with the file itself. Stay tuned, It’ll be clear as mud. 	 0 Comments
2010 ggplot2 Case Study Competition Winners	https://www.r-bloggers.com/2010/10/2010-ggplot2-case-study-competition-winners/	October 16, 2010	C		 0 Comments
Accuracy in Inventories and Nightlights Part II	https://www.r-bloggers.com/2010/10/accuracy-in-inventories-and-nightlights-part-ii/	October 16, 2010	Steven Mosher	In part 1 we discussed UHI in a general way and introduced NASA’s use of nightlights to identify Rural, periurban and Urban stations. Very simply, the latitude and longitude data in the station inventory is used to look up at radiance value in the nightlights file. If that value is  10 or less, the site is Rural. If it is 11-35, the site is Periurban, If the site has lights greater than 35 it is Urban. These values are then used in NASA’s adjustment process. In that process, periurban sites and Urban sites are adjusted to comport with their Rural neighbors. Logically then we will want to know several things. First how accurate is the station latitude and longitude data, second how accurate is the nighlights data, third how accurate is the look up, and finally do nighlights really “pick out” stations that dont suffer from UHI. Or rather, can you have UHI in a lightless place? The accuracy of the latitude and longitude data is hard to determine. Basically, we want to ask is the station in the real world at the same location that the database says it is. Well, if we knew exactly where the real stations were independently of the inventory data, then we could just compare the two sources. In some cases this is possible. Let’s start with stations in the US. In the US station location is now recorded down to 4 decimal places, eg  43.6754. using a rule of thumb that says a degree is roughly 111 km ( at the equator) we can see that 1/10 is roughly 10km, 1/100 is roughly 1km,  and so the US stations are represented down to roughly 100 meter precision. But is it accurate?   That question can be addressed by looking at alternative  sources. In this case, surfacestations.org. In that project many sites were visited and volunteers took GPS readings at the site. A complete comparision of the field measurements and the data held by NCDC has not  been made public. So, one can only do manual spot checks. In any case the first thing one can realize about the Giss inventory is that they appear not to use the more precise GHCN data. In their inventory for the US they continue to use the data down to 1/100 of a degree precision.  Or roughly 1km. Given the latitude of the US, however, one can say the precision is slightly better than this. Still, with more precise data available one wonders why not use it. Particularly because if a station location happens to fall on a grid line, the rounding differences between machines can cause one system to look up one cell, and another system to look up the adjoining cell. This is not an issue, if the adjoining cells are all roughly the same, however, why introduce this kind of uncertainty when you have better data. Instead of focusing on the US stations, I will examine the ROW. Primarly because Hansen2010 has extended the use of nightlights beyond the US and because the accuracy problems are more easily illustrated there. In order to illustarte the accuracy problem in the ROW we will utilize Google map. In short, we will look up the station location used by GISS and then see if there is station there. That phenomena is clearest in places like coastlines, and deserts. It clear because at coastal locations you will see the google push pin in the water. As noted peter has done much of the work documenting this and notifying NASA, who apparently think it doesnt matter. It’s also clear in deserts because you can see a push pin located out in the middle of a pathless field usually with a small town nearby. At airports its clear when we have other supporting evidence, such as ground based photos of the station which we can then place using google maps. A few minutes with the Inventory will demonstrate that something is wrong with NOAA’s inventory, or wrong with Google Earth. Since coastal locations show the problem most clearly, I took the Inventory data and read it into a fusion table and then filtered the data to look for coastal locations within 1 miles of the coast. Pulling that into Google earth one can tour the stations. Using the ruler tool one can see that location errors in the 1/100ths of a degree take stations and put them into the sea. Sometime the error is much larger: Consider this station, off by 2 degrees  For download you can find a google earth tour some of  the suspect coastal station locations in my drop down box. or visit google fusion table I created for this. The bottomline is this. The station latitudes and longitudes are precise to 1/100th but inaccurate up to 10′s of kilometers. I find no evidence to support Hansen’s contention that the station data is accurate to .01 degrees. That means when we look at the nightlights at say 34.55,45.23 we get the nightlights at that location, BUT the station may actually be at 34.876753423, 45.198763254. And that location may have different nightlights. The problem actually gets worse than this because even with the exact same coordinates and the exact same nightlights data, I can get a different value when I do my lookup than when NASA does theirs. One reason for this is rounding. If I look up a site location that falls exactly on a boundary between two cells, I have to choose which cell to pull data from. Even with the same package, raster, on different machines i could get different results. To see how large this problem is, I repeated an experiment that Ron Broberg did a while ago. Using the GISS inventory and the same nightlights that NASA uses checked the values they got for nightlights with the values produced by using raster. 	 0 Comments
Benoît B Mandelbrot, 1924-2010	https://www.r-bloggers.com/2010/10/benoit-b-mandelbrot-1924-2010/	October 16, 2010	David Smith	Benoît Mandelbrot, the father of fractals, died Thursday at the age of 85. His obituary in the New York Times covers his life and work, and is also a well-written introduction to fractals. Mandelbrot's famous book, The Fractal Geometry of Nature, was an inspiration to me in high school: that a simple question like “How Long Is The Coast of Britain” might have no simple answer was a revelation, and led me to study fractals for my honours thesis. Amongst non-mathematicians, Mandlebrot is probably most famous for the fractal object that bears his name. You can generate the Mandelbrot Set yourself using R, as shown in this article. New York Times: Benoit Mandelbrot, Mathematician, Dies at 85 	 0 Comments
Cramer’s Stock Pick Recommendations Analyzed	https://www.r-bloggers.com/2010/10/cramers-stock-pick-recommendations-analyzed/	October 16, 2010	C		 0 Comments
RcppArmadillo 0.2.8	https://www.r-bloggers.com/2010/10/rcpparmadillo-0-2-8/	October 16, 2010	Thinking inside the box	"
The short NEWS file extract follows, also containing Conrad’s entry for 0.9.90::

 
More information is on the 
RcppArmadillo page.
Questions, comments etc should go to the
rcpp-devel mailing list
off the R-Forge page.

 "	 0 Comments
Mandelbrot and Akaike:  from taxonomy to smooth runways (pioneering work in fractals and self-similarity)	https://www.r-bloggers.com/2010/10/mandelbrot-and-akaike-from-taxonomy-to-smooth-runways-pioneering-work-in-fractals-and-self-similarity/	October 16, 2010	Andrew Gelman		 0 Comments
Benoit Mandelbrot (the Father of Fractals) dead at 85	https://www.r-bloggers.com/2010/10/benoit-mandelbrot-the-father-of-fractals-dead-at-85/	October 16, 2010	C		 0 Comments
Busting gay stereotypes with data	https://www.r-bloggers.com/2010/10/busting-gay-stereotypes-with-data/	October 15, 2010	David Smith	As a gay guy, you sometimes have to put up with some pretty offensive stereotypes that get thrown your way by extremists in the community and the media. These stereotypes are usually deployed in the form of anecdotes about how gay people are “promiscuous” or “corrupting”. These misrepresentative anecdotes have serious consequences, not just in the continued denial of fundamental civil rights to gay people in places like Iran and Nigeria and the United States, but also at the individual level. Recently here in the US, there's recently been a rash of suicides by gay teens. (Some of the stories were recounted in this heartwrenching statement (video) by Forth Worth (TX) Councilman Joel Burns, and the It Gets Better Project is a wonderful new resource for gay teens subjected to bullying.)    But as every good data analyst knows, the plural of “anecdote” is not “data”. But without data published outside of the academic literature, it's been hard to combat the anecdotes with reality. Until now: Enter the clever folks at dating site Ok Cupid, who have used their database of 3.1 million user profiles to put these stereotypes to the test. Because each profile includes a self-reported sexual orientation along with a wealth of demographic, profile and essay information, data analysis reveals that: To be sure, this isn't a peer-reviewed academic analysis. You can argue that the population of Ok Cupid members isn't representative of the population as a whole: it certainly skews young and single. (For example, I'm nearing 40 and have been married to my wonderful husband for 6 years: I don't have a profile on Ok Cupid.) But, it's data, and as always the data say what the data say: amongst the population of people with profiles on Ok Cupid, these offensive stereotypes just don't hold water. That said, some stereotypes are based in truth, looking at this analysis of personality traits (based on key phrases appearing in profiles):  Straight men are more interested in sports? Knock me down with a feather. For the complete details of this analysis and much more besides, check out the link below. oktrends: Gay Sex vs. Straight Sex 	 0 Comments
R 2.12.0 released	https://www.r-bloggers.com/2010/10/r-2-12-0-released/	October 15, 2010	David Smith	As announced today, The new R 2.12.0 is now available in source form, and you'll soon be able to download R as an installable binary for Windows, Mac and Linux from your local CRAN mirror. In the meantime, if you're not building R yourself you can check out the list of new features in the NEWS file. As usual, there's a raft of new functions and fixes, but the ones that stood out for me were: More details in the official announcement linked below. R-announce mailing list: R 2.12.0 is released 	 0 Comments
Are class representatives twice as satisfied?	https://www.r-bloggers.com/2010/10/are-class-representatives-twice-as-satisfied/	October 15, 2010	Social data blog	"

 So Agnes asked (via Facebook), looking at my previous post, http://stevepowell99.posterous.com/which-parents-are-satisfied-with-their-childs, if class representatives are more satisfied than other parents? This picture shows that they are: the parents with Rep=TRUE, i.e. the representatives, are also more satisfied, especially in schools where parents are generally less satisfied. So perhaps this explains away the effect in the previous post – parent reps can be considered to know themselves (!) and they tend to be more satisfied?No. I redid the graphic from the previous post excluding all those who are parent reps and the picture is almost indistinguishable. So there are two separate things going on here. Permalink 

	| Leave a comment  »
 "	 0 Comments
Margin of error, and comparing proportions in the same sample	https://www.r-bloggers.com/2010/10/margin-of-error-and-comparing-proportions-in-the-same-sample/	October 15, 2010	arthur charpentier	"
I recently tried to answer a simple question, asked by @adelaigue.
Actually, I thought that the answer would be obvious… but it is a
little bit more compexe than what I thought. In a recent pool about
elections in Brazil, it was mentionned in a French newspapper that “Mme
Rousseff, 62 ans, de 46,8% des intentions de vote et José Serra,
68 ans, de 42,7%” (i.e. proportions obtained from the survey). It is also mentioned that “la marge d’erreur du sondage est de 2,2% ” i.e. the margin of error is 2.2%, which means (for the journalist) that there is a “grande probabilité que les 2 candidats soient à égalité” (there is a “large probability” to have equal proportions).Usually,
in sampling theory, we look at the margin of error of a single
proportion. The idea is that the variance of widehat{p}, obtained from
a sample of size  is         "	 0 Comments
Prediction with Multilevel Regression Models, and Pizza	https://www.r-bloggers.com/2010/10/prediction-with-multilevel-regression-models-and-pizza/	October 15, 2010	Harlan	The Meetup phenomenon, which is now substantial and longstanding enough to be more of a cultural change than a flash in the pan, continues to impress me. Even more so than tools like LinkedIn, Meetups have changed the nature of professional networking, making it more informal, diverse, and decentralized. Last night, statistics consultant (and cheap eats guru) Jared Lander and I presented a talk on a statistical technique tangentially related to my professional work (more closely associated with Jared’s). The origin of this presentation is worth noting. On Meetup’s web site, members of a group can suggest topics for meetings. Before even attending a single NYC Predictive Analytics event, I posted several topics that I thought might be interesting for the group. A bit later, the organizers (Bruno and Alex) contacted me to see if I’d be willing to present on prediction with Multilevel models. I said that I would, but only if I could co-present with someone who actually knew something about the topic a complementary set of skills and experiences. Knowing Jared from the NYC R Meetup group, and knowing that he learned about multilevel models from the professor who wrote the best book on the topic, and knowing that he’s pretty good in front of an audience, I suggested we collaborate. Despite requiring a lot of work, and a lot of learning of details on my part, we managed to throw together a pretty decent talk. (As of this morning, there’s four ratings of the event on Meetup, and we got 5/5 stars! Yay us! Not statistically conclusive, though…) We used as an example topic for data analysis the difficult and critically important problem of predicting reviews of pizza restaurants in downtown NYC. Jared is actually an expert on this topic, having written his Masters thesis on ratings from Menupages.com. For the talk, Jared would present a few slides, then I’d present a few. In a few cases we’d both try to explain topics from slightly different points of view. I’d repeatedly try to use the keyboard instead of the remote-control gadget to control Powerpoint, causing the computer to melt down into a pile of slag and refuse to change the slide. Jared would send me withering glares when I started to move towards the keyboard. It ended up OK, though, we got through everything, and even answered about half of the (excellent) questions! Oh, and shout-out to the AV guy at AOL HQ. I don’t know how they pay his salary, but he rocked. Jared has posted the slides from the talk here (ppt), and I’ve put the data we made up (for pedagogical purposes) and the code we used to analyze it and generate graphs for the talk here on Github. Alex video-recorded the presentation, and I’ll update this sentence to link to the video once it’s posted somewhere. Hope folks find it valuable! 	 0 Comments
Shravan Vasishth’s Slog (Statistics blog) 2010-10-15 06:47:00	https://www.r-bloggers.com/2010/10/shravan-vasishths-slog-statistics-blog-2010-10-15-064700/	October 15, 2010	Shravan Vasishth		 0 Comments
R 2.12.0 is released!	https://www.r-bloggers.com/2010/10/r-2-12-0-is-released/	October 15, 2010	Paolo Sonego		 0 Comments
Rcpp 0.8.7	https://www.r-bloggers.com/2010/10/rcpp-0-8-7/	October 15, 2010	Thinking inside the box	"
This Rcpp release depends on R 2.12.0 as two things have changed.  First, we
play along with change in R concerning the ordering of inheritance for time
classes. But secondly, and more importantly, we support in Rcpp the
corresponding change R itself which brings the new ReferenceClasses. Here is
corresponding bit from R’s NEWS file for R 2.12.0:
 
We also made a number of internal changes some of which leads to speed-ups
and internal improvement.  The NEWS entry follows below:
 
As always, even fuller details are in 
Rcpp Changelog page and the 
Rcpp page which also
leads to the downloads, the 
browseable
doxygen docs and zip files of doxygen output for the standard formats.
A local directory  has
source and documentation too. 
Questions, comments etc should go to the
rcpp-devel mailing list
off the R-Forge page

 "	 0 Comments
Nightlights: First Principles	https://www.r-bloggers.com/2010/10/nightlights-first-principles/	October 15, 2010	Steven Mosher	"With the publication of Hansen2010 forthcoming it is critical to examine the subject afresh. The global temperature index product from NASA is known as GISSTEMP .GISSTEMP, like the temperature index from Hadley/CRU and NCDC attempts to estimate the average temperature of the globe using historical data archived in the GHCN ( Global Climate Historical Network) as well as some other other sources, namely SCAR and USHCN. Moshtemp also compiles an average using  GHCN data and a method derived from CRU. One issue that generates a fair amount of controversy is the issue of UHI or the Urban Heat Island. As climate science tells us Urban areas tend to be warmer than non urban areas. The physical reasons behind this are related to the geometry of urban areas, the material properties of urban areas, and human activity. Briefly, the geometry of the urban landscape, buildings, influence the amount of sun that strikes the surface, the clear view of the sky at night ( radiation back to the sky) and it alters the airflow over the surface which impacts the  transport of heat skyward. The geometry can also lessen the UHI, for example, if we consider aspect such as tree canopies which cool the urban environment. According to that study the second most important regressor after canopy cover was building height. Canopy cover created “cool parks” while building height was instrumental in creating hot zones. The importance of building height is well established in the literature for some time  And the relationship here not simply a statistical correlation but rather has a physical explanation.Buildings change the flow of air  and they change the way heat is transported
“Air traversing rough warm urban areas thus encounters changes in surface and 
atmospheric characteristics that alter existing mean and turbulent wind velocity fields 
(Craig 2002).  In particular, UHIs cause pressure field deformations, buildings produce 
barrier effects, and large urban surface roughness length values increase surface frictional 
drag.  Bornstein and Johnson (1977) showed that wind speeds over NYC were decreased 
below (increased above) observed rural values during high (low) regional-speed periods. 
Loose and Bornstein (1977), and Gaffen and Bornstein (1988) also showed that NYC re- 
tards synoptic fronts as they pass over the city during non-UHI periods, while such fronts 
were accelerated over the city during periods of well-developed UHIs; similar effects on 
sea breeze movement over NYC were found by Bornstein and Thompson (1981). 
 Urban areas also alter wind directions, e.g., in otherwise calm synoptic flows, UHIs 
produce inward directed “country breezes” that result in speed maxima and confluence 
over the city.  With non-zero synoptic flows, the UHI is advected downwind and urban- induced flows are superimposed onto prevailing winds, the combination of which results 
in displacement of urban effects to the downwind urban edge. 
During non-UHI periods, building-barrier induced diffluence at the upwind urban 
edge divides regional flows as they approach a city, especially during stable nocturnal 
conditions (Bornstein and LeRoy 1990).  The diffluence is followed by downstream cyc- 
lonic-turning on the right-hand urban edge (looking downstream) and anticyclonic turn- 
ing on the left hand urban edge (Bornstein et al. 1993).  Barriers also produce confluence 
at both lateral urban edges (where air deflected around the city converges with the undis- 
turbed prevailing flow) and downwind of the city (where the flow re-unites).  These hori- 
zontal convergence/divergence effects produce urban vertical velocity patterns consistent 
with 3-D mass consistency and are larger during unstable daytime hours than stable 
nighttime hours. “ (MODELING THE EFFECTS OF LAND-USE/LAND-COVER MODIFICATIONS ON
THE URBAN HEAT ISLAND PHENOMENA IN HOUSTON, TEXAS,2006 ) My intention here is not to explain the phenomena of UHI, suffice it to say that the existence of UHi is generally accepted and studied in detail. The effect is not uniform as this crosssection of Budapest indicates:  The effect can be modulated by wind speed, cloudiness, and precipitation. In addition, Oke suggests that the effect is modulated by the wetness of the surrounding rural environment. Lastly, the effect is  a function of the number of people. In general, more people means more buildings, more water taken from surrounding rural areas, more economic activity, more surfaces that retain more heat. Consequently, some view population as a good proxy for the actual physical differences that cause an urban environment to be warmer. However, “The use of urban population as a predictor of the amount of UHI bias has been studied (Oke 1973; Oke 1982; Karl et al. 1988). Oke (1973) examined relationships between the urban and rural temperature bias and population statistics of North American and European cities and identified two different relationships for the two continents. Other limitations to the use of population statistics as an estimator of the UHI bias include the lack of globally consistent statistics. Additionally, population data given for a geographically arbitrary boundary can be difficult to relate to the population in the immediate vicinity of a weather station. Thus, population alone does not appear to be a globally applicable method for evaluating and removing the UHI bias.” (The Use of NOAA AVHRR Data for Assessment of the Urban Heat Island Effect, Gallo, Karl, et al) Nevertheless, Gisstemp uses  a proxy of population, nightlights, to categorize temperature stations into catagories of urban and rural throughout the world. Until recently Gisstemp had just used Nightlights for the US stations and population for the ROW. That has changed, they now use nightlights for the entire globe The Nightlights data are used as a proxy for population and population is used as a proxy for the actual physical changes that cause UHI. For present purposes we will accept this notion and our question will be instead “how accurate are nightlights and how well can they separate the urban from the rural. This is simply a data accuracy issue and an uncertainty issue. Namely, when GISS use Nightlights to classify a station as “lit” is that determination error free? and if there are errors how widespread are they and finally what if anything do these errors mean in the determination of a global temperature. For the purposes of this exercise we will accept the data that NASA relies on and we will accept their classification scheme. The accuracy of that radiance data is not examined here. Further, NASA defines the following categories: a radiance value of 0-10 is defined as rural,11-35 as periurban, and above 35 is defined as urban. In Giss code, rural stations are used to adjust periurban and urban stations. That adjustment will not be discussed. The very narrow question we will address is this:  Given a station at latitude X and longitude Y, how accurate is the giss assessment that a station is rural or not. The first part of this question is how accurate are the station locations. Giss station locations are found in the v2.inv file, which is derived from a similar file at GHCN. There are over 7000 stations in that file with and each has an associated altitude and longitude,as seen below in an example from greenland. the station at GODTHAB NUUK View Larger Map That station is listed as being at Lat: 64.17 Lon: -51.75. In actuality the site is here 64.191852,-51.675487. The error here appears small. .02 degrees in latitude and .08 degrees in longitude. View Larger Map Taking a larger view we can see that the location error is on the order of several km. Roughly, at the equator a degree is 111km and so .1 degrees will be on the order of 10km ( this varies with latitude). A cursory look at the stations in this way quickly reveals that the errors can be as large as .5degrees.  Nasa have been advised or these errors and they appear to be ignoring them. As Peter details here in his exchange with Jim hansen. Hansen’s claim that the station data is accurate to within .01 degrees is clearly wrong as 5 minutes with the station data and google earth will attest. This station in the inventory is a good case.  For more cases see Peter’s post above   This story is repeated over the ROW. In the USA, however, the station data is reported down to four decimal degrees. While there are still inaccuracies with that data, I’ll focus of the ROW. So lets set the stage by stating the problem. The nightlights data is collected so that every pixel represents 2.7km of the earths surface. This data is then projected at 1km. Generally you can think of that as 1/100th of a degree. The station location data on the other hand is far less accurate, with errors up to .5deg or more. Simply, a station that says it is at 0.0,0.0 could actually be at .5,0. Now if nightlights are uniform over big patches, we have less of a problem. If every pixel with .5 degrees of a station is lit with the same light, then station location accuracy is not that critical. But if brite pixels and dark pixels are intertwined then the location aliasing is more critical. In the posts that follow I will examine this location question in detail. The code for examining every station in detail over the whole globe is done, and we will start to plow through the data. To the code: The data sources. the station data and the nightlights data. "	 0 Comments
The S3 OOP system	https://www.r-bloggers.com/2010/10/the-s3-oop-system/	October 15, 2010	R on Guangchuang Yu	"R currently supports two internal OOP systems (S3 and S4), and several others as add-on packages, such as R.oo, and OOP. S3 is easy to use but not reliable enough for large software projects. The emphasis of the S3 system was on generic functions and polymorphism. It’s a function centric system which is different to class centric system like JAVA. Computations are carried out by methods. In many OOP languages, that is, a call to obj.method() which will find the first ancestor class of obj that has a method .method(). R makes the same dicision in a different way, it turns the idea of inheritance “inside out”. In R, a special type of function called generic function decides which method to invoke. Methods are defined in the same way as a normal function, but with a tag describing what type of object they were designed to process.
 Object Class The class of an object is defined by a character vector of class names. Objects can have many classes: Generic functions and method dispatch A generic function is a function that examines the class of its first argument, and thus decides which specific method to dispatch to. Generic functions are defined by UseMethod which names the generic name and the object to dispatch on.  The following example shows how to define a generic function: Methods are defined with a naming convention: generic.class: The methods function can find out which classes a generic function was designed for. R looks for methods in the order in which they appear in the class vector until it found the appropriate method to operate on.  Inheritance In S3 system, evaluation may be passed to a less specific method by calling NextMethod which provide a simple inheritance mechanism. NextMethod will dispatch on the second element of the class vector: "	 0 Comments
Nightlights: cool data, bad geocoding	https://www.r-bloggers.com/2010/10/nightlights-cool-data-bad-geocoding/	October 14, 2010	Jeffrey Breen	A global source of population density has been on my low-priority wish list for some time, so I was very excited when I found Steve Mosher’s work with the Nighlights data set.  “Nightlights” refers to the artificial lights seen from space at night.  Astronomers call it “light pollution” which is pretty accurate since it’s decidedly not the light which we all use to see and avoid peril at night.  Rather, it’s the light (and energy) wasted in that pursuit by being emitted or reflected straight up into the sky. Steve has since spent some quality time with other R packages like Rgooglemap exploring this data set and has noticed some problems with the geocoding of the nightlights data.  I noticed the same thing, though much more naively, just trying to check out the data around my home: which looks amazing… right up until you overlay the county boundaries from the standard ‘maps’ package:  Alas. To my eye, there’s a clear shift to the northwest (see Provincetown at the tip of Cape Cod), and perhaps a bit of a clockwise rotation as well (see the big bulge of Cape Ann north of Boston). I have a lot to learn about this data, but in my poking around, I did find more recent observations available on a “Version 4 DMSP-OLS Nighttime Lights Time Series” page. But warning — these files are big.  The tar file I download next is over 350MB:  which looks a lot better, though still probably not perfect. I am a huge fan of ggplot2, but since this was my first exposure to the raster package, I just copied and pasted Steve’s code to make the plots.  But I couldn’t help myself to try to reproduce them in ggplot2. Getting your data into a data.frame is the key to using ggplot2. Fortunately, the raster package includes a rasterToPoints() function which outputs a list which is easily cast to a data.frame: which makes the actual plotting so easy, even qplot() will do it:  The only technical glitch is in the overlay, as zooming in truncates the northernmost coastline points but the geom_polygon() layer created by borders() seems compelled to close the shape and connects the northern Mass. coast with Rhode Island. 	 0 Comments
Which parents are satisfied with their child’s education? Those who know their class representative well. Especially in poor schools.	https://www.r-bloggers.com/2010/10/which-parents-are-satisfied-with-their-childs-education-those-who-know-their-class-representative-well-especially-in-poor-schools-2/	October 14, 2010	steve	"Another result from our OSI / ESP survey of nearly 11000 parents in ten countries. Dots are individual parents.
The y-axis is individual parents’ overall satisfaction with their children’s education.
Red dots are parents who know their parent representatives well, blue dots are parents who do not; i.e. colour is mapped to the variable par.rep.1 which ranges from 1 to 4.
The x-axis is how well the parents in each school on average know their reps. So you can see that in schools where the parents know their reps well nearly all the parents are happy. In schools where parents on average know their reps less well, overall satisfaction with education is lower. But also there is a bigger range in satisfaction and this range is well explained by how well the individual parents know their reps. This is shown by the four lines which go through the means for each level of individual satisfaction.   So in short, if you are a parent in a school where most parents know their reps, you are probably happy with the education.  But if you are in one of the other schools, some are still satisfied with the education: those are the parents who know their reps well.   This graphic also shows the power of raw data plots as provided by ggplot2 (http://had.co.nz/ggplot2/) for R (http://r-project.org/). "	 0 Comments
Which parents are satisfied with their child’s education? Those who know their class representative well. Especially in poor schools.	https://www.r-bloggers.com/2010/10/which-parents-are-satisfied-with-their-childs-education-those-who-know-their-class-representative-well-especially-in-poor-schools/	October 14, 2010	Social data blog	"

 Another result from our OSI / ESP survey of nearly 11000 parents in ten countries. Dots are individual parents.The y-axis is individual parents' overall satisfaction with their children's education. Red dots are parents who know their parent representatives well, blue dots are parents who do not; i.e. colour is mapped to the variable par.rep.1 which ranges from 1 to 4.  The x-axis is how well the parents in each school on average know their reps. So you can see that in schools where the parents know their reps well nearly all the parents are happy. In schools where parents on average know their reps less well, overall satisfaction with education is lower. But also there is a bigger range in satisfaction and this range is well explained by how well the individual parents know their reps. This is shown by the four lines which go through the means for each level of individual satisfaction.   Permalink 

	| Leave a comment  »
 "	 0 Comments
Liquidity Premium vs Liquidity of Corporate Bonds	https://www.r-bloggers.com/2010/10/liquidity-premium-vs-liquidity-of-corporate-bonds/	October 14, 2010	Quantitative Finance Collector		 0 Comments
How to build a world-beating predictive model using R	https://www.r-bloggers.com/2010/10/how-to-build-a-world-beating-predictive-model-using-r/	October 14, 2010	Sydney Users of R Forum - SURF	 Many modern data analysis problems in both industry and academia involve building a model that can predict the future based on historical variables. The 2009 KDD Cup was an international data mining competition devoted to this type of problem, where contestants attempted to predict the behaviour of mobile phone customers using an extensive database of historical information. The University of Melbourne team managed to win one part of this challenge, using R almost exclusively. In this talk I’ll give some background to the area and the specific problem, and discuss how we went about building our models. The talk will be fairly accessible, and deal with many of the practical issues encountered in this type of work. Presentation  SURF Meet Up Group 	 0 Comments
Boxplots or raw data graphs?	https://www.r-bloggers.com/2010/10/boxplots-or-raw-data-graphs/	October 14, 2010	Social data blog	"
 We recently had a dilemma for an OSI publication about the design for the graphs. There will be dozens of these graphs showing the mean score on a given variable for nearly 11000 parents from 10 countries. This example is for household wealth which has values ranging from 0 to 16. These are the three alternative designs we considered, all constructed with the wonderful ggplot2. My personal favourite is the first as all of the 10 thousand persons in the database is represented by a dot. No information is lost. The means are shown by larger dots. The second option was preferred by many because it looks more familiar. However I had to disallow it because although they look like boxplots, actually the centre line is the mean and the height of the box is two standard deviations, whereas for a boxplot that should be the median and the interquartile range. So we settled on the third option though I had to tinker a bit with the code because some of the standard deviations actually exceed the range of the y-axis – the kind of problem you wouldn’t have with the first option. 


 Permalink 

	| Leave a comment  »
 "	 0 Comments
R is Hot: Part 2	https://www.r-bloggers.com/2010/10/r-is-hot-part-2/	October 14, 2010	David Smith	This is Part 2 of a five-part article series, with new parts published each Thursday. You can download the complete article from the Revolution Analytics website.    R was created in 1993 by Ross Ihaka and Robert Gentleman at the University of Aukland in New Zealand. It’s called R for the simple reason that both of its creators have first names beginning with the letter “R.” Some believe that R’s single-letter name represents a sort of homage to the S language, since R is an open-source descendent of S and much of the code written for S runs unaltered in R. The S language was developed by a Bell Labs team that included John Chambers, for which he won the prestigious ACM Software System award in 1998. Ihaka and Gentleman had set out create a language that would make it easier for them to teach their introductory data analysis courses. But news of the new language spread quickly, and in 1995 they were convinced to make the R source code available under the terms of the Free Software Foundation's GNU General Public License. Their decision to share R freely was a seminal moment in the annals of analytic software development.  As interest in R surged, a core group of dedicated volunteers coalesced around the project. This core group of leading statisticians and computer scientists from around the world is now the project’s official leadership team. They are the guardians of the R language, and oversee changes and implementations of new features to the R source code on a six-month cycle. They also provide guidance, support and advice to R users through a very active mailing list. “R got lucky because the core group is an extremely talented collection of statisticians who have a great vision and who really think about what they’re doing,” says Abhijit Dasgupta, a consulting biostatistician at the National Institutes of Health. “It also built up a very active user community around this core group. As a result, the customer support for R through various online forums is amazingly good. That’s one of the truly great benefits of R – it has a fantastic user community that keeps growing. It’s infectious.” According to Muenchen, who has actually measured the popularity of data analysis software through the careful analysis of Internet traffic, “R is the most discussed software by roughly a two-to-one margin, followed by Stata then SAS.”  R’s skyrocketing popularity translates into more than just bragging rights – the R user community is now so large that it generates new R programs (called “packages”) at an astonishing pace. It’s almost as if the R community has achieved critical mass and been transformed into one gigantic, self-organizing virtual factory that produces new R software with clockwork regularity. How does this self-organizing virtual software factory work? Let’s look at one more or less typical member of the R user community, Glenn Meyers.  Meyers is a vice president of research at ISO Innovative Analytics. He holds a bachelor’s degree in mathematics and physics from Alma College in Alma, Michigan, a master’s degree in mathematics from Oakland University, and a Ph.D. in mathematics from the State University of New York at Albany.  He is a Fellow of the Casualty Actuarial Society and a member of the American Academy of Actuaries.  If you’re a casualty actuary, you are probably already familiar with Meyers. He’s won many of the top actuarial prizes and awards, he gives speeches and he sits on international committees.  Meyers also writes a regular column for Actuarial Review. When he writes about a new technique for analyzing data, he often includes the code for the new method in his column. Most of his code is written in R, because R has become the common language – the lingua franca – of statistical analysis.  “You put out your R code and it becomes immediately usable,” says Meyers. “People download the code and just start using it.”  Large commercial software vendors will rarely develop new programs unless there’s a large enough market to justify their development costs. And it can take years for large vendors to bring new programs to market. The R community, on the other hand, develops and releases new software continuously, thanks to the contributions of thousands of people like Meyers. “The most powerful reason for using R is the community,” he says.  Or as Robert Sudol of AllianceBernstein puts it, “the more people who use R, the more powerful it becomes.” Sudol uses R to predict future economic trends by analyzing seasonal data and looking for patterns or anomalies.  It’s a complicated job that requires creative thinking and improvisation. “If you’re trying to do something that’s not in the code set, you go out and find an R package … and then you snap it right in and start using it,” Sudol explains. “There are so many people out there making modifications and enhancements, you’re going to find something you can use.”  Sudol says that he sees “a lot of parallels between R and Linux. R is becoming ubiquitous, so if you’re starting a huge project and you don’t have a lot of programmers … you go to colleges and hire people who can be productive when they walk in the door. That’s a huge benefit.” R also offers benefits to companies that are trying to reduce the amount of money they spend on renewing licenses with traditional enterprise software vendors such as SAS and SPSS. “The nicest thing about R is that it’s free,” says Sudol. Choosing an R package over a traditional software product can literally save you “hundreds of thousands of dollars,” says Sudol.  Continued Thursdays 	 0 Comments
Postdoc position in computational Bayesian statistics	https://www.r-bloggers.com/2010/10/postdoc-position-in-computational-bayesian-statistics/	October 14, 2010	xi'an	"Here is an announcement I received that should interest potential postdocs (willing to come to Paris). The location is on the Orsay campus, south of Paris. In the framework of the ANR-funded Metacoli project which aims at identifying the metabolic underpinnings of the lifestyle diversity in the E. coli species, Genoscope (the genomics institute of the French atomic energy commission) opens a 1-year post-doctoral position in Computational Bayesian Statistics. While Escherichia coli is mostly commensal bacterium of the gut, certain strains from this species are human pathogens causing multiple diseases, especially when escaping from the gut and colonizing other areas of the human body. The Metacoli project is specifically aimed at understanding how the operation of metabolism in certain pathogenic strains allows them to achieve this colonization, while other strains cannot. In the course of this project, a wealth of experimental data have been collected by two partners of the consortium (INSERM U722 “Ecology and evolution of microorganisms” and CNRS/INRA/Univ Paris XI mixt research unit “Plant Genetics”), while metabolic networks and models for 20+ strains have been reconstructed. Integrating these data using these models is an open challenge, to be tackled using Bayesian methods. An adequate Bayesian model has been devised, partly based on past works published by Calvetti & Somersalo. Yet, no well-normalized analytical expression of the posterior is available, and one has to rely on sampling methods to gain insights into the posterior distribution of the metabolic concentrations and fluxes for each (strain, growth medium) combination. The post-doctoral fellow will be in charge of developing a suitable sampler to achieve this task, and will be involved in the downstream statistical analysis of the samples produced. Applicants should have experience with bayesian computing and strong programming capabilities. The successful applicant will work at Genoscope within the “Metabolic genomics” lab headed by Claudine Médigue, where a major part of the upstream modeling effort has already been conducted, and collaborate closely with the “Plant Genetics” unit, where part of the modeling expertise is located. The salary is approximately 2000 euros/month. Contacts
Prof.Christine Dillmann – Project coordinator Université Paris Sud & INRA & CNRS
dillmann [ à] moulon.inra.fr
Dr. Claudine Médigue – Principal Investigator Genoscope – CEA/Life Sciences division
claudine.medigue [à] genoscope.cns.fr "	 0 Comments
Kuwait Airport	https://www.r-bloggers.com/2010/10/kuwait-airport/	October 14, 2010	Steven Mosher	  Kuwait International airport. Giss has it as nightlights =0, so do I. By looking at comparisons of nightlights with the station centered and a static google map with the station centered, there are mismatches between GISS and Me and between Nightlights and the  world. Subtle shift here and there. Annoying. Also, you can see how subtle shifts could turn a station from rural to urban.                                                	 0 Comments
Nightlights, Contours, and Rgooglemap	https://www.r-bloggers.com/2010/10/nightlights-contours-and-rgooglemap/	October 14, 2010	Steven Mosher	"I am continuing the investigation of nightlights using some additional packages from Cran. Here we add Rgooglemaps to the mix. Rgooglemaps is a neat tool that gives you a simple ( needs better docs) interface to the static map server. Perhaps, I’ll modify the code to my likeing, so For now I use it as delivered. Lets recap the issue. Station positions ( Lat lon) from GHCN inventory are not precise. Consequently, the nightlights you read may not be accurate. To show that I’ve collected stations where the Lat/Lon of the station is Dark, but where the surrounding area ( radius of about 55km) contains urban stations.  First off some basic stats:  Next, we look for those stations that have no urban lights within a 55km radius. Rural cells, as I term it: And then we look at the mixed cells. Cells where the station lat/lon is dark, but urban lights are in the “hood” Recall that “rural is defined as a DN ( radiance number) of less than 10. What we see is that within the “mixed” cells we have situations were the rural location is close to urban locations. And the question is “how close” and is the station really at the location in the inventory or is it really in a close by urban site. 
  At this stage the work is exploratory, getting tools together and refining an analysis approach. One issue is the registration of nightlights. More on that later, lets just look at some charts: A nightlights contour, “S” marks the station spot.  "	 0 Comments
R wanted for an intern at Barron’s	https://www.r-bloggers.com/2010/10/r-wanted-for-an-intern-at-barrons/	October 13, 2010	jackman	R/SQL/scripting, oodles of data, a willing outlet for write-ups.  Any takers? Do some good.  Intern at Barron’s, the New York financial publication with a decades-long tradition of investigative journalism and a more recent commitment to data analytic exposure of fraud in finance, business and healthcare.  Bring us your zeal and your data munging skills and we’ll teach you how to be a writer, stock analyst and gumshoe investigator.  We assemble unique data sets to scrutinize activities like Jim Cramer’s CNBC stock picking, for-profit colleges, Medicare rip-offs and the hyping of US-listed China stocks.  Our stories achieve instant reform of bad situations. Please help us make Wall Street safe for the little guy. We will hire one programmer-journalist intern to work from June through August of 2011. Desirable skills would include programming fluency in R (for data analysis and visualization), a scripting language (like Perl or Python) and SQL.  If you can write, you will also get stories published under your byline. As part of the Dow Jones News Fund, the program provides free pre-internship training seminars, 10 weeks’ salary, $1,000 scholarships for interns returning to school, and a travel allowance. The salary is not princely, so it helps if you have a place to stay in the New York area. APPLICATION DEADLINE: November 1, 2010 Apply online at https://www.newsfund.org…Click Programs…then Internship Applications…make sure to complete the business reporting form. Also, send a resume, writing and code samples to me  — Bill Alpert — at [email protected] and call me with any questions at 212.416.2742. 	 0 Comments
Impact of Google Instant on paid search	https://www.r-bloggers.com/2010/10/impact-of-google-instant-on-paid-search/	October 13, 2010	David Smith	When Google introduced Google Instant (where search results are displayed as you type), it was certainly a boon for searchers. Personally, I've started visiting the Google homepage after years of just using the search box in Firefox (and now Chrome), and enjoying the improved search experience. (And I get to see those neat Doodles, too.) But not everyone was happy. People who advertise on Google Search were worried that the ads (which now recycle with each click stroke) might not be as effective. But Weisner Vos has done an analysis using R, and it seems that advertisers don't have much to worry about. For example, click-through rates for ads in any position on the first page are basically unchanged since Google Instant was introduced:  Take a look at the complete analysis at the link below. Click2Customers: The Paid Search Impact of Google Instant – Some Initial Data 	 0 Comments
Reassembling logical operations on boolean vectors in Gnu R	https://www.r-bloggers.com/2010/10/reassembling-logical-operations-on-boolean-vectors-in-gnu-r/	October 13, 2010	Martin Scharm	"What a headline.. It’s about combining boolean vectors in R. 
I just had some problems with computing a boolean vector as a result of applying AND to two boolean vectors: As you can see, it’s a nice result, but not what I want.. My hack was the following: When Rumpel, my personal R-freak, saw that hack, he just laughed and told me the short version of this hack: Nice, isn’t it 😉 "	 0 Comments
Animated plots in R and LaTeX	https://www.r-bloggers.com/2010/10/animated-plots-in-r-and-latex/	October 12, 2010	Rob J Hyndman	I like to use animated plots in my talks on functional time series, partly because it is the only way to really see what is going on with changes in the shapes of curves over time, and also because audiences love them! Here is how it is done. For LaTeX, you need to create every frame as a separate graphics file. Here is an example. First the R code: This creates a series of pdf files in the figs directory, named frmale1.pdf, …, frmale191.pdf.  In the LaTeX file, you need to load the animate package. Then the following command will do the magic: This is how the graph on slide 2 of this presentation was produced. For web usage, it is better to produce an animated gif version in R: Click the graph below for the animated version.  For an explanation of the colours, see my rainbow plot paper. The animation package for R also allows graphics to be saved in other formats. See AniWiki for some examples of animations. 	 0 Comments
Lists of English Words	https://www.r-bloggers.com/2010/10/lists-of-english-words/	October 12, 2010	Ryan	When I was a kid, I went through an 80s music phase…well, some things never change. “People just love to play with words…” Know that song? Anyway… One of the biggest pains of text mining and NLP is colloquialism — language that is only appropriate in casual language and not in formal speech or writing. Words such as informal contractions (“gonna”, “wanna”, “whatcha”, “ain’t”, “y’all”) are colloquialisms and are everywhere on the Web. There is also a great deal of slang common on the Web including acronyms/emoticons (“LOL”, “WTF”) and smilies that add sentiment to text. There is also a less used slang called leetspeak that replaces letters with numbers (“n00b” rather than “noob”, “pwned” instead of “owned” and “pr0n” instead of “porn”). There are also regionalisms which are a pain for semantic analysis but not so much for probabilistic analysis. Some examples are pancakes (“flapjacks”, “griddlecakes”) or carbonated beverages (“soda”, “pop”, “Coke”). Or, little did I know, “maple bars” vs. “Long Johns”. Now I am hungry. There are also words that have a formal and informal meeting such as “kid” (a young goat, or a child…same thing).  Linguists consider colloquialisms different than slang. Slang is informal language used by a specific group of people: Internet users, gamers, teenagers, college students, men/women, surfers, skaters, boarders, etc. These words can be used to put users into social groups, but beyond the point of this post. Text mining becomes a lot less overwhelming if we can filter out known English words and focus on mapping colloquialisms, slang and Internet jargon to known English words. By using a list of known English words, we can do just that. I got some great recommendations for lists of English words that go beyond the typical list of words which is about length 58,000. This list may evolve over time, but it is what I have for now, and it was very useful to me. What about you? What are your favorite word lists? 	 0 Comments
RHIPE in the SD Times	https://www.r-bloggers.com/2010/10/rhipe-in-the-sd-times/	October 12, 2010	David Smith	Saptarshi Guha, who we profiled yesterday, is at the Hadoop World conference in New York City today. At 4PM, Saptarshi will give a presentation on RHIPE, his link between R and Hadoop. Saptarashi was interviewed yesterday by Alex Handy of the SD Times, where he talked about his background and his motivation to create RHIPE.  Saptarshi was sponsored by Revolution Analytics to improve RHIPE and to present at Hadoop World, and we're pleased to announce that he will be working closely with the Revolution team on projects involving Hadoop and R. From the interview: Mike Minelli, VP of sales at Revolution Analytics, said that Hadoop will now be a focus for R. “[Guha] is now working with us closely. We hired him because Revolution Analytics will be working to basically marry up Hadoop and ScaleR, which is a component that makes R scream as far as speed and the ability to handle large sets of data. Read the full interview at the link below. SD Times: RHIPE combines Hadoop and the R analytics language  	 0 Comments
Example 8.9: Contrasts	https://www.r-bloggers.com/2010/10/example-8-9-contrasts/	October 12, 2010	Ken Kleinman		 0 Comments
Export R data to tex code	https://www.r-bloggers.com/2010/10/export-r-data-to-tex-code/	October 12, 2010	Martin Scharm	"We often use Gnu R to work on different things and to solve various exercises. It’s always a disgusting job to export e.g. a matrix with probabilities to a  document to send it to our supervisors, but Rumpel just gave me a little hint. 
The trick is called xtable and it can be found in the deb repository: It’s an add on for R and does right that what I need: It is not only limited to matrices and doesn’t only export to latex, but for further information take a look at ?xtable 😉 Btw. I just noticed that the GeSHi acronym for Gnu R syntax highlighting is rsplus… "	 0 Comments
ClusterProfiles	https://www.r-bloggers.com/2010/10/clusterprofiles/	October 12, 2010	R on Guangchuang Yu	"It is very common to cluster genes based on their expression profiles, and also very common to integrate Gene Ontology to observe the distribution of biological processes, molecular functions and cellular components for a given gene list. But, what if the two in combination? The Gene Ontology distributions across a variety of gene clusters may give us a new insight to find out which specific GO terms may related to our biological problem. I was inspired by the MCP paper which was also mentioned in my previous post, and developed an R function to get this job done. Here comes the codes: 
The input *geneClusters* is a list of clusters which contain gene IDs.
Other parameters can refer to the reference manual of Bioconductor package goProfiles. I post an example below to illustrate how to use it. The function return a list which contain the cluster profiles annotated with GO which may be useful for further analysis, and a graph which can plot the GO distributions as shown below. 
 The dot sizes was based on the percentage of frequency in each row. I plan to develop the version 2 of this function to let user specify which GO categories they are interested in looking at. Any comments or suggestions are welcomed. p.s: My thanks goes to Tal Galili for inviting me to joint R-bloggers.  I am very happy to see my post appeared in R-bloggers. "	 0 Comments
Chicago Marathon 2010	https://www.r-bloggers.com/2010/10/chicago-marathon-2010/	October 11, 2010	Thinking inside the box	"

This was the sixth time I ran this race (and my 14th marathon overall). And I still can’t run this course all that well: never got a
Boston qualification here. As I had mentioned when I blogged
about my third Boston Marathon 
earlier in the year and the recent
Chicago Half-Marathon, I have
had some recurrent issue with a sore achilles which limited my running throughout the year. It had gotten better but a quick summary of the
miles in my running log showed that I had been running only about 80% of the training miles I had in prior years. And not a single
20-miler. I knew I’d have to pay for that.

 
Plus, as so often, the weather. Not quite as hot as
the record-heat of 2007.
But close enough: high 60s at the start and high 70s or even low 80s towards the end.  But I have to compliment to the race organisers.
The race was very well organised (following the experience of 2007) with extra water stops, extra sponges handed out at several spots (!!)
and very good communication when during the race the alert level was raised to yellow given the heat and humidity. The searchable results now show a fair number of
non-finishers, but at least nobody seems to have died. But it looked ugly on the course. I think I ran by three or four sets of paramedics assisting
runners who were ‘down and out’..

 
So how did I do? Fair, I suppose — I ran pretty well for sixteen miles, then needed a first short walking break and continued to run well towards and past
the 18 mile waterstop where a bunch of friends and fellow Oak Park runners were helping. But not long after that, I crumbled and needed to
alternate walking and running for most of the remainder. With that I came in at 3:41:41, or a 8:28 min/mile pace. And which is by two
seconds slower than the previous ‘worst’ from 2007.  But
heck, at least it’s still more than three minutes faster than Dubya in Houston in 1993  …  I also got beat by a few local running friends
as well as by Chicago’s own marathon juggler.  So there. Maybe I’ll train a bit more next time.


 "	 0 Comments
Parallel processing of independent Metropolis-Hastings algorithms	https://www.r-bloggers.com/2010/10/parallel-processing-of-independent-metropolis-hastings-algorithms/	October 11, 2010	xi'an	"With Pierre Jacob, my PhD student, and Murray Smith, from National Institute of Water and Atmospheric Research, Wellington, who actually started us on this project at the last and latest Valencia meeting, we have completed a paper on using parallel computing in independent Metropolis-Hastings algorithms. The paper is arXived and the abstract goes as follows: In this paper, we consider the implications of the fact that parallel raw-power can be exploited by a generic
Metropolis–Hastings algorithm if the proposed values are independent. In particular, we present improvements to the
independent Metropolis–Hastings algorithm that significantly decrease the variance of any estimator derived from the
MCMC output, for a null computing cost since those improvements are based on a fixed number of target density
evaluations. Furthermore, the techniques developed in this paper do not jeopardize the Markovian convergence properties of the algorithm,
since they are based on the Rao–Blackwell principles of Gelfand and Smith (1990), already exploited in Casella and Robert 91996), Atchadé and Perron (2005) and Douc and Robert (2010). We illustrate those improvement both on a toy normal example and on a classical
probit regression model but insist on the fact that they are universally applicable. I am quite excited about the results in this paper, which took advantage of (a) older works of mine on Rao-Blackwellisation, (b) Murray’s interests in costly likelihoods, and (c) our mutual excitement when hearing about GPU parallel possibilities from Chris Holmes’ talk in Valencia. (As well as directions drafted in an exciting session in Vancouver!) The (free) gains over standard independent Metropolis-Hastings estimates are equivalent to using importance sampling gains, while keeping the Markov structure of the original chain. Given that 100 or more parallel threads can be enhanced from current GPU cards, this is clearly a field with much potential! The graph below gives the variance improvements brought by three Rao-Blackwell estimates taking advantage of parallelisation over the initial MCMC estimate (first entry) with the importance sampling estimate (last entry) using only 10 parallel threads. "	 0 Comments
The R-Files: Saptarshi Guha	https://www.r-bloggers.com/2010/10/the-r-files-saptarshi-guha/	October 11, 2010	David Smith	“The R-Files” is an occasional series from Revolution Analytics, where we profile prominent members of the R Community.   Name: Saptarshi Guha Background: Ph.D. in Statistics, Purdue University Nationality: India Years Using R: 6 Known for: Developing RHIPE package for R + Hadoop integration At just 31 years old, Saptarshi Guha has emerged as a cutting-edge contributor to the R community. Saptarshi earned a Ph.D. in Statistics from Purdue University in June 2010 (his advisor was William S. Cleveland, one of the pioneers of modern data visualization) and his current research focus is the analysis of large data sets. He is working to develop innovative approaches to the visualization and computing of statistical analyses. He has also worked on modeling network traffic for security and developing algorithms to detect human presence in SSH connections. Saptarshi is best known for developing the popular RHIPE package that integrates the R statistical environment with the Hadoop framework. RHIPE allows R users to compute on terabyte-sized data sets a cluster using the MapReduce framework, thus offering the best of both worlds to users seeking to leverage the strength of R and Hadoop. People with very large data sets stored in the Hadoop Distributed File System can now easily process the data on hundreds or even thousands of nodes in parallel, using only the R language (no need to learn Java). They can even apply the statistical algorithms in R to boot. While Saptarshi has studied the intersection of computer science and statistics for well over a decade — in the U.S. as well as his native India — it was not until he began his doctoral research at Purdue that he learned R. Given his statistical background, he quickly took to the language. “R is one of the most expressive languages I’ve ever encountered, and it’s perfectly geared towards comprehensive data analysis,” he says. In addition to RHIPE, Saptarshi has worked on packages that serialize R objects for operability with other languages, including Python, Java and C. He also wrote a package that saves R objects in a flexible data format so that individual objects can be lazy loaded. He has used R to perform statistical analysis on a wide array of topics, from network security modeling to generating reports and graphs for monthly expenditures and weight loss programs. “The real beauty with R is that it’s constantly evolving,” Saptarshi says. “Is it perfect? No. But it’s being constantly refined by some of the most brilliant statistical minds today.” To that end, Saptarshi is working on several packages that will bring features of distributed computing to users working within the R environment. He’s also working on a package that integrates R with HBase and give users a fast query distributed data store that applies MapReduce computations across the data using RHIPE. On October 12, Saptarshi will be delivering a talk at Hadoop World, where he’ll demonstrate the use of RHIPE to analyze 190 Gb of VoIP network data. (This project was joint work with Jin Xia and William S. Cleveland.) When you make a call over Skype, Google Voice, or even a landline routed over the internet, the call quality is of primary importance. One of the major factors influencing call quality is timing: when you speak, your voice is sampled every 20 milliseconds, but on the receiver’s end it might not arrive quite so regularly – perhaps 5ms too early or too late. This “jitter” in packet arrival times degrades the audio quality; to investigate this, Saptarshi used R code to identify which packets corresponded to a single call. He used R’s robust regression algorithm to remove the effect of the gateway. In this way, he was able to process the data in a matter of minutes stored across a cluster of eightcomputers to assess the overall call quality metrics of the system. If you’re interested in attending and are in the Greater New York area, you can register here. Cloudera also published an interview with Saptarshi last month. 	 0 Comments
R Tutorial Series: One-Way Omnibus ANOVA	https://www.r-bloggers.com/2010/10/r-tutorial-series-one-way-omnibus-anova/	October 11, 2010	John Quick		 0 Comments
New R User Group in Toronto	https://www.r-bloggers.com/2010/10/new-r-user-group-in-toronto/	October 11, 2010	David Smith	Canadian R user Leo Guelman contacted me last week to ask if there was an R User Group in Toronto. There wasn't one last week, but there's one now: Leo has taken the initiative to start a new useR group. The Greater Toronto Area (GTA) R User's Group is now active on meetup.com, and taking suggestions for their first meeting. If you're in the region, you can sign up now at the link below. meetup.com: Greater Toronto Area (GTA) R User's Group 	 0 Comments
How to pick the best cricketer in any format of the game?	https://www.r-bloggers.com/2010/10/how-to-pick-the-best-cricketer-in-any-format-of-the-game/	October 11, 2010	prasoonsharma		 0 Comments
WP-CodeBox:  A better R syntax highlighter plugin for WordPress	https://www.r-bloggers.com/2010/10/wp-codebox-a-better-r-syntax-highlighter-plugin-for-wordpress/	October 11, 2010	Tal Galili	About 10 months ago, I was looking for a plugin to enable me to highlight R code on my self hosted WordPress blog. The solution I came up with then was to use the wp-syntax plugin (with the need for some modifications). Today I was informed of (what I believe is) a better WordPress plugin for R syntax highlighting called WP-CodeBox.  This plugin doesn’t require any modification (as opposed to WP-Syntax), so it can simply be downloaded and installed to the WP as is from the “Add New” section in the plugins menu). WP-CodeBox provides some nice features (some AJAX based) to the display of the code in the post: Lastly, my thanks goes to guangchuang yu who’s comment on my original post, and he’s post on wp-codebox and R, has introduced me to this better plugin. p.s: in case you where wondering, there is also a solution for R syntax highlighting for bloggers on WordPress.com. 	 0 Comments
highlight R syntax in wordpress using wp-codebox	https://www.r-bloggers.com/2010/10/highlight-r-syntax-in-wordpress-using-wp-codebox/	October 10, 2010	R on Guangchuang Yu	Tal Galili’s blog post mentioned that WP-Syntax can highlight R codes. I downloaded the modified version of WP-Syntax in his blog site. The plugin throw an error when activated. I did not try the original version hosted in WordPress. I found that WP-CodeBox use GeShi for syntax highlighting as WP-Syntax does.I used this plugin when I posted perl codes, but did not found it support R syntax yet. In Tal’s another blog post, I found that the lang tag must use “rsplus” instead of “r”. And yes, WP-CodeBox can hightlight R syntax and it works fine as shown in this post. 	 0 Comments
Nightlights in China	https://www.r-bloggers.com/2010/10/nightlights-in-china/	October 10, 2010	Steven Mosher	Update: Some are not aware that GISS has switched to using Nightlights in the ROW. According to their updates they have moved to nightlights for the ROW. The station inventories can be found here The station I examine below is listed  like this in the new giss inventory That final Zero is Giss’ field for nighlights. For this station our figures match. My preliminary analysis shows no station at that location, however there is a nearby settlement that may be its location. As most who follow the construction of a Global temperature index know the Index created by GISS uses Nightlights as a determinate of whether a station is urban or rural. To determine if a site is rural GISS apparently looks at the Night lights value and if the value is less than 10, the site is declared as rural. There is an inherent problem with this that results from the location errors in the GHCN inventory. Simply, the Nightlights image is collected  at a 30 arcsecond accuracy while the GHCN inventory is far less accurate. A GHCN site that is supposed to be at 42.0 90, may actually be at 42.3,90. The exact distribution of errors has not been quantified, but others looking at the issue have reported errors up to .5 degrees. At the equator nightlights data is roughly 1km data. .5degrees at the equator is roughly 55km. That means when GISS looks at nighlights they could actually be looking at the lights from a position 55km away from the actual site. They have not taken aliasing into account to INSURE that a station is in fact rural. In an effort to come to some understanding of the magnitude of this problem I’ve built several tools. The first was a program to download and analyze Nightlights. Using the calibrated dataset referred to by GISS, I downloaded and worked with the HiRes version. My approach was simple and brute force. For every station in the GHCN inventory I cropped the surrounding area to approximately a 111km square with the GHCN station in the center.  It turns out after I wrote the code that raster has a command to do exactly what I wanted [ see xyValues(..buffer=) The resulting sub raster was then processed to extract the following: min value, max value, mean value, and value at the location indicated by GHCN. These rasters can then be plotted. In the abstract we are concerned about urban sites being mislocated as rural sites. That is, a site in a city having a recorded location that is outside the city. Such a site would show up dark, but actually be in a lit area. The goal then would be to develop an approach that would allow for the rapid assessment of this possibility. More on that later. First, we will start with an approach that eliminates that possibility. We do that by constraining the stations to those that have no lights brighter than 10 in the whole sub grid. Starting with 7280 stations if we select those stations that have a Nighlights value of less than 10, and if we require that no pixel within ~55Km of that location is also less than 10, we have a good assurance that the site is dark regardless of the location errors, provided the location error is less than ~.5degrees. Filtered thusly there are  1099 such stations in GHCN. My Data is all open and viewable in google fusion tables. Fusion tables will allow you to subset the data any way you want and geo view the results. Rural Area by nighlights As you can note that means the other 6000 sites are all in areas where there is some sign of urban development within ~55kmkm or so of the location. To screen for the phenomena I’m asserting exists, I filtered the data to look for sites where the  Area had “high frequency” that is, values of 0 and values over 100. The reporting site has a value of Zero according to its location in the inventory AND the surrounding area has values over 100. There are 179 entries meeting these condition. That view  is located here  Looking at that map I was drawn to the site in China. For a couple reasons that should be obvious. Pulling up the inventory on that station we see the following: Please note the fields my analysis adds. ( again with the mountain valley sites!)  “AreaLights” is the area in square km that I perform the cropping at, roughly 111km per side. The putative station location is centered and I crop a equal area square around every station center. The figure of 111km is selected based on reports of errors as large as .5 degrees in some station locations. SumLights is the sum of all radiance values in the sub grid. DSMP is the figure at the GHCN location which matches the inventory value in GISS. Using the lat lon from GHCN we an get the google map. with the green arrow indicating where GHCN ( And GISS) thinks the station is: View Larger Map You can zoom in on the location at the green arrow. See any station? Here is what this world look like to nightlights. the blue circle represents the location given by GHCN. its dark. Yet, a few km away we find an urban location. At approximately 43.08,90.45 we see the hot spot  And here is the GE view of the hotspot View Larger Map So what do we know. We know that Nightlights is reported at a 1Km resolution. When know that GHCN has a coarser resolution. This leads to aliasing. When we read Nightlights  given the locations in GHCN we have no assurance that the lights figure we obtain is the correct one. If we try to protect against this by filtering stations more aggressively, we are left with 1099 stations. That is 1099 stations that have no light in their area. On the other hand, there are 179 sites where a bright urban source is close by. By chance I picked good site to illustrate the issue, or maybe that station is somewhere in that pathless desert. My bet is the location and the assesement of this site as “dark” is wrong. Since this is classed as an “rural” station it recieves no adjustment but would adjust other stations in its sphere of influence: here is the Giss chart of the station:  The solution is for NOAA to update its inventory with accurate location data. This is tedious work, there is a much better way to fix these simple problems If Anybody wants a good are to study I’ll suggest this collection of sites where the max lights are less than 10 for a nice tight collection of stations. But here too be aware of the  land use change. And for a list of US sites, start with this map.  MaxLights in the area less than 10. consider the constellation of three sites in the same vincinity.  In particular “hanksville” which Giss sees as Dark, and which my program determined  has no lights within a 55km radius. In the chinese case we have a station mislocation, and below we see than even the requirement of no lights within 55km is not a perfect filter On the ground: View Larger Map 	 0 Comments
Le Monde puzzle [40]	https://www.r-bloggers.com/2010/10/le-monde-puzzle-40/	October 10, 2010	xi'an	"The puzzle in Le Monde this week[end] is called the “square five” (sic!): Two players each have twenty-five cards with five times each of the digits 1,2,3,4,5. They alternate putting one card on top of the pile, except that they can instead take an arbitrary number of consecutive cards from the top of the pile whose product is a squared number. In which case the corresponding player wins! What is the maximum number of moves and who can find a winning strategy?
 At any time, a player cannot put a 1 or a 4 because those are squared numbers. The choice is therefore always among 2,3,5, with a prohibition of squares in the sequence. Testing blindly for the longest possible sequence can be done via the following R code which gives a maximum length of 8, a figure that makes sense since picking any sequence of length 8 from 2,3,5 leads to a square by a tree decomposition. It does not seem there is a winning strategy for the second player if the first one plays in an optimal manner… "	 0 Comments
Data Tools Contest Update	https://www.r-bloggers.com/2010/10/data-tools-contest-update/	October 10, 2010	johnmyleswhite	At midnight this morning, Kaggle began accepting submissions for the data hacking contest that we announced on Thursday. Hopefully you’ve used the last few days to build predictions for the test data set. Once you submit your predictions, you’ll be able to see your position on the leaderboard. Good luck! 	 0 Comments
R Recommendation Contest Launches on Kaggle	https://www.r-bloggers.com/2010/10/r-recommendation-contest-launches-on-kaggle/	October 10, 2010	John Myles White	The R Recommendation Engine contest is now live on Kaggle. Please head over there and start submitting your predictions for the test data set. Once you do, you can check the leaderboard to see how your algorithm compares with other people’s work. We know that there’s still plenty of progress that can be made, because we have other models that are much better than the benchmark code we released to the public. In the future, I’ll be posting some hints on this blog about ways to improve your models for the contest using less well-known methods in machine learning. Because you can use CRAN itself as a data source, this contest offers a lot of opportunities to exploit the state-of-the-art in machine learning based on text and network analysis. If you any questions or comments about the contest, please use the contest forum on Kaggle so that others will benefit from the discussion. 	 0 Comments
Add an animated R plot to your LaTeX document	https://www.r-bloggers.com/2010/10/add-an-animated-r-plot-to-your-latex-document/	October 10, 2010	tanvir	" My first introduction with LaTeX was not very pleasant. I got tired and frustrated by writing so many codes for producing a simple document; but a few days ago I could write a function for producing an animated plot in R and also could export it to my LaTeX document with all its animations intact (using animation package of R).  That really made me interested in learning LaTeX; just think a PDF book has animated plots and the readers can rewind and reverse and also can control the motion of the plots just by clicking mouse! It will be very helpful for both the writer and the readers.  Now let’s discuss about producing a very simple animated plot in R. I have three variables X, Y and Z. And I regressed Z on X and Y and extracted the residuals. Now I want to examine the standardized residuals where some attractive animations will help us to visualize their characteristics. To be specific, I want that the residuals with positive values and negative values will be denoted by the letters “P” and “N” respectively, and the size of the letters will increase gradually. The next animation effect will show the largest values of the residuals (both positive and negative) and will denote these two values by the words “maximum (+)” and “maximum (-)” respectively, and the sizes of the words will increase gradually and at the same time will rotate at 360 degrees. These two points will also be pointed by red blinking circles. At last I want to find if there is any standardized residual that falls above 3 or below -3; and if any point does it should be denoted by a text “warning” (whose size will also increase at every animation frame). The function is given below-  It is to be noted that at the first for loop I put 30 animation frames just because I want the letters P and N to increase to size 1.2 after 30 steps. We can change this number to alter the motion of our animation. In the second for loop 45 frames are put for the same reason; and the “if” argument is put because I want the red circles to increase in size for the first 30 animation frames and for the next 15 frames the size will decrease gradually and at last will settle at size 1.5. Look at the “srt” argument in the function “text” in second for loop, it make the text rotate at 8 degrees at each frame. The final frame of the animated plot is like following-  This function will produce a nice animated plot that might be very useful in visualizing and understanding various aspects of our data. This plot does not require any additional package but the package “animation” can be very much useful in producing more complex animated plots. Now we want to add this animated plot to our LaTeX document with all its effects. The package animation has a very useful function named “saveLatex” for this purpose. The function I used for exporting the plot to LaTeX is –   Look at the highlighted parts of the function; these are the additional things I did for exporting the plot to LaTeX.  Here I put “documentclass” as article, make it “beamer” if you want to make a multimedia presentation. The function “outdir=getwd()” will put the outcome files to your working directory (by default it is home folder for Linux and My Documents for windows). The argument “nmax” denote the maximum number of animation frames; here we have 30 frames for first for loop and for the second loop have 45; so, here “nmax” should be at least 75.  This function will produce several files including a PDF file (which will contain the animated plot) and a tex file which you can always modify. To see the animation in the PDF file you need to install Adobe Reader in your computer as no other PDF reader support JavaScript correctly (am I wrong? Is there any open source available?) . Linux users please download Adobe Reader for Linux from
here The PDF file will contain a plot which will initially look like following-

By clicking on the play button we can see the animations, there are also some additional buttons with which we can control the speed of our animation and also can rewind or reverse the animation (For the final frame look at the main image of the article).   It is also possible to convert the plot to a GIF image that we can add to our Microsoft Office PowerPoint or open office impress slide. For this we have to use the “saveMovie” function and for running this function we need to have “imageMagick” installed in our computer. Download and install “imageMagick” from here. It is also possible to convert this animated plot to a flash video file (that we can upload to YouTube). For this purpose, use “saveSWF” function. Softwares like “SWFTools” need to be installed before doing this. Download SWFTools from here here. ""saveMovie"" function adn ""saveSWF"" functions are very much similar to ""saveLatex""; so, for details please read the R animation package help manual. Have fun with R and LaTeX. "	 0 Comments
dcemriS4 0.34	https://www.r-bloggers.com/2010/10/dcemris4-0-34/	October 9, 2010	Brandon Whitcher		 0 Comments
oro.nifti 0.2.1	https://www.r-bloggers.com/2010/10/oro-nifti-0-2-1/	October 9, 2010	Brandon Whitcher		 0 Comments
oro.dicom 0.2.7	https://www.r-bloggers.com/2010/10/oro-dicom-0-2-7/	October 9, 2010	Brandon Whitcher		 0 Comments
A competition to recommend “relevant” R packages – and the future of R	https://www.r-bloggers.com/2010/10/a-competition-to-recommend-%e2%80%9crelevant%e2%80%9d-r-packages-%e2%80%93-and-the-future-of-r/	October 9, 2010	Tal Galili	Update: the competition was just launched. * * * Drew Conway and John Myles Whyte have collected data from (52) R users about the packages they have installed.  The data is now available on github for download and the contest will be run on the kaggle platform. For more details, head over to dataists. And for fun, here is the dependency graph for R packages they have assembled so far:  Since I started getting involved in the R bloggers community, I can recall two major discussion that have attracted more then two bloggers writing about them. The first one was people in the R community arguing against Dr. AnnMaria De Mars post “The Next Big Thing”, where she wrote that “R is an epic fail.”  (my response to it then was the post ““The next big thing”, R, and Statistics in the cloud“) The second one was tackling the question “Is R “that bad” that it should be rewritten from scratch?”.  Many responses went to the post by Ross Ihaka who was arguing for the need to rewrite R from scratch (a very wide spectrum of replies to that can be viewed on the stackoverflow discussion I started on the topic.) And in the past few days I noticed a starting of a cascade of posts, all promoting the post at “dataists“. This leads me to three simple statements: 1) I think it is beautiful that the R community has advocates that defend R’s role in the future of statistics 2) I think it is important that the R community has so many (smart) people (beyond the amazing R core team) who reflects on how R is doing, and of the challenges that the R language and environment will face in the future. 3) I think it is a fascinating thing that the R community is a community of researchers who have the skills to research themselves.  Each community of a discipline can use it’s skill on itself – psychologists may psychoanalyze themselves, WordPress bloggers may write about WordPress, and R users can plan studies and analyse data about themselves – this potential is only beginning to be untapped – and I am excited to see where it might lead in the years to come. 	 0 Comments
Mapping BioStar users onto the world map	https://www.r-bloggers.com/2010/10/mapping-biostar-users-onto-the-world-map/	October 9, 2010	Egon Willighagen		 0 Comments
BioStar users (of the world, unite)	https://www.r-bloggers.com/2010/10/biostar-users-of-the-world-unite/	October 9, 2010	nsaunders	"Egon writes: 
Can someone please plot the BioStar users on a Google Map?
 Sounds like a challenge.  Let’s go.

1. Harvesting user IP addresses
BioStar user profiles (here’s mine) include a location field.  It’s free text and optional, which means that location is missing or inaccurate for many users.  However, if you’re logged into BioStar (and perhaps, if you’re a moderator – I’m not sure), you’ll see a field that says: where “XXX.XXX.XXX.XXX” is either an IP address or, for your own page, the text “this IP address” (assuming your latest activity was from your current machine). IP addresses can be used for geolocation – we’ll see how shortly.  The problem is that they are only present when logged into BioStar, which uses OpenID for authentication.  So to write code which automates the collection of user IP addresses, you’d have to convince BioStar that you were logged in. I’m sure that it’s possible to write code which stores OAuth credentials and sends them to BioStar, but it would take some time to develop.  So instead, I used a very ugly and largely manual approach.  First, I wrote this simple Greasemonkey script: It captures the content of the DIV with class summaryinfo and writes it to the Javascript console.  That content looks something like this: Again, XXX.XXX.XXX.XXX is the IP address. So I opened Firefox, installed the Greasemonkey and Firebug extensions, installed my user script, navigated to the BioStar users page, opened the Firebug console and started clicking through users.  By choosing “Persist” and increasing the console log limit, I was able to record the IP address of each user in the console.  When finished, I copied the console contents to a text file. There is no worse solution, for a bioinformatician, than one that involves manual labour, copy and paste.  Currently, there are 17 pages of users (16 x 35 + 1 x 11 = 571 total).  My file contains 567 of them: at least one did not display an IP address and perhaps I missed a couple.  This is why we learn to script. 2. Location using GeoIP
So how do we find location using IP?  The answer is GeoIP. First, head over to the MaxMind website and download their GeoIP C API.  I installed it (for Ubuntu) like so: GeoIP comes with a free database of countries, located in /opt/GeoIP/share/GeoIP/GeoIP.dat.  I also installed their free city database, as shown above. Next, the Ruby gem for GeoIP: Now, quick and very dirty Ruby code to read the text file containing IP addresses and look them up in the GeoIP database: That prints out a tab-delimited file, which looks like this: 3. Plotting maps using R
Before we go all Google-y, let’s look at plotting geographical data using R.  There are many libraries and mapping solutions, but here’s a simple script to plot our users on a world map.  It requires the packages ggplot2 and maps.  Assuming that the output from the Ruby script is saved in a file, biostar.tab: BioStar user locations 4. Plotting on a Google Map
There are many options for getting data into Google Maps.  I figured that there must be a site where you can upload a simple CSV file containing latitude + longitude and display a Google Map.  There is – it’s called ZeeMaps.  It has many features – some free, some paid – which I’m yet to investigate fully. BioStar users at ZeeMaps Of course, IPs can be spoofed, users move around and the location of a machine might not reflect the location of the user.  However, I think it’s a more reliable geolocation approach than an arbitrary text description.  Now, if I could just automate that IP-harvesting code… "	 0 Comments
Nightlights II	https://www.r-bloggers.com/2010/10/nightlights-ii/	October 8, 2010	Steven Mosher	I’ve modified some routines so that we are always grabbing a roughly equal area regardless of the latitude. Basically, you do this: Simply we feed getExtent a “raster”, a  point, and a “halfLength”  The last parameter tells you how wide and tall your plot will be. So the default of .5 means that your “point” will be bracketed by 1/2 a degree of latitude  (+- 1/2 degree) and the longitude will be scaled depending upon the latitude. At the equator, this entails 1/2 degree in each direction. Toward the pole, I adjust the width to keep the areas generally similar. Note, the underlying data is not globally complete, there are small gaps at the pole. In a better implementation of “getExtent” I would wrap at the extrema. Then I played with colors. Arrg I hate doing color by number:  And by changing the call to “getExtent() I can request a grid that only looks out .25 degrees in latitude and longitude.  	 0 Comments
Creating structured and flexible models: some open problems	https://www.r-bloggers.com/2010/10/creating-structured-and-flexible-models-some-open-problems/	October 8, 2010	VCASMO - drewconway		 0 Comments
A competition to predict popular R packages	https://www.r-bloggers.com/2010/10/a-competition-to-predict-popular-r-packages/	October 8, 2010	David Smith	What makes an R package popular? The number of people that use a given R package is a common point of discussion, but it turns out that it's kind of tricky to get hard and fast data to answer this question. You can look at the “I use this” number on crantastic.org, but that's a self-reported number (many more than 38 people have installed ggplot2, for example). To try and answer this question, Drew Conway and John Myles Whyte have collected data from 52 R users about the packages they have installed, and have provided this data as the basis for building a recommendation engine to predict which R packages are most likely to be installed, based on factors like the number of other packages the package author maintains, what packages it depends on, and so on. When blown out as a matrix of all the packages and users, that turns out to be about 100,000 rows of data. The dependency graph for R packages is quite complex, by the way, as illustrated by the snippet below:   Can you use all this data to predict which R packages are installed the most? That's the challenge of the data hacking competition, which will be launched on Kaggle on Sunday. If you haven't heard of it, Kaggle is a website that hosts many such prediction competitions, encouraging data hackers from around to compete to find the best system to predict things like World Cup winners, Eurovision Song Contest votes and grandmaster chess rankings. Many of the competitors use R to build their models (for example, the winner of the HIV progression used the caret package), and it would of course make sense to use R for this competition. (Hence the Yo Dawg reference.) The competition opens on Sunday and runs for four months. Full details at the link below. dataists: Using Data Tools to Find Data Tools, the Yo Dawg of Data Hacking  	 0 Comments
Data mining competition with R	https://www.r-bloggers.com/2010/10/data-mining-competition-with-r/	October 8, 2010	Larry D'Agostino		 0 Comments
Contest for developing an R package recommendation system	https://www.r-bloggers.com/2010/10/contest-for-developing-an-r-package-recommendation-system/	October 7, 2010	Andrew Gelman		 0 Comments
Nightlights	https://www.r-bloggers.com/2010/10/nightlights/	October 7, 2010	Steven Mosher	"Those who follow the discussions about UHI understand that “nightlights” plays a large role in defining whether or not a station is considered Rural or Urban. In the work of GISS nightlights are determined by looking at the DSMP product. The product is available in 30 arcsecond format. That’s .00833 degrees. The following issue arises. The GHCN metadata is reported out at two resolutions: For the US,  the V3 inventory reports down to the 4th digit. X.yyyy. For the ROW the precision is X.yy. Further, we know that some of the locations are wrong. Herein lies the question. If you take the reading of nightlights from an aliased location or a faulty location, how good is your estimate? I started out looking at this with raster. Of course I didnt read the manual entirely so I spent some time re inventing wheels. In the end I will illustrate two commands and how they help up start down the path to the answer: Very quickly. We download the lights file, untar it and then use a utility I wrote to ‘gunzip” the entire directory. Next we load the raster. It’s huge. Nightlights at 1km. Don’t even bother plotting it, you’ll see clouds of little points. Next, I pull out the cellId for every station. Then using those cells, I pull the value of nightlights at that location. Then I just add them to the inventory. To answer the question about the potential errors we need to look at the neighborhood. I tinkered around with calls to “adjacency” and “focal” but in the end settled on the following. First, extent: e 
 For this command I merely picked the coordinates of the first station: 6.95,36.95. I then bracket the lat and lon. Then we call ‘”crop” r 
 then plot: plot(r) points(36.93 6.95,col=’red’) map(“world”,xlim=c(6,8),ylim=c(36,38),add=T) later I’ll apply an SST mask, but this is what you see:  "	 0 Comments
Pre-ordinary meeting	https://www.r-bloggers.com/2010/10/pre-ordinary%c2%a0meeting/	October 7, 2010	xi'an	Those are the slides for the (basic) introduction of the paper by Mark Girolami and Ben Calderhead at the RSS next week. Not to be confused with my comments on the paper.  	 0 Comments
Students in predominantly ethnic minority classes want segregated education very much. The others don’t.	https://www.r-bloggers.com/2010/10/students-in-predominantly-ethnic-minority-classes-want-segregated-education-very-much-the-others-dont-2/	October 7, 2010	steve	"
 I just did this for what will hopefully be a book chapter on our Divided Education – Divided Citizens research project with NEPC. Explanation further below for anyone more interested in the actual topic   About the graphic:  I like raw-data plots like this, made possible by Hadley Wickhams’s amazing ggplot2 package for the stats package R. Every school class in the survey is on there somewhere. The line types need making a bit more distinct though. Oh, and it would presumably get printed in greyscale, so no colours. I have spent weeks trying to analyse this data properly – spent ages with a stuctural equation model, and then scrapped that and went for a series of mixed effects models because of the class-level variance, but I found trying to implement that kind of model very difficult. In the end I will probably just go for this graph and one very simple regression at class level and perhaps just one mixed effects regression. I think together they say all that needs to be said. In statistics as everywhere else the difficulty is in not saying too much. I am not sure I succeeded here yet, true there is only one graph but it is a pretty complicated one. About the topic: It’s about how support amongst students for segregated education is much stronger in minority schools. Minority students, or rather, students in predominantly minority classes, want segrated education very much. The others don’t. The graph shows mean support for segregated schooling in each class by percentage of majority children in each class. Dots represent individual classes. Lines represent smoothed means per country.  The sample consisted of 2417 students from 134 schools in Bosnia & Herzegovina, Estonia, Latvia and Slovakia. To represent the school-level differences, the means of student support for segregation in each school were calculated along with the percentage of majority students in each school. The figure shows that nearly all the classes had either heavily majority or heavily minority composition; very few classes lie between the 20% and 80% lines, and there are many classes which are 100% majority or minority children. Also, there are rather more minority classes with a few majority children than the other way round. The figure also shows clearly that the larger the proportion of minority students in a school, the stronger the support for segregation. In  Bosnia & Hergovina the children are not such strong supporters of segregation. A linear regression predicting support for segregation at class level on the basis only of country and percentage majority children as predictors confirms this, with a remarkable 63% of the class-level variance explained (adjusted R2). Then another analysis shows that majority children in predominantly minority classes do not differ from their classmates in their strong support for segregation, whereas minority children in predominantly majority classes are a little stronger than their classmates in their support for segregation, but not as strong as their peers in minority classes.       "	 0 Comments
Students in predominantly ethnic minority classes want segregated education very much. The others don’t.	https://www.r-bloggers.com/2010/10/students-in-predominantly-ethnic-minority-classes-want-segregated-education-very-much-the-others-dont/	October 7, 2010	Social data blog	"
 
  I just did this for what will hopefully be a book chapter on our Divided Education – Divided Citizens research project with NEPC. Explanation further below for anyone more interested in the actual topic 😉 About the graphic:  I like raw-data plots like this, made possible by Hadley Wickhams’s amazing ggplot2 package for the stats package R. Every school class in the survey is on there somewhere. The line types need making a bit more distinct though. Oh, and it would presumably get printed in greyscale, so no colours. I have spent weeks trying to analyse this data properly – spent ages with a stuctural equation model, and then scrapped that and went for a series of mixed effects models because of the class-level variance, but I found trying to implement that kind of model very difficult. In the end I will probably just go for this graph and one very simple regression at class level and perhaps just one mixed effects regression. I think together they say all that needs to be said. In statistics as everywhere else the difficulty is in not saying too much. I am not sure I succeeded here yet, true there is only one graph but it is a pretty complicated one. About the topic: It’s about how support amongst students for segregated education is much stronger in minority schools. Minority students, or rather, students in predominantly minority classes, want segrated education very much. The others don’t. The graph shows mean support for segregated schooling in each class by percentage of majority children in each class. Dots represent individual classes. Lines represent smoothed means per country.  The sample consisted of 2417 students from 134 schools in Bosnia & Herzegovina, Estonia, Latvia and Slovakia. To represent the school-level differences, the means of student support for segregation in each school were calculated along with the percentage of majority students in each school. The figure shows that nearly all the classes had either heavily majority or heavily minority composition; very few classes lie between the 20% and 80% lines, and there are many classes which are 100% majority or minority children. Also, there are rather more minority classes with a few majority children than the other way round. The figure also shows clearly that the larger the proportion of minority students in a school, the stronger the support for segregation. In  Bosnia & Hergovina the children are not such strong supporters of segregation. A linear regression predicting support for segregation at class level on the basis only of country and percentage majority children as predictors confirms this, with a remarkable 63% of the class-level variance explained (adjusted R2). Then another analysis shows that majority children in predominantly minority classes do not differ from their classmates in their strong support for segregation, whereas minority children in predominantly majority classes are a little stronger than their classmates in their support for segregation, but not as strong as their peers in minority classes.       Permalink 

	| Leave a comment  »
 "	 0 Comments
Studying joint effects in a regression	https://www.r-bloggers.com/2010/10/studying-joint-effects-in-a-regression/	October 7, 2010	arthur charpentier	"We’ve seen in the previous post (here)  how important the *-cartesian
product to model joint effected in the regression. Consider the case of
two explanatory variates, one continuous (, the age of the driver) and one qualitative (, gasoline versus diesel).       "	 0 Comments
Build a Recommendation System for R Packages	https://www.r-bloggers.com/2010/10/build-a-recommendation-system-for-r-packages/	October 7, 2010	John Myles White	On Dataists, a new collaborative blog for data hackers that I’m contributing to, we’ve just announced a data contest that’s custom made for R users. To win the contest, you need to build a recommendation system for R packages. To find out more, check out the official announcement on Dataists. Then go to GitHub to get the data sets we’re providing, including the official training data set that you should use to build your model. We’re even providing you with a baseline model to get you started. On Sunday, the contest will officially go live on Kaggle, where you’ll want to make submissions to see how your recommendation algorithm compares with other contestants’ submissions. In February 2011, the contest will end and the team with the best system will win three UseR! books of their choosing. Happy hacking! 	 0 Comments
Using Data Tools to Find Data Tools, the Yo Dawg of Data Hacking	https://www.r-bloggers.com/2010/10/using-data-tools-to-find-data-tools-the-yo-dawg-of-data-hacking/	October 7, 2010	johnmyleswhite	"by John Myles White and Drew Conway Editors’ Note: One theme likely to recur on dataists.com is that data hackers love using their tools to analyze, visualize, and predict everything. Data hackers also love discovering and learning about new tools. So it should come as no surprise that Dataist contributors John Myles White and Drew Conway thought to develop a model that can predict which R packages a particular user would like. And in the spirit of friendly competition, they’re opening it up for others to participate!  As part of the kickoff for dataists, we’re announcing a data hacking contest tailored to the statistical computing community. Contestants will build a recommendation engine for R packages. The contest is being administered in collaboration with Kaggle. If you’re interested in the details of the contest, please read on. By sponsoring this contest, we’re hoping to encourage the data hacking community to use its skills to build a recommendation engine that will help R programmers to find the best packages on CRAN, the standard repository for R libraries. Like many data-driven projects, the question has evolved with the availability of data. We started with the question, “which packages are best?” and replaced it with the empirical question, “which packages are used most often?” This is quite a difficult question to answer as well, because the appropriate data set is neither readily available nor can it be easily acquired. For that reason, we’ve settled on the more manageable question, “which packages are most often installed by normal R users?” This last question could potentially be answered in a variety of ways. Our current approach uses a convenience sample of installation data that we’ve collected from volunteers in the R community, who kindly agreed to send us a list of the packages they have on their systems. We’ve anonymized this data and compiled a set of metadata-based predictors that allow us to predict the installation probabilities quite well. We’re releasing all of our current work, including the data we have and all of the code we’ve used so far for our exploratory analyses. The contest itself will go live on Kaggle on Sunday and will end four months from Sunday on February 10, 2011. The rules, prizes and official data sets are all described below. To win the contest, you need to predict the probability that a user U has a package P installed on their system for every pair, (U, P). We’ll assess your performance using ROC methods, which will be evaluated against a held out test data set. The winning team will receive 3 UseR! books of their choosing. In order to win the contest, you’ll have to provide your analysis code to us by creating a fork of our GitHub repository. You’ll also be required to provide a written description of your approach. We’re asking for so much openness from the winning team because we want this contest to serve as a stepping stone for the R community. We’re also hoping that enterprising data hackers will extend the lessons learned through this contest to other programming languages. To get started, you can go to GitHub to download the primary data sets and code. The sections below describe the data sets that you can download and the baseline model you should try to beat. For this contest, there are really three data sets. At the start, you’ll want to download the heavily preprocessed data set that we’ll be providing to you through Kaggle. This data set is also available on GitHub, where it is labeled as training_data.csv. This file contains a matrix with roughly 100,000 rows and 16 columns, representing installation information for all existing R packages and 52 users. The test data set against which your performance will be evaluated contains approximately another 30,000 rows. Each row of this matrix contains the following information: In addition to these central predictors, we are including logarithmic transforms of the non-binary predictors as we find that this improves the model’s fit to the full data set. For that reason, the last five columns of our data set are,
 The Kaggle data set is really the minimal amount of data you should use to build your model. For most users, you’ll quickly want to move on to the raw metadata that we’re providing on GitHub. This second-level data set is contained in several normalized CSV files inside of the data directory: To give you a taste of this richer data set we’re providing, we’ve built a visualization of the suggestions graph found in suggestions.csv: In the graph (above), the package names are sized and colored by in-degree centrality (i.e., larger sized and darker colored nodes have higher centrality), which you can think of as a very rough proxy for importance. If you’re interested in producing similar visualizations of this data, you can use Gephi to produce new graphics like this. To better explore the graph toggle to full-screen mode. For those interested, we’re also providing the R scripts we used to generate the metadata predictors we’re providing, in case you’d like to use them as examples of how to work with the raw data from CRAN. The relevant scripts are: All of the other data sets described earlier were compiled by hand. Please note that these data sets are normalized, so we are also providing preprocessing scripts that build one large data frame that contains all of the information we’ve used to build our predictive model. The lib/preprocess_data.R script performs the relevant merging and transformation operations, including logarithmic transformations that we’ve found improve predictive accuracy. The result of this merging is the training data set that we’re providing through Kaggle. For the truly dedicated, you should consider CRAN itself to be the raw data set for this contest. If you want to use predictors beyond those we’re giving you, you’ll want to download a copy of CRAN that you can work with locally. You can do this using the Perl script, fetch_cran.pl, that we’re providing. To be kind to the CRAN maintainers, this download script sleeps for ten seconds between each step in the spidering process. Obviously you can change this, but please be considerate about the amount of bandwidth you use if you do make changes. Please note: until you are familiar with the preprocessed data sets that we’re providing, we suggest that you do not download a copy of CRAN. For many users, working directly with a raw copy of CRAN will not be efficient. We think this contest can help focus data hackers on an unsolved problem: using our current data tools to help us find the best data tools. We hope you’ll consider participating and even extending this work to new contexts. Happy hacking! If you have further questions about this contest, please direct them to John Myles White. "	 0 Comments
R is Hot: Part 1	https://www.r-bloggers.com/2010/10/r-is-hot-part-1/	October 7, 2010	David Smith	This is Part 1 of a five-part article series, with new parts published each Thursday. You can download the complete article from the Revolution Analytics website.    Much in the same way that social networking, reality TV and craft beer were considered marginal fads before gaining widespread acceptance from the mainstream culture, the fast-growing popularity of R strongly suggests that it is heading toward a similar level of acceptance by the analytic community.  R has already won praise and plaudits from established media outlets such as the New York Times, Forbes, Intelligent Enterprise, InfoWorld and The Register. When you consider that R is a high-level computer programming language designed mostly for quants (the nickname for a subspecies of geeks who focus on quantitative analysis), the adoring media attention seems nothing short of astounding.  So it’s entirely fair to ask: Why all the hoopla? Why is an esoteric programming language created in the early 1990s by two academics in New Zealand suddenly all the rage? Why is R so hot? Let’s examine some of the reasons behind the rising popularity of R. As is the case with almost every new trend, there are underlying economic and social factors – nothing just “happens,” there are always root causes. For example, it’s no secret that our digital information systems generate new data at an unimaginably fast pace – sometimes it seems as though we’re drowning in data. Despite this apparently inexhaustible supply of new data, the perceived value of data is rising, which has led to the development of quicker, better and more powerful methods for analyzing complex sets of numbers.  The current generation of analytic solutions are cumbersome and costly, however, which has opened the door to newer and less expensive techniques for crunching big numbers. Many of these newer and less costly techniques are written in R, which has rapidly become the “common language” of people whose careers or livelihoods are driven by data. “R is the most powerful and flexible statistical programming language in the world,” says Norman Nie, a nationally recognized scholar in the fields of survey research, quantitative social science and political behavior. A co-founder of SPSS in the late 1960s, Nie is currently CEO and president of Revolution Analytics, a company based in Palo Alto that provides commercialized versions of R programs. “What was once a secret of drug-development statisticians at pharmaceutical companies, quants on Wall Street, and PhD-level statistical researchers around the globe (not to mention pioneers at Web 2.0 companies like Google and Facebook) is suddenly becoming mainstream,” says Nie. Robert A. Muenchen, the author of R for SAS and SPSS Users, writes that R has already had a profound impact on research in a variety of fields that rely on quantitative analysis to generate usable information. Since its release in 1996, R has dramatically changed the landscape of research software. There are very few things that SAS or SPSS will do that R cannot, while R can do a wide range of things that the others cannot. Given that R is free and the others quite expensive, R is definitely worth investigating.    Unlike traditional analytic software products, R is a fully-fledged programming language. But R has already evolved into more than just a language – R represents a radically different approach to the challenges posed by increasingly large and complex sets of data.  In that respect, it is something of a cultural phenomenon.  R is an open source project, which means that it depends on a worldwide community of active developers to grow and evolve. Like Linux, the most famous open source project, R isn’t “owned” by any single person or entity. R is maintained and supported by thousands of individuals who use it and who contribute to its ongoing development. The members of this global community serve as R’s parents and custodians – and they take their responsibilities seriously. Like doting parents, they take pride in the achievements of their offspring – and they are quick to leap in when they perceive a problem. “I can’t think of any programming language that has such an incredible community of users,” says Mike King, a quantitative analyst at Bank of America. King uses R to write programs for capital adequacy modeling, decision systems design and predictive analytics. “If you have a question, you can get it answered quickly by leaders in the field. That means very little downtime.”  Continued Thursdays   	 0 Comments
LondonR Rcpp slides	https://www.r-bloggers.com/2010/10/londonr-rcpp-slides/	October 7, 2010	romain francois	"I’m just back to london where I presented about Rcpp at 
mango‘s LondonR event.  This was the third time (after rmetrics and useR!) I presented these slides, so I allowed myself some new metaphores about my long term relationship with R and my indiscretions with other languages such as C++. I’ve uploaded my slides to my slideshare account:  I had some time to browse around in South Bank and Covent Garden before the event. I took some pictures from my iphone "	 0 Comments
Science is vital – what we don’t know yet	https://www.r-bloggers.com/2010/10/science-is-vital-%e2%80%93-what-we-don%e2%80%99t-know-yet/	October 6, 2010	respiratoryclub	This post is not about R (for a change).  For working UK scientists, science is vital – sign the on-line petition to preserve science funding. For my contribution of what we don’t know yet – We don’t know whether we can use biomarkers of kidney injury to personalise the doses of medications to maximise the dose for the patient whilst minimizing any renal side effects. 	 0 Comments
Creating GUIs in R with gWidgets	https://www.r-bloggers.com/2010/10/creating-guis-in-r-with-gwidgets/	October 6, 2010	richierocks	The gWidgets framework is a way of creating graphical user interfaces in a toolkit independent way.  That means that you can choose between tcl/tk, Gtk, Java or Qt underneath the bonnet. There’s also a web-version based upon RApache and ExtJS. Since the code is the same in each case, you can change your mind and swap toolkits at a later date, without having to rewrite everything.  Different versions of the toolkit are in different states of development; Gtk is the most complete, but the tcl/tk and Java versions are usable. The Web version has had a recent rewrite, which I haven’t used so I can’t vouch for it’s status.  Finally, the Qt version is still experimental (and not yet available on CRAN). Personally, I use the tcl/tk version, since all the necessary components ship with the Windows edition of R. The framework is  fairly high level, making it quick for prototyping user interfaces.  The drawback is that you don’t get quite as much control over the styling of your interface.  If you need finer control, you may prefer one of the lower level packages: RGtk2, tcltk or rJava.  In those cases, you will lose the toolkit independence. To learn how gWidgets works, we’ll build a dialog box with controls to upload a tab delimited file.  To begin, we load the necessary packages. The textboxes and checkboxes and so forth that we need are known as widgets (hence “gWidgets”).  They need to be contained inside a window, which we create using the function gwindow. By default, the widgets will be stacked up vertically.  We can create groups of widgets that are stacked horizontally with ggroup(which is a widget in itself).  Notice that all widgets must specify their container; in this case it’s just the window. A glabel is a widget that represents a text label.  Notice that it is contained inside the group we just created. A gedit is a single line textbox.  (Not to be confused with a gtext, which is a multiline textbox.) Another horizontal group, for the upload button. For widgets that we want to respond to an action, we need to add a handler argument.  This is always a function accepting a list as its first argument (named h by convention), and dots.  The gbutton handler is called whenever the button is clicked.  Don’t worry about the contents of the handler function for now; we’ll add them in a moment. Since tab delimited files can have decimal places represented as full-stops or commas (depending upon local conventions), we need a checkbox to choose between cases.  We define a function to get the default value from the system locale settings.  Conveniently, checkboxes have their own label built-in so we don’t need to create our own this time. The last widget we’ll include is a status bar, so that users don’t have to refer back to the R main window for messages. Finally, here’s the content for the button handler.  It creates a file open dialog box, which in turn has its own handler function.  The action argument names the function to be applied to the file that is opened.  The svalue function returns the “most useful thing” from a widget.  For a checkbox, the svalue is whether or not it is checked.  For a textbox or status bar, the svalue is its text.  The filter argument populates the “Files of type” drop down list in the file open dialog. And there we have it.  If you’re feeling enthusiastic, see if you can adapt this to work with CSV files, or even better a general delimited file. One last trick to finish the post:  You can create a GUI interface to any function using ggenericwidget.  Try 	 0 Comments
R is Hot	https://www.r-bloggers.com/2010/10/r-is-hot/	October 6, 2010	David Smith	Our mission at Revolution Analytics is to make R the statistical analysis tool of choice in the workplace. But even though R is pervasive in academia and rising in popularity generally, we still sometimes get blank faces when we demonstrate R to potential new clients. Sure, most people have heard of R — it's been hard to miss in the news lately — but some haven't yet heard how leading-edge companies are using R today to improve their data analysis processes and how they communicate the results.  That's why we started a project to document how R is Hot. We interviewed a dozen R users, mostly from industry but also from the R user community generally, to try and understand what makes R different, and how you can do things with R that just can't be done in other statistical software. The result is a story, told in 8 pages, of how a statistical programming language Invented in New Zealand became a global sensation. The article tells how R is more than just a programming language: its power and elegance are helping companies build their business around modern data analysis practices. The fact that there's no need to reinvent the wheel (every statistical analysis you might ever need is already there) and that R makes high-quality data visualization easy, has given R critical mass and seen it go viral, being widely used around the world. But R isn't standing still: as a vibrant, open-source project it is changing, transforming and evolving to keep up with the latest advances in data analysis. Starting tomorrow and over the next five weeks, we'll be serializing the article here on the blog each Thursday, and you'll be able to find new posts in the R is Hot section. If you want to get the scoop early, you can download the full article now from the Revolution Analytics website at www.revolutionanalytics.com/R-is-Hot (in exchange for your email address and the opportunity to sign up for our monthly newsletter). We've released the content under a Creative Commons license, so please feel free to share these stories about R with others. I'd like to give thanks to everyone who participated in interviews for this project, including:     I'd also like to give a special thanks to Mike Barlow who conducted the interviews and was instrumental in putting this article together. We hope you enjoy the article, and let us know what you think in the comments. Revolution Analytics: R is Hot 	 0 Comments
Belgian Astronomers and Exercise Machines	https://www.r-bloggers.com/2010/10/belgian-astronomers-and-exercise-machines/	October 6, 2010	C		 0 Comments
Convert decimal to IEEE-754 in R	https://www.r-bloggers.com/2010/10/convert-decimal-to-ieee-754-in-r/	October 6, 2010	Todos Logos		 0 Comments
How fast is JAGS?	https://www.r-bloggers.com/2010/10/how-fast-is-jags/	October 6, 2010	jackman	From Martyn Plummer, on the JAGS news blog.  Key graph below, showing a few outlying cases in which JAGS is substantially slower than OpenBUGS, but generally, JAGS performs quite favorably. Key point from Martyn: Incidentally, these figures are for JAGS with the glm module loaded. The glm module is not loaded by default.  If you are not using it for generalized linear mixed models then you might be missing out.  	 0 Comments
Typos…	https://www.r-bloggers.com/2010/10/typos%e2%80%a6/	October 5, 2010	xi'an	  Edward Kao just sent another typo found both in   Monte Carlo Statistical Methods (Problem 3.21) and in Introducing Monte Carlo Methods with R (Exercise 3.17), namely that  should be  I also got another email from Jerry Sin mentioning that matrix summation in the matrix commands of Figure 1.2 of Introducing Monte Carlo Methods with R should be matrix multiplication. And asking for an errata sheet on the webpage of the books, which is clearly necessary and overdue! Here are also a few more typos found by Pierre Jacob and Robin Ryder when working on the translation of Introducing Monte Carlo Methods with R: – on page 153, “step” should be replaced with “iteration” in the first paragraph; – on page 154 in  Example 5.14, the parenthesis ends up after “equal to 0″; – on page 156, in Example 5.15,  “likelihood surface” should be “log-likelihood surface”; – on page 158, in Example 5.16, “or, equivalently, by” should be “or, equivalently, with”; – on page 162, in the caption of Figure 5.14,  “MLE estimator” should be “MLE”; – in Algorithm 8, page 206, there are two commas before given; – in the caption of Figure 7.6, these are the allele probabilities, not the genotype probabilities; – in Exercise 7.22 c, the matrix is positive definite if and only if the condition is satisfied; – on page 239, “the slower chain” should be “the slowest chain”; – on page 241, “you can load all coda functions” instead of “…download…”; – in Exercise 8.1, we compare an estimator  with , not ; – in the caption of Figure 8.2, the upper quantile is a 97.5% quantile, not a .975% quantile; – in Exercise 8.11, the  at the end of page 267 should be ; – in Exercise 8.16, jpg is mistakenly qualified to be open (!). 	 0 Comments
The Data Science Venn Diagram	https://www.r-bloggers.com/2010/10/the-data-science-venn-diagram/	October 5, 2010	David Smith	"Whenever I'm asked, “Who uses R?”, I usually rattle off a long list of job titles: statistician, analyst, quant, researcher … and that's before all the domain-specific titles. It would be nice if there were a simple, succinct phrase to describe the process of working with, analyzing, and communicating with real data. At the new blog, “dataists“, the inaugural post by Hilary Mason and Chris Wiggins describes a new term which seems like it fits the bill: Data Scientist. Personally, I like the term, as it encompasses more than “mere” data analysis, which can sometimes imply a black-box approach. The dataists describe a process for data science: Obtain, Scrub, Explore, Model, and Interpret; steps which, when taken together, allow the data scientist to tell a complete story about discoveries they have made in the data. The post describes each of these steps in detail and is well worth a read. R blogger Drew Conway takes this concept a set further with his Venn Diagram of Data Science: 
  Data Science is right there at the middle, combining the skills of Hacking, Expertise, and Math/Stats Knowledge. I especially like the way it highlights the danger of applying statistical tools (including R) to an applied problem without a rigorous statistical background. Drew highlights this Danger Zone and other aspects of Data Science in his post, which you should also check out. dataists: The Data Science Venn Diagram "	 0 Comments
Example 8.8: more Hosmer and Lemeshow	https://www.r-bloggers.com/2010/10/example-8-8-more-hosmer-and-lemeshow/	October 5, 2010	Ken Kleinman		 0 Comments
India Australia test cricket matches over the years	https://www.r-bloggers.com/2010/10/india-australia-test-cricket-matches-over-the-years/	October 5, 2010	prasoonsharma		 0 Comments
Why R is better than Excel for teaching statistics	https://www.r-bloggers.com/2010/10/why-r-is-better-than-excel-for-teaching-statistics/	October 4, 2010	Rob J Hyndman	This was the topic of a recent conversation on the Australian and New Zealand R mailing list. Here is an edited list of some of the comments made. Further comments on this theme are available at the following sites:   	 0 Comments
S4 classes in R: printing function definition and getting help	https://www.r-bloggers.com/2010/10/s4-classes-in-r-printing-function-definition-and-getting-help/	October 4, 2010	Vinh Nguyen	I’m not very familiar with S4 classes and methods, but I assume it’s the recommended way to write new packages since it is newer than S3; this of course is open to debate. I’ll outline my experience of programming with S4 classes and methods in a later post, but in the mean time, I want to write down some notes on how to get help (via ? in R) and getting function definitions from S4 methods.   For S3 classes and methods, suppose I want to learn more about a certain method, say print of some class. Let’s use class lm as an example. I could type ?print.lm to get documentation on the function, and type print.lm in the R console to get the function definition printed. This allows me to learn more about the method and learn from them (perks of open source!). To recap:   However, with S4, this is not the class. I’ve used a few packages that are written in S4 and could not get documentation open within R and get the function definitions printed to learn about the underlying code based on the previous techniques. As I learn to write and document an R package based on S4, I read this section of the R manual for writing packages. I misinterpreted the reading and thought to get help on a method I had to type something like methods?generic,signature_list-method to get help. However, I received an error due to the - symbol (it’s an operator in R). I believe the stated convention is just for the documentation piece of S4 methods in the .Rd files. After some more searching, I came across this link (examples section) that showed me how to get help. Let’s illustrate with the show method (S4′s equivalent of the print method) for the mer class in the lme4 package.   For our particular example, the show method for the mer class calls a printMer function in the lme4 namespace. Thus, we need to call lme4:::printMer to see the definition.   Hope this out others out there.  	 0 Comments
Use R to Analyze Players for your Fantasy Hockey League	https://www.r-bloggers.com/2010/10/use-r-to-analyze-players-for-your-fantasy-hockey-league/	October 4, 2010	--	I am in a fantasy hockey league for the first time this seasons and I wanted to use R to analyze players.  Since I am relatively new to R, I am quite certain this code could be improved.  The code below is functional, however, and while this isn’t my complete analysis, I think it outlines how powerful R truly is. NOTE:  Have a wordpress.com and post R code?  Check out this post: http://www.r-statistics.com/2010/09/r-syntax-highlighting-for-bloggers-on-wordpress-com/ 	 0 Comments
Max Heart Rate Calculations Compared	https://www.r-bloggers.com/2010/10/max-heart-rate-calculations-compared/	October 4, 2010	C		 0 Comments
Means By	https://www.r-bloggers.com/2010/10/means-by/	October 4, 2010	Tal Galili	"The other day I was asked by a coworker hos to do a SAS Means By statement in R.  I embarrassingly did not know how to so I wrote something up, and this is what I came up with, it takes a data.frame and an indexing variable and computes the means for each group defined by INDEX.  This isn’t a solution that I’m terribly happy about since is involves unlisting and transposing matrices, and what not. I got to thinking that someone out there will know how to do this so I put it on StackOverflow hoping that someone out there will know of someone who has faced and addressed this issue before.  It is a common type of statistics question to ask so I have a hard time believing that everyone does this sort of thing every time.  Please post responses on StackOverflow. Update:
Thanks to the people on StackoverFlow I found the answer that I was looking for aggregate (also mentioned was a similar plyr function ddply), an interesting note that I found is that neither of these are as intelligent as I would like.  For instance if the variable you are using is inside the data frame, it still tries to compute the aggregating function over the variable, where it would make sense to exclude the variable.   It would also make sense to assume that the variable could be in the data frame. Here iagg is an intelligent aggregate function that makes those assumptions:  1. grouping variables will be able to be part of the data frame. 2. grouping variables are excluded from the computations. 3. grouping variables are added back appropriately named with the unique values. The full code fore iagg is here "	 0 Comments
Code Snippets in Revolution R	https://www.r-bloggers.com/2010/10/code-snippets-in-revolution-r/	October 4, 2010	David Smith	Ajay Ohri has been trying out the Code Snippets feature in Revolution R (available free to academics), and has found it handy for writing code for statistical analysis: Now even if you are averse to using a GUI /or GUI creators don’t have your particular analysis you can basically type in code at an extremely fast pace. It is useful to people who do not have to type in the entire code, but it is a boon to beginners as the parameters in function inserted by code snippet are automatically selected in multiple colors. If you want to see Code Snippets in action, check out the video below by Revolution's Sue Ranney.    Sue starts working with Code Snippets at around the 1:45 mark. DecisionStats: Using Code Snippets in Revolution R 	 0 Comments
The ARIMAX model muddle	https://www.r-bloggers.com/2010/10/the-arimax-model-muddle/	October 4, 2010	Rob J Hyndman	"There is often confusion about how to include covariates in ARIMA models, and the presentation of the subject in various textbooks and in R help files has not helped the confusion. So I thought I’d give my take on the issue. To keep it simple, I will only describe non-seasonal ARIMA models although the ideas are easily extended to include seasonal terms. I will include only one covariate in the models although it is easy to extend the results to multiple covariates. And, to start with, I will  assume the data are stationary, so we only consider ARMA models. Let the time series be denoted by . First, we will define an ARMA model with no covariates:
 An ARMAX model simply adds in the covariate on the right hand side:
 If we write the model using backshift operators, the ARMAX model is given by
 For this reason, I prefer to use regression models with ARMA errors, defined as follows.
 Using backshift operators, this model can be written as
 Transfer function models Both of these models can be considered as special cases of transfer function models, popularized by Box and Jenkins:
 Sometimes these are called “dynamic regression models”, although different books use that term for different models. The method for selecting the orders of a transfer function model that is described in Box and Jenkins is cumbersome and difficult, but continues to be described in textbooks. A much better procedure is given in Pankratz (1991), and repeated in my 1998 forecasting textbook. For ARIMA errors, we simply replace  with  where  denotes the differencing operator. Notice that this is equivalent to differencing both  and  before fitting the model with ARMA errors. In fact, it is necessary to difference all variables first as estimation of a model with non-stationary errors is not consistent and can lead to “spurious regression”. The arima() function in R (and Arima() and auto.arima() from the forecast package) fits a regression with ARIMA errors. Unfortunately there is a bug in the code so that if a non-stationary error is specified, the data are not differenced before estimation of the regression coefficients. Consequently, you should do your own differencing when required. Note also that R reverses the signs of the moving average coefficients compared to the standard parameterization given above. The arimax() function from the TSA package fits the transfer function model (but not the ARIMAX model). This is a new package and I have not yet used it, but it is nice to finally be able to fit transfer function models in R. Sometime I plan to write a function to allow automated order selection for transfer functions as I have done with auto.arima() for regression with ARMA errors (part of the forecast package)   "	 0 Comments
A tale of two returns	https://www.r-bloggers.com/2010/10/a-tale-of-two-returns/	October 4, 2010	Pat	"It was the best of times, it was the worst of times. As you may have guessed, this is a mashup of a novel by Charles Dickens and an explanation of financial returns. The key plot element of A Tale of Two Cities is that there are two men, Charles Darnay and Sydney Carton, who look almost identical.  They are different, but in some respects interchangeable. Simple returns and log returns are different, but in some respects interchangeable. Traditionally simple returns are denoted with a capital R and log returns with a lower-case r.  These are defined as: Rt = (Pt – Pt-1) / Pt-1 = Pt / Pt-1 – 1 rt = log(Pt / Pt-1) = log(Pt) – log(Pt-1)
 where Pt is the price of the asset at time t.  We are defining the return from time t-1 to time t.  The log function here is the natural logarithm. In the R language if you have a vector of prices (for the same asset at different times), you can compute the simple returns with: > R  It is slightly different if you have a price matrix (with times in the rows and assets along the columns): > R  For log returns you can do the same command in either case: > r  Caveat: This assumes the positions are all long.  Read on for an explanation of returns for short positions. We have two similar concepts and we need to distinguish between them.  Unfortunately this is about as messy as the French revolution in which our Dickensian characters are embroiled. First off, some would think that what we are calling “return” is an abbreviation of “rate of return”. Table 1: Names for return concepts. I find it hard to believe that Table 1 lists all the words that are used.  Let me know if there are others. Extreme care is needed. Some of these words are used inconsistently by different people. The word “total” can (and in my experience more probably does) mean the return that includes dividends as well as prices.  A (simple) total return in this sense is: Rt = (Pt + Dt – Pt-1) / Pt-1 where Dt is the dividend or interest that is paid between times t-1 and t.  This concept is sometimes expressed as the “overall” return. The terms “gross” and “net” are more commonly used to mean something like before and after tax. Take-away message: There is no magic potion.  You’ll need to threaten violence on whomever is speaking until they make clear what they mean. It is easy to convert one type of return into the other.  Shortly we’ll see why that is useful. To go from simple to log returns, do: r = log(R + 1) To go from log return to simple return, do: R = exp(r) – 1 These formulas work exactly as is in R — whether the returns are vectors or matrices. Figure 1: Comparison of simple and log returns.  Figure 1 compares simple and log returns.  It shows that log returns are always smaller than simple returns.  (You can remember which is smaller because the smallest possible simple return is -100%, that is minus infinity for the log return.)  For daily returns or intraday returns the differences between the two types are going to be trivial, generally. The two types act very differently when it comes to aggregation.  Each has an advantage over the other: The simple return of a portfolio is the weighted sum of the simple returns of the constituents of the portfolio. The log return for a time period is the sum of the log returns of partitions of the time period.  For example the log return for a year is the sum of the log returns of the days within the year. Physicists tend to think that time only goes one way.  In finance time can go backwards — you can short an asset. Or you can think of it as having been misled so far.  What has been identified as the initial time in the return formulas should really be called the buying time, and what has been identified as the final time should be the selling time.  When we short an asset, the buying time is after the selling time. The log return of a short position is the negative of the log return of the long position.  The relationship of the simple return of a short position relative to that of the long position is a little more complicated: -R / (R + 1) The computations we saw before in the R language assume that the positions are long and the times are in order.  When computing log returns of short positions, you can always just take the negative of the computation for the long positions.  For simple returns, you have some choices. The return you experience for an asset that is priced in a foreign currency depends both on the return in the foreign currency and the return of that currency relative to your home currency. There is a simple relationship for log returns: it is the sum of the log return of the asset in the foreign currency and the log return of the currency.  The formula for simple returns is slightly complicated. If you have the return for euros per dollar, then what is the return for dollars per euro?  Again, it is easy for log returns: one is the negative of the other. The material in this post is fairly pedestrian, and most people in finance at least sort of know a lot of it.  So why is this information so hard for a novice to find? A Tale of Two Cities ends with Carton taking the place of Darnay in prison and at the guillotine.  He does this for the woman that they both love who chose Darnay and not Carton to marry.  The final sentence of the novel is the thoughts of Carton just before he is executed: It is a far, far better thing I do, than I have ever done; it is a far, far better rest that I go to, than I have ever known. To create Figure 1 for your own consumption with R you might do commands like:

> retseq 
> plot(retseq, log(retseq + 1), type='l')
> abline(0,1, lty=2)
 The commands that actually created Figure 1 were:

> retseq 
> plot(retseq, log(retseq + 1), type=""l"",
+    axes=FALSE, col=""blue"", lwd=3, xlab=""Simple return"",
+    ylab=""Log return"")
> axis(1, at=c(-.5, 0, .5), label=c(""-50%"", ""0%"", ""50%""))
> axis(2, at=c(-1, -.5, 0, .5),
+    label=c(""-100%"", ""-50%"", ""0%"", ""50%""))
> box()
> abline(0,1, lty=2, col=""gold"")
 "	 0 Comments
V3 Station mash up	https://www.r-bloggers.com/2010/10/v3-station-mash-up/	October 3, 2010	Steven Mosher	Hmm, lets see how a fusion table works with map maker. The process works like this. We read in the GHCN inventory file into R. ( see code below ) We then clean it up ( the names fields and missing values) and we output it to a  *csv  file. Then open the csv in excell and save it as XLS. ( I can avoid this step in the future by exporting xls direct ) Anyways, when you have an xls, you import that into a Fusion table. http://tables.googlelabs.com/DataSource?dsrcid=267862 Then we can select or filter for Mountain valleys and export the KLM link: View Larger Map View Larger Map View Larger Map 	 0 Comments
More BLAS, BLASter, BLAStest: Updates on gcbd	https://www.r-bloggers.com/2010/10/more-blas-blaster-blastest-updates-on-gcbd/	October 3, 2010	Thinking inside the box	"
There is now a new version 0.2.4 of gcbd on CRAN. 
I revised the paper ever so slightly based on some more feedback, and
focussed the results sections by concentrating on just the log-axes lattice
blot and the corresponding lattice plot of raw results—where the y-axis is
capped at 30 seconds:

 
 


This chart–in levels rather than using logarithmic axes is done
here–nicely 
illustrates just how large the performance difference can be for for matrix
multiplication and LU decomposition. QR and SVD are closer but accelerated
BLAS libraries still win. GPUs can be compelling for some tasks and large
sizes.

 
More discussion is still available in the 
paper which is
also included in the gcbd package
for R.
 

 "	 0 Comments
Le Monde puzzle [34]	https://www.r-bloggers.com/2010/10/le-monde-puzzle-34/	October 3, 2010	xi'an	Since the puzzle in this week (-end) edition of Le Monde is not (easily) solvable via an R program, I chose to go back to an older puzzle that my students can solve. Eleven [distinguishable] token are [arbitrarily] distributed around a 200 meter perimeter-long ring. They all start moving at the same speed, 18km/h, in arbitrary directions. When two tokens hit one another they instantaneously reverse directions while keeping the same absolute speed. How long does it take for all tokens to be back in their initial positions? The logical [non-computational] solution was given in an earlier edition: If all tokens move in the same direction, it takes 60×60/(18×5)=40 seconds to complete a turn and hence to see all tokens back to their initial positions. If they are moving into different directions, hits have no impact on the distribution of the tokens when they are not distinguished. Therefore, after 40 seconds, the repartition of the tokens is the same as at time 0, except for a permutation of the individual tokens. Given that there are 11 tokens, it takes at most 11 permutations to see all tokens back to their initial positions, i.e. 440 seconds. Here is the basic R resolution with a dynamic rendering of the moving tokens. It obviously gives 440 seconds as its output. 	 0 Comments
ICOADS	https://www.r-bloggers.com/2010/10/icoads/	October 3, 2010	Steven Mosher	Just a short update on ICOADS. I started out an ICOADS project with grand plan to reprocess the observation files into a “raster” format. That ends up being a really big job. Along the way I had to learn a new package Rcurl and relearn some old Unix skills for compiling RCurl. Along the way I found more R tricks for doing things but I hit a brick wall on downloading the Icoads observation data. ( my problem, not the data supplier ). During the course of getting the observation data i hit some problems with the FTP and so I wrote to Scott Woodruff of NOAA, Scott was very helpful and pointed me at some great resources and documentation. Currently, some work is being completed around the various datasets, so it makes sense for me to delay my project, but I was able to use some of the assets Scott pointed me to to put together a quick animation of ICOADS data, all in R. I thought it would be an easy task, maybe 1 hour, if that. Let’s go to the code and I’ll explain some of the issues. It ended up taking 3 days. All that is pretty straightforward. What’s hidden is the details. ICOADS is so large that in R it has to stay on disk. That means to get certain values we require for plotting we have to perform functions on the disk file which are painfully slow in R on a MAC. Functions like “setMinMax” and the function “rotate.” Because ICOADS uses 0-360 in longitude that has to be transformed to -180 to 180 to work with world map plots. That process is painfully slow, and once the file is read into memory, everything in R on a MAC grinds to a halt. There are some ways around this which I’ll explore, but for now I’ll just  use one approach to get the data out and build an animation from it. Note, the data goes from 0-360 in longitude. That can be changed with a “rotate” command. First off, I’ve had to modify the sourcecode for the animation package. Since this is R that’s easy, I just download the source. The animation package works like this. First, a call is made to ani.options() where options are set for the animation, including the names of output directories. Then a call to ani.start() is made. In this call the standard package will create those directories and do some other set up work… INCLUDING changing the working directory. Next the package expects you to call a plot routine. But when we are plotting a brick that is on disk, our access path is tied to the working directory, which the call to ani.start() has changed. So the plot will fail to find the file. There is a way to get around this problem in raster that is currently being addressed, but I decided to change the animation code as well because I’m not too keen about a design that changes directories and imposes API ordering. Anyway, ani.stop() changes directories back to finish the work. ani.stop() basically uses the ani.options() to output a html file. You click on that file and  you will get the animation to play. I modified the ani() functions to remove the changing of directories and made a few other minor changes and I renamed them Ani.option(),Ani.start() and Ani.stop(). the code needs some clean up, and credit given, but it functions. Autobrowse has been removed as an option. When you execute that code you get 50 frames. In the entire brick there are over 2500 frames of data so if you want to animate the whole thing, it takes a while. Then we will need to add “credits” for the ICOADS team and we have a final version..A couple screen caps below   	 0 Comments
Plotting time vs date in R	https://www.r-bloggers.com/2010/10/plotting-time-vs-date-in-r/	October 2, 2010	datadebrief		 0 Comments
Here is how to improve your charts, graphs, maps, and…	https://www.r-bloggers.com/2010/10/here-is-how-to-improve-your-charts-graphs-maps-and/	October 2, 2010	Isomorphismes	Here is how to improve your charts, graphs, maps, and plots: For example, here’s how he would use the eraser, not the pen to improve on the typical bar chart or histogram.  (3-D bar charts are right out.)  Additionally, Tufte wants news publications to use sophisticated graphics that let the data tell their intricate story, rather than simplistic graphics that attempt to “dazzle” the viewer. Lastly, regarding wide versus tall graphics: 	 0 Comments
ProjectTemplate Version 0.1-3 Released	https://www.r-bloggers.com/2010/10/projecttemplate-version-0-1-3-released/	October 2, 2010	John Myles White	I’ve just released the newest version of ProjectTemplate. The primary change is a completely redesigned mechanism for automatically loading data. ProjectTemplate can now read compressed CSV files, access CSV data files over HTTP, read Stata, SPSS and RData binary files and even load MySQL database tables automatically. For my own projects, this is a big step forward. To access the more esoteric data sources like remote datasets and MySQL databases, the end user only needs to provide a YAML file that specifies a few details about the data source that you’ll be accessing. Hopefully the approach I’ve taken works for a large range of problems. If you’re interested in data available over HTTP, a sample configuration file, called a.url is shown below: And for those interested in accessing data from MySQL, a sample configuration file, called b.sql is shown below: My inspiration for these changes came from two people that I’d like to thank: Diego Valle-Jones and David Edgar Liebke. A month ago, Diego submitted a patch for load_data.R that added RData and compressed CSV file type support. At that time, I started thinking about how to make a more extensible data loader, but wasn’t able to return to the topic until this week. Last night, while I was reading David’s very helpful tutorial on Incanter, I realized that ProjectTemplate could automate many more types of data loading. I hope that I’ve made load_data.R capable of least some of the magic that Incanter’s get-dataset does. The full list of file types that is now supported is shown below: The other major change to ProjectTemplate in this release is that many fewer packages are now being loaded or even installed by default. I am not sure whether this is the ideal practice moving forward, but it was explicitly requested by a user. I’ve decided to see how the change is received by other users before making a final design decision. If you have strong views for or against this change, please speak up here or on the Google Groups mailing list. 	 0 Comments
>_ StatET, eclipse plug-in for R	https://www.r-bloggers.com/2010/10/_-statet-eclipse-plug-in-for-r/	October 2, 2010	oneliner	"Install the plug-in is quite simple, we only need to follow the instructions that we’ll find at the project’s page: http://www.walware.de/goto/statet The requirements are Java 5 (1.5) or higher, Eclipse IDE and the R package: rJava. The rj package is also recommended. In my case, I have installed Eclipse Helios for PHP (eclipse-php-helios-linux-gtk-x86_64.tar.gz). Eclipse download page: http://eclipse.org/downloads Install Java using for example synaptics and then run >_$ sudo R CMD javareconf Install the R package “rJava” as usually, i.e into R: > install.packages(“rJava”); Open Eclipse, go to: help menu > install new software,  add  the repository for our version: Configure the R environment, at the menu option: Window > Preferences and unfold the Run/Debug option, in ‘R environments’ add an environment  we can select ‘try find automatically’ for our R Home variable, in my case /usr/lib/R and select the proper arch (32 or 64 bits) and the other options. In order to use the console inside eclipse we need to do few adjustments: Go to menu: Run > Run configurations, double click in ‘R console’, give a name in main tab and select RJ console, here we can select the working directory and add start up options like –silent that
causes R to start up without printing messages  then we are ready to play with R in our Eclipse IDE. "	 0 Comments
Typo in Example 5.18	https://www.r-bloggers.com/2010/10/typo-in-example-5-18/	October 2, 2010	xi'an	Edward Kao pointed out several typos in Example 5.18 of Monte Carlo Statistical Methods. First, the customers in area i should be double-indexed, i.e.  which implies in turn that . Then the summary T should be defined as  and  as  given that the first m customers have the fifth plan missing. 	 0 Comments
Millionaire’s advice	https://www.r-bloggers.com/2010/10/millionaire%e2%80%99s-advice/	October 2, 2010	kafka	"Viktor Uspaskich is euro-parliamentarian delegated by Lithuania. His was born in Russia, Arkhangelsk Oblast and later on he moved to Lithuania where he made his fortune and first millions.
Recently, I saw an interview with him and I found interesting to test his claim, that  gold and oil are negatively correlated. Meaning that, when the price of gold is appreciating, the price of oil has to depreciate and opposite.
First of all lets check simple correlation between daily price of gold and oil, from 2003 to 2010.  Surprise, surprise – here’s correlation, but it is positive (opposite, that millionaire adviser was claiming)  . It is not strong, only 0.3 or 30% and r-square 0.0919. But lets move on – maybe he didn’t reveal all facts.
I used all my data points in the first graph to get correlation. The other way of looking at data is roll the window of 252 data points and calculate moving correlation. Here we go:  It is clear, that during 4 years correlation between gold and oil was positive and in some case was more than 50%. No luck.
Let’s try something different – lagged correlation. That is – one day the price of the gold goes up/down and x days later the price of oil will go down/up. Here we have:  The graph shows correlation between two assets with lagged 30 days and moved front 30 days. The highest value 30% is on day 0 (the same day), the rest is below 10%, with means here’s no hidden correlation between lagged data. Summary: negative correlation between gold and oil doesn’t exist. So, welder’s who became millionaire advice is not worth a penny. "	 0 Comments
head and tail for strings	https://www.r-bloggers.com/2010/10/head-and-tail-for-strings/	October 2, 2010	Karsten W.	The functions head and tail are very useful for working with lists, tables, data frames and even functions.But they do not work on strings. It is easy to define such functions and start using them: It is not a good idea to name these functions head.character and tail.character because this has unexpected effects if applied to a vector of strings. Other useful string functions are defined in the the stringr package. 	 0 Comments
CouchDB and R	https://www.r-bloggers.com/2010/10/couchdb-and-r/	October 2, 2010	Christopher Bare	Here are some quick crib notes on getting R talking to CouchDB using Couch’s ReSTful HTTP API. We’ll do it in two different ways. First, we’ll construct HTTP calls with RCurl, then move on to the R4CouchDB package for a higher level interface. I’ll assume you’ve already gotten started with CouchDB and are familiar with the basic ReST actions: GET PUT POST and DELETE. First install RCurl and RJSONIO. You’ll have to download the tar.gz’s if you’re on a Mac. For the second part, we’ll need to install R4CouchDB, which depends on the previous two. I checked it out from GitHub and used R CMD INSTALL. That’s nice, but we want to get the result back as a real R data structure. Try this: Sweet! One way to add a new record is with http PUT. Notice that RJSONIO has no high level PUT method, so you have to fake it using the costumrequest parameter. I'd never have figured that out without an example from R4CouchDB's source. The API of libCurl is odd, I have to say, and RCurl mostly just reflects it right into R. If you don't like the idea of sending a put request with a get function, you could use RCurl's curlPerform. Trouble is, curlPerform returns an integer status code rather than the response body. You're supposed to provide an R function to collect the response body text. Not really worth the bother, unless you're getting into some of the advanced tricks described in the paper, R as a Web Client - the RCurl package. Now that there's something in there, how do we get it back? That's super easy. Updating is done by using PUT on an existing document. For example, let's give Bozo, some mad skillz: If you POST to the database, you're adding a document and letting CouchDB assign its _id field. For DELETE, you pass the doc's revision number in the query string. Sorry, Bender. R4CouchDB provides a layer on top of the techniques we've just described. R4CouchDB uses a slightly strange idiom. You pass a cdb object, really just a list of parameters, into every R4CouchDB call and every call returns that object again, maybe modified. Results are returned in cdb$res. Maybe, they did this because R uses pass by value. Here's how you would initialize the object. First we take the document id and rev from the existing document. Then, save our revised document back to the DB. Shortly thereafter, Bozo mysteriously disappeared. 	 0 Comments
Comparison of Sunil Gavaskar and Javed Miandad’s Performance in International Cricket	https://www.r-bloggers.com/2010/10/comparison-of-sunil-gavaskar-and-javed-miandads-performance-in-international-cricket/	October 2, 2010	prasoonsharma		 0 Comments
A new version of ff released (version 2.2.0)	https://www.r-bloggers.com/2010/10/a-new-version-of-ff-released-version-2-2-0/	October 2, 2010	Tal Galili	A few hours ago, Jens Oehlschlägel has announced on the R-help mailing list of the release of a new version of the ff package. The ff package provides data structures that are stored on disk but behave (almost) as if they were in RAM by transparently mapping only a section (pagesize) in main memory – the effective virtual memory consumption per ff object. Here are the new features of ff, as Jens wrote in his announcement: —- Dear R community, The next release of package ff is available on CRAN. With kind help of Brian Ripley it now supports the Win64 and Sun versions of R. It has three major functional enhancements: a) new fast in-memory sorting and ordering functions (single-threaded) b) ff now supports on-disk sorting and ordering of ff vectors and ffdf dataframes c) ff integer vectors now can be used as subscripts of ff vectors and ffdf dataframes a) is achieved by careful implementation of NA-handling and exploiting context information b) although permanently stored, sorting and ordering of ff objects can be faster than the standard routines in R c) applying an order to ff vectors and ffdf dataframes is substantially slower than in pure R because it involves disk-access AND sorting index positions (to avoid random access). There is still room for improvement, however, the current status should already be useful. I run some comparisons with SAS (see end of mail): – both could sort German census size (81e6 rows) on a 3GB notebook – ff sorts and orders faster on single columns – sorting big multicolumn-tables is faster in SAS  Win64 binaries and version 2.2.1 supporting Sun should appear during the next days on CRAN. For the impatient: checkout from r-forge with revision 67 or higher. Non-Windows users: please note that you need to set appropriate values for options ‘ffbatchbytes’ and ‘ffmaxbytes’ yourself. Note that  virtual window support is deprecated now because it leads to too complex code. Let us know if you urgently need this and why. Feedback, ideas and contributions appreciated. To those who offered code during the last months: please forgive us that integrating and documenting was not possible with this release. Jens & Daniel P.S. NEWS CHANGES IN ff VERSION 2.2.0 NEW FEATURES o   ff now supports the 64 bit Windows and Sun versions of R (thanks to Brian Ripley) o   ff now supports sorting and ordering of ff vectors and dataframes (see ramsort, ffsort, ffdfsort, ramorder, fforder, ffdforder) o   ff now supports ff vectors as subscripts of ff objects (currently positive integers only, booleans are planned) o   New option ‘ffmaxbytes’ which allows certain ff procedures like sorting using larger limit of RAM than ‘ffbatchbytes’ in chunked processing. Such higher limit is useful for (single-R-process) sorting compared to some multi-R-process chunked processing. It is a good idea to reduce ‘ffmaxbytes’ on slaves or avoid ff sorting there completely. o   New generic ‘pagesize’ with method ‘pagesize.ff’ which returns the current pagesize as defined on opening the ff object. USER VISIBLE CHANGES o   [.ff now returns with the same vmode as the ff-object o   Certain operations are faster now because we worked around unnecessary copying triggered by many of R’s assignment functions. For example reading a factor from a (well-cached) file is now 20% faster and thus as fast as just creating this factor in-RAM using levels() o   ff() can now open files larger than .Machine$integer.max elements (but gives access only to the first .Machine$integer.max elements) o   ff now has default pattern NULL translating to the pattern in ‘filename’ (and only to the previous default ‘ff’ if no filename is given) o   ff now sets the pattern in synch with a requested ‘filename’ o   clone.ff now always creates a file consistent with the previous pattern o   clone.ff now always creates a finalizer consistent with the file location o   clone.ffdf has a new argument ‘nrow’ which allows to create an empty copy with a different number of rows (currently requires ‘initdata=NULL’) o   clone.default now deep-copies lists and atomic vectors DEPRECATED o   virtual window support is deprecated. Let us know if you urgently need this and why. BUG FIXES o   read.table.ffdf now also works if transFUN filters and returns less rows Older version changes can be viewed in the package’s NEWS/changelog. P.P.S. Below are some timings in seconds at 3e6, 9e6, 27e6 and 81e6 elements from a Lenovo 410s notebook (3GB RAM, i5 m520, 2 real cores, 4 hyperthreaded cores, SSD drive, Windows7 32bit) Legend for software ram:  new in-ram inplace operations receiving enough RAM to optimize for speed, not for memory ff:  new on-disk operations limiting RAM for this operation at ~500GB R:  timings from standard sort() and order() SAS:  timings from SAS 9.2 allowing for multithreaded sorting Legend for type of random data rboolean:  bi-boolean with 50% FALSE and TRUE rlogical:  tri-boolean with 33% NA, FALSE and TRUE rubyte:  integers from 0..255 rbyte:  33% NA and 67% -127..127 rushort:  integers from 0..65535 rshort:  33% NA and 67% -32767..32767 ruinteger:  50% NA and 50% integers rinteger:  random integers rusingle:  50% NA and 50% singles rsingle:  random singles rudouble:  50% NA and 50% doubles rdouble:  doubles rfactor:  factor with 64 levels of length 66 (being different at bytes 65 and 66) rchar:  64 strings of length 66 (being different at bytes 65 and 66) Legend for abbreviations OOM:  out of memory OOD:  out of disk NT:  not timed because too slow NA:  not available Results for sorting a single column ===================================== , , 3e6 rboolean rlogical rubyte rbyte rushort rshort ruinteger rinteger rusingle rsingle rudouble rdouble rfactor rchar ram     0.02     0.03   0.02  0.04    0.02   0.02      0.17     0.11     0.66    0.36     0.66    0.36    0.03    NA ff      0.25     0.33   0.22  0.25    0.28   0.26      0.38     0.30     1.02    0.65     0.92    0.67    0.39    NA R         NA     0.35     NA    NA      NA     NA      0.83     0.54       NA      NA     1.28    0.90   64.83 51.20 SAS       NA       NA     NA    NA      NA     NA      1.61     1.32       NA      NA     1.57    1.29      NA 17.01 , , 9e6 rboolean rlogical rubyte rbyte rushort rshort ruinteger rinteger rusingle rsingle rudouble rdouble rfactor rchar ram     0.04     0.07   0.03  0.08    0.03   0.07      0.50     0.31     1.88    0.97     1.87    0.97    0.04    NA ff      0.72     0.93   0.61  0.73    0.84   0.75      1.08     0.86     2.68    1.62     2.57    1.67    0.78    NA R         NA     0.90     NA    NA      NA     NA      2.84     1.78       NA      NA     3.51    2.12      NA    NT SAS       NA       NA     NA    NA      NA     NA      4.99     3.90       NA      NA     4.91    4.48      NA 62.76 , , 27e6 rboolean rlogical rubyte rbyte rushort rshort ruinteger rinteger rusingle rsingle rudouble rdouble rfactor  rchar ram     0.10     0.24   0.09  0.23    0.11   0.23      1.58     1.00     6.06    3.15     6.00    3.23    0.16     NA ff      2.19     2.98   1.92  2.21    2.56   2.31      3.22     2.68     8.49    5.18     8.10    5.35    2.58     NA R         NA     2.72     NA    NA      NA     NA      9.69     5.80       NA      NA    12.34    6.97      NA     NT SAS       NA       NA     NA    NA      NA     NA     17.02    12.67       NA      NA    17.05   14.07      NA 176.63 , , 81e6 rboolean rlogical rubyte rbyte rushort rshort ruinteger rinteger rusingle rsingle rudouble rdouble rfactor rchar ram     0.27     0.67   0.28  0.67    0.33   0.72      5.58     3.23       NA      NA       NA      NA    0.49    NA ff      6.56     9.06   5.93  6.88    8.52   7.15     10.70     8.54    51.35   28.98    70.20   44.13    7.91    NA R        OOM      OOM    OOM   OOM     OOM    OOM       OOM      OOM      OOM     OOM      OOM     OOM     OOM   OOM SAS       NA       NA     NA    NA      NA     NA     61.45    44.94       NA      NA    63.14   46.56      NA   OOD Results for calculating the order on a single column ==================================================== , , 3e6 rboolean rlogical rubyte rbyte rushort rshort ruinteger rinteger rusingle rsingle rudouble rdouble rfactor  rchar ram     0.05     0.07   0.04  0.07    0.09   0.11      0.92     0.53     1.46    0.81     1.31    0.64    0.06     NA ff      0.14     0.19   0.77  0.58    0.87   0.67      1.04     0.60     1.66    0.81     1.43    0.85    0.74     NA R         NA     3.23     NA    NA      NA     NA      4.57     4.07       NA      NA     5.27    4.61    4.59 193.75 SAS       NA       NA     NA    NA      NA     NA      1.86     1.48       NA      NA     1.63    1.39      NA  16.83 , , 9e6 rboolean rlogical rubyte rbyte rushort rshort ruinteger rinteger rusingle rsingle rudouble rdouble rfactor rchar ram     0.16     0.21   0.17  0.20    0.30   0.28      3.07     1.61     4.24    2.16     4.22    2.19    0.19    NA ff      0.48     0.51   2.45  1.84    2.91   2.15      3.38     1.92     4.72    2.48     4.54    2.45    1.91    NA R         NA    12.31     NA    NA      NA     NA     17.02    15.56       NA      NA    16.96   15.47      NT    NT SAS       NA       NA     NA    NA      NA     NA      6.71     5.97       NA      NA     6.25    5.41      NA 59.27 , , 27e6 rboolean rlogical rubyte rbyte rushort rshort ruinteger rinteger rusingle rsingle rudouble rdouble rfactor  rchar ram     0.51     0.67    0.5  0.69    0.92   0.94      9.89     5.31    15.13    7.69    15.15    7.70    0.58     NA ff      1.33     1.51    7.6  5.77    9.25   6.79     10.72     6.12    15.98    8.53    15.96    8.92    5.80     NA R         NA    46.37     NA    NA      NA     NA     65.57    59.17       NA      NA    63.74   58.37      NT     NT SAS       NA       NA     NA    NA      NA     NA     21.41    18.77       NA      NA    20.22   18.84      NA 182.74 , , 81e6 rboolean rlogical rubyte rbyte rushort rshort ruinteger rinteger rusingle rsingle rudouble rdouble rfactor rchar ram     1.49     2.03    1.5  2.06    3.15   2.98     34.33    17.89       NA      NA       NA      NA    1.90    NT ff      3.98     4.65   22.9 17.42   30.33  21.82     36.68    20.36    77.16   49.55   125.01   59.27   17.39    NT R        OOM      OOM    OOM   OOM     OOM    OOM       OOM      OOM      OOM     OOM      OOM     OOM     OOM   OOM SAS       NA       NA     NA    NA      NA     NA     86.24    70.32       NA      NA    84.40   68.66      NA    NA Results for sorting all columns of a table with m columns of random double data (without NAs) ============================================================================================= , , 3e6 ncol   1    2    5   10    20 SAS 1.65 1.83 3.71 6.90 14.06 ff  1.97 2.37 3.75 6.21 10.86 R   4.70 5.67 5.65 6.46  8.06 , , 9e6 ncol    1     2     5    10    20 SAS  5.18  6.70 14.02 19.25 41.65 ff   6.38  7.96 12.12 19.58 45.43 R   18.86 19.20 20.58   OOM   OOM , , 27e6 ncol    1     2     5    10     20 SAS 17.79 19.52 35.03 83.30 142.09 ff  22.68 25.79 46.25 87.55 157.62 R   65.56   OOM   OOM   OOM    OOM , , 81e6 ncol     1      2      5     10     20 SAS  64.78  83.39 143.59 242.23 408.72 ff  167.52 220.03 324.03 502.42 884.03 R      OOM    OOM    OOM    OOM    OOM 	 0 Comments
OpenMx 1.0	https://www.r-bloggers.com/2010/10/openmx-1-0/	October 1, 2010	Shige		 0 Comments
Because it’s Friday: I hate pennies	https://www.r-bloggers.com/2010/10/because-its-friday-i-hate-pennies/	October 1, 2010	David Smith	I was brought up in Australia, where we got rid of 1¢ and 2¢ coins in my childhood. Nobody missed them, at least as I recall. I certainly didn't: they were useless, even at the age when I might by lollies (candy) for 2¢ apiece. But then again, I'd never buy one, and with the new changes if I bought six of them, I'd get them for a discount as the price would be rounded down to 10¢. Besides, petrol (gas) was priced including fractions of a cent per litre, and nobody worried we didn't have 0.1¢ coins. So why worry about those little copper coins? Besides, few transactions are done with cash these days anyway, and with debit cards and bank transfers you can still account for the pennies, even if you can't count them using metal tokens. I guess things are different here in the States where there's some passionate clinging to those little copper coins that serve only to be collected in random jars and drawers around my house after I dump them from my pockets. But this guy has the most passionate argument I've seen for getting rid of pennies:    I agree: down with pennies! 	 0 Comments
Fibonacci 1-liners	https://www.r-bloggers.com/2010/10/fibonacci-1-liners/	October 1, 2010	David Smith	The other day, as an excuse to play around with custom iterators, I created some completely over-engineered code to calculate the Fibonacci sequences. But surely such a simple function can be implemented in fewer than my 15 lines? (Rick Wicklin, who writes the SAS blog The Do Loop, thinks so too.) We could use such a function to more easily show that the ratio of successive Fibonacci numbers tends to the Golden Ratio.  Barry Rowlingson suggests this function to calculate the ratio between the nth and n+1th Fibonacci number:  A slight tweak to Barry's code yields a function to calculate the Nth Fibonacci number in 55 characters: I can do it in 1 fewer characters with the closed-form solution: But given that this equation is written in terms of the Golden Ratio, using it to derive the Golden Ratio pretty much defeats the purpose. jebyrnes suggests another means of calculating the Golden Ratio from fibonacci numbers, this time using the sapply function: jebyrnes also pointed me to this page of Fibonacci 1-liners in many languages including C, Java and Python. Unless I'm mistaken, R is now very close to holding the record for shortest 1-liner.   	 0 Comments
R Beginner’s Guide Book Update 10/1/2010	https://www.r-bloggers.com/2010/10/r-beginners-guide-book-update-1012010/	October 1, 2010	John Quick	Update: Statistical Analysis with R is now available! I recently submitted the final drafts of all chapters of my R Beginner’s Guide book, which is to be published through Packt. The official publishing timeline is set to December 2010, although the book may release ahead of schedule if all continues to go well. Below is an updated list of the major topics covered in the R Beginner’s Guide. Over the course of this book, you will acquire the knowledge and skills necessary to: 	 0 Comments
Three-Quarter Truths: Correlation Is Not Causation	https://www.r-bloggers.com/2010/10/three-quarter-truths-correlation-is-not-causation/	October 1, 2010	John Myles White	Other than our culture’s implicit association between lies, damned lies and statistics, I think no idea has stifled the growth of statistical literacy as much as the endless repetition of the words correlation is not causation. This phrase seems to be primarily used to suppress intellectual inquiry by encouraging the unspoken assumption that correlational knowledge is somehow an inferior form of knowledge. I’d like to defend correlation for a bit. Here are four reasons why I think we should learn to love correlation and stop worrying so much about causation. The majority of reliable human knowledge is already correlational. Spend a few days making a list of things that you know for certain about the world. I claim that you will find that a solid majority of them will be correlational statements rather than causal statements. For example, you might notice that you know that teenagers who own skateboards generally like punk music more than world music, though you are certainly aware that listening to the Sex Pistols isn’t the cause of their desire to learn how to ollie. And you almost certainly know that ‘s’ is followed by ‘t’ more often than ‘s’ is followed by ‘r’ in English, though you would never claim that an ‘s’ causes an ‘r’. 1 Hopefully those two examples are enough to make you suspect that you have an enormous quantity of correlational information stored inside your head. I’d like to further suggest that, despite its low status in our scientific culture, this sort of correlational knowledge has enormous practical value to you, because it allows you to make sense of a world in which you have incomplete information and are constantly required to fill in the blanks. For example, if you’re out at night in the deep South and suddenly see someone charging towards you dressed in white sheets, you’ll almost certainly run away, even though you don’t believe that white sheets cause lynchings. 2 Correlational knowledge can keep you alive when worrying about causality would get you killed. Taking this point a step further, it’s worth noting that assessing the value of information is a far more difficult problem than one might think. In practice, you always need to ask yourself what you’re trying to do with information.  In many cases, you aren’t trying to control things, which I would claim is the only scenario in which causal knowledge could not be replaced with correlational knowledge in principle. In most real world problems, correlational knowledge is enough to make predictions with very high accuracy. For example, imagine that you run a bank and want to predict whether a person will default on their loan. You find that their zipcode predicts their rate of default quite well. You know full well that a zipcode cannot possibly cause a person to default on their loan, because it’s just a number based on a fairly arbitrary way of cutting up neighborhoods. But the absence of a causal relationship is completely irrelevant to you as a banker, since your interest lies in making money — and not in learning something about the hidden causes of human behavior. If you want to predict something, rather than control it, the most important thing to ask is how well the information you can acquire will allow you to make predictions. After addressing this problem, you will also need to consider the relative costs of acquiring different sorts of information. For example, suppose that you want to predict a person’s height. Most of us accept that our genes are the ultimate cause of our height, barring serious illness or malnutrition as children. That’s why the heights of identical twins are so similar, while the heights of fraternal twins can be quite different. Focusing on causal pathway from genes to phenotype might suggest that you should try to measure someone’s genes to predict their height. People have done this and it doesn’t work very well. More importantly, it provides mediocre results at a fairly high cost. Acquiring a genotype is constantly going down in price, but it still costs a few hundred dollars. Another approach comes from the inventor of the concept of correlation: Francis Galton. Galton’s method simply takes your parents’ heights and uses a correlational model to predict your height. This approach is correlational because no one believes that your parents’ heights cause your height: your parents’ genes caused their heights, then their genes caused your genes, and finally your genes caused your height. This is a perfect example of the way in which two things can be correlated because they share a common cause. By making clever use of correlational information, Galton’s method only requires data that is available at almost zero cost, and yet it is more than ten times as accurate as the genetic screening method described above. Sometimes cheap correlational information provides high predictive accuracy, while costly causal information provides almost no predictive power. If you want to do something with information, you should always consider the possibility that a correlational pathway may be cheaper to observe than a causal one — at the same time that it provides comparable predictive power or even greater predictive power. Causation is not an entirely well-defined concept. It is an intuitive notion like justice or intelligence, and therefore may not have any definition that corresponds to all of the ways in which the word “cause” is used in normal language. Despite considerable work by philosophers and mathematicians, our accumulated understanding of what causation means is still very weak. This vagueness works in causation’s favor. Because correlation is so much more precise as a concept than causation, it’s easier to come up with examples in which correlation doesn’t provide us with useful information than it is to come up with examples of the irrelevance of causal knowledge. This discrepancy in falsifiability is really a general property of mathematical models when compared with intuitive arguments: the precision of mathematical models makes them much more vulnerable to attack than vague ideas. But this brittleness is really a unrecognized virtue, because it is inseparable from the exactness that makes mathematical models directly comparable, precisely communicable and easily modified and extended. Despite their intuitive appeal, ideas whose true or falsehood is hard to assess are less amenable to the incremental improvements that has made scientific knowledge so valuable to humanity. Last, but not least, I think correlation and causation are themselves correlated. By this I mean that if you were to list pairs of related things like height and weight, ethnicity and voting preferences, or zipcodes and mortgage default rates; and then classified each relationship as correlational and causal, you’d find that many instances of correlation were accompanied by causation. And you’d find that even more instances of causation were accompanied by correlation. Following Drew Conway’s lead, I’ll draw a Venn diagram of the relationship that I believe holds between correlation and causation: This claim is incredibly hard to test: it is merely meant to remind us how wasteful it can be to focus exclusively on the differences between correlation and causation when they also have important similarities. It is true that correlation is not causation. But it is also true that human beings are not chimpanzees. And yet, in spite of that, we’ve been able to learn a lot about the human brain from studying the brains of chimpanzees, because there are many cases in which the similarities between humans and chimps are more important than the differences. Similarly, studying correlations can give us valuable information, including information about where to start looking for causal relationships. And even when it can’t do this, there is nothing wrong with correlational knowledge that is not also causal knowledge. Knowledge of causation is only necessary when we want to control the world. But there are many aspects of the world that we are largely unable to control, even in principle. In those cases, we simply need to have accurate predictions, because prediction without causation is enough for us to make the best of what is going to happen in the future. Assessing our ability to make predictions is vitally important, and it is the habit of making testable and precise predictions that an education in statistics can give to us. So let’s embrace a world with rich data sets that can provide us  with formal, testable knowledge based on unambiguous, formal models — even if those models won’t ultimately provide us with causal mechanisms. With all of that said, if you really want to understand the distinctions between correlation and causality, there is a rich academic literature that is far subtler and more interesting than the folk philosophy of science that I’ve been attacking. The current classic is Judea Pearl‘s masterwork, entitled simply “Causality”. It is very challenging material, but well worth the effort. And understanding it will require you to master so much of the machinery of prediction that you’ll walk away enlightened even if you decide in the end that causality doesn’t really interest you. For most people, though, I have a different closing message. Please don’t allow the absence of causation to be used as a justification for remaining ignorant about the correlational structure of our world. Though there are cases in which knowing that A is related to B is much less useful than knowing that A causes B, knowing that A and B are related at all is still far better than knowing nothing at all — and we currently know nothing about many things. We should stop focusing on the ways in which correlation is not causation and instead follow Voltaire’s advice: do not allow the perfect to become the enemy of the good. 3  Running those commands should show you that there are 156 examples of ‘sr’ and 21,407 examples of ‘st’ in the standard UNIX dictionary. 	 0 Comments
R Optimization Function Test	https://www.r-bloggers.com/2010/10/r-optimization-function-test/	October 1, 2010	Quantitative Finance Collector		 0 Comments
The avalanche of publications mentioning GO	https://www.r-bloggers.com/2010/11/the-avalanche-of-publications-mentioning-go/	November 30, 2010	R on Guangchuang Yu	Gene Ontology is the de facto standard for annotation of gene products. It has been widely used in biological data mining, and I believe it will play more central role in the future. Publications mentioning GO was collected and deposited in GO ftp, and can be accessed (ftp://ftp.geneontology.org/go/doc/). I count the number of publicans by year, and draw a histogram, which showed that the growing trend was remarkable.  	 0 Comments
RQuantLib 0.3.5	https://www.r-bloggers.com/2010/11/rquantlib-0-3-5/	November 30, 2010	Thinking inside the box	"

Most of the changes were made two and four weeks ago: first in response to
some warnings triggered by R 2.12.0 on the included manual pages which needed
a brush-up, and then again is some consolidation of manual pages and some
other minor tweaks. The release was
then held back at CRAN as we noticed that manual pages, when collated to a
single large document, triggered a segmentation fault in the latex
compiler. Oddly enough only in Europe (if the a4paper option was
used) and not here (where I use uspaper). Long story short, this
turns out to be a bug in the latex toolchain (which we reported as
Debian bug report 604754)
which is apparently is known but has no known fix yet (a sample file was
supplied with the bug report if you want to take a look).

 
With that, special thanks go to Kurt Hornik and Brian Ripley on the R Core
team who made a change to how R processes the manual which made it resilient
to the latex bug so that normal release of the package could proceed (and the
shiny manual
is available too).

 
Thanks to
CRANberries, there is
also a 
 diff to the previous release 0.3.4.
Full changelog details, examples and more details about this package are at
my RQuantLib page.


 "	 0 Comments
Updates to the ‘aqp’ Package for R (version 0.98-3)	https://www.r-bloggers.com/2010/11/updates-to-the-aqp-package-for-r-version-0-98-3/	November 30, 2010	dylan	Just released an updated version of our ‘aqp’ package for quantitative soils investigations, available on CRAN and R-Forge. Some of the major changes are listed below: read more 	 0 Comments
What’s Next for Revolution R and Hadoop?	https://www.r-bloggers.com/2010/11/what%e2%80%99s-next-for-revolution-r-and-hadoop/	November 30, 2010	David Smith	"
 "	 0 Comments
Controlling Amazon Web Services using rJava and the AWS Java SDK	https://www.r-bloggers.com/2010/11/controlling-amazon-web-services-using-rjava-and-the-aws-java-sdk/	November 30, 2010	JD Long	" I’ve been messing around with using Amazon Web Services for a while. I’ve had some projects where I wanted to upload files to S3 or fire off EMR jobs. I’ve been controlling AWS services using a hodgepodge of command line tools and the R system() function to call the tools from the command line. This has some real disadvantages, however. Using the command line tools means each tool has to be configured individually which is painful on a new machine. It’s also much harder to roll my R code up into a CRAN package because I have to check dependencies on the command line tools and ensure that the user has properly configured each tool. Clearly a pain in the ass. So I was looking for more simple/elegant solutions. After thinking the Boto library for Python might be helpful, I realized that the easiest way to use that would be with rJython which meant having to interact with R, Python, AND Java. Considering I don’t program in Python or Java, that seemed like a fair bit of complexity. Then I realized that the canonical implementation of the AWS API was the AWS Java SDK. The rJava package makes interacting with Java from R a viable option. Since I’ve never written a single line of Java code in my pathetic life, this was somewhat harder than it could have been. But with some help from Romain Francois I was able to cobble together “something that works.” The code below gives a simple example of interfacing with S3. The example will look to see if a given bucket exists on S3, if not it will create the bucket. Then it will upload a single file from your PC into that bucket. You will have to download the SDK, unzip it in the location of your choice, and then change the script to reflect your configuration. If you are running R in Ubuntu, you should install rJava using apt-get instead of using install.packages() from inside of R: sudo apt-get install r-cran-rjava Here’s the codez. And a direct link for you guys reading this through an RSS reader:
 I realize that Duncan Temple Lang has created the RAmazonS3 package which can easily do what the above code sample does. The advantage of using rJava and the AWS Java SDK is the ability to apply the same approach to ALL the AWS services. And since Amazon maintains the SDK this guarantees that future AWS services and features will be supported as well. "	 0 Comments
how to install 64-bit rggobi on Mac OS X 10.5	https://www.r-bloggers.com/2010/11/how-to-install-64-bit-rggobi-on-mac-os-x-10-5/	November 30, 2010	Jeffrey Breen	"I was surprised when everything seemed to “just work” when I made the jump to 64-bit R 2.11.1 a while back. “Surprised” because my previous attempt to join the 64-bit world under 2.10.x was a dead-end: the must-have RMySQL package didn’t like the 32-bit MySQL drivers, but Leopard’s 32-bit perl couldn’t deal with 64-bit drivers, etc., etc.  Too much hassle for my new non-IT self to deal with. So today I found the first thing which didn’t “just work” in the jump: the rggobi package. Long story short, the trick is to install the rggobi package from source.  Hadley Wickham (that guy is everywhere, isn’t he?) spells out the prerequisites and steps on the GGobi users Google group: http://groups.google.com/group/ggobi/msg/5ae98c8b87d14a61 The only change you’ll need to make, if you’re still stuck in the 32/64-bit limbo of 10.5 like me is to make sure you build the package from the 64-bit version of R: $ R64 CMD INSTALL ~/Downloads/rggobi_2.1.16.tar.gz

 "	 0 Comments
Example 8.16: Exact logistic regression	https://www.r-bloggers.com/2010/11/example-8-16-exact-logistic-regression/	November 30, 2010	Ken Kleinman		 0 Comments
Sweave Tutorial 3: Console Input and Output – Multiple Choice Test Analysis	https://www.r-bloggers.com/2010/11/sweave-tutorial-3-console-input-and-output-multiple-choice-test-analysis/	November 30, 2010	Jeromy Anglim	The repository with all source files is available at: A copy of the resulting PDF can be viewed here. For general information on program requirements and running the code  see this earlier post I find it useful to distinguish between different kinds of  Sweave documents. One key distinction is between  Reports that display the console are suited to distinct applications,  including: It would be further possible to distinguish between  reports that do and do not show the console input (echo=true). Developing Sweave reports that display the console still benefit from thoughtful variable names,  selective display of output and so forth. However, naturally less time is invested in  putting the polish on figures, tables, and inline text. The previous Sweave Tutorials were examples  of Sweave documents that do not display the console. Tutorial 1 was a data driven documentTutorial 2 was a set of batch polished reports. The present tutorial is an example of a Sweave document that displays console input and output. The remainder of the post discusses various aspects of the   source code. ... ... ... This post is the third installment in a Sweave Tutorial Series: 	 0 Comments
Hägerstrand Time-Space Cube	https://www.r-bloggers.com/2010/11/hagerstrand-timespacecube/	November 30, 2010	Benedikt Orlowski	"Hägerstrand time-space cube with R   With the rgl package it’s possible to interact with the 3d visualization of the timespace tracks. Code example:
plot3d(lon,lat,timedate, xlim=range(lon), ylim=range(lat), zlim=range(timedate), ticktype=”detailed”, xlab=”longitude”, ylab=”latitude”, zlab=”Date”, col= as.POSIXlt(daten[,”Date”])$mday, type=”l”, main=plottitle) In the posted example individual waypoints were added by drawing vertical lines. "	 0 Comments
GOSemSim redesign in terms of S4 classes	https://www.r-bloggers.com/2010/11/gosemsim-redesign-in-terms-of-s4-classes/	November 30, 2010	R on Guangchuang Yu	I started to develop GOSemSim package two years ago when I was not quite familiar with R. I am very happy to see that someone use it and found it helpful.  I try to learn S4 and redesign GOSemSim with S4 classes and methods in the pass two weeks, and the very first version was implemented. As I’m not very familiar with S4, the package may need improve in many aspect. The newest version of GOSemSim can be installed by: Here come some example: 	 0 Comments
Analysis of retractions in PubMed	https://www.r-bloggers.com/2010/11/analysis-of-retractions-in-pubmed/	November 30, 2010	nsaunders	"As so often happens these days, a brief post at FriendFeed got me thinking about data analysis.  Entitled “So how many retractions are there every year, anyway?”, the post links to this article at Retraction Watch.  It discusses ways to estimate the number of retractions and in particular, a recent article in the Journal of Medical Ethics (subscription only, sorry) which addresses the issue. As Christina pointed out in a comment at Retraction Watch, there are thousands of scientific journals of which PubMed indexes only a fraction.  However, PubMed is relatively easy to analyse using a little Ruby and R.  So, here we go…

Code and raw data used for this post are available at Github. 1. Searching for retractions
In the Journal of Medical Ethics article, the authors state: “Every research paper noted as retracted in the PubMed database from 2000 to 2010 was evaluated.  PubMed was searched on 22 January 2010 with the limits of ‘items with abstracts, retracted publication, English.’ A total of 788 retracted papers were identified…” Not a bad approach.  There’s another way:  at the PubMed website, find a retraction and examine the record in XML format.  You’ll see this: The equivalent in Medline format is: This means that retractions have a particular type:  Publication Type, or PTYP for short.  If you search at the PubMed website using the term “Retraction of Publication[Publication Type]“, you will retrieve (at the time of writing) ~ 1621 records. 2. Retrieving retraction counts by year
Armed with this information, we can modify the Ruby code that I’ve posted previously to retrieve total and retracted publications between 1900 and 2010.  This generates a tab-delimited file with 3 columns:  year, total publications and retracted publications. 3. Retraction count analysis
Here’s the R code to analyse the retraction counts.  There are no recorded retractions until 1977, so we’ll start from that year. So, retractions are increasing rapidly.  No surprise there, since the total number of publications per year is also increasing rapidly.  We need some kind of normalization.
 PubMed retractions 1977 – 2010 Indeed, it seems that with each year, retractions constitute a greater proportion of publications for that year.
 PubMed retractions 1977 – 2010 (per 100K by year) This shows a smoother upwards trend, with a rapid increase from 2005 onwards.
 PubMed retractions 1977 – 2010 (per 100K, cumulative) This is somewhat alarming.  Whilst there are about 4x as many total publications in Pubmed now as there were in 1977, the total number of retractions has risen almost 550x.
 Percent increase relative to 1977, cumulative 4. Analysis of Medline data
Using the search term described earlier in the post to retrieve retractions, we can download a file in Medline format.  Medline records contain various fields of interest, including the ROF (retraction of) line, describing the publication that was retracted. Or – as it turns out in some cases – publications.  One retraction record may include the retraction of several publications, as we can see with a simple grep: We won’t worry about that too much, since the majority of retraction records reference one publication. Here is some R code that performs two simple, similar analyses of the Medline file.  First, the top 10 journals for retractions: A brief glance at that list suggests that higher impact factor = more retractions.  We would want to know the total number of publications for those journals to make more sense of that. Second, the top 10 countries: Not especially surprising; the ones with the most researchers/scientific output.  Again, we’d want more data before drawing any conclusions. Final thoughts "	 0 Comments
Data visualization videos	https://www.r-bloggers.com/2010/11/data-visualization-videos/	November 29, 2010	Rob J Hyndman	Probably everyone has seen Hans Rosling’s famous TED talk by now. I recently came across a couple of other exceptional talks on data visualization: Hans Rosling again: Let my dataset change your mindset. If only all statistics lecturers were this dynamic! David McCandless: The beauty of data visualization. Not so exciting as Hans, but some great examples. And for those in the UK, look out for “The Joy of Stats”, coming soon on BBC4. 	 0 Comments
Initializing the Holt-Winters method	https://www.r-bloggers.com/2010/11/initializing-the-holt-winters-method/	November 29, 2010	Rob J Hyndman	"The Holt-Winters method is a popular and effective approach to forecasting seasonal time series. But different implementations will give different forecasts, depending on how the method is initialized and how the smoothing parameters are selected. In this post I will discuss various initialization methods. Suppose the time series is denoted by  and the seasonal period is  (e.g.,  for monthly data). Let  be the -step forecast made using data to time . Then the additive formulation of Holt-Winters’ method is given by the following equations  
ell_t & = alpha(y_t – s_{t-m}) + (1-alpha)(ell_{t-1}+b{t-1})\
b_t & = beta(ell_t – ell_{t-1}) + (1-beta)b_{t-1}\
s_t &= gamma(y_t-ell_{t-1} – b_{t-1}) + (1-gamma)s_{t-m}\
hat{y}_{t+h|t} &= ell_t + b_th + s_{t+h-m},
end{align*}”title=”Rendered by QuickLaTeX.com” style=”vertical-align: 0px; border: none;”/> and the multiplicative version is given by  
ell_t & = alpha(y_t/ s_{t-m}) + (1-alpha)(ell_{t-1}+b{t-1})\
b_t & = beta(ell_t – ell_{t-1}) + (1-beta)b_{t-1}\
s_t &= gamma(y_t/(ell_{t-1} – b_{t-1})) + (1-gamma)s_{t-m}\
hat{y}_{t+h|t} &= (ell_t + b_th)s_{t+h-m}.
end{align*}”title=”Rendered by QuickLaTeX.com” style=”vertical-align: 0px; border: none;”/> In many books, the seasonal equation (with  on the LHS) is slightly different from these, but I prefer the version above because it makes it easier to write the system in state space form. In practice, the modified form makes very little difference to the forecasts. In my 1998 textbook, the following initialization was proposed. Set  
ell_m & = (y_1+cdots+y_m)/m\
b_m &= left[(y_{m+1}+y_{m+2}+cdots+y_{m+m})-(y_1+y_2+cdots+y_{m})right]/m^2.
end{align*}”title=”Rendered by QuickLaTeX.com” style=”vertical-align: 0px; border: none;”/> The level is obviously the average of the first year of data. The slope is set to be the average of the slopes for each period in the first two years:  
(y_{m+1}-y_1)/m,quad (y_{m+2}-y_2)/m,quad dots,quad (y_{m+m}-y_m)/m.
]”title=”Rendered by QuickLaTeX.com” style=”vertical-align: 0px; border: none;”/> Then, for additive seasonality set  and for multiplicative seasonality set , where . This works pretty well, and is easy to implement, but for short and noisy series it can give occasional dodgy results. It also has the disadvantage of providing state estimates for period , so that the first forecast is for period  rather than period 1. In some books (e.g., Bowerman, O’Connell and Koehler, 2005), a regression-based procedure is used instead. They suggest fitting a regression with linear trend to the first few years of data (usually 3 or 4 years are used). Then the initial level  is set to the intercept, and the initial slope  is set to the regression slope. The initial seasonal values  are computed from the detrended data. This is a very poor method that should not be used as the trend will be biased by the seasonal pattern. Imagine a seasonal pattern, for example, where the last period of the year is always the largest value for the year. Then the trend will be biased upwards. Unfortunately, Bowerman, O’Connell and Koehler (2005) are not alone in recommending bad methods. I’ve seen similar, and worse, procedures recommended in other books. While it would be possible to implement a reasonably good regression method, a much better procedure is based on a decomposition. This is what was recommended in my 2008 Springer book and is implemented in the HoltWinters and ets functions in R.  This is generally quite good and fast to implement and allows “forecasts” to be produced from period 1. (Of course, they are not really forecasts as the data to be forecast has been used in constructing them.) However, it does require 2 or 3 years of data. For very short time series, an alternative (implemented in the ets function in R) is to set the initial seasonal values to be the last year of available data with the mean subtracted (for additive HW) or divided out (for multiplicative HW). Then proceed with steps 3 and 4 as above. Whichever method is used, these initial values should be seen as rough estimates only. They can be improved by optimizing them along with the smoothing parameters using maximum likelihood estimation, for example. The only implementation of the Holt-Winters’ method that does that, to my knowledge, is the ets function in R. In that function, the above procedure is used to find starting values for the estimation. Some recent work (De Livera, Hyndman and Snyder, 2010) shows that all of the above may soon be redundant. In Section 3.1, we show that the initial state values can be concentrated out of the likelihood and estimated directly using a regression procedure. Although we present the idea in the context of complex seasonality, it is valid for any exponential smoothing model. I am planning on modifying the ets function to implement this idea, but it will probably have to a wait for a couple of months as my “to do” list is rather long. "	 0 Comments
Sorting out Sweave in Eclipse/StatET	https://www.r-bloggers.com/2010/11/sorting-out-sweave-in-eclipsestatet/	November 29, 2010	Luke Miller		 0 Comments
Slices and crumbs [arXiv:1011.4722]	https://www.r-bloggers.com/2010/11/slices-and-crumbs-arxiv1011-4722/	November 29, 2010	xi'an	An interesting note was arXived a few days ago by Madeleine Thompson and Radford Neal. Beside the nice touch of mixing crumbs and slices, the neat idea is to have multiple-try proposals for simulating within a slice and to decrease the dimension of the simulation space at each try. This dimension diminution is achieved via the construction of an orthogonal basis based on the gradient of the log densities at previously-rejected proposals.  until all dimensions are exhausted, in which case the scale of the Gaussian proposal is reduced. (The paper comes with R and C codes.) Provided the gradient can be computed (or at least approximated), this is a fairly general method (even though I have not tested it so cannot say how much calibration it requires). An interesting point is that, contrariwise to the delayed-rejection method of Antonietta Mira and co-authors,  the repeated proposals do not induce a complexification in the slice acceptance probability. I am less convinced by the authors’ conclusion that the method compares with adaptive Metropolis techniques, in the sense the “shrinking rank” method forgets about past experiences as it starts from scratch at each iteration: it is thus not really learning… (Now, in terms of performances, this may be the case!) 	 0 Comments
The Joy of Visualizations	https://www.r-bloggers.com/2010/11/the-joy-of-visualizations/	November 29, 2010	David Smith	"This is a clip from the forthcoming BBC4 program, The Joy of Stats: 





  The clip shows Hans Rosling (who we've profiled here before), plotting life expectancy versus income for various countries, and animating over time. The clip amply demonstrates that with the right presentation and story, even the simplest of scatterplots can be a joy to behold. I'm looking forward to seeing the entire program; hopefully it will be broadcast here in the States sometime soon. "	 0 Comments
Altering Eclipse user name	https://www.r-bloggers.com/2010/11/altering-eclipse-user-name/	November 29, 2010	Luke Miller		 0 Comments
John Chambers on R and Multilingualism	https://www.r-bloggers.com/2010/11/john-chambers-on-r-and-multilingualism/	November 29, 2010	David Smith	John Chambers, one of the creators of R's predecessor S and a current member of the R Core Group, gave a seminar at Stanford last week titled “R, other languages and object-oriented programming”. Unfortunately, I was away for the Thanksgiving break and couldn't make it myself, but John has kindly made his slides (PDF) available for download. The talk covers the benefits of multilingualism: in this case, being able to call other languages from R. It covers in particular the Rcpp interface between R and C++, and the new ability in R 2.12 to create C++-like classes that pass data by reference instead of by value. The slides give a concise introduction to reference classes in R, and note that Rcpp is the first implementation of using R reference classes from another language. Also, be sure to check out slides 13 and 14 for an important piece of history: the first whiteboard concepts of a new language in 1976, that would lead to the development of S and, ultimately, R. John Chambers: R, Other Languages and Object-Oriented Programming (PDF) 	 0 Comments
Statistique de l’assurance STT6705V, partie 11	https://www.r-bloggers.com/2010/11/statistique-de-lassurance-stt6705v-partie-11/	November 29, 2010	arthur charpentier	Last course will be uploaded soon (the links will be here and there). The R code considered is given below. First, we had to work a little bit on the datasets, 	 0 Comments
SAS and R joins SAS-x	https://www.r-bloggers.com/2010/11/sas-and-r-joins-sas-x/	November 29, 2010	Ken Kleinman		 0 Comments
INFORMS Data Mining Competition leaders used Open Source software	https://www.r-bloggers.com/2010/11/informs-data-mining-competition-leaders-used-open-source-software/	November 29, 2010	Larry D'Agostino		 0 Comments
Joy of Stats coming soon	https://www.r-bloggers.com/2010/11/joy-of-stats-coming-soon/	November 29, 2010	Pat	The Joy of Stats really is a joy.  It will be shown on BBC4, apparently scheduled for December 7.  (That date comes from Hans Rosling on twitter, I haven’t found scheduling evidence at the BBC.) I saw its debut at the Royal Statistical Society on World Statistics Day.  Here is a five minute excerpt:  You can see more of Hans Rosling via Ideas for World Statistics Day. David Spiegelhalter is another person who appears in the film.  He is involved with the Understanding Uncertainty website. The software that is visible in the film is Gapminder, but it is a safe bet that R is used in several of the projects that are discussed.  You can see a bit of a bridge between the two from the talk by Markus Gesmann and Eric Wambach “Google motion charts with R” at LondonR. 	 0 Comments
Sweave Tutorial 2: Batch Individual Personality Reports using R, Sweave, and LaTeX	https://www.r-bloggers.com/2010/11/sweave-tutorial-2-batch-individual-personality-reports-using-r-sweave-and-latex/	November 29, 2010	Jeromy Anglim	This post documents an example of using Sweave to generate individualised personality reports based on  responses to a personality test. Each report provides information on both the responses of the general  sample and responses of the specific respondent. All source code is provided, and selected aspects are discussed, including makefiles use of \Sexpr, figures, and LaTeX tables using Sweave.    All source code is available on GitHub: Three examples of compiled PDF reports can be viewed as follows:  ID1ID2and ID4. The resulting report is a simple proof of concept example. main.R loads external functions and packages, imports data, imports metadata and processes the data. Test scores are calculated using the function score.items. 	 0 Comments
Getting Started with Git, EGit, Eclipse, and GitHub: Version Control for R Projects	https://www.r-bloggers.com/2010/11/getting-started-with-git-egit-eclipse-and-github-version-control-for-r-projects/	November 28, 2010	Jeromy Anglim	Version control works really well with R, Sweave, and LaTeX projects. There are many benefits to version control for the data analyst.  Version control allows you to: I also found that adopting version control  facilitated several conceptual benefits. It encouraged greater consideration of: There are many version control systems (see Plastic SCM for a discussion). I’ve chosen to use Git for the following reasons: Finally, the big difference is between using a version control system  and not using a version control system. EGit is a Git plugin for Eclipse. I use Eclipse and StatET to write R code and Sweave documents. I found EGit a particularly easy tool for getting started with Git  and version control. The documentation is straightforward and the interface is easily integrated into my Eclipse workflow. There are many ways to interact with Git. Installing EGit in eclipse involves using the update manager. Vogella.de has a tutorial. To get started with your first Git repository in Eclipse, check out the EGit user Guide. When I was first getting started I used a simple R project rather than a Java Hello World application. GitHub is one of several sites for sharing git repositories  (for example, see Hadley Wickham’s baby names analysis, or my own example of using Sweave to write Multiple Choice Questions). It also has many useful social networking features. Set up a free account on https://github.com/ Work through the tutorial on creating a repository at GitHub While the above tutorial briefly mentions SSH Configuration,   it does not go into detail. When setting up my SSH key, I did the following:   Gists provide a quick way to get started with GitHub. Gists are useful for storing and sharing snippets of code. The result can be embedded into blog posts. To get formatted R code,   give the file name a “.r” file extension (e.g., “test.r”)   (thanks to Hadley Wikham) A simple example of an embedded gist is shown below: Good examples of people sharing R projects on GitHub include: See the suggestions on Stats.SE. I also have an accountin case you are interested.  	 0 Comments
Computing evidence	https://www.r-bloggers.com/2010/11/computing-evidence/	November 28, 2010	xi'an	"The book Random effects and latent variable model selection, edited by David Dunson in 2008 as a Springer Lecture Note. contains several chapters dealing with evidence approximation in mixed effect models. (Incidentally, I would be interested in the story behind the  Lecture Note as I found no explanation in the backcover or in the preface. Some chapters but not all refer to a SAMSI workshop on model uncertainty…) The final chapter (similar to a corresponding paper in JCGS) contains in particular the interesting identity that the Bayes factor opposing model h to model h-1 can be approximated by (the average of the terms)  when I was first surprised when reading this result and wondered whether or not it was suffering either of the Savage-Dickey difficulty or of the harmonic mean instability, but this does not seem to be the case. The ratio is correctly defined thanks to the projection property and the ratio has no particular reason to suffer from infinite variance. To check the practical performances of the method, I tried it on a simple linear model with known variance  comparing the model including the (normal+log-trend) simulated regressor with the model without the regressor. I used a g-prior  on the full model and its(marginal)  projection  which (rather counter-intuitively) involves the regressor values. This modelling should give a Bayes factor equal to  but the difference between the simulated Bayes factor and the actual value is quite large in my simulations, both under the full  > DD
[1] 1.769114e-10
> B01
[1] 0.004805112 and the restricted  > DD
[1] 0.6218985
> B01
[1] 2.516249 models. Here is the R code: I think one explanation for the discrepancy is that the a‘s derived from the full model posterior are quite different from a‘s  that would be generated from the restricted model posterior. The more I  think about the above approximation, the less convinced I am that it  can apply in full generality because of this drawback that the projected  posterior has nothing to do with the posterior associated with the  projected prior. (Since, formally, the same unbiasedness result applies  when comparing a full model with k variables with a submodel  with none but the constant, it is clear that the approximation may be  poor in non-orthogonal situations.) "	 0 Comments
Analyst First – SURF	https://www.r-bloggers.com/2010/11/analyst-first-%e2%80%93-surf/	November 28, 2010	Sydney Users of R Forum - SURF	This presentation is aimed at all those working in commercial and government analytics, irrespective of what tools they use, and also to those students intending on such a career. R and other open source tools play a powerful, unique and disruptive role in business analytics, and are even now changing the landscape. Analyst First  	 0 Comments
Random variable generation (Pt 1 of 3)	https://www.r-bloggers.com/2010/11/random-variable-generation-pt-1-of-3/	November 28, 2010	csgillespie	"As I mentioned in a recent post, I’ve just received a copy of Advanced Markov Chain Monte Carlo Methods. Chapter 1.4 in the book (very quickly) covers random variable generation. A standard algorithm for generating random numbers is the inverse cdf method. The continuous version of the algorithm is as follows: 1. Generate a uniform random variable  2. Compute and return  where  is the inverse of the CDF. Well known examples of this method are the exponential distribution and the Box-Muller transform. I teach this algorithm in one of my classes and I’m always on the look-out for new examples. Something that escaped my notice is that it is easy to generate RN’s using this technique from the Logistic distribution. This distribution has CDF 
and so we can generate a random number from the logistic distribution using the following formula:
 Which is easily converted to R code:


myRLogistic = function(mu, s){

  u = runif(1)

  return(mu + s log(u/(1-u)))

} "	 0 Comments
parser 0.0-12	https://www.r-bloggers.com/2010/11/parser-0-0-12/	November 28, 2010	romain francois	I’ve pushed a new version of the parser package to CRAN.  This is the first release that depends on Rcpp, which allowed me to reduce the code size and increase its maintainability.  This also features a faster version of nlines, a function that retrieves the number of lines of a text file.  	 0 Comments
Rcpp 0.8.9	https://www.r-bloggers.com/2010/11/rcpp-0-8-9/	November 28, 2010	romain francois	Rcpp 0.8.9 was pushed to CRAN recently. Apart from minor bug fixes, this release concentrates on modules, with lots of new features to expose C++ functions and classes through R reference classes. The Rcpp-modules vignette has all the details, and the mjor points are highlighted in the NEWS entry below:  	 0 Comments
Computational efficiency of great-circle distance calculations in R	https://www.r-bloggers.com/2010/11/computational-efficiency-of-great-circle-distance-calculations-in-r/	November 28, 2010	Mario Pineda-Krch	An obvious omission in my previous post on Great-circle distance calculations in R was a lack of discussion on the computational efficiency of the various methods, and in particular comparing different implementations of the same method. One of the comments pointed out that the geosphere package implements spherical trigonometry functions for geographic applications, including great-circle distance calculations. My aim here is to explore the computational efficiency of the various methods, in particular comparing the different implementation of them.Specifically, I will be comparing three methods for calculating great-circle distance, the Spherical Law of Cosines, the Haversine formula and the Vincenty inverse formula for ellipsoids (the previous post has more details on the methods). For the Spherical Law of Cosines, the Haversine formula and the Vincenty inverse formula for ellipsoids I will compare the implementations provided in the geosphere and fields packages with my own implementations (previous post). Note that the fields package only implements the Spherical Law of Cosines. I will do the comparison by timing the distance calculation between 100000 randomly chosen pairs of points where each of the methods is using the same set of points. First I generate the set of points, For the geosphere package I can then time the distance calculations using the Haversine formula as follows, Using the Spherical Law of Cosines, Using the Vincenty inverse formula for ellipsoids, Using the rdist.earth from the fields package, which implements the Spherical Law of Cosines, Using my implementations described in the previous post (the following functions have to be defined deg2rad, gcd.slc, gcd.hf and gcd.vif), After crunching all of this the moment of truth is here. The timings (in seconds) are, Needless to say, I was rather surprised by the results, in particular that packages performed so poorly. My implementations was just a “back  of the envelope” attempt at coding up some of these methods without any conscious goal of being computationally efficient. In retrospect I realize that I did have only one criterion, to keep things short. For example, my version of the Spherical Law of Cosines is a 5 line function (of which only one line does the actual calculations) while the distCosine function is 19 lines (15 lines performing calculations) and the rdist.earth function is 24 lines (9 lines of calculations). Could this explain the large differences in the performances? I am incredulous to this proposition and if this would be the case it would be a serious weakness of R. I recall seeing a recent discussion online somewhere about performance effects of brackets in R with something along the lines of more brackets (e.g. in mathematical expressions) slowing down the evaluation. Is there a connection? I don’t know what to think, but it is disconcerting. On a somewhat different note, clearly the way to go (particularly in a package) would be to implement the core of these type of calculations in C. Since I have yet to find an R implementation that does this, this might be incentive enough for me to actually spend an evening doing it. If someone would be interested in incorporating these into a package I would be quite happy providing the C code for this. This is from the “Mario’s Entangled Bank” blog (http://pineda-krch.com) of Mario Pineda-Krch, a theoretical biologist at the University of Alberta. 	 0 Comments
LaTeX Typesetting – Basics	https://www.r-bloggers.com/2010/11/latex-typesetting-%e2%80%93-basics/	November 28, 2010	Ralph	The LaTeX typesetting is used to create professional looking documents on a home computer. It may have a steeper learning curve than using a Word Processor, but this initial effort will often pay off reasonably quickly. The system is almost a necessity for anyone writing documents with a large amount of mathematics as most alternatives are painful to use efficiently. Fast Tube by Casper In a LaTeX document we need to specify a class, which is a file that defines the formatting styles applied to a document. The standard classes available include article, book, letter and custom classes can be defined to create different visual styles for a LaTeX document. At the top of the document we use the documentclass command to specify the class to use for the document. For an article we would have: The preamble after this command and before the main body of the document can be used to include other packages, define environments or various other actions. The body of the document is included within these commands: It is straightforward to include title information in the document by specifying an author, title and date then followed by a maketitle command. Note that note all of these are compulsory for a document. A simple example of creating a title: We write the text of our LaTeX document as we would a basic text document and then add commands where we want to make changes to the appearance of text. For example we might want to emphasise some of the text using the emph: There are a few special symbols worth being aware of when creating LaTeX documents: To align a block of text the center, flushright and flushleft environments can be used. Examples of using these: The size of the font can be changed using various declarations, which include small, normalsize, large, Large and LARGE. Useful software for working with LaTeX includes: Other useful resources are provided on the Supplementary Material page. A google search will also through up a large number of good online resources for LaTeX. 	 0 Comments
Advanced Markov Chain Monte Carlo Methods (AMCMC)	https://www.r-bloggers.com/2010/11/advanced-markov-chain-monte-carlo-methods-amcmc/	November 27, 2010	csgillespie	 I’ve just received my copy of Advanced Markov Chain Monte Carlo Methods, by Liang, Liu, & Carroll. Although my PhD didn’t really involve any Bayesian methodology (and my undergrad was devoid of any Bayesian influence), I’ve found that the sort of problems I’m now tackling in systems biology demand a Bayesian/MCMC approach. There are a number of introductory books on MCMC, but not that many on advanced techniques. This book suggests that it could be used as a possible textbook or reference guide in a one-semester statistics graduate course. I’m not entirely convinced that it would be a good textbook, but as a reference it looks very promising. A word of warning, if you’re not familiar with MCMC then you should try the Dani Gamerman MCMC book first. Some later chapters look particularly interesting, including topics on adaptive proposals, population-based MCMC methods and dynamic weightings. Anyway, I intend to work through the book (well at least a few of the chapters) and post my results/code as I go. Well that’s the plan anyway.   	 0 Comments
RcppArmadillo 0.2.10	https://www.r-bloggers.com/2010/11/rcpparmadillo-0-2-10/	November 26, 2010	Thinking inside the box	"
The short NEWS file extract follows, also containing Conrad’s entry for 1.0.0:

 
More information is on the 
RcppArmadillo page.
Questions, comments etc should go to the
rcpp-devel mailing list
off the R-Forge page.

 "	 0 Comments
Yet another inferno	https://www.r-bloggers.com/2010/11/yet-another-inferno/	November 26, 2010	Pat	Many from the R world will know The R Inferno. Abstract: If you are using R and you think you’re in hell, this is a map for you. A newly minted inferno is The 9 circles of scientific hell. Most amusing to me is Circle 4: p-value fishing, the punishment of which is brilliant. As a bonus one of the comments gives a glossary of the meanings of some phrases in scientific papers.  Example: “It has long been known that …” really means “I haven’t bothered to look up the reference, but …” 	 0 Comments
Computational tools for Bayesian analysis	https://www.r-bloggers.com/2010/11/computational-tools-for-bayesian-analysis/	November 26, 2010	Shige		 0 Comments
Sweave Tutorial 1: Using Sweave, R, and Make to Generate a PDF of Multiple Choice Questions	https://www.r-bloggers.com/2010/11/sweave-tutorial-1-using-sweave-r-and-make-to-generate-a-pdf-of-multiple-choice-questions/	November 26, 2010	Jeromy Anglim	The repository with all source files is available at: The repository allows you to download all files as an archive or view the files individually on the web.A copy of the final PDF generated from the process is available here I ran the code on Windows with the following programs installed. It should run on MAC and Linux with appropriate R, make, and LaTeX tools installed. Assuming you have the above installed, to run the code The remainder of this post explains the code in each of the main files in the repository. The makefile is used to build the PDF from the Rnw Source.It also performs other useful tasks.A copy of the makefile is shown below: I recently posted on the benefits of makefiles when developing Sweave documents. The make file starts with three variables. The file then has four goals. The default goal is called all:.If make is called without argument from the command line in the project directory, the recipe immediately below all: is run. Note that all apparent indentations are tab indentations (a set of spaces would cause an error). The tex: goal can be called by running make tex at the command line.I use it in case I want there is an error in the when compiling the pdf from the tex file.Sometimes its easier to work out where the bug is by manipulating the intervening tex file.Of course once the problem has been identified, it needs to be incorporated into the Rnw source. The clean: goal removes all files in the output directory (i.e., all the derived files) The backup: goal copies the resulting pdf into the backup folder.I figured this might be useful in order to include a copy of the final product in the repository. The .gitignore file prevents all files in the /.output directory (i.e., the derived files) and the file .project from being placed under version control in git. I’m preparing a post on version control, git, and github which will be posted shortly. Sweave_MCQ.Rnw is the R noweb file that contains chunks of LaTeX and R code.When Sweave is run on this file, the R code chunks are converted into tex and, potentially, image files are generated. The latex preamble is mostly general code that ensures proper display of the reuslting document. The combination of make, R, Sweave, and LaTeX is tremendously powerful.Hopefully, this post encourages a few more people to have a play.To learn more check out some of the following posts and pages: Question on Stats.SE 'Complete substantive examples of reproducible research using R' UPDATE: After posting this article on my blog, Christophe Lalanne let me know about a paper by Bettina Grün and Achim Zeileis called 'Automatic Generation of Exams in R' in JSS. It also uses Sweave, LaTeX, and the exams document class. 	 0 Comments
Hierarchical Cluster Analysis	https://www.r-bloggers.com/2010/11/hierarchial-cluster-analysis/	November 25, 2010	rtutor.chiyau	"

With the distance matrix found in previous tutorial, we can use various techniques of
cluster analysis for relationship discovery. For example, in the data set mtcars, we
can run the distance matrix with hclust, and plot a dendrogram that displays a
hierarchical relationship among the vehicles. 
 read more "	 0 Comments
Benchmarking feature selection with Boruta and caret	https://www.r-bloggers.com/2010/11/benchmarking-feature-selection-with-boruta-and-caret-3/	November 25, 2010	Allan Engelhardt	"
Feature selection is the data mining process of selecting the variables from our data set that may have an impact on the outcome we are considering.  For commercial data mining, which is often characterised by having too many variables for model building, this is an important step in the analysis process.  And since we often work on very large data sets the performance of our process is very important to us.
 
Having looked at feature selection using the Boruta package and feature selection using the caret package separately, we now consider the performance of the two approaches.
 
For our tests we will use an artificially constructed trivial data sets that the automated process should have no problems with (but we will be disappointed later on this expectation, as we will see).  The data set has an equal number of normal and uniform random variables with mean 0 and variance 1 of which 20% are used for the target variable.  There are 10 time as many observations as variables.  We create a function to set this up:
 
Then we run a test using the Boruta package for different sizes:
 
As it turned out, our expectations for the size of data set we could handle were wildly optimistic and we killed the process at size 30.  We add to the data set a field with the total number of variables in the x data set and plot the results.
  Benchmarking results for feature selection with Boruta package shows linear scaling (slope is 1.01 with standard error 0.025 and adjusted R² 0.993) A quick check using summary(lm(log(elapsed) ~ log(n.elem), data = bench)) shows us a linear scaling with the number of elements (slope is 1.01 with standard error 0.025 and adjusted R² 0.993).  The algorithm selects all the right features up to n.vars = 10 when it starts to miss some of them:
 
A higher accuracy in the feature selection for the larger problems could presumably be achieved by adjusting the maxRuns and perhaps confidence parameters on the Boruta call.
 
In summary, the Boruta package performs well up to about 20 features out of 100 (n.vars = 10) which runs in about 11 minutes on my machine.  If we changed the technical implementation to support multicore, MPI, and other parallel frameworks, then the out of the box settings would be useful up to n.vars of 20 or 30 (40-60 features out of 200-300) which an 8-core machine should be able to complete in 20 minutes or so.
 
This is still a lot less than the size of data sets we normally work with.  (Our usual benchmark is 15,000 variables and 50,000 observations.)
 
One of the nice features of the caret package is that is supports most parallel processing frameworks out of the box, but for comparison with the previous analysis we will (somewhat unfairly) not use that here.  The setup is then quite simple, using the same make.data function as before.
 
This uses the randomForest classifier from the package of the same name.  To use the ipredbagg bagging classifier from Andrea Peters and Torsten Hothorn's ipred: Improved Predictors package we simply change the control object to:
 
As usual, we were widely optimistic in our guesses for the size of problems we could handle, and had to abort the run.
  Benchmarking results for feature selection with caret package using randomForest classifier (slope is 1.17 with standard error 0.024 and adjusted R² 0.996)  Benchmarking results for feature selection with caret package using treebag classifier shows non-power behaviour (nevertheless, a linear log-log fit gives a slope of 1.12 with standard error 0.067 and adjusted R² 0.96) 
Remember that the right number of significant features are 2 * n.vars and we see that the caret package apparently always miss one feature in its selection, which is very odd and possibly a bug.  It is less likely to select the wrong features than Boruta, but that could be partially due to ""Tentative"" data in Boruta.  Timing-wise, performance is a little worse in the non-parallel situation but realistically of course a lot better than Boruta depending on the number of cores on your processor or nodes in your cluster.
 
Neither approach is suitable out of the box for the sizes of data sets that we normally work with.
 "	 0 Comments
Random graphs with fixed numbers of neighbours	https://www.r-bloggers.com/2010/11/random-graphs-with-fixed-numbers-of-neighbours/	November 24, 2010	xi'an	"In connection with Le Monde puzzle #46, I eventually managed to write an R program that generates graphs with a given number n of nodes and a given number k of edges leaving each of those nodes. (My early attempt was simply too myopic to achieve any level of success when n was larger than 10!) Here is the core of the R code:  and it uses a sort of annealed backward step to avoid simulating a complete new collection of neighbours when reaching culs-de-sac…. There is nothing fancy or optimised about this code so I figure there are much better versions to be found elsewhere… Ps-As noticed before, sample does not work on a set of length one, which is a bug in my opinion…. Instead, sample(4.5,1) returns a random permutation of (1,2,3,4). > sample(4.5)
[1] 4 3 1 2 "	 0 Comments
R preferred by Kaggle competitors	https://www.r-bloggers.com/2010/11/r-preferred-by-kaggle-competitors/	November 24, 2010	David Smith	Kaggle, the predictive-analytics competition site, has analyzed the preferences of the 2,500 data scientists who participate in its competitions, and R was the most-preferred software of the competitors at 22.5%. The next-nearest alternative was Matlab, at 16%. On a related note, the premier of the Australian state of New South Wales has just launched a competition on Kaggle to predict the traffic on Sydney's M4 motorway. It's great to see government promoting the use of data analysis to solve (or at least better understand) civic problems, and this competition comes with some serious prizemoney: AUD$10,000 (about the same in $USD). Might be worth your time spending the Thanksgiving break doing a little modeling in R… No Free Hunch: Profiling Kaggle’s user base 	 0 Comments
Life Is Short, Use Python	https://www.r-bloggers.com/2010/11/life-is-short-use-python/	November 24, 2010	Quantitative Finance Collector		 0 Comments
The joys of teaching R	https://www.r-bloggers.com/2010/11/the-joys-of-teaching-r/	November 23, 2010	xi'an	Just read a funny but much to the point blog entry on the difficulties of teaching proper programming skills to first year students! I will certainly make use of the style file as grading 180 exams is indeed a recurrent nightmare… 	 0 Comments
Great-circle distance calculations in R	https://www.r-bloggers.com/2010/11/great-circle-distance-calculations-in-r/	November 23, 2010	Mario Pineda-Krch	Recently I found myself needing to calculate the distance between a large number of longitude and latitude locations. As it turns out, because the earth is a three-dimensional object, you cannot simply pretend that you are in Flatland, albeit some beg to differ. Of course, earth is not a perfect sphere either (it’s slightly ellipsoidal with a bit of extra width around the equator), so the exercise is not entirely trivial. There are several approaches one can take, each one with its pros and cons. As one may expect, many of these methods have been implemented in various programing languages with code fragments available at various places on the web. I was not, however, able to find R code implementing the different methods, and in particular, comparing their performance. Well, that’s all the incentive I need to try to remedy this apparent deficiency – after all geodesic distance calculations is surely something people need to do on a regular basis. My aim here is to implement a few selected methods in R and do a “back of the envelope” comparison of their performance, i.e. do their results differ. More specifically I want to calculate the great-circle distance between the two points – that is, the shortest distance over the earth’s surface – giving an ‘as-the-crow-flies’ distance between the points (ignoring any hills). The great-circle distance is why those route maps in the inflight magazines look parabolic when it appears that it would be simpler to use a ruler to connect London with Vancouver. Apparently Air Canada pilots do not bring along straight rulers Many of the formulas calculating the distance between longitude/latitude points assume a spherical earth, which, as we will see, simplifies things greatly and for most purposes is quite an adequate approximation. Perhaps the simples formula for calculating the great-circle distance is the Spherical Law of Cosines which is R looks like so, Here the longitude and latitude coordinates are given in radian, i.e. the latitude and longitude decimal degrees (DD) converted to radians like so, Note that for the decimal degrees positive latitudes are north of the equator, negative latitudes are south of the equator. Positive longitudes are east of Prime Meridian, negative longitudes are west of the Prime Meridian. The Spherical Law of Cosines performs well as long as the distance is not to small (some sources claim it’s accuracy deteriorates at about the 1 metre scale). At very small distances the inverting of the cosine magnifies rounding errors. An alternative formulation that is more robust at small distances is the Haversine formula which in R looks like so (assuming the latitudes and longitudes already have been converted from degrees to radians), While both the Spherical Law of Cosines and the Haversine formula are simple they both assume a spherical earth. Taking into account that Earth is not perfectly spherical makes things a bit more messy. Vincenty inverse formula for ellipsoids is an iterative methods used to calculate the ellipsoidal distance between two points on the surface of a spheroid. A direct “translation” of an implementation by Chris Veness in JavaScript, To “automate” the conversion from to radians and to simplify comparison of the results we wrap up all the methods like so, Generating 10 random pairs of points for which the distances are calculated shows that both the Haversine and the Spherical Law of Cosines give identical results while the Vincenty inverse formula for ellipsoids gives slightly different results. Randomly located point locations for which the distances are calculated using the different methods So why bother with all this coding goodness when you could just use the rdist.earth() function in the fields package. As it turns out the rdist.earth() function gives an estimate that is different from the other methods, After poking around in the rdist.earth() source code it turns out that it assumes the earth’s radius to be 6378.388 km. According to Wikipedia this number seems to be the equatorial radius (the maximum radius). Because earth is not a perfect sphere, however, the radius declines as one moves to the poles reaching a polar minimum of about 6,357 km. The mean radius is 6371 km and is what I have been using in my calculations. When using the mean radius in rdist.earth() one obtains, which is the same result the Haversine and the Spherical Law of Cosines gives. There is much more that can be said for the different methods of calculating the great-circle distance between two points with a vast amount of much more technical discussions available online. To sum up, as expected, all the methods assuming a spherical Earth, i.e. Haversine, Spherical Law of Cosines, and rdist.earth() (once the Earth’s radius has been corrected), give the same results. In contrast, as expected, Vincenty inverse formula for ellipsoids gives different results but appear to be within 100km of the spherical Earth methods. Which method to choose will ultimately depend on factors such as the scale of the distance calculations and the computational performance of the different methods (which I do not address here). For points located close to each other (think short haul flights) the computationally simpler (read, faster) spherical Earth methods will be quite adequate while for  points located far apart (think long haul flights, in particular connecting opposite hemispheres) the substantially more computationally complex (i.e. probably slower) Vincenty inverse formula for ellipsoids will give a better result. This is from the “Mario’s Entangled Bank” blog (http://pineda-krch.com) of Mario Pineda-Krch, a theoretical biologist at the University of Alberta. 	 0 Comments
Principal Component Analysis: Which variables contribute most to principal components ?	https://www.r-bloggers.com/2010/11/principal-component-analysis-which-variables-contribute-most-to-principal-components/	November 23, 2010	altuna		 0 Comments
Slides from first Utah.edu & R.P. RUG meeting	https://www.r-bloggers.com/2010/11/slides-from-first-utah-edu-r-p-rug-meeting/	November 23, 2010	R Blog	"Here are the slides from the first University of Utah and Research Park R Users Group meeting.  They discuss getting help and finding packages.
R 



Related
ShareTweet




To leave a comment for the author, please follow the link and comment on their blog:  R Blog.

R-bloggers.com offers daily e-mail updates about R news and tutorials about learning R and many other topics. Click here if you're looking to post or find an R/data-science job.

Want to share your content on R-bloggers? click here if you have a blog, or  here if you don't.
 "	 0 Comments
How to make beautiful bubble charts with R	https://www.r-bloggers.com/2010/11/how-to-make-beautiful-bubble-charts-with-r/	November 23, 2010	David Smith	Nathan Yau has just published at FlowingData a step-by-step guide on making bubble charts in R. It's actually pretty simple: read in data, sqrt-transform the “bubble” variable (to scale the bubbles by area, not radius), and use the symbols function to plot. It's the last step, though, that really ups the presentation quality: read R's PDF file into Illustrator and clean up for publication: You can mess around with this in R, if you like, but I've found it's way easier to save my file as a PDF and do what I want with Illustrator. I uncluttered the state labels to make them more readable, rotated the y-axis labels, so that they're horizontal, added a legend for population, and removed the outside border. I also brought Georgia to the front, because most of it was hidden by Texas. The final chart is, indeed, beautiful:  FlowingData: How to Make Bubble Charts 	 0 Comments
R and AOL in NYC	https://www.r-bloggers.com/2010/11/r-and-aol-in-nyc/	November 23, 2010	David Smith	R and the NYC R User Group get brief mentions in this article about AOL's offices in New York City. The NYC UseRs meet at AOL and (ironically) the next meeting on Dec 9 is on the topic of R at Google. New York Observer: Bringing Some Sizzle to the Dial-Up King (via) 	 0 Comments
R Style Guide	https://www.r-bloggers.com/2010/11/r-style-guide/	November 23, 2010	csgillespie	"Each year I have the pleasure (actually it’s quite fun) of teaching R programming to first year mathematics and statistics students. The vast majority of these students have no experience of programming, yet think they are good with computers because they use facebook! Debugging students' R scripts The class has around 100 students, and there are eight practicals. In some of  these practicals  the students have to submit code. Although the code is “marked” by a script, this only detects if the code is correct. Therefore, I have to go through a lot of R functions by hand and find bugs. This style guide is intended to be very light touch. It’s intended to give students the basis of good programming style, not be a guide for submitting to cran. File names should end in .R and, of course, be meaningful.  Files should be stored in a meaningful directory – not your Desktop!


GOOD: predict_ad_revenue.R

BAD: foo.R

 Variable names should be lowercase. Use _ to separate words within a name. Strive for concise but meaningful names (this is not easy!)


GOOD: no_of_rolls

BAD: noOfRolls, free


Function names have initial capital letters and are written in CamelCase


GOOD: CalculateAvgClicks

BAD: calculate_avg_clicks , calculateAvgClicks


If possible, make function names verbs. An opening curly brace should never go on its own line; a closing curly brace should always go on its own line. Functions must have a single return function just before the final brace  Of course, the above function could be simplified to test = x < 0
 Comment your code. Entire commented lines should begin with # and one space.  Comments should explain the why, not the what. I decided against putting a section in on ""spacing"" , i.e. place spaces around all binary operators (=, +, -, etc.). I think thus may be taking style a bit too far for first year course. Comments welcome! "	 0 Comments
Programming with R – Processing Football League Data Part I	https://www.r-bloggers.com/2010/11/programming-with-r-%e2%80%93-processing-football-league-data-part-i/	November 23, 2010	Ralph	In this post we will make use of football results data from the football-data.co.uk website to demonstrate creating functions in R to automate a series of standard operations that would be required for results data from various leagues and divisions. The first step is to consider what control options should be available as part of the function and here is a list of some arguments that will be used for this implementation of a football result data processing function: Some of this information might appear optional but is included so that we can write a custom print function at a later date to display a meaningful summary of the object (list) that will be created by the function. The first part of our function is concerned with checking the various values provided to the function arguments. Our skeleton function is as follows: Here we have specified default options for three of the arguments with the most likely number of points for each match outcome, i.e. 3 points for a win and 1 point for a draw. To illustrate the working of the result processing function we will use a small exert from the start of the 2010/2011 English Premiership season which is shown below: This is stored in a file E0test.csv so that we can use the read.csv function to import the results data and then process it. The first series of commands that we add to the function are for checking various function arguments specified by the user to ensure that they are sensible. First up we check whether a results data file has been specified as we cannot do any processing without any results. The simple test is for whether a file name has been specified: It might be sensible to check whether the object datafile is actually a character string specifying a file, but this hasn’t been done for now. We then check whether the country name and division have been specified and set them to blank strings if they haven’t been set by the user. Next up we import the data file and only save the columns of interest (at this point of the development of the function at least. There are many more columns of information that we need in the raw data from the website, The square brackets are used to subset on a part set of columns and only save these. Then we check whether the team names have been specified by the user and if not extract them from the data provided: The sort function is used to order the team names alphabetically which is the order often used in league tables, especially when no games have been played. We then convert the columns HomeTeam and AwayTeam into factors, which allows teams that haven’t played a fixture yet to be included in the table. To round off the first part of creating the result processing function we create a list object to return at the end of the function. The function so far: We then test this function with the data file shown above. First up we create our own list of teams in the English Premiership for 2010/2011 and specify some of the other function arguments while using the defaults for points. 	 0 Comments
Robust adaptive Metropolis algorithm [arXiv:10114381]	https://www.r-bloggers.com/2010/11/robust-adaptive-metropolis-algorithm-arxiv10114381/	November 23, 2010	xi'an	 Matti Vihola has posted a new paper on arXiv about adaptive (random walk) Metropolis-Hastings algorithms. The update in the (lower diagonal) scale matrix is  where The spirit of the adaptation is therefore a Robbins-Monro type adaptation of the covariance matrix in the random walk, with a target acceptance rate. It follows the lines Christophe Andrieu and I had drafted in our [most famous!] unpublished paper, Controlled MCMC for optimal sampling. The current paper shows that the fixed point for  is proportional to the scale of the target if the latter is elliptically symmetric (but does not establish a sufficient condition for convergence). It concludes with a Law of Large Numbers for the empirical average of the  under rather strong assumptions (on f, the target, and the matrices ). The simulations run on formalised examples show a clear improvement over the existing adaptive algorithms (see above) and the method is implemented within Matti Vihola’s Grapham software. I presume Matti will present this latest work during his invited talk at Adap’skiii. Ps-Took me at least 15 minutes to spot the error in the above LaTeX formula, ending up with S^text{T}_{n−1}: Copy-pasting from the pdf file had produced an unconventional minus sign in n−1 that was impossible to spot! 	 0 Comments
Learn Logistic Regression (and beyond)	https://www.r-bloggers.com/2010/11/learn-logistic-regression-and-beyond/	November 23, 2010	John Mount	"One of the current best tools in the machine learning toolbox is the 1930s statistical technique called logistic regression.  We explain how to add professional quality logistic regression to your analytic repertoire and describe a bit beyond that.
A statistical analyst working on data tends to deliberately start simple move cautiously to more complicated methods.  When first encountering data it is best to think in terms of visualization and exploratory data analysis (in the sense of Tukey).  But in the end the analyst is expected to deliver actionable decision procedures- not just pretty charts.  To get actionable advice the analyst will move up to more complicated tools like pivot tables and Naive Bayes.  Once the problem requires control of interactions and dependencies the analyst must move away from these tools and towards the more complicated statistical tools like standard regression, decision trees and logistic regression.  Beyond that we have machine learning tools such as: kernel methods, boosting, bagging, decision trees and support vector machines.  Which tool is best depends a lot on the situation- and the prepared analyst can quickly try many techniques.  Logistic regression is often a winning method and we will use this article to discuss logistic regression a bit deeper.  By the end of this writeup you should be able to use standard tools to perform a logistic regression and know some of the limitations you will want to work beyond. Logistic regression was invented in the late 1930s by statisticians Ronald Fisher and Frank Yates.  The definitive resource on this (and other generalized linear models) is Alan Agresti “Categorical Data Analysis” 2002, New York, Wiley-Interscience.   Logistic regression is a “categorical” tool in that it is used to predict categories (fraud/not-fraud, good/bad …) instead of numeric scores (like standard regression).  For example: consider the “car” data set from the UCI machine learning database ( data, description ).  The data is taken from a consumer review of cars.  Each car is summarized by 6 attributes ( price, maintenance costs, doors, storage size, seating and safety ); there is also a conclusion column that contains the final overall recommendation (unacceptable, acceptable,  good, very good).  The machine learning problem is to infer the reviewer’s relative importance or weight of each feature.  This could be used to influence a cost constrained design of a future car.  This dataset was originally used to demonstrate hierarchical and decision tree based expert systems.  But logistic regression can quickly derive interesting results. Let us perform a complete analysis together (at least in our imaginations if not with our actual computers).  First download and install the excellent free analysts workbench called “R”.  This software package is an implementation of John Chambers’ S language (a statistical language designed to allow for self-service statistics to relieve some of Chambers’ consulting responsibilities) and a near relative of the SPlus system.  This system is powerful and has a number of very good references (our current favorite being Joseph Adler “R in a nutshell” 2009, O’Reilly Media).  Using R  we can build an interesting model in 2 lines of code. After installing R start the program and copy the following line into the R command shell. This has downloaded the car data directly from the UCI database and added a header line so we can refer to variables by name.  To see roll-ups or important summaries of our data we could just type in ""summary(CarData).""  But we will move on with the promised modeling.  Now type in the following line: We now have a complete logistic model.  To examine this model we type ""summary(logisticModel)"".  And see the following (rather intimidating) summary: We also saw the cryptic warning message ""glm.fit: fitted probabilities numerically 0 or 1 occurred"" which we will discuss later.  Returning to our result we see a left column that is formed by concatenating variable names and variable values (values are called ""levels"" when they are strings).  For example the label ""buyinglow"" is a combination of ""buying"" and ""low"" meaning a low purchase price.  The next column (and the last one we will dig into) is the score associated with this combination of variable and level.  The interpretation is that a care that has ""buyinglow"" is given a 5.0481 point score bonus.  Whereas a car with ""safetylow"" is given a -30.5045 scoring penalty.  In fact the complete prediction procedure for a new car is to look the levels specified for all 6 variables and add up the correct scores (plus the ""(Intercept)"" score of  -28.4255 which is used as an initial score).  Any value not found is assumed to be zero.  This summed-up score is called the ""link"" and is essentially the model prediction.  Positive link values are associated with acceptable cars and negative link values are associated with unacceptable cars.  For example the first car in our data set is: According to the columns: we see above our scoring procedure assigns a very bad score of -68 to this car- correctly predicting the ""unacc"" rating.  We can examine the error rates of our model with the single line: While yields the result: This diagram is called a ""contingency table"" and is a very powerful summary.  The rows are labeled with the ratings assigned at training time (unacceptable, acceptable, good and very good).  The columns FALSE and TRUE  denote the model predicted the car was unacceptable or at least acceptable.  From the row ""unacc"" we see that 1166 of the 1166+44 unacceptable cars were correctly predicted FALSE (or not at least acceptable).  Also notice the only face negatives are the 32 FALSEs in the ""acc"" row- none of the good or very good cars were predicted to be unacceptable.  We can also look at this graphically: Which yields the following graph:  This is an area density chart.  Each car that was defined as being unacceptable adds a single blue circle to the bottom of the chart.  Each car that was defined as being acceptable adds a single magenta circle to the bottom of the chart.  The left-right position of each circle is the link score the model assigned to the circle.  There are so many circles that they start to overlap into solid smudges.  To help with this charting software adds the density curves above the circles.  Density curves are a lot like histograms- the height of the curve indicates what fraction of the circles of the same color are under that region of the curve.  So we can see most of the blue circles are in 3 clusters centered at -55, -30 and -5 while the magenta circles are largely clustered around +5.  From a chart like this you can see that a decision procedure of saying a link score above zero is good and below zero is bad would be pretty accurate (most of the blue is to the left and most of the magenta is to the right).  In fact this rule would be over 95% accurate (though accuracy is not a be-all measure, see: Accuracy Measures). So far so good.  We have built a model that accurately predicts conclusions based on inputs (so it could be used to generate new ratings) and furthermore our model has visible ""Estimate"" coefficients that report the relative strength of each feature (which we could use prescriptively in valuing trade-offs in designing a new car).  Except we need to go back to the warning message: ""glm.fit: fitted probabilities numerically 0 or 1 occurred.""  For a logistic regression the only way the model fitter can encounter ""probabilities numerically 0 or 1"" is if the link score was out of a reasonable range of zero (say +-20).  Remember we saw link scores as low as -68.  With a link score of -68 the probability of the car in question being acceptable is around 2*10^-16.  This from a training set that only included 1728 items (so really can not be expected to see events much rarer than one in a thousand. We are deliberately confusing two different types of probabilities here- but it is a good way to think).  What is the cause of these extreme link scores?  Extreme coefficient estimates.  The +29.96 preference for ""persons4"" (cars that seat 4) is a huge vote that swamps out effects from purchase price and maintenance cost.  The model has over fit and made some dangerous extreme assumptions.  What causes this are variable and level combinations that have no falsification in the data set.  For example: suppose only one car had the variable level ""person4"" and that car happened to be acceptable.  The logistic modeling package could always raise the link score of that single car by putting a bit more weight on the ""person4"" estimate.  Since this variable level shows up only in positive  examples (and in this case only one example) there is absolutely no penalty for increasing the coefficient.  Logistic regression models are built by an optimizer.  And when an optimizer finds a situation with no penalty- it abuses the situation to no end.  This is what the warning was reporting.  All link numbers map to probabilities between zero and one; only ridiculously large link values map to probabilities near one (and only ridiculously small values map to probabilities near zero).  The optimizer was caught trying to make some of the coefficients ""run away"" or ""run to infinity.""  There are some additional diagnostic signs (such as the large coefficients, large standard errors and low significant levels), but there is no advice offered by the system in how to deal with this.  The standard methods are to suppress problem variables and levels (or suppress data with the problem variables and levels present) from the model.  But this is inefficient in that the only way we have of preventing a variable from appearing to be too useful is not to use it.  These are exactly the variables we do not want to eliminate from the model, but they are unsafe to keep in the model (their presence can cause unreliable predictions on new data not seen during training). What can we do to fix this?  We need to ensure that running a coefficient to infinity is not without cost.  One way to achieve this would be something like Laplace smoothing where we enter two made-up data items: one that has every level set on and is acceptable and one that has every level set on and is unacceptable.  Unfortunately there is no easy way to do this from the user-layer in R.  For example each datum can only have one value set for each categorical variable- so we can't define a datum that has all features on.  Another way to fix this would be to directly penalize large coefficients (like Tychonoff regularization in linear algebra).  Explicit regularization is a good idea and very much in the current style.  Again, unfortunately, the R user layer does not expose a regularization control to the user.  But one of the advantages of logistic regression is that it is relatively easy to implement (harder than Naive Bayes or standard regression in that it needs and optimizer, but easier than SVM in that the optimizer is fairly trivial). The logistic regression optimizer works to find a model of probabilities p() that maximizes the sum:  Or in english: assign large probabilities to examples known to be positive and small probabilities to examples known to be negative.  Now the logistic model assigns probabilities using a function of the form:  The beta is the model parameters and the x is the data associated with a given example.  The dot product of beta and x is the link score we saw earlier.  The rest of the function is called the sigmoid (also used by neural nets) and its inverse is called the ""logit"" which is where logistic regression gets its name.  Now this particular function (and others have the so-called ""canonical link"") has the property that the gradient (the vector of derivative directions indicating the direction of most rapid score improvement) is particularly beautiful.  The vector gradient is:  This quantity is a vector because it is a weighted sum over the data_i (which are all vectors of feature values and value/level indicators).  As we expect the gradient to be zero at an optimal point we now have a set of equations we expect to be simultaneously satisfied at the optimal model parameters.  In fact these equations are enough to essentially determine the model- find parameter values that satisfy all of this vector equation and you have found the model (this is usually done by the Newton–Raphson method or by Fisher Scoring).  As an aside this is also the set of equations that the maximum entropy method must specify; which is why for probability problems maximum entropy and logistic regression models are essentially identical.   If we directly add a penalty for large coefficients to our original scoring function as below:  Beta is (again) our model weights (laid out in the same order as the per-datum variables) and epsilon (>=0) is the new user control for regularization.  A small positive epsilon will cause regularization without great damage to model performance (for our example we used epsilon = 0.1). Now our optimal gradient equations (or set of conditions we must meet) become:  Or- instead of the model having to reproduce all know feature summaries (if a feature is on in 30% of the positive training data then it must be on for 30% of the modeled positive probabilities) we now have a slop term of 2 epsilon beta.  To the extent a coefficient is large its matching summary is allowed slack (making it harder for the summary to drive the coefficient to infinity).  This system of equations is as easy to solve as the original system (a slightly different update is used in the Newton-Raphson method) and we get a solution as below: This model has essentially identical performance and much smaller coefficients.  From a performance point of view this is essentially the same model.  What has changed is the model no longer is able to pick up a small bonus by ""piling on"" a coefficient.  For example moving a link value to infinity moves the probabilities from 0.999 to 1.0 which in turn moves the data penalty (assuming the data point is positive) from log(0.999) to log(1.0) or from -0.001 to 0.0.  The approximate 1/1000 score improvement is offset by a penalty proportional to the size of the coefficient- making the useless ""adding of nines"" no longer worth it.  Or, as has become famous with large margin classifiers, it is important to improve what the model does on probability estimates near 0.5 not estimates already near 0 or 1.  As expected: the coefficients are significantly different than the standard logistic regression.  For example the original model has 3 variables with extreme levels (the intercept, number of passengers and safety) while the new model sees only extreme values (but much smaller) for number of persons and safety (which are likely correlated).  Also consider the difference between the buying levels low and very high in the original model (5 - -2 = 7) and in the new model (2 - -3.5 = 5.5) differ by 1.5 or around 3 of the reported standard deviations (indicating the significance summaries are not enough to certify the location of model parameters).  It is not just that all of the coefficients have shifted, many of the differences are smaller (and others are not- changing the emphasis of the model).  We don not want to overstate differences- we are not so much looking for something better than standard logistic regression as adding an automatic safety that saves us both the effort and loss of predictive power found in fixing models by suppressing unusually distributed (but useful) variables and levels. An analyst is well served to have logistic regression (and the density plots plus contingency table summaries) as ready tools.  These methods will take you quite far.  And if you start hitting the limits of these tools you can, as we do, bring in custom tools that allow for explicit regularization yielding effective and reliable results. Related posts: "	 0 Comments
makefiles for Sweave, R and LaTeX using Eclipse on Windows	https://www.r-bloggers.com/2010/11/makefiles-for-sweave-r-and-latex-using-eclipse-on-windows/	November 22, 2010	Jeromy Anglim	make is software that uses makefiles to build projects. make has many benefits. make has won the ACM Software System Award. It shares this honour with two of my other favourite tools: TeX and S. make has also encouraged me to think more about dependencies, one-click builds, and project file structures. Specifically, I use make in conjunction with R, Sweave, LaTeX, and other command line tools to build reproducible research output. I’ll post some examples in the near future.  In order to run Sweave on the command line on Windows using: R CMD Sweave foo.Rnw: Introduction to R has more information about running R at the command line.   I use Eclipse to edit R and Sweave files. To configure make in Eclipse:  Thus, with a makefile in the root directory of a project, Run - External Tools - Make runs the makefile. Also note: 	 0 Comments
R.I.P. StatProb?	https://www.r-bloggers.com/2010/11/r-i-p-%c2%a0statprob/	November 22, 2010	xi'an	As posted in early August from JSM 2010 in Vancouver, StatProb was launched as a way to promote an on-line encyclopedia/wiki with the scientific backup of expert reviewers. This was completely novel and I was quite excited to take part in the venture as a representative of the Royal Statistical Society. Most unfortunately, the separation of the originator of the project, John Kimmel, and of the editor Springer-Verlag (which is backing up the project) a few weeks later put an almost sure stop to the experiment by exposing the lack of incentive in investing a not-inconsiderable amount of our time in editing the entries and the need for part-time operators that would handle LaTeX and other editorial issues… The core of the matter is, I think, that the “reward” in getting involved in the wiki is sadly too limited from an academic perspective to balance the investment (the more because most members of the editorial board were senior researchers). This was clear for instance in the search of a person in charge of the LaTeX aspects of the submissions: I could not find a strong enough reason to convince a younger colleague to dedicate part of his (limitless!) energy to this task, apart from service to the community… So, in the end, and in agreement with the Royal Statistical Society, I have sadly resigned from the board of StatProb along with George Casella and Nando de Freitas. 	 0 Comments
Access the InfoChimps API from R	https://www.r-bloggers.com/2010/11/access-the-infochimps-api-from-r/	November 22, 2010	David Smith	InfoChimps.com is mainly known as a clearinghouse for finding large data sets, for free or for sale. But they have also released (in beta, at least) an API that lets you find some pretty useful information on-demand. Normally, you'd have you use RESTful calls to access the API, but now Drew Conway has created an R package (and released gist sources) that lets you query the API using simple R commands. With the infochimps package, you can: To use the infochimps package in R, you'll need to sign up for an API key. (The free subscription is limited in the number of API calls allowed, and requires attribution.) inside-R Package Reference: infochimps 	 0 Comments
Example 8.15: Firth logistic regression	https://www.r-bloggers.com/2010/11/example-8-15-firth-logistic-regression/	November 22, 2010	Ken Kleinman		 0 Comments
Homage to floating points	https://www.r-bloggers.com/2010/11/homage-to-floating-points/	November 22, 2010	Martin Scharm	"I recently got very close to the floating point trap, again, so here is a little tribute with some small examples! 
Because Gnu R is very nice in suppressing these errors, all examples are presented in R. Those of you that are ignorant like me, might think that 0.1 equals 0.1 and expect 0.1==0.1 to be true, it isn’t! Just see the following: You might think it comes from the division, so you might expect seq(0, 1, by=0.1) == 0.3 contains exactly one vale that is TRUE!? Harrharr, nothing like that! Furthermore, what do you think is the size of unique(c(0.3, 0.4 - 0.1, 0.5 - 0.2, 0.6 - 0.3, 0.7 - 0.4))!? Is it one? Not even close to it: Your machine is that stupid, that it isn’t able to save such simple numbers 😉
And another example should show you how these errors sum up: As you can see, R tells you that you summed up to exactly one, suppressing the small numerical error. This error will increase with larger calculations! So be careful with any comparisons.
To not fail the next time, for example use the R build-in function all.equal for comparison: Or, if you’re dealing with integers, you should use round or as.integer to make sure they really are integers. I hope I could prevent some of you falling into this floating point trap! So stop arguing about numerical errors and start caring for logical fails 😉 Those of you interested in further wondering are referred to [Mon08]. "	 0 Comments
Retrieving transcriptome sequences for RNASeq analysis	https://www.r-bloggers.com/2010/11/retrieving-transcriptome-sequences-for-rnaseq-analysis/	November 22, 2010	JC		 0 Comments
Were stock returns really better in 2007 than 2008?	https://www.r-bloggers.com/2010/11/were-stock-returns-really-better-in-2007-than-2008/	November 22, 2010	Pat	"We know that the S&P 500 was up a little in 2007 and down a lot in 2008.  So on the surface the question seems really stupid.  But randomness played a part.  Let’s have a go at deciding how much of a part. Figure 1: Comparison of 2007 and 2008 for the S&P 500.
 The bootstrap is an easy way to find the variability of a statistical procedure.  (Easy, that is, if you have an appropriate computing environment, such as R.)  The idea is to recompute your procedure over and over again using samples from your original data. We get the annual log return by summing the daily log returns (see A tale of two returns).  So we can do a bootstrap for this in R like:

> bs07 
> for(i in 1:10000) bs07[i] 
+     251, replace=TRUE))
> plot(density(bs07)) # essentially Figure 2 Figure 2: Bootstrap distribution for S&P 500 for 2007. You can use the multiverse interpretation to understand Figure 2.  We are looking at ten thousand different universes — each of them has the same distribution of daily returns as our universe in 2007.  Some universes are lucky, some are not. Figure 3: Bootstrap distributions for the S&P 500 for each of 2007 and 2008.
 Figure 3 shows the bootstrap distributions of both 2007 and 2008.  The spread for 2008 is amazing.  Some of the most extremely low values lost about 80% of the value (a simple return of –.8 corresponds to a log return of about –1.6).  I’m intrigued with the other extreme: 2008 has more mass on the right than 2007. Volatility was much higher in 2008 than 2007.  That is why the 2008 distribution is so much wider. Figure 4: Bootstrap distribution of year 2007 minus year 2008. Figure 4 shows the return in 2007 minus the return in 2008 for each of the ten thousand universes.  Our presumption is that this should be unambiguously above zero.  It isn’t — about 12% of the distribution is below zero. One of the really nice things about the bootstrap is that there is a minimum of assumptions.  The one key assumption that it does have is that the data are independent and identically distributed. Our return data are not independent and identically distributed. We know for sure that there is volatility clustering — periods of low volatility and periods of high volatility.  A lot of people presume that the expected value of the return changes over time as well — that there are bull and bear markets.  Belief in that may make it true even if it weren’t already true. One approach of handling the problem of changes in expected returns is to use the loess nonparametric regression in R — we estimate expected return for each point in time.  This doesn’t take care of volatility clustering, but that is much less problematic for the inference that we are trying to make. Figure 5: Two loess views of expected returns. The red line in Figure 5 shows the default loess fit.  The other line uses robust estimation and much less smoothing.  This is probably about as wiggly of a line as we would want.  We see that at the worst point, it is expecting the return to be less than -1% per day. We can use the fit and residuals from this model to do bootstrapping again.  The resulting bootstrap distributions are virtually identical to the original ones.  Figure 6 gives a hint of why this is the case. Figure 6: 2008 loess residuals compared to the returns. Figure 6 shows that the distribution of the residuals (from the wiggly fit) and the original returns are not very different.  The extreme negative fit is associated with large positive returns as well as large negative returns.  We haven’t managed to squeeze much noise into signal. Surprisingly, we haven’t found much evidence that 2007 was really better than 2008.  Perhaps someone else sees a clever way of partitioning signal and noise. Here is more on statistical bootstrapping and its relatives. 
> spxdf 
> losym10 
+     family=""symmetric"", span=.1)

The first line above creates a suitable data frame consisting of a column that is the number of trading days from the start of 2007 and a column that is the daily returns. The second line fits the loess model using a span of 10% of the data (instead of the default of 75%) and using the symmetric family rather than the gaussian family.  This is the fit that appears prominently in Figure 5. "	 0 Comments
Graphical comparison of MCMC performance [arXiv:1011.445]	https://www.r-bloggers.com/2010/11/graphical-comparison-of-mcmc-performance-arxiv1011-445/	November 22, 2010	xi'an	"   
 A new posting on arXiv by Madeleine Thompson on a graphical tool for assessing performance. She has developed a software called SamplerCompare, implemented in R and C. The graphical evaluation plots “log density evaluations per iteration times autocorrelation time against a tuning parameter in a grid of plots where rows represent distributions and columns represent methods”. The autocorrelation time is evaluated in the same way as coda, which is the central package used in the convergence assessment chapter of Introducing Monte Carlo Methods with R because of its array of partial (if imperfect) indicators. Note that there is an approximation factor in the evaluation of the autocorrelation time because the MCMC output is represented as an AR(p) series, with a possible divergence artifact in the corresponding confidence interval if the AR(p) process is found to be non-stationary. When the simulation method (corresponding to columns in the above graphs) allows for an optimal value of its (cyber-)parameters, the performances exhibit a clear parabolic pattern (right graph), but this is not always the case (left graph). Graphical tools are always to be preferred to tables (a point Andrew would not rebuke!), However I do not see the point in simultaneously graphing the performances of different MCMC algorithms for different targets. This “wasted” dimension could instead be used for increasing to at least three the number of cyber-parameters evaluated by the method. "	 0 Comments
Animate .gif images in R / ImageMagick	https://www.r-bloggers.com/2010/11/animate-gif-images-in-r-imagemagick/	November 21, 2010	markheckmann	"Yesterday I surfed the web looking for 3D wireframe examples to explain linear models in class. I stumbled across this site where animated 3D wireframe plots are outputted by SAS.  Below I did something similar in R. This post shows the few steps of how to create an animated .gif file using R and ImageMagick. Here I assume that you have ImageMagick installed on your computer. As far as I know it is also possible to produce animated .gif files using R only, e.g. with write.gif() from the caTools package. But using ImageMagick is straighforward, gives you control over the conversion and .gif production and is the free standard program for conversion. First a simple countdown example. To be sure not to overwrite anything I will create a new folder and set the working directory to the new folder﻿.  Above a loop is used to do the plotting. A new .png file for each plot is created automatically. The ""%02d"" part in the  filenamepart is a placeholder here for a two character counter (01,02 etc.). So we do not have to hard-code the filename each time. Now I want a linear model to be visualized as a 3d mesh.  A 3D surface can easily be plotted using the wireframe() function from the lattice package (or other functions available in R; also see the rgl package for rotatable 3D output). Now let’s create multiple files while changing the rotation angle. Note that wireframe() returns a trellis object which needs to be printed explicitly here using print(). As the code below produces over 150 images and merges them into one .gif file note that this may take a minute or two.   Now I want the same as above but for a model with an interaction and I want to make the plot a bit more pretty. This time I use .pdf as output file. This is just to demonstrate that other formats  than .png can be used. Note that the ""%02d"" part of the filename has disappeared as I only create one .pdf file with multiple pages, not multiple .pdf files.  The last example is a visual comparison of the interaction and a non-interaction model. Here we now have the models on the same scale. Before I did not specify the scale limits.  Above I chose a small image size (300 x 300 pts). Smaller steps for rotation and a bigger picture size increases file sizes for examples 2, 3 and 4 to 3-5mb which is far too big for a web format. I am not familiar with image optimization and I suppose a smaller file sizes for the .gif file can easily be achieved by some optimization flags in ImageMagick. Any hints are welcome! "	 0 Comments
My First R Package: infochimps	https://www.r-bloggers.com/2010/11/my-first-r-package-infochimps/	November 20, 2010	Drew Conway	"I have finally taken the plunge and created my first R package!  As frequent readers will know, I often sing the praises of infochimps, a startup out of Austin, TX attempting to be the world’s data clearinghouse.  While infochimps is an excellent resource for data sets, they also provide their own set excellent data APIs, which provide information about Twitter, U.S. Census data, and IP geo-location. I use these APIs quite a bit, and after writing several functions to pull data from them I decided it was time to write a full-blown R wrapper to all of these APIs.  For your enjoyment, I submit the infochimps R package, which provides R functions to download and parse data from all of the current infochimps APIs. I submitted the package to CRAN this afternoon, but am still awaiting approval.  In the meantime, if you would like to test out the package you are welcome to download and install it.  This process is very straightforward, simply download the source code and run the following command at the R console: > install.packages(""infochimps_0.1.tar.gz"", repos=NULL, type=""source"") Happy data mining, and please let me know what breaks! "	 0 Comments
R function for reading big tables	https://www.r-bloggers.com/2010/11/r-function-for-reading-big-tables/	November 20, 2010	frenkiboy		 0 Comments
ShortCut[R]: locator	https://www.r-bloggers.com/2010/11/shortcutr-locator/	November 20, 2010	Martin Scharm	"Welcome to my new category: ShortCut! Here I’ll shortly explain some smart features, unknown extensions or uncommon pathways of going for gold.
Today it’s about the Gnu R tool locator. 
With locator you are able to detect the mouse position inside you plot. Just run locator() and click some points, when you’re finished click the right button and locator will print the x– and y-values of the clicked positions.
With this tool it’s possible to visually validate some numerical calculation. With a little bit more code, you can output the coordinates right into you plot: Figure 1: locator example With a click into the plot you’ll be able to create a result like figure 1. "	 0 Comments
Running R on remote computer via local emacs	https://www.r-bloggers.com/2010/11/running-r-on-remote-computer-via-local-emacs/	November 19, 2010	t-man		 0 Comments
"Finally! A practical R book on Data Mining:  ""Data Mining With R, Learning with Case Studies,"" by Luis Torgo"	https://www.r-bloggers.com/2010/11/finally-a-practical-r-book-on-data-mining-data-mining-with-r-learning-with-case-studies-by-luis-torgo/	November 19, 2010	Intelligent Trading		 0 Comments
Is there a Market for Premium R Packages?	https://www.r-bloggers.com/2010/11/is-there-a-market-for-premium-r-packages/	November 19, 2010	Thomas Hopper	Nathan Yau, of the excellent FlowingData blog, recently asked on his Twitter stream: I wonder if there’s a market for premium R packages, like there is for say, @wordpress themes and plugins There are some great packages available for R, all of which are currently free. I think it would be great if authors like Hadley Wickham and Ian Fellows received remuneration for their efforts. However, I see a trap here. From my perspective, R has two main barriers to adoption: the learning curve and IT support. The learning curve is steep enough that casual users will not get very far, and infrequent users tend to slide backwards and have to relearn (I’ve had to develop a mind map of common functions to help mitigate this problem for myself). R packages generally address the learning curve. There aren’t many packages that provide functions that the user couldn’t have created for themselves with base R, but the packages make using their functions much easier. ggplot2 and its support packages plyr and reshape make a perfect example. The default R graphical output is pretty good, but ggplot2 offers better aesthetic defaults and provides an easier path to advanced functions, like transforming data and adding fitted curves and “ribbons.” IT departments will not have any readily available, professional support should problems arise with any R installation. I’ve seen a couple of IT departments balk at supporting open source software for this very reason, and one of them balked at supporting R for this reason. IT departments must evaluate software through the lenses of incident response and down time. However good the community, open source software leaves a big uncertainty when planning for support budgets. The only solution that I’ve found for IT is to convince them that they don’t have to support R; I can do it myself. I’m sure some of you are luckier that way, and it seems that Revolution is slowly addressing this issue, but it’s not an issue that has been generally addressed in the community. In addition, IT departments are usually responsible for ensuring that all software installed on their organization’s computers is legally licensed to the organization. With everything currently free, that’s a problem that is easily overcome. Make ggplot2, or any other package, available only to those who can pay, and you exacerbate the two main problems with R: great functionality that flattens the learning curve will be lost to a large segment of users (i.e. casual or infrequent users and cash-strapped users like students), and IT departments will have to choose between actively supporting R and simply banning it. Providing user-installable R packages where some are freely licensed and others are not would create an environment where some IT departments would simply ban R rather than have to sort out the licensing issues. I suspect that many would ban R. We need a way to repay package authors for their time, without losing the benefits of freely available packages. Donationware seems like a good first step, even though the response rate is typically very low. 	 0 Comments
Airport security: science vs backlash	https://www.r-bloggers.com/2010/11/airport-security-science-vs-backlash/	November 19, 2010	David Smith	The United States has recently introduced millimeter wave and backscatter x-ray scanners to the security screening process in many airports, prompting a backlash in some quarters. Much of the opposition is centered around the invasion of privacy: the scanners generate an image of the traveller's naked body. There are also health concerns, at least for the backscatter x-ray variants of the scanners. I had initially dismissed these concerns as part of the usual overblown fears of anything related to radiation, but some scientists have recently raised concerns that while the radiation dose is low, unlike a medical x-ray or cosmic radiation, the entire dose is concentrated on the skin, intensifying the local dose by orders of magnitude. Personally, I agree with the that more study is warranted. But are the scanners worth it, overall? Whether they are successful in deterring terrorists is one question that's been endlessly debated. But what about the effect of these security procedures on the economy? Nate Silver of the New York Times likens these security procedures as a “tax upon air travel”: Teleconferences are often a poor substitute for person-to-person interaction, and when people are reluctant to travel, some business deals don’t get done that otherwise would have. Recreational travelers, meanwhile, may skip out on vacations that otherwise would have brought them pleasure and stress-relief (while improving revenues for tourism-dependent economies). The tenuous profits of the airline industry are also affected, of course. Revenue losses from the new bag-checking procedures may have measured in the billions. It certainly seems plausible that there is a quantifiable drain on the economy. But could we measure it? Nate suggests we could, via a statistical experiment. If, for instance, Chicago O’Hare installs new machines and Chicago Midway does not, and passenger traffic at O’Hare drops by 9 percent while traffic at Midway holds steady, that would provide considerably more tangible evidence of how travelers are reacting to the new protocols than polls ever could. It's a great idea. It's just a shame that political considerations would never allow it to be implemented. New York Times: The Hidden Costs of Extra Airport Security 	 0 Comments
Making R growl	https://www.r-bloggers.com/2010/11/making-r-growl/	November 18, 2010	Mario Pineda-Krch	Spending the day churning through large data set or doing some heavy-duty number crunching? What is one to do while the computer is running in overdrive? We’ll, for one, you could get a steaming cup of joe and write a post telling the world how you will know when the chomping and crunching is done. Some of you may say, just keep an eye on the program and/or the output. I say, that’s boring and not so practical when simultaneously running a dozen applications, each one with a fistful of windows spread out across 9 virtual desktops. Also, did I say it’s boring? Anyway, imagine the following scenario; an Apple running snow leopard, R doing the number crunching, and a mint installation of Growl. Now, it would only make sense if R could display notifications through Growl, e.g. when the chomping and crunching is done. Well, as it turns out it can, all one has to do is to, more or less, follow the instructions of Michael Ewens. Turns out the current installation process is slightly different (easier?) and the implementation in R can be spiced up a snap. So, for the record, here is my version of the set up, Now you are able to make R send notifications through Growl by declaring a one liner, There are a few twists to this version compared to previous incarnations, specifically, it gets the R application icon right, there is a system beep as part of the notification, and last but not least all this goodness is conveniently prepackaged as a short (aka oneliner) function, i.e. declare once and then unleash all the R growling you can think of like so or even Oops, there it goes. Gotta go…  This is from the “Mario’s Entangled Bank” blog (http://pineda-krch.com) of Mario Pineda-Krch, a theoretical biologist at the University of Alberta. 	 0 Comments
Competitive Data Science: An Update	https://www.r-bloggers.com/2010/11/competitive-data-science-an-update/	November 18, 2010	David Smith	A quick reminder that two competitions based around data analysis, both very suited to R, are currently underway. First, there's still plenty of time to enter the competition to predict popular R packages, announced by the The Dataists and hosted at Kaggle. According to organizer Drew Conway, the competition has already received 114 entries from 21 teams. But with 13 weeks of competition remaining, there's still plenty of time to oust the leader (who is currently scoring 0.982349 AUC on the test data). I've also heard that new data may be added to the competition soon, which should shake things up a bit. And to encourage even more participation, Revolution Analytics has chipped in an additional prize for the winner: a 32Gb iPod Touch. See the announcement at The Dataists for info on how to join the fray. Second, the How Do People Use Firefox data visualization competition, announced earlier this month, is now underway. The data from the Mozilla Test Pilot program is now live, ripe for some insightful visualizations – the ggplot2 package in R will likely be the tool of choice for many of the entrants. Check out the discussion forum to see what others are doing, and the main competition page for details on how to enter. The deadline for submissions is December 15.  	 0 Comments
Rapidminer + R Example for Trading	https://www.r-bloggers.com/2010/11/rapidminer-r-example-for-trading/	November 18, 2010	a Physicist		 0 Comments
Stat Computing Visions from the Past	https://www.r-bloggers.com/2010/11/stat-computing-visions-from-the-past/	November 18, 2010	martin	I recently stumbled upon an old paper of a presentation I gave at the Interface conference in 1998, entitled “JAVA – the next Generation of Statistical Computing?”:  It is very interesting to compare the things I envisioned 12 years ago and what actually came true. Here are some topics: What did you think what statcomp would look in the future 12 years ago? Should we be happy or should we be disappointed? PS: Yes, I stole the idea of the paper thumbnail from Robert’s eagereyes – but I am confident he won’t mind … 	 0 Comments
Logistic regression – simulation for a power calculation…	https://www.r-bloggers.com/2010/11/logistic-regression-simulation-for-a-power-calculation/	November 18, 2010	respiratoryclub	"
Please note – I’ve spotted a problem with the approach taken in this post – it seems to underestimate power in certain circumstances.  I’ll post again with a correction or a more full explanation when I’ve sorted it.   So, I posted an answer on cross validation regarding logistic regression.   I thought I’d post it in a little more depth here, with a few illustrative figures.  It’s based on the approach which Stephen Kolassa described.   Power calculations for logistic regression are discussed in some detail in Hosmer and Lemeshow (Ch 8.5).  One approach with R is to simulate a dataset a few thousand times, and see how often your dataset gets the p value right.  If it does 95% of the time, then you have 95% power. In this code we use the approach which Kleinman and Horton use to simulate data for a logistic regression.  We then initially calculate the overall proportion of events.   To change the number of events adjust odds.ratio.  The independent variable is assumed to be normally distributed with mean 0 and variance 1.   This plot shows how the intercept and odds ratio affect the overall proportion of events per trial:
 When you’re happy that the proportion of events is right (with some prior knowledge of the dataset), you can then fit a model and calculate a p value for that model.  We use R’s inbuilt function replicate to do this 10,000 times, and count the proportion where it gets it right (i.e. p < 0.05).  The proportion of the time that the simulation correctly get's the p < 0.05 is essentially the power of the logistic regression for your number of cases, odds ratio and intercept. I checked it against the examples given in Hsieh, 1999.  It seemed to work pretty well calculating the power to be within ~ 1% of the power of the examples given in table II of that paper. We can do some interesting things with R.  I simulated a range of odds ratios and a range of sample sizes.  The plot of these looks like this (each line represents an odd ratio):-
 We can also keep the odds ratio constant, but adjust the proportion of events per trial.  This looks like this (each line represents an event rate):
 As ever, if anyone can spot an error or suggest a simpler way to do this then let me know.  I haven’t tested my simulation against any packages which calculate power for logistic regression, but if anyone can it would be great to hear from you. "	 0 Comments
dcemriS4 0.40	https://www.r-bloggers.com/2010/11/dcemris4-0-40/	November 18, 2010	Brandon Whitcher		 0 Comments
Introducing Monte Carlo in PaRis [more slides]	https://www.r-bloggers.com/2010/11/introducing-monte-carlo-in-paris-more-slides/	November 17, 2010	xi'an	The class started yesterday with a small but focussed and responsive audience! Given the background of the students, and in particular their clear proficiency in R!, I switched between the original slides of Introducing Monte Carlo Methods with R and those of my Monte Carlo Statistical Methods: course, updated by Olivier Cappé who is teaching the course in Paris-Dauphine this year.  	 0 Comments
Wanted: R hackers for Revolution	https://www.r-bloggers.com/2010/11/wanted-r-hackers-for-revolution/	November 17, 2010	David Smith	Revolution Analytics is growing, and we're looking for some skilled R Hackers to work in our pre-Sales team. A big part of our task is showing companies how R is such a great tool for modern data analysis (especially compared to those older tools with 3- or 4-letter acronyms). So if you have a knack for applying R to new problems and data sets (especially on a short deadline) and the enthusiasm and personality to show it off in person, let us know. You can apply at the link below, and the full job description is after the jump. On a personal note, I took a similar role as my first job after my post-grad work. It was for S-PLUS (this was before R even existed), and I totally loved it. It was a blast being able to apply the data analysis skills I'd spent so long learning to real problems on a day by day basis. On top of that, I got to learn the specifics of data analysis in many “verticals” (quantitative finance, drug discovery, etc.) and meet hundreds of interesting people in new places. If you're looking for a fun job that broadens your skills in practical ways, I can personally recommend joining the pre-Sales team. Revolution Analytics careers: Pre-Sales / Technical Sales Revolution Analytics is the leading commercial provider of software and support for the open source “R” statistical computing language. Our products, including Revolution R and Revolution R Enterprise, enable statisticians, scientists and others to create superior predictive models and derive meaning from mission-critical data in record time. Revolution Analytics works closely with the R community to incorporate the latest developments in open source R and with our clients to support their efforts to produce ground-breaking innovations in life sciences, financial services, defense technology and other industries where high-level analytics are crucial to success. The ideal candidate will be passionate about R, have experience with data analysis in an enterprise data environment, and love working with customers.  In addition, candidates should be solution oriented with an ability to stay cool while managing multiple tasks and changing priorities. Duties and Responsibilities     	 0 Comments
Its 9am, do you know what the traders are thinking?	https://www.r-bloggers.com/2010/11/its-9am-do-you-know-what-the-traders-are-thinking/	November 17, 2010	Lloyd Spencer		 0 Comments
Syntax Highlighting R Code, Revisited	https://www.r-bloggers.com/2010/11/syntax-highlighting-r-code-revisited/	November 17, 2010	Stephen Turner		 0 Comments
Updated SoilWeb Usage Statistics	https://www.r-bloggers.com/2010/11/updated-soilweb-usage-statistics/	November 16, 2010	dylan	Google Earth Access Trends: Daily Requests read more 	 0 Comments
ACM Data Mining Camp	https://www.r-bloggers.com/2010/11/acm-data-mining-camp/	November 16, 2010	Joseph Rickert	By guest blogger Joseph Rickert. I was very happy to be a part of the ACM Data Mining camp held last Saturday (November 13th) at eBay. It was a big day for discussing hot topics in data mining, Mahout, parallel SVMs etc, and also a pretty big day for R.  Because Revolution Analytics was a sponsor for the camp, I got to give a three minute company pitch and was very pleased to have people applaud my “I ‘heart’ R” slide.  (A cynic might point out that “I heart R” was my last slide and people were just relieved that I was finished speaking – three minutes of listening to me being a significant time investment. However, since the I ♥ R stickers went flying off the table where sponsors were allowed to leave their collateral material; I do think that at least a few data miners are developing some affection for R.) In addition to my brief presentation, I led a session on manipulating large data sets in R in that was attended by maybe 100 people. I was expecting to run some code real -time for a group of about twenty or so, but found myself instead up at the podium in the large conference room.  Some adjustment was necessary for the larger audience, but I did show Revolution’s RevoScaleR package running cubes (crosstabs) and regressions and plotting histograms etc. while working directly with the 123 million row airlines data set that was used in the 2009 ASA completion.  I was very happy to show off Revolution R, and I was doubly pleased to find out later that, concurrent with my session, Tricia Hoffman and Mike Bowles were leading a session on R programming.  Nothing could be more encouraging for an R enthusiast than the apparent necessity to have competing R sessions. Please have a look at the examples Tricia and Mike presented which are posted on the conference website.   I also attended the session where Anup Parikh presented Red-R and lead a discussion on user interfaces. Red-R has some very nice features. At the very least, I can see R developers using it to document the flow of their analyses. Finally, I spoke briefly with Stanford’s Susan Holmes who mentioned that basic proficiency in R was a requirement for the data mining course she is teaching.  As part of an answer to a question that came up during the expert panel discussion, Susan compared learning data mining skills to learning to play the piano. I am paraphrasing here, but Susan made the point that you could learn to play music by studying music theory first, or with a little help, you could sit at the piano and try to pick out a tune. Continuing this theme, I think that playing with R might be a natural way to tune your data mining skills. 	 0 Comments
Visualizing US House Results with a Seats-Votes curve	https://www.r-bloggers.com/2010/11/visualizing-us-house-results-with-a-seats-votes-curve/	November 16, 2010	jjh	"A few weeks ago I wrote about ways to compare major-party returns in US House elections. I experimented with several visualizations, none as useful as the seats-votes curve. A traditional seats-votes cure measures average party performance against individual US House results. Our simplified curve uses a density plot to measure major-party (Democratic, in this case) support across all seats up for election. The seats-votes curve we use will help measure the following characteristics of the US House for a given election: number of uncontested or weakly contested seats, number safe seats, and the number of close or tossup seats. By comparing plots from different years we can track changes in major party support and electoral attitudes, both of which can have a dramatic effect on future elections and legislative priorities in the US House. This exercise will explain the different components of seats votes plot, and then look at how Democratic party support has changed from 2002 to 2010.  Though it may look simple, the modified seats-votes curve is very information-heavy and can be somewhat confusing. A small change in the contours of the curve can convey a lot of information. Please refer to the annotated graph (figure 1), and the items below for instructions on how to read a seats votes-curve.
 Figure 1 Now that we can interpret the plot in terms of base, competitive, and safe seats lets look at the modified seats votes plot from the previous article but with the latest election returns. Using the information from annotated reference chart (Figure 1.) to interpret to contours of the seats votes curve we can build a narrative for the mood of the electoral for any given year. Using the latest results chart (Figure 2.) we can expand that narrative across electoral cycles.   Figure 2 For elections leading up to 2010 we see a prominent bimodal curve, with a reasonably stable amount of safe seats for each party. In 2002 and 2004 we see a large number of seats in the 30-40 percent Democratic vote share, which corresponds to a strong Republican majority. In 2006 we see the number of competitive seats increase, and 2008 we see an increase in the number of safe Democratic seats. The changes in 2006 and 2008 track nicely with the Democrats narrowly control of the US House in 2006 and then strongly expanding their majority in 2008.  The 2002-2008 plots show a Republican between 15-20%, a Democratic base between 15-18%, and competitive seats between 9-15% of the total US House. In 2010 the structure of the curve changed dramatically. The Republican base seats were right around 18%, which is in line with previous years. But the Democratic base seats look nothing like previous years. There is still a contingent of the lump of safe Democratic seats, but the count dropped by about half. The rest of the seats shifted towards the 50% line where they merged with the other competitive seats. This plot of the returns lines up nicely with the pre-election narrative of an outsize number of tossup races by CQ Politics, FiveThirtyEight, Cook Political Report, and The Rothenberg Political Report.  The smoothed density function used in the seats-votes plot is an estimation, so it is difficult to determine exactly how many seats fall within a given vote range. We could use a histogram, but I like a cummulative distribution function (CDF) plot instead. To create a CDF plot for each years results we’ll use the built-in R ECDF function and ggplot: Figure 3 Using the CDF plot we can see that Democrats received 50% or less in 60% of the races in 2010, but received 50% or less in only 42% of the races in 2008. In 2010 a full 40% of the total races fell into the competitive category, defined as received between 40% and 60% of the vote. In 2008 and 2006  that number was closer to 12%, an increase of 50% in a single year.  The combination of the seats-votes plot and CDF allow us pretty powerful insights into the current electoral power of each major party in the US House. We have hard numbers and a narrative for the 2010 US House Democratic loss that goes beyond parroting the number of seats lots. We also have some historical perspective on major-party electoral returns, and it will be interesting to see if the 2010 competitive seats remain that way in 2012.  "	 0 Comments
Feature selection: Using the caret package	https://www.r-bloggers.com/2010/11/feature-selection-using-the-caret-package-3/	November 16, 2010	Allan Engelhardt	Feature selection is an important step for practical commercial data mining which is often characterised by data sets with far too many variables for model building. In a previous post we looked at all-relevant feature selection using the Boruta package while in this post we consider the same (artificial, toy) examples using the caret package. Max Kuhn kindly listed me as a contributor for some performance enhancements I submitted, but the genius behind the package is all his. The caret package provides a very flexible framework for the analysis as we shall see, but first we set up the artificial test data set as in the previous article. The flexibility of the caret package is to a large extent implemented by using control objects. Here we specify to use the randomForest classification algorithm (which is also what Boruta uses) and if the multicore package is available then we use that for extra perfomance (you can also use MPI etc ­– see the documentation): We will consider from one to six features (using the sizes variable) and then we simply let it lose: The results are: If you recall the feature selection with Boruta article, then the results there were To show the flexibility of caret, we can run the analysis with another of the built-in classifiers: This gives: And of course, if you have your own favourite model class that is not already implemented, then you can easily do that yourself. We like gbm from the package of the same name, which is kind of silly to use here because it provides variable importance automatically as part of the fitting process, but may still be useful. It needs numeric predictors so we do: And we get the results below: It is all good and very flexible, for sure, but I can’t really say it is better than the Boruta approach for these simple examples. 	 0 Comments
Feature selection: Using the caret package	https://www.r-bloggers.com/2010/11/feature-selection-using-the-caret-package/	November 16, 2010	Allan Engelhardt	"
Feature selection is an important step for practical commercial data mining which is often characterised by data sets with far too many variables for model building.  In a previous post we looked at all-relevant feature selection using the Boruta package while in this post we consider the same (artificial, toy) examples using the caret package.  Max Kuhn kindly listed me as a contributor for some performance enhancements I submitted, but the genius behind the package is all his.
 
The caret package provides a very flexible framework for the analysis as we shall see, but first we set up the artificial test data set as in the previous article.
 
The flexibility of the caret package is to a large extent implemented by using control objects.  Here we specify to use the randomForest classification algorithm (which is also what Boruta uses) and if the multicore package is available then we use that for extra perfomance (you can also use MPI etc ­– see the documentation):
 
We will consider from one to six features (using the sizes variable) and then we simply let it lose:
 
The results are:
 
If you recall the feature selection with Boruta article, then the results there were
 To show the flexibility of caret, we can run the analysis with another of the built-in classifiers: This gives: And of course, if you have your own favourite model class that is not already implemented, then you can easily do that yourself.  We like gbm from the package of the same name, which is kind of silly to use here because it provides variable importance automatically as part of the fitting process, but may still be useful.  It needs numeric predictors so we do: And we get the results below: It is all good and very flexible, for sure, but I can’t really say it is better than the Boruta approach for these simple examples. Jump to comments. 

Benchmarking feature selection with Boruta and caret
 Feature selection is the data mining process of selecting the variables from our data set that may have an impact on the outcome we are considering. For commercial data mining, which is often characterised by having too many variables for model building, this is an important step in the analysis process. And since we often work on very large data sets the performance of our process is very important to us. Having looked at feature selection using the Boruta package and feature selection using the caret package separately, we now consider the performance of the two approaches. Neither approach is suitable out of the box for the sizes of data sets that we normally work with. 

Feature selection: All-relevant selection with the Boruta package
 Feature selection is an important step for practical commercial data mining which is often characterised by data sets with far too many variables for model building. There are two main approaches to selecting the features (variables) we will use for the analysis: the minimal-optimal feature selection which identifies a small (ideally minimal) set of variables that gives the best possible classification result (for a class of classification models) and the all-relevant feature selection which identifies all variables that are in some circumstances relevant for the classification. In this article we take a first look at the problem of all-relevant feature selection using the Boruta package by Miron B. Kursa and Witold R. Rudnicki. This package is developed fo… 

R code for Chapter 1 of Non-Life Insurance Pricing with GLM
 Insurance pricing is backwards and primitive, harking back to an era before computers. One standard (and good) textbook on the topic is Non-Life Insurance Pricing with Generalized Linear Models by Esbjorn Ohlsson and Born Johansson. We have been doing some work in this area recently. Needing a robust internal training course and documented methodology, we have been working our way through the book again and converting the examples and exercises to R , the statistical computing and analysis platform. This is part of a series of posts containing elements of the R code. 

R code for Chapter 2 of Non-Life Insurance Pricing with GLM
 We continue working our way through the examples, case studies, and exercises of what is affectionately known here as “the two bears book” (Swedish björn = bear) and more formally as Non-Life Insurance Pricing with Generalized Linear Models by Esbjörn Ohlsson and Börn Johansson (Amazon UK | US ). At this stage, our purpose is to reproduce the analysis from the book using the R statistical computing and analysis platform, and to answer the data analysis elements of the exercises and case studies. Any critique of the approach and of pricing and modeling in the Insurance industry in general will wait for a later article. 

Area Plots with Intensity Coloring
 I am not sure apeescape’s ggplot2 area plot with intensity colouring is really the best way of presenting the information, but it had me intrigued enough to replicate it using base R graphics. The key technique is to draw a gradient line which R does not support natively so we have to roll our own code for that. Unfortunately, lines(..., type=l) does not recycle the colour col= argument, so we end up with rather more loops than I thought would be necessary. We also get a nice opportunity to use the under-appreciated read.fwf function. "	 0 Comments
Assignment operators in R: ‘=’ vs. ‘<-’	https://www.r-bloggers.com/2010/11/assignment-operators-in-r-%e2%80%98%e2%80%99-vs-%e2%80%98-%e2%80%99/	November 16, 2010	csgillespie	"In R, you can use  both ‘=’ and ‘
 The main difference between the two assignment operators is scope. It’s easiest to see the difference with an example:


##Delete x (if it exists)

> rm(x)

> mean(x=1:10) #[1] 5.5

> x #Error: object 'x' not found


Here x is declared within the function’s scope of the function, so it doesn’t exist in the user workspace. Now, let’s run the same piece of code with using the 


> mean(x <- 1:10)# [1] 5.5

> x # [1]  1  2  3  4  5  6  7  8  9 10


This time the x variable is declared within the user workspace. Well there’s quite a strong following for the “
 However, I tend always use the “=” operator for the following reasons: Also Introducing Monte Carlo Methods with R, by Robert and Casella recommends using “=”. If I’m missing something or you disagree, please leave a comment – I would be very interested. "	 0 Comments
Data Science meets Humanities	https://www.r-bloggers.com/2010/11/data-science-meets-humanities/	November 16, 2010	David Smith	There's an interesting article in the NYT today about the emerging discipline of “digital humanities“: extracting digital data from historical archives to answer questions from the Arts and Humanities. From the article: Members of a new generation of digitally savvy humanists argue it is time to stop looking for inspiration in the next political or philosophical “ism” and start exploring how technology is changing our understanding of the liberal arts. This latest frontier is about method, they say, using powerful technologies and vast stores of digitized materials that previous humanities scholars did not have. These researchers are digitally mapping Civil War battlefields to understand what role topography played in victory, using databases of thousands of jam sessions to track how musical collaborations influenced jazz, and searching through large numbers of scientific texts and textbooks to track where concepts first appeared and how they spread. Unsurprisingly, this has kicked off a debate over the role of quantification and analysis in the liberal arts, vesus the more traditional approach of individual interpretations of documents and artifacts. While many in the old-school view such quantitative analysis as “whimsical”, there are some great discoveries to be made through data science. Read the full article for some great examples. New York Times: Digital Keys for Unlocking the Humanities’ Riches   	 0 Comments
Postdoc in Wharton	https://www.r-bloggers.com/2010/11/postdoc-in-wharton/	November 16, 2010	xi'an	"Just received this email from José Bernardo about an exciting postdoc position in Wharton: POST-DOCTORAL FELLOW – DEPARTMENT OF STATISTICS, THE WHARTON SCHOOL The Department of Statistics at The Wharton School of the University of Pennsylvania is seeking candidates for a Post-Doctoral Fellowship. This research fellowship provides full funding without any teaching requirements at a competitive salary for two years beginning in Summer 2011. Applicants are expected to show outstanding capacity for research as well as excellent communication skills. Although our department is located in the Wharton School, we provide services to the entire University of Pennsylvania and hold research interests across diverse scientific fields. We have strong research programs in many areas of statistics, including
– Hierarchical Bayesian modeling
– Statistical decision theory and learning theory
– Multivariate analysis and high-dimensional inference
– Nonparametric function estimation
– Variable selection and model uncertainty
– Mathematical finance and time-series analysis
– Statistical computing and graphics
– Causal inference and observational studies
– Applications in public health, genetics and biology Applications should be submitted via our online application system, for which a link can be found on our department website. We will begin reviewing applications after December 31, 2010. Interested candidates should submit the following materials: cover letter, curriculum vitae, research statement, selected publications, and three letters of recommendation. Applicants must have a Ph.D. (expected completion by June 30, 2011 is acceptable) from an accredited institution. Additional inquiries should be directed by email to Shane Jensen at: stat.postdoc.hiring[[@]]wharton.upenn.edu "	 0 Comments
Loops in R: Think different	https://www.r-bloggers.com/2010/11/loops-in-r-think-different/	November 15, 2010	David Smith	Especially for programmers that come to R from other languages, R sometimes gets dinged about the speed of its for loops. But a lot of the time, where you might have needed an iterative loop in another language to solve a specific task, you don't need a for loop in R at all.  Often, there's a pre-build function to accomplish the specific task at hand. Other times, you can use the implicit iteration of vector or matrix operations, which is much faster than using an explicit loop. And in other cases, you can use some of R's other looping constructs (like apply and lapply, for example) to achieve a similar goal more elegantly. Basically, when it comes to looping in R, it's often best to think beyond the basic for loop. This post from Yihui Xie explains this point well: students, when asked to code an iterative task in R, often turned to a for loop when another method would have required less code and run faster. The alternate formulations makes for an educational example for all R programmers, not just students. Statistics, R, Graphics and Fun: On the Gory Loops in R     	 0 Comments
Example 8.14: generating standardized regression coefficients	https://www.r-bloggers.com/2010/11/example-8-14-generating-standardized-regression-coefficients/	November 15, 2010	Nick Horton		 0 Comments
Feature selection: All-relevant selection with the Boruta package	https://www.r-bloggers.com/2010/11/feature-selection-all-relevant-selection-with-the-boruta-package-3/	November 15, 2010	Allan Engelhardt	Feature selection is an important step for practical commercial data mining which is often characterised by data sets with far too many variables for model building. There are two main approaches to selecting the features (variables) we will use for the analysis: the minimal-optimal feature selection which identifies a small (ideally minimal) set of variables that gives the best possible classification result (for a class of classification models) and the all-relevant feature selection which identifies all variables that are in some circumstances relevant for the classification. In this article we take a first look at the problem of all-relevant feature selection using the Boruta package by Miron B. Kursa and Witold R. Rudnicki. This package is developed for the R statistical computing and analysis platform. All-relevant feature selection is extremely useful for commercial data miners. We deploy it when we want to understand the mechanisms behind the behaviour or subject of interest, rather than just building a black-box predictive model. This understanding leads us to a better appreciation of our customers (or other subject under investigation) and not just how, but why they behave as they do, which is useful for all areas of the business, including strategy and product development. More narrowly, it also help us define the variables that we want to observe which is what will really make a difference in our ability to predict behaviour (as opposed to, say, run the data mining application a little longer). I really like the theoretical approach that the Boruta package tries to implement. It is based on the more general idea that by adding randomness to a system and then collecting results from random samples of the bigger system, one can actually reduce the misleading impact of randomness in the original sample. For the implementation, the Boruta package relies on a random forest classification algorithm. This provides an intrinsic measure of the importance of each feature, known as the Z score. While this score is not directly a statistical measure of the significance of the feature, we can compare it to random permutations of (a selection of) the variables to test if it is higher than the scores from random variables. This is the essence of the implementation in Boruta. This article is a first investigation into the performance of the Boruta package. For this initial examination we will use a test data sample that we can control so we know what is important and what is not. We will consider 200 observations of 20 normally distributed random variables: Normal distribution has the advantage of simplicity, but for commercial application where highly non-normally distributed features like money spent are important may not be the best test. Nevertheless, we will use it for now and define a simple utility function before we get on to the tests: For a simple classification based on a single variable, Boruta performs well: while it identifies three variables as being potentially important, this does include the true variable (V.1) and the plot clearly shows it as being by far the most significant. With a test of a linear combination of the first four variables where the weights are decreasing from 4 to 1, we begin to get closer to the limitations of the approach. The implementation correctly identified the first three variables (with weights 4, 3, and 2, respectively) as being important, but it had the fourth variable as possible along with the two random variables V.8 and V.9. Still, six variables are more approachable than twenty. For this text and the following we consider less obvious combinations of the first four variables. If we just count how many of them are positive, then we get to a situation where Boruta excels (because random forests excel at this type of problem). For a spectacular fail of the Boruta approach we will have to consider a classification in the hyperplane of the four variables. For this simple example, we simply count if there are an even or odd number of positive values among the first four variables: Ouch. The package rejects the four known significant variables. It is too hard for the random forest approach. Increasing the number of observations to 1,000 does not help though at 5,000 observations Boruta identifies the four variables right. Some limitations of the Boruta package are worth highlighting: It only works with classification (factor) target variables. I am not sure why: as far as I remember, the random forest algorithm also provides a variable significance score when it is used as a predictor, not just when it is run as a classifier. It does not handle missing (NA) values at all. This is quite a problem when working with real data sets, and a shame as random forests are in principle very good at handling missing values. A simple re-write of the package using the party package instead of randomForest should be able to fix this issue. It does not seem to be completely stable. I have crashed it on several real-world data sets and am working on a minimal set to send to the authors. But this is a really promising approach, if somewhat slow on large sets. I will have a look at some real-world data in a future post. 	 0 Comments
Feature selection: All-relevant selection with the Boruta package	https://www.r-bloggers.com/2010/11/feature-selection-all-relevant-selection-with-the-boruta-package/	November 15, 2010	Allan Engelhardt	"
Feature selection is an important step for practical commercial data mining which is often characterised by data sets with far too many variables for model building.  There are two main approaches to selecting the features (variables) we will use for the analysis: the minimal-optimal feature selection which identifies a small (ideally minimal) set of variables that gives the best possible classification result (for a class of classification models) and the all-relevant feature selection which identifies all variables that are in some circumstances relevant for the classification.
 
In this article we take a first look at the problem of all-relevant feature selection using the Boruta package by Miron B. Kursa and Witold R. Rudnicki.  This package is developed for the R statistical computing and analysis platform.
 
All-relevant feature selection is extremely useful for commercial data miners.  We deploy it when we want to understand the mechanisms behind the behaviour or subject of interest, rather than just building a black-box predictive model.  This understanding leads us to a better appreciation of our customers (or other subject under investigation) and not just how, but why they behave as they do, which is useful for all areas of the business, including strategy and product development.  More narrowly, it also help us define the variables that we want to observe which is what will really make a difference in our ability to predict behaviour (as opposed to, say, run the data mining application a little longer).
 
I really like the theoretical approach that the Boruta package tries to implement.  It is based on the more general idea that by adding randomness to a system and then collecting results from random samples of the bigger system, one can actually reduce the misleading impact of randomness in the original sample.
 
For the implementation, the Boruta package relies on a random forest classification algorithm.  This provides an intrinsic measure of the importance of each feature, known as the Z score.  While this score is not directly a statistical measure of the significance of the feature, we can compare it to random permutations of (a selection of) the variables to test if it is higher than the scores from random variables.  This is the essence of the implementation in Boruta.
 
This article is a first investigation into the performance of the Boruta package.  For this initial examination we will use a test data sample that we can control so we know what is important and what is not.  We will consider 200 observations of 20 normally distributed random variables:
 
Normal distribution has the advantage of simplicity, but for commercial application where highly non-normally distributed features like money spent are important may not be the best test.  Nevertheless, we will use it for now and define a simple utility function before we get on to the tests:
 
For a simple classification based on a single variable, Boruta performs well: while it identifies three variables as being potentially important, this does include the true variable (V.1) and the plot clearly shows it as being by far the most significant.
  Figure 1: Simple test of Boruta feature selection with single variable. 
With a test of a linear combination of the first four variables where the weights are decreasing from 4 to 1, we begin to get closer to the limitations of the approach.
  Figure 2: Simple test of Boruta feature selection with linear combination of four variables. 
The implementation correctly identified the first three variables (with weights 4, 3, and 2, respectively) as being important, but it had the fourth variable as possible along with the two random variables V.8 and V.9.  Still, six variables are more approachable than twenty.
 
For this text and the following we consider less obvious combinations of the first four variables.  If we just count how many of them are positive, then we get to a situation where Boruta excels (because random forests excel at this type of problem).
  Figure 3: Simple test of Boruta feature selection counting the positives of four variables. 
For a spectacular fail of the Boruta approach we will have to consider a classification in the hyperplane of the four variables.  For this simple example, we simply count if there are an even or odd number of positive values among the first four variables:
  Figure 4: Simple test of Boruta feature selection with non-linear combination of four variables 
Ouch.  The package rejects the four known significant variables.  It is too hard for the random forest approach.  Increasing the number of observations to 1,000 does not help though at 5,000 observations Boruta identifies the four variables right.
 
Some limitations of the Boruta package are worth highlighting:
 
But this is a really promising approach, if somewhat slow on large sets.  I will have a look at some real-world data in a future post.
 Jump to comments. 

Benchmarking feature selection with Boruta and caret
 Feature selection is the data mining process of selecting the variables from our data set that may have an impact on the outcome we are considering. For commercial data mining, which is often characterised by having too many variables for model building, this is an important step in the analysis process. And since we often work on very large data sets the performance of our process is very important to us. Having looked at feature selection using the Boruta package and feature selection using the caret package separately, we now consider the performance of the two approaches. Neither approach is suitable out of the box for the sizes of data sets that we normally work with. 

Feature selection: Using the caret package
 Feature selection is an important step for practical commercial data mining which is often characterised by data sets with far too many variables for model building. In a previous post we looked at all-relevant feature selection using the Boruta package while in this post we consider the same (artificial, toy) examples using the caret package. Max Kuhn kindly listed me as a contributor for some performance enhancements I submitted, but the genius behind the package is all his. 

R code for Chapter 2 of Non-Life Insurance Pricing with GLM
 We continue working our way through the examples, case studies, and exercises of what is affectionately known here as “the two bears book” (Swedish björn = bear) and more formally as Non-Life Insurance Pricing with Generalized Linear Models by Esbjörn Ohl… 

R code for Chapter 1 of Non-Life Insurance Pricing with GLM
 Insurance pricing is backwards and primitive, harking back to an era before computers. One standard (and good) textbook on the topic is Non-Life Insurance Pricing with Generalized Linear Models by Esbjorn Ohlsson and Born Johansson. We have been doing som… 

R: Eliminating observed values with zero variance
 I needed a fast way of eliminating observed values with zero variance from large data sets using the R statistical computing and analysis platform . In other words, I want to find the columns in a data frame that has zero variance. And as fast as possible… "	 0 Comments
Isarithmic History of the Two-Party Vote	https://www.r-bloggers.com/2010/11/isarithmic-history-of-the-two-party-vote/	November 15, 2010	d sparks	A few weeks ago, I shared a series of choropleth maps of U.S. presidential election returns, illustrating the relative support for Democratic, Republican, and third Party candidates since 1920. The granularity of these county level results led me to wonder whether it would be possible to develop an isarithmic map of presidential voting using the same data. Isarithmic maps are essentially topographic or contour maps, wherein a third variable is represented in two dimensions by color, or by contour lines, indicating gradations. I had never seen such a map depicting political data — certainly not election returns, and thus sought to create them. There is a trade-off between an isarithmic depiction versus a choroplethic depiction, in which a third variable is shown within discrete political boundaries. Namely, that though a politically-delineated presentation better facilitates the connection of the variable of interest to the level at which it was measured, the superimposition of geographically arbitrary political boundaries may cloud the existence of more general regional patterns. Election-year maps can be seen in a slideshow here (and compared to the three-color choropleth maps here). The isarithmic depiction does an excellent job of highlighting several broad patterns in modern U.S. political history. First, it does a good job of depicting local “peaks” and “valleys” of partisan support clustered around urban areas. In the 2008 map, for example, Salt Lake City, Denver, Chicago, Miami, Memphis, and many other cities stand apart from their surrounding environs, highlighted by a relatively intense concentration of voters with distinct partisan leanings. In 1980, this method shows that though Reagan enjoyed broad support in California, the revolution was not felt in the Bay Area. Comparison of these maps across time also underscores well-known political trends, but offers more resolution than state-level choropleths and greater clarity than county-level choropleths. Note the nearly inverted maps for 1924 and 2004, between which elections the Solid South went from solidly Democratic to solidly Republican. Interestingly, though that particular regional pattern has been remarkably consistent since 1984, the South favored a Democratic candidate as recently as 1980. These patterns over time are even better observed in motion. Interpolating support between elections, I have generated a video in which these maps shift smoothly from one election year to the next. The result is the story of 20th century presidential politics on a grand scale, condensed into a little 0ver a minute of data visualization. The video can also be seen at YouTube (I recommend the “expanded” or “full screen” view), or at Vimeo. The images were rendered at 1280 x 720 pixels, to allow the video to be seen in HD. This animated interpretation accentuates certain phenomena: the breadth and duration of support for Roosevelt, the shift from a Democratic to a Republican South, the move from an ostensibly east-west division to the contemporary coasts-versus-heartland division, and the stability of the latter. More broadly, this video is a reminder that what constitutes “politics as usual” is always in flux, shifting sometimes abruptly. The landscape of American politics is constantly evolving,  as members of the two great parties battle for electoral supremacy. Appendix on creating the visualization Using county-level presidential returns from the CQ Press Voting and Elections Collection, I associated each county’s support in a given election year for the Democratic and Republican candidates with an approximation of that county’s centroid in degrees latitude and longitude, using the shapefiles loaded with the package mapdata. I then used simple linear interpolation to create a smoothed transition from election-to-election, creating 99-interelectoral estimates of partisanship for each county. Using a custom function and the interp function from akima, I created a spatially smoothed image of interpolated partisanship at points other than the county centroids. This resulted in inferred votes over the Gulf of Mexico, the Atlantic and Pacific Oceans, the Great Lakes, Canada and Mexico — so I had to clip any interpolated points outside of the U.S. border using the very handy pinpoly function from the spatialkernel package. Finally, I created a custom color palette, a modification of the RdBu scheme from Colorbrewer, using colorRampPalette(), and plotted the interpolated data along with state borders using the excellent ggplot2. I would like to note that I would have preferred using the Albers Equal Area Conic projection, but settled on the default Mercator projection, as drawing the Albers map with ggplot2 was prohibitively time-consuming, given that I was generating 2,201 individual frames. 	 0 Comments
Introducing Monte Carlo in PaRis	https://www.r-bloggers.com/2010/11/introducing-monte-carlo-in%c2%a0paris/	November 14, 2010	xi'an	As already announced on Statisfaction, I will start a short [14 hour] course in English based on Introducing Monte Carlo Methods with R at ENSAE next Tuesday. The slides were written by George Casella for a course he gave in Italy last spring and he kindly agreed on making them available on slideshare:  	 0 Comments
ZAT! 2010	https://www.r-bloggers.com/2010/11/zat-2010/	November 13, 2010	romain francois	Tomorrow is the last day to enjoy the first edition of Montpellier’s ZAT! (Zones Artistiques Temporaires). I was there this afternoon and tonight, but I found it much more picture worthy tonight:  Other people have also taken pictures and shared them on flickr:  	 0 Comments
Reporting Standard Errors for USL Coefficients	https://www.r-bloggers.com/2010/11/reporting-standard-errors-for-usl-coefficients/	November 13, 2010	Neil Gunther		 0 Comments
My Day at ACM Data Mining Camp III	https://www.r-bloggers.com/2010/11/my-day-at-acm-data-mining-camp-iii-dmcamp/	November 13, 2010	Ryan Rosario	"My first time at ACM Data Mining Camp was so awesome, that I was thrilled the make the trip up to San Jose for the November 2010 version. In July, I gave a talk at the Emerging Technologies for Online Learning Symposium conference with a faculty member in the Department of Statistics, at the Fairmont. The place was amazing, and I told myself I would save up to stay there. This trip gave me an opportunity to check it out, and pretend that I am posh for a weekend  . The night I arrived I had the best dinner and drinks at this place called Gordon Biersch. I had the best garlic fries and BBQ burger I have ever had. I ate it with a Dragonfruit Strawberry Mojito, the Barbados Rum Runner, and finished off with a Long Island Iced Tea, so the drinks were awesome as well. Anyway, to the point of this post… The next morning I made the short trek to the PayPal headquarters for a very long 9am-8pm day. Since I came up here for the camp, I wanted to make the most of it and paid the $30 for the morning session, even though I had not intended on going originally. Overview of Data Mining Algorithms with Dean Abbott The paid morning session from 9-11:30 was led by Dean Abbott (@deanabb), the president of Abbott Analytics. It was an excellent overview of the basic data mining algorithms, but obviously 2 hours is not enough time to cover the algorithms in detail. When I first scanned through the slides I was concerned that I would be bored, but I actually learned a few things that made it worth it. One of the first concepts I learned about was CHAID, (CHi-squared Automated Interaction Detector) a decision tree algorithm that can build wide n-ary trees rather than just binary trees like in CART. CHAID can also output a p-value, making diagnostic analysis more practical. I also did not know that decision trees could be used as a pre-analysis step to find interactions among variables. The output from this step can be used to construct better regression models including the proper interaction terms. We moved on to linear regression and logistic regression which were obviously very basic. Next, we spent some time discussing neural networks. It is no secret that I detest neural networks. I don’t know what it is, but they annoy me to no end. It seems like there is very little science behind how to choose quantities such as the parameters, number of neurons or number of hidden layers. Maybe it is just me, but neural networks feel like a hack. Besides, anything that can be done with a neural network can be done using plain old statistics. Dean also discussed other methods including nearest neighbors, Radial Basis Functions, Bayes Classifiers and Naive Bayes, Support Vector Machines (SVM).  At this point, we had to start rushing which was too bad. We briefly discussed ensemble methods including bagging, boosting (AdaBoost), and Random Forests. We spent about 5 minutes on unsupervised methods as well including k-means, Kohonen maps
(self-organizing maps). I am not sure what happened to principal components analysis (PCA), multidimensional scaling (MDS)
or independent components analysis (ICA). As I mentioned to a friend, unsupervised learning always gets the shaft. We had slides for association rules (Apriori algorithm), but we did not have time to discuss it. I was hoping semi-supervised learning, reinforcement learning and
recommendation systems would be mentioned, but there was not enough time even for what was on the agenda. I wish we had more time. Unfortunately, there were way too many questions and a few individuals that wished to waste minutes debating and challenging the speaker. Dean Abbott teaches a full, two-day course (not free) in data mining that may be of interest. Click here for more information. I usually would not post something like this, but he is an excellent, and practical speaker

What I found a bit surprising was that this session was at a Data Mining event. I would hope that most of the people in attendance had familiarity with a good amount of the material. The Netflix session at the previous ACM Data Mining Camp seemed to align better with the target audience of the day’s events. On the other hand, there were a ton of people in the session. Perhaps this was a good money-maker for the Bay Area ACM, because perhaps some people got their training on in the morning, and then left after lunch. The eBay sponsored lunch was phenomenal, just like last time. I got a smoked ham sandwich and my little box also contained a bag of potato chips, an apple, and an oatmeal-raisin muffin looking thing (it was supposed to be a cookie but the baker got carried away). Main Session Next up was the main session which mainly consisted of a QA session with some experts in the field and also some job announcements from companies that sponsored the event.

 Large Data with R

Given that I gave a talk to the Los Angeles R Users’ Group on working with large datasets in R, I figured this would be an enlightening session. Unfortunately, the R skills that were covered were very basic, and it was little more than a commercial for Revolution Computing’s version of R. The take away from the session was basically just that the Revolution version has optimized methods that read the data into memory in chunks and operate on each chunk (perhaps) independently. This is nothing that a nice integration with Hadoop could not provide. No mention was made of the free open-source solutions for large datasets in R: bigmemory and ff.  If I had a time machine, I would have instead attended Rob Zinkov‘s talk on Sentiment Analysis. Rob is a second-year Ph.D. student in Computer Science at University of Southern California’s Information Sciences Institute and a member of the Los Angeles R Users’ Group. Mahout Next up was Ted Dunning discussing Mahout. I was elated to see practically each hand in the room shoot up when we were asked to vote on which sessions we wanted to attend. Mahout is a Java framework that provides scalable machine learning and data mining algorithms. Mahout code interacts with Hadoop to provide map-reduce functionality for algorithms. The purpose of Mahout is to provide early production quality scalable data mining. Some classification methods currently in Mahout include mixture modeling, Latent Dirichlet Allocation (LDA), logistic regression, naive Bayes, Complementary Naive Bayes, latent factor loglinear algorithms, stochastic gradient descent SVM, and random forests. Some of these methods are parallel, and some are sequential. Large scale SVD is currently being worked on, and still has some rough edges. The biggest news in this talk was how well Mahout has been snapped up by industry. AOL uses Mahout’s methods for its product recommendation services. “A large scale electronics company” (name was secret) uses Mahout for music recommendations. Other uses of Mahout in industry include frequent itemset mining, and spam website detection. Dunning mentioned that Mahout does seem to work well with sparse matrices assuming that if an element of the matrix is unspecified, it is equal to 0. If I understood his statement correctly, this means that Mahout works well with most sparse matrices. Some more technical  gems  I learned is that Mahout can do stochastic gradient descent (although it is sequential), and its implementation uses per-term annealing which can then be used for supervised learning with logistic regression. These implementations optimize for high dimensional sparser data, possibly with interactions. These methods are scalable and fast to train. Ted mentioned that for a particular test case, the optimization converged in less than 10,000 examples. For large datasets, it is possible that the method will converge before seeing all of the data. With that said, in the “best” case, an algorithm using stochastic gradient descent can be sublinear in the number of training examples. Towards the end of the session, Ted answered some questions personally, and it gave me some insight into data mining methods. He is not a fan of “most common” itemset algorithms (Apriori, Eclat, etc.) because they are difficult to parallelize due to their quadratic nature. Instead, he prefers  co-occurence analysis methods. He also prefers R to Weka, and he loves Python. I also prefer R to Weka, and love Python  . Large Scale Supervised Learning The next talk I attended was rehearsed with slides etc. and was presented by Junling Hu, from eBay. Junling has a Ph.D. in Computer Science from University of Michigan, Ann Arbor. Although the talk began with (another) quick review of data mining algorithms, the meat of the talk was on how to parallelize some of these algorithms. The challenge of parallelization is that we must maintain a global  function, and messages must be passed to update this global function. One basic way to do this is how map-reduce does it: split the data into subsets, perform some function on each subset and reduce the computations into one result. Each method has its own way it can be parallelized. Decision Trees. One type of parallelization for decision trees is based on the tree nodes. One can write a map-reduce function to recursively split nodes, like the PLANET method proposed by Google.  We have some sequence of nodes in a tree and we maintain a model. With the proposed framework, we start with an empty set of nodes. We maintain a map-reduce queue for all the nodes we are going to divide and we also maintain an in-memory queue of nodes. The goal is  to find the best splits based on some measure, in PLANET’s case, variance. We run some controller that controls map-reduce jobs. Each map-reduce job sends back data and we update the global variables: the model, the map-reduce queue and the in-memory queue. Then, new map-reduce jobs are constructed and the process continues. Due to time constraints, I had a difficult time following all of what was going on, but more information about the algorithm discussed can be found here. Support Vector Machines. Hu mentioned two types of SVMs: primal SVM and parallel SVM. The idea behind parallelizing SVMs is to use block-based optimization by dividing either the data, or the features, into blocks. Stochasic gradient descent can be used for block minimization for the primal SVM. Some other ways mentioned included randomly splitting the data (bootstrapping perhaps), or using data compression (dimension reduction, perhaps). One resource for parallel SVM is the psvm project on Google Code which provides distributed gradient computation for maximum entropy, and parallel logistic regression. Junling listed a few resources: Monetizing Images The final time slot was slim pickings for me, and the second time slot (when I attended Mahout) hosted 4 sessions I wanted to attend that all conflicted with each other. The discussion about the association between tweets and stock prices sounded interesting, except for the stock prices part. So, I attended the Monetizing Images session. This session was more of a discussion about data mining with images in general. We also discussed forensic photography and the ability to detect if an image has been doctored. We also discussed some techniques for measuring image similarity. David Lowe from the University of British Columbia maintains a list of uses and companies regarding computer vision on his website. It is interesting to note that after the fact, Ken Weiner (@kweiner) from GumGum in Los Angeles indicated that monetizing images is exactly what they are doing. Sounds interesting! At this point I was exhausted. I like meeting Twitter friends and followers, but people very quiet! It was a pleasure to meet Scott Waterman (@tswaterman) and Tommy Chheng (@tommychheng). I also got to reconnect with my friend Shaun Ahmadian (@ssahmadian) from the UCLA Department of Computer Science as well as Rob Zinkov (@zaxtax) who also made the trek from Los Angeles to San Jose.  "	 0 Comments
New R Users Group for University of Utah and Research Park	https://www.r-bloggers.com/2010/11/new-r-users-group-for-university-of-utah-and-research-park/	November 13, 2010	andrew	I’m organizing a new R Users Group for the University of Utah and Research Park sponsored by the Study Design and Biostatistics Center.  We welcome all to come.  The first meeting will be dedicated to finding out what users needs and abilities are.  We also welcome all skill levels.  But I will also give a short presentation about some of the basics of R. We will be meeting from 12 -1PM in the Williams Building, Room 223, 295 Chipeta Way, Salt lake City, UT 84132. Please email me at Andrew.Redd at hsc.utah.edu for more information. 	 0 Comments
Know any R blogs in your own language?	https://www.r-bloggers.com/2010/11/r-bloggers-now-in-your-language/	November 13, 2010	Tal Galili		 0 Comments
Programming with R – Checking Data Types	https://www.r-bloggers.com/2010/11/programming-with-r-%e2%80%93-checking-data-types/	November 13, 2010	Ralph	There are a number of useful functions in R that test the variable type or convert between different variable types. These can be used to validate function input to ensure that sensible answers are returned from a function or to ensure that the function doesn’t fail. Following on from a previous post on a simple function to calculate the volume of a cylinder we can include a test with the is.numeric function. The usage of this function is best shown with a couple of examples: The function returns either TRUE or FALSE depending on whether the value is numeric. If a vector is specified to this function then a vector or TRUE and FALSE elements is returned. We can add two statements to our volume calculation function to test that the height and radius specified by the user are indeed numeric values: We add these tests after checking whether the height and radius have been specified and before the test for whether they are positive values. The function now becomes: A couple of examples show that the function works as expected: These various validation checks can be combined in different ways to ensure that a user does not try to use a function in a way that was not intended and should lead to greater confidence in the output from the function. This is one approach to checking function arguments and there are likely other slicker ways of doing things. Other useful resources are provided on the Supplementary Material page. 	 0 Comments
Because it’s Friday: Asteroids	https://www.r-bloggers.com/2010/11/because-its-friday-asteroids/	November 12, 2010	David Smith	"A huge mass of rock hurtling in from space could really make a mess of your weekend plans. So it's comforting to know that the world's astronomers are out there keeping an eye for any potential earth-grazers. See their discoveries over the past 30 years in this beautifully-designed animation: 





  Earth crossers are in red; earth approachers are in yellow; other asteroids are in green. I love the way that the Earth with all its telescopes appears almost as a lighthouse beacon, peering into the murky depths of space. "	 0 Comments
New R User Group in Cincinnati / Dayton	https://www.r-bloggers.com/2010/11/new-r-user-group-in-cincinnati-dayton/	November 12, 2010	David Smith	The latest local R user group to join the fold is CinDay RUG, serving the Cincinnati/Dayton area in Ohio. The group was founded by Stu Rodgers, who decided to set it up after posting a query on LinkedIn and finding several other R users in the area. Even if you think there's not enough likeminded folks in your area to support an R user group, it's worth asking around — you might be surprised! CinDay RUG's first meeting is scheduled for December 1. Follow the link below for details. MeetUp.com: CinDay RUG 	 0 Comments
Update: Forbes wants your R stories by Nov 17	https://www.r-bloggers.com/2010/11/update-forbes-wants-your-r-stories-by-nov-17/	November 12, 2010	David Smith	I mentioned recently that Forbes is seeking stories about R for a forthcoming issue. Well, the story will now be in the December issue (bumped up from the January issue), so be sure to get your post your stories about R to the Mean Business blog by November 17. Forbes: Names You Need to Know in 2011: R Data Analysis Software 	 0 Comments
Risk-Opportunity Analysis	https://www.r-bloggers.com/2010/11/risk-opportunity-analysis/	November 12, 2010	Joshua Ulrich	"
 "	 0 Comments
What would impressionnists do with R ?	https://www.r-bloggers.com/2010/11/what-would-impressionnists-do-with-r/	November 12, 2010	romain francois	"I’ve been playing with images recently, probably inspired from my trip in San Francisco. There was an exhibit at the De Young museum of fine arts with pieces borrowed from the Musée d’Orsay. I did not go to the exhibit because it is easy enough for me to just go to Paris and the Musée d’Orsay, but I guess this somewhat inspired me, along with the golden gate bridge, to do some R based impressionnism The starting point is this picture of the golden gate bridge The png package makes it straightforward to import png pictures into R (There are other ways as well). Then, I generate randomly spaced circles so that they don’t overlap, and fill each circle with the average color (on the RGB space) of all te pixels that are inside the circle Then I do this many times, with translucent circles, and after some iterations ,the golden gate bridge starts to reveal itself The code for this is included below Here are other examples

 

 

 "	 0 Comments
Bayesian Inference for Latent Gaussian Models	https://www.r-bloggers.com/2010/11/bayesian-inference-for-latent-gaussian-models/	November 12, 2010	xi'an	An exciting conference in Zurich next February, 02-05. (I think I will attend! And not for skiing reasons!) Latent Gaussian models have numerous applications, for example in spatial and spatio-temporal epidemiology and climate modelling. This workshop brings together researchers who develop and apply Bayesian inference in this broad model class. One methodological focus is on model computation, using either classical MCMC techniques or more recent deterministic approaches such as integrated nested Laplace approximations (INLA). A second theme of the workshop is model uncertainty, ranging from model criticism to model selection and model averaging. Havard Rue will give an INLA tutorial on the first day. Further confirmed invited speakers are Renato Assuncao, Gonzalo Garcia-Donato, Sylvia Frühwirth-Schnatter, Alan Gelfand, Chris Holmes, Finn Lindgren, Douglas Nychka, Christopher Paciorek and Stephen Sain. Contributed talks and a poster session complete the four-day program. 	 0 Comments
Speeding up Optmatch while improving match quality	https://www.r-bloggers.com/2010/11/speeding-up-optmatch-while-improving-match-quality/	November 12, 2010	Mark Fredrickson	“Fast, cheap, correct: Pick two.” Does this phrase apply to statistical matching algorithms? In the case of Optmatch, you can have all three. “Cheap” is easy: it is open source. You can download it for free. Today I’m going to explain how to make the matching process both faster and more substantively relevant using a technique we call “pre-stratification”: splitting your data into smaller matching problems. Ben Hansen and I often receive messages from Optmatch users of the form: “I have a very large matching problem, and Optmatch is taking a very, very long time to complete. Is there anything I can do?” For example, say that you have some data of the form: Where z is a indicator of whether the unit received “treatment” and x is a covariate you wish to match on (or it could be a summary of covariates, such as a propensity score). If one were to invoke Optmatch on this data directly, it could take a long time. I don’t suggest you try it, but it might look something like this: In this code, mdist prepares a treatment by control matrix where each entry is the Mahalanobis distance between each pair. pairmatch finds the best set of treatment-control pairs, minimizing the average distance within pairs. We’ll see another example of mdist below and more examples of both functions are contained in the online documentation (e.g. > ?mdist). Of course, this example will be slow. You are telling optmatch to compare 3000 treated units with 3000 control units, which is a very, very large search space. We would recommend limiting the comparisons of treated and control units based on another covariate, stratifying the data into smaller subgroups prior to matching. Usually the best way is to use a categorical variable of substantive purpose. For example, say you have a continuous covariate (x), a treatment indicator (z), and a factor indicating male or female (gender) Perhaps previous studies indicate gender to be an important determinant to whether subjects self-select into treatment (this can be verified by checking the balance of male and female treatment and control subjects). Limiting matches to same gender pairs will likely improve the quality of your matches (as compared to ignoring gender) and will also speed up the matching process. Using mdist again, we can create a set of distances for male subjects (treated and control) and separately female subjects by indicating that gender should be a stratifying variable and the distances can be fed to optmatch: Watching the R process as this ran, I saw it took about 200mb of RAM to compute the distances and the pair match, and it only took a few seconds. By comparison, the 3000 by 3000 matching task took all my available memory (forcing other apps to be pushed to swap) and did not complete in the 10 minutes I allowed it to run. Clearly, stratification improves the execution time of matches. I’ve also found that stratified matches do very well compared to propensity score models that would also include the stratification variable. In other words, if were to build a propensity score model of treatment and include gender, I could so in this fashion: But like the original method, this would require a 3000 by 3000 entry matrix to search. Again, the faster way is to both include the stratifying variable from the propensity model and use it directly in the match: Like the stratification above using Mahalanobis distance, the propensity score example completes much more quickly when gender is used to stratify the data before the match. While I’ve been arguing from a speed perspective so far, I also think that stratification improves the substantive quality of matches. Matches that stratify along variables with strong theoretical importance or that have been shown to be strong predictors of treatment selection in previous studies make good choices for stratifying variables as they improve the rational for the matching strategy. Even readers unfamiliar with the matching literature understand that stratifying limits comparisons to comparable units. Ultimately, convincing others that particular matching strategy allows for valid causal inference is a matter of rhetoric. Stratification can be another tool in creating believable matching scenarios. The rhetorical aspect of matching can also be improved by quantitative analysis of the match quality, specifically balance testing. I’ve written on testing balance on this website before. The Optmatch and RItools documentation provides more examples of matching and balance testing strategies. When you have sped up the matching process using stratification, it is easy to compare balance on many different matching strategies to find the one that best fits your data.This again is an example of faster matching providing higher quality results. “Fast, cheap, correct” – Optmatch has them all. 	 0 Comments
How to calculate confidence intervals of correlations with R	https://www.r-bloggers.com/2010/11/how-to-calculate-confidence-intervals-of-correlations-with-r/	November 11, 2010	Jeromy Anglim	This post sets out how to calculate confidence intervals for correlations using R. Because I often get this question from people unfamiliar with R, it assumes no prior knowledge of R.  Download and Install R (R Home Page; Windows Installation) Start R and run the following commands, updating the CIr command as required:  	 0 Comments
RcppArmadillo 0.2.9	https://www.r-bloggers.com/2010/11/rcpparmadillo-0-2-9/	November 11, 2010	Thinking inside the box	"
The short NEWS file extract follows, also containing Conrad’s entry for 0.9.92::

 
More information is on the 
RcppArmadillo page.
Questions, comments etc should go to the
rcpp-devel mailing list
off the R-Forge page.

 "	 0 Comments
Remembering on 11/11	https://www.r-bloggers.com/2010/11/remembering-on-1111/	November 11, 2010	David Smith	Today is Veterans Day in the US, and Remembrance Day or Armistice Day elsewhere in the world. Whatever you know this day as, it's a day for remembering the sacrifices of those who served. Drew Conway has commemorated the day in a touching yet saddening way, by visualizing with R the sparse distribution of funds to support the many homeless veterans in the US:  Read Drew's thoughtful post at the link below, and remember your veterans today. Zero Intelligence Agents: Supporting Homeless Veterans 	 0 Comments
Help Mozilla visualize how people use Firefox	https://www.r-bloggers.com/2010/11/help-mozilla-visualize-how-people-use-firefox/	November 11, 2010	David Smith	You might recall we posted a couple of weeks ago this chart summarizing the times of the day Firefox users switch on Private Browsing mode:  The chart, based on data from the Mozilla Test Pilot program tells an interesting story about the habits of Web users. But what other interesting stories could be told, to reveal more insights into how people interact with the Web via a browser? To drive discussion around that question, Mozilla has launched an open data visualization competition, How Do People Use Firefox? Participants in the competition will have access to Test Pilot data from more than 1 million volunteer Firefox users, and will use those data to create, in the words of the organizers, “compelling visualizations that tell detailed, meaningful and yet easy-to-interpret stories about interesting user activities”.  The competition begins on November 17 when the data are made available, and the deadline for submissions is December 5. Winners will be announced on December 1. I am honored to have been asked to serve as one of the seven judges, along with members of Mozilla Labs and Mozilla Metrics. For inspiration, check out the examples under “What we are looking for” on the competition page. Two of the four examples were created in R, including Heike Hoffman's “Flying over the USA” contribution to the ggplot2 visualization competition. And if you need more inspiration, the Grand Prize is a $300 Amazon gift card, with Tufte books for the runners-up.  Check the competition page at the link below for full details, follow @MozTestPilot on Twitter for updates, and get those R graphics devices fired up! Mozilla Design Challenge: How Do People Use Firefox 	 0 Comments
Also in Bosnia & Herzegovina, satisfied parents are those who believe their school representatives are effective	https://www.r-bloggers.com/2010/11/also-in-bosnia-herzegovina-satisfied-parents-are-those-who-believe-their-school-representatives-are-effective/	November 11, 2010	Social data blog	"
 In the mixed effects regression for all 10 countries in the research I mentioned previously on this blog http://stevepowell99.posterous.com/which-parents-are-satisfied-with-their-childs, while school mean of “parents believing their representatives are effective” was significantly positively related to their overall satisfaction with education, this effect was dwarfed by the contribution of this variable at individual level, even when allowing for a host of other dependent variables, from wealth to child’s educational performance etc.     And the coefficients for the dependent variables are similar to the all-countries model, although the contribution of individual-level belief in the effectiveness of representatives is not quite as massive.    This could be interpreted as follows: those individual parents who have faith in their representatives are those who use their relationships with those representatives for the benefit of their children’s education, whether to get information, communicate with the school, influence decisions and so on, which leads them to being substantially more satisfied.  This suggests that parental participation does have a really substantial impact on satisfaction with education, but not necessarily on the level of individual parents’ interaction with the school directly (which actually has a significant negative effect in these models). Rather, good schools are those which have effective parent representatives; but sometimes those representatives are effective for some but not all of the parents. So it might be possible to substantially improve parental satisfaction with education by helping to ensure that all parents are in a position to make use of their representatives; but it might equally be that these “special relationships” between representatives and some parents work by giving preferential treatment to some but not to all, and so inherently are not capable of being extended to all the parents in a class or school. Permalink 

	| Leave a comment  »
 "	 0 Comments
R is a name you need to know	https://www.r-bloggers.com/2010/11/r-is-a-name-you-need-to-know/	November 11, 2010	Pat	As if that is news to some of you. Forbes has a Mean Business blog post by Steve McNally titled “Names You Need to Know in 2011: R Data Analysis Software”. The post includes several links to why R is wonderful. It also includes a pretty — but seemingly useless — statistical graph.  Correct me if I’m wrong. This is the Green Room in The first rule of statistics is you don’t write about statistics takes exception to the simplistic statement about Facebook using R to learn about customer retention. 	 0 Comments
Mortgage Calculator (and Amortization Charts) with R	https://www.r-bloggers.com/2010/11/mortgage-calculator-and-amortization-charts-with-r/	November 10, 2010	C		 0 Comments
New ebook: Asian Option Pricing with R/Rmetrics	https://www.r-bloggers.com/2010/11/new-ebook-asian-option-pricing-with-rrmetrics/	November 10, 2010	ellis		 0 Comments
Tell Forbes how you use R	https://www.r-bloggers.com/2010/11/tell-forbes-how-you-use-r/	November 10, 2010	David Smith	Steve McNally of the Forbes Mean Business blog says R is a name you need to know for 2011. He cites some great examples of R in action: Facebook has used R to figure out that “just two data points are significantly predictive of whether a user remains on Facebook: (i) having more than one session as a new user, and (ii) entering basic profile information.” For non-data-wonk professionals, R can make pretty pictures of complex information. Anyone capable of creating a spreadsheet can load that same sheet into R and begin playing with plots, charts, and graphs. This should not be undervalued: making a management presentation fraught with data (think: ad page sales by geography; segmented by industry; year-to-date; vs. last year-to-date) is a lot more palatable when there’s a stunning infograph in your deck. Forbes is also crowdsourcing content for their January issue and is looking for graphs, charts and success stories about R. I've suggested the examples from the “R is Hot” series and the Revolution Analytics case studies, but I'm sure you have other great suggestions. Let Forbes know in their comments (Forbes.com registration required) or here. Forbes: Names You Need to Know in 2011: R Data Analysis Software 	 0 Comments
Bayesian estimation with Markov Chain Monte Carlo using PyMC	https://www.r-bloggers.com/2010/11/bayesian-estimation-with-markov-chain-monte-carlo-using-pymc/	November 10, 2010	VCASMO - drewconway		 0 Comments
Quantitative Ecology 2010-11-10 14:56:00	https://www.r-bloggers.com/2010/11/quantitative-ecology-2010-11-10-145600/	November 10, 2010	Forester		 0 Comments
Co-authorship Network of SSRN Conflict Studies eJournal	https://www.r-bloggers.com/2010/11/co-authorship-network-of-ssrn-conflict-studies-ejournal/	November 10, 2010	Drew Conway	As part of my on-going research simulating network structure using graph motifs I have been collecting novel data sets to test and benchmark the method.  Since I am a political scientist studying conflict, it was suggested to me to collect a co-authorship network within this sub-discipline.  Such a network is useful for several reasons; for example, it is a census of all possible ties, i.e., there are no missing or hidden edges among nodes because the network “is what it is” based on authorships.  This makes testing and benchmarking much easier to interpret.  In addition, while co-authorship networks are common in other disciplines, no such data exists for political science, or conflict studies more specifically (to my knowledge).  Finally, it is just plain fun to have data wherein your colleagues are the units! Previously, collecting such data was an arduous, hand-coded, task.  I do not get down on hand-coding, but luckily I do not have to.  The Social Science Research Network is an outstanding resource for working papers across all of the social sciences—including political science.  Of particular interest for me is the Conflict Studies eJournal, which aggregates and organizes new papers in this area.  The SSRN also has a convenient coding schema for both articles and authors, which is all that is needed to quickly generate a co-authorship network from their database of articles.  Above is a visualization of this network data generated in Gephi and hosted at Microsoft’s zoom.it (go full-screen to really explore).  The nodes are colored by type, with authors in red and articles in blue.  They are also sized by their relative PageRank, which is slightly less helpful in this case because there are so many disconnected components.  Interestingly, there are several duplicate entries in the SSRN database such that an author-article relation can may be present up to 14 times.  I am afraid this is due to some error within the SSRN data, but to mitigate it I have collapsed those relationships and created a weighted graph, which is actually what is used to generate the above visualization.  The edge thickness reflects this slightly, though the vast majority of edges have a weighting of one. Depending on your perspective, you might view this illustration as the “Conflict Studies Supernova,” or perhaps the “Moment of Conception for Conflict Studies;” either way, the visualization provides evidence of the scale and density of co-authorship relationships in this sub-discipline.   Some quick facts about the network, there 5,234 nodes and 4,240 edges.  It contains 1,406 weakly connected components, the largest of which contains 1,157 nodes (pictured at the center).  It is extremely sparse, with a density well below 1%, and even isolating the main component only yields a density of 0.001.  Given this sparsity, the average degree is actually less then one, at about 0.8 ties.  Clearly there is room for much more collaboration among conflict studies scholars within political science! The network was generated in R using the XML and igraph packages primarily, with a bit of data slicing in plyr.  You can download the data and code on my github repository (code sharing, FTW!), and are welcome to play around with it as you like.  The node labels in the data correspond to author and abstract IDs, which you can look-up at the SSRN website.  Keep in mind, the data online only represents the network as on last night (2010-11-09), and any papers added today or later will not be present.  Of course, you are welcome to re-run the code to update the data for your pruposes. I’ll be spending the next several weeks with this and other data, so I hope you have as much fun with it as I plan to! 	 0 Comments
Generating a quasi Poisson distribution, version 2	https://www.r-bloggers.com/2010/11/generating-a-quasi-poisson-distribution-version-2/	November 10, 2010	arthur charpentier	"Here
and there, I mentioned two codes to generated quasiPoisson random
variables. And in both of them, the negative binomial approximation
seems to be wrong. Recall that the negative binomial distribution is "	 0 Comments
Don’t be a Turkey	https://www.r-bloggers.com/2010/11/dont-be-a-turkey/	November 9, 2010	C		 0 Comments
Forecast estimation, evaluation and transformation	https://www.r-bloggers.com/2010/11/forecast-estimation-evaluation-and-transformation/	November 9, 2010	Rob J Hyndman	I’ve had a few emails lately about forecast evaluation and estimation criteria. Here is one I received today, along with some comments. I have a rather simple question regarding the use of MSE as opposed to MAD and MAPE. If the parameters of a time series model are estimated by minimizing MSE, why do we evaluate the model using some other metric, e.g., MAD and MAPE. I could see that MAPE is not scale dependent. But MAPE is a percentage version of MAD. So why don’t we use the percentage version of MSE? MSE (mean squared error) is not scale-free. If your data are in dollars, then the MSE is in squared dollars. Often you will want to compare forecast accuracy across a number of time series having different units. In this case, MSE makes no sense. MAE (mean absolute error) is also scale-dependent and so cannot be used for comparisons across series of different units.  The MAD (mean absolute deviation) is just another name for the MAE. The MAPE (mean absolute percentage error) is not scale-dependent and is often useful for forecast evaluation. However, it has a number of limitations. For example, It is possible to have a percentage version of MSE, the Mean Squared Percentage Error, but this isn’t used very often. The MASE (mean absolute scaled error) was intended to avoid these problems. For further discussion on these and related points, see Hyndman & Koehler (IJF, 2006). A preprint version is also available. Also, suppose we have a lognormal model, where the estimation is done on the log-transformed scale and the prediction is done on the original, untransformed scale. One could either predict with the conditional mean or the conditional median. It seems to me that you would predict with the mean if the MSE is your metric, but you would predict with the median if the MAD is your metric. My thought is that the mean would minimize MSE, while the median would minimize MAD. So whether you use the mean or the median depends on which metric you use for evaluating the model. In most cases, the mean and median will coincide on the transformed scale because the transformation should have produced a symmetric error distribution. I would usually estimate with the MSE because it is more efficient (assuming the errors look normal). It might help to estimate with the MAD if there are outliers, but I would prefer to explicitly deal with them. When forecasting on the original, untransformed scale, the simple thing to do is to back-transform the forecasts (and the prediction interval limits). The point forecasts will then be the conditional median (assuming symmetry on the transformed scale), and the prediction interval will still have the desired coverage. To get the conditional mean on the original scale, it is necessary to adjust the point forecasts. If  is the variable on the log-scale and  is the variable on the original scale, then  where  is the point forecast on the log-scale and  is the forecast variance on the log-scale. The prediction interval remains unchanged whether you use a conditional mean or conditional median for the point forecast. Occasionally, there may be some reason to prefer a conditional mean point forecast; for example, if you are forecasting a number of related products and you need the point forecasts to sum to give the forecast of total number of products. But in most situations, the conditional median will be suitable. In R, the plot.forecast() function (from the forecast package) will back-transform point forecasts and prediction intervals using an inverse Box-Cox transformation. Just include the argument lambda. For example: 	 0 Comments
Particle learning [rejoinder]	https://www.r-bloggers.com/2010/11/particle-learning-rejoinder/	November 9, 2010	xi'an	Following the posting on arXiv of the Statistical Science paper of Carvalho et al., and the publication by the same authors in Bayesian Analysis of Particle Learning for general mixtures I noticed on Hedibert Lopes’ website his rejoinder to the discussion of his Valencia 9 paper has been posted. Since the discussion involved several points made by members of the CREST statistics lab (and covered the mixture paper as much as the Valencia 9 paper), I was quite eager to read Hedie’s reply. Unsurprisingly, this rejoinder is however unlikely to modify my reservations about particle learning. The following is a detailed examination of the arguments found in the rejoinder but requires a preliminary reading of the above papers as well as our discussion.. “Particle  learning based on the product estimate and MCMC based on Chib’s formula  produce relatively similar results either for small or large samples” This statement about the estimation of the marginal likelihood (or the evidence) and the example A that is associated with it thus comes to contradict our (rather intensive) simulation experiment which, as reported in the discussion, concludes to the strong bias in evidence induced by using particle learning, whether or not the product estimator is used. We observed there that there were two levels of degeneracy, one due to the product solution (errors in a product being more prone to go and…multiply) and one due to the particle nature of the sequential method (which does not refresh particles from earlier periods). The above graph is at odds with the one presented in the rejoinder, maybe because we consider 10,000 observations rather than 100. (I also fail to understand how the “Log-predictive (TRUE)” is derived.) “Black-box sequential importance sampling algorithms and related central limit theorems are of little use in practice.” Another quote from the rejoinder I do not get. What’s wrong with the central limit theorem?! One major lesson from the central limit theorem is that it provides a scale for the speed of convergence and thus an indicator on the number of particles needed for a given precision level. The authors of the rejoinder then criticise our use of “1000 particles in 5000 dimensional problems” as we “shouldn’t be surprised at all with some of our findings”. I find no trace in the discussion of such a case: we use 10,000 particles in all examples and the target is either the distribution of the 4 mixture parameters, the evidence  or the distribution of a one-dimensional sufficient statistic. Furthermore, these values of n and N are those used in their example D… “This argument [that the Monte Carlo variance will `blow-up’] is incorrect and extremely misleading.” This point is central to both the discussion and the rejoinder, as the authors maintain that the inevitable particle degeneracy does not impact the distribution of the sufficient statistics. The argument about using time averages over particle paths rather than sums appears reasonable at first. Actually, taking an empirical average in almost stationary situations should produce an approximately normal distribution. With an asymptotic variance different from 0. (Thanks to the central limit theorem by the way!) However, this is not the main argument used in the discussions. Degeneracy in the particle path means that the early terms in the average are less and less diverse in the sample average. Therefore it is not that surprising that the variance is decreasing to too small a value! As shown in Figure 8 of the discussion, degeneracy due to resampling may induce severe biases in the distribution of empirical averages while giving the impression of less variability. Furthermore, the fact that parameters are simulated [rather than fixed] in the filter means that the  process is not geometrically ergodic, hence that Monte Carlo errors tend to accumulate along iterations, rather than compensate… (This is why the comparison between PL and sampling importance resampling is particularly relevant, because it does not address this accumulation.) The rejoinder also quotes Olsson et al. (2008) for justifying the decrease in the Monte Carlo variance. This is somehow surprising in that (a) Olsson et al. (2008) show that there is degeneracy without a fixed-lag smoothing and (b) they require a geometric forgetting property on the filtering dynamics. In addition, I note that Example E used to illustrate the point about variance reduction is not very appropriate for this issue because the hidden Markov chain is a Gaussian random walk, hence cannot be stationary (a fact noted by the authors). And again a decrease in the “MC error” does not mean a converging algorithm because degeneracy naturally induces empirical variance decrease. (I also fail to see why the “prior” on  is improper.) The final argument that “PL parameters do not degenerate” is somehow puzzling: by nature, those parameters are simulated from a distribution conditional on the sufficient parameters. So obviously the simulated parameters all differ. But this does not mean that they are marginally distributed from the right distribution. “MCMC schemes depend upon the not so trivial task of assessing convergence. How long should the burn-in G0 be?” The rejoinder concludes with recommendations that sound more like a drafted to-do note the authors forgot to remove than an accumulation of true recommendations. It seems to me that the comparison between MCMC and particle filters is not particularly relevant, simply because particle filters apply in [sequential] settings where MCMC cannot be implemented. To try to promote PL over MCM by arguing that MCMC produces dependent draws while having convergence troubles is not needed (besides, PL also produces [unconditional] dependent draws). To advance that the Monte Carlo error for PL is in  is not more relevant because  is exponential in  and because MCMC also has an error in . 	 0 Comments
Any R packages to solve Vehicle Routing Problem?	https://www.r-bloggers.com/2010/11/any-r-packages-to-solve-vehicle-routing-problem/	November 9, 2010	prasoonsharma		 0 Comments
R co-creator Ross Ihaka wins Lifetime Achievement Award in Open Source	https://www.r-bloggers.com/2010/11/r-co-creator-ross-ihaka-wins-lifetime-achievement-award-in-open-source/	November 9, 2010	David Smith	The co-creator of R, University of Auckland Associate Professor of Statistics Dr. Ross Ihaka, was yesterday awarded the Catalyst Lifetime Achievement in Open Source Award at the 2010 New Zealand Open Source Awards. From the announcement: Dr. Ihaka is one of the originators of the world-renown ‘R’ programming language and software environment for statistical computing and graphics. In 2008 Dr. Ihaka was the recipient of the Royal Society of New Zealand's Pickering Medal, also for his work on ‘R’. The award was presented last night in Wellington at a gala event attended by more than 200 people. Congratulations to Ross for this well-deserved award. GeekZone: NZ Open Source Awards winners announced 	 0 Comments
Promote your favorite R functions	https://www.r-bloggers.com/2010/11/promote-your-favorite-r-functions/	November 9, 2010	David Smith	The 27 base and recommended libraries of the standard R 2.12 distribution together contain 3556 functions (you can check using the code posted after the jump). Many of the functions are commonly used: c, data.frame, rnorm, lm. But some of those functions, while being extremely useful, may be less well known to many R users. Some examples I'd wish I'd learned about earlier include tapply, agrep, and formatC. There are also several really useful help pages that aren't associated with specific functions at all, like Syntax and .Machine, that don't get the exposure they deserve. To help get some of these “hidden gems” of the R documentation better known, we've added a “Function of the Day” section to the home page of inside-R.org. What R functions and help pages do you wish you'd known about earlier? Add a comment to your favorite pages in the Language Reference explaining why they deserve more love, and we'll consider the comments nominations for the Function of the Day. inside-R.org: Language Reference 	 0 Comments
New R User Group in Houston	https://www.r-bloggers.com/2010/11/new-r-user-group-in-houston/	November 9, 2010	David Smith	The latest local R user group to form is located in Houston, Texas. The first meeting of the Houston R Users Group is tonight at Rice University (in conjunction with the Houston chapter of the ASA). R hackr (typo intended!) Hadley Wickham will be giving a presentation on writing R packages, and you can check out the slides on his SlideShare page. If you're in the Houston area, be sure to sign up to the Meetup group to be notified of future meetings. Meetup.com: Houston R Users Group 	 0 Comments
Mapping drug war related homicides in 2010	https://www.r-bloggers.com/2010/11/mapping-drug-war-related-homicides-in-2010/	November 9, 2010	Diego Valle-Jones		 0 Comments
The ARORA guessing game	https://www.r-bloggers.com/2010/11/the-arora-guessing-game/	November 9, 2010	Pat	"ARORA (A random or real array) is a website that gives you two time series at a time.  Your job is to guess which series is real market data and which is permuted data.  It’s fun — try it. With some practice you will probably be able to guess which is which well above chance.  I have a hypothesis or two about why.  But before you read my hypotheses, you should try it out yourself without my contamination. The original paper describing the experiment is Is it real or is it randomized?: A financial Turing test by Jasmina Hasanhodzic, Andrew Lo and Emanuele Viola. My hypotheses are explained in the working paper Some hypotheses about ARORA, the financial Turing test. Do you think I’m right? Do you have other hypotheses? Generating a random series in (presumably) the way that ARORA does it is easy in R.  Here is some code that imitates a static version of a single ARORA test:

> par(mfcol=c(2,1))
> plot(priceSeries, type=""l"", axes=FALSE, xlab='',
+        ylab=''); box()
> plot(exp(c(0, cumsum(sample(diff(log(priceSeries)))))),
+     type=""l"", axes=FALSE, xlab='', ylab=''); box()
 The par command sets up the graphics page to have two plots on it, one above the other.  (In this case it doesn’t matter if you use mfcol or mfrow.) The first plot command plots the price series as a line with the usual labeling of the axes removed.  The box command draws a box around the plot — this is usually done for such plots but not when the axes are not drawn. The second plot command contains all of the computation of the random series.  We can explain it by starting on the inside and working outwards. diff(log(priceSeries)) computes log returns from the series (see A tail of two returns).  Then the sample command does a random permutation of those numbers. The cumsum function performs a cumulative sum of the permuted returns.  We add a zero onto the front of that vector of numbers, and finally use exp to go from log returns back to prices (starting at 1). A more polished version would add a mar argument to the call to par in order to not waste so much space in the resulting graphic. Figure 1: Panthera onca.
 Does it matter if you can tell a panthera onca from a panthera pardus?  Probably not (though knowing what continent you’re on might be useful).  What matters is if you can outrun her if she decides to eat you.  (Probably not.) The ability to distinguish the real data series has been used to give credence to chartists.  While the opposite result would tend to rule out the efficacy of chart-reading, I’m not convinced that this is especially supportive of chartism. The real task is to tell where a price series is going. A test of that is the Technical Analysis Challenge on the Burns Statistics website.  This is another multiple choice game that you can play yourself.  However, it isn’t as nicely presented as ARORA.  You are given a price series and four possible extensions of that series.  Only one of the four, of course, is the correct extension. Of the few people who officially entered the challenge, there was no indication of skill at guessing the extensions.  (Except for  a certain someone who industriously cheated.) Thanks to Lisa Goldberg for pointing out ARORA. Other blogs that have spoken about this include Mind Your Decisions and Technology Review. Photo from stock.xchng. "	 0 Comments
Computational position in Texas	https://www.r-bloggers.com/2010/11/computational-position-in-texas/	November 8, 2010	xi'an	"José Bernardo forwaded this announcement that sounds quite attractive (conditional upon living in a remote part of Texas!) Senior Faculty Position in Computational Statistics At Texas A&M University As part of a recognition of the increasing importance in the modeling and computational sciences, the Department of Statistics at Texas A&M University is recruiting for a senior faculty position in computational statistics as broadly defined. This position is one of three new senior lines dedicated to computational science that were created as part of an initiative led by the Institute for Applied Mathematics and Computational Science. Considerable startup funding is available. Computational science has become inherently multidisciplinary. As a result, successful candidates for this position should be able to demonstrate a strong record of research accomplishments and leadership, both within the statistics discipline and in multidisciplinary initiatives. Documentation of such success should include a record of publication in both statistics and a multidisciplinary application area as well as examples of collaboration and program building. Special emphasis will be placed on computational methods involving hierarchical modeling, uncertainty quantification and systems biology. Texas A&M University is a university of approximately 50,000 students and 3,000 faculty members. The Department of Statistics (37 members) is one of the largest departments in the US and is listed among the Top 5 departments from public institutions in the most recent rankings of US News and World Report. Additional information can be obtained by contacting the search committee chair at searchcommittee[[@]]stat.tamu.edu. Individuals who wish to be considered for this position should send a copy of their CV and a letter of interest to:
Recruiting Committee Chair, Computational Statistics Search Committee
Department of Statistics
3143 TAMU
Texas A&M University
College Station TX 77843-3143. Electronic submissions will also be accepted and should be sent to:  Searchcommittee[[@]]stat.tamu.edu, with Computational Statistics in the Subject Line. Additional information and letters of reference will be solicited after a preliminary review. Review of the applicant pool will begin January 15, 2011.  Start dates are flexible and the position will remain open until filled. Texas A&M University is an Equal Opportunity Employer and has a policy of being responsive to the needs of dual-career couples. Obviously, I quite like the focus on computational stats posted in this offer. (The last line is also interesting in that it reflects a growing concern for academic couples!) "	 0 Comments
Using R and Hadoop to analyze VOIP data	https://www.r-bloggers.com/2010/11/using-r-and-hadoop-to-analyze-voip-data/	November 8, 2010	David Smith	Last month, the newest member of Revolution's engineering team, Saptarshi Guha, gave a presentation at Hadoop World 2010 on using R and Hadoop to analyze 1.3 billion voice-over-IP packets to identify calls and measure call quality. Saptarshi, of course, is the author of RHIPE, which lets R programmers write map-reduce algorithms in the Hadoop framework without needing to learn Java. With R running on each Hadoop node, Saptarshi used R's data analysis functions (such as robust regression) to process almost 100 Gb of data in just a few minutes. The slides for Saptarshi's talk are now available to view at the Hadoop World website (linked below), or you can download a PDF version (7.3Mb). Hadoop World 2010: Voice over IP: Studying Traffice Characteristics for Quality of Service using R and Hadoop   	 0 Comments
The Dataists answer your questions	https://www.r-bloggers.com/2010/11/the-dataists-answer-your-questions/	November 8, 2010	David Smith	The fine bloggers (and R experts) at the Dataists have volunteered to answer questions about data analysis on Reddit: A few months ago, a group of likeminded folks in New York and the San Francisco Bay area decided it was time to start a blog about data, and we can up with the Dataists. Since then we thought about a taxonomy of data science, careful statistical computing, what data visualization should do, and even started a predictive analytics contest for the R community. Many of our posts have been featured here at /r/MachineLearning, so the moderators thought it would be fun to run a community Q&A. So, how can we help you? Visit the Reddit post at the link below to ask your questions. (Free Reddit account required.) Reddit: We are the Dataists; we love statistics, machine learning, visualization, and all things data. How can we help you?   	 0 Comments
Example 8.13: Bike ride plot, part 2	https://www.r-bloggers.com/2010/11/example-8-13-bike-ride-plot-part-2/	November 8, 2010	Ken Kleinman		 0 Comments
The NYC Marathon	https://www.r-bloggers.com/2010/11/the-nyc-marathon/	November 8, 2010	John Myles White	New York’s annual marathon took place yesterday. Watching a bit of it on television with my friends, I was struck by the much earlier starting time for women than men. Specifically, professional women started running yesterday at 9:10 AM, while professional men start running at 9:40 AM. (This information comes from the runner’s handbook.) I wanted to get a sense of how much this head start depended on real differences in their performance, because I found it very hard to imagine why professional women would run significantly slower than professional men. Of course, I have seen discussions of the speed difference between men and women before, but I was still very surprised by it yesterday. To get a sense of the scope of the differences, I found some data this morning from the ING Marathon website and made a quick density estimate plot, which you can see below: It’s clear that men and women had quite difference average speeds yesterday, and that their times had very different distributions. Of course, these plots are each based on 100 observations, so I’m hesitant to make any strong conclusions. Having confirmed for myself that there are real differences in the performance of men and women, I have to confess that I still find it surprising. For those interested in following up on this, the code I used to produce this plot and the data set I used are both available on GitHub. I’m sure there are other interesting questions one can ask of this data beyond simple comparisons across genders. 	 0 Comments
R Beginner’s Guide Book Update: Statistical Analysis with R Released	https://www.r-bloggers.com/2010/11/r-beginners-guide-book-update-statistical-analysis-with-r-released/	November 8, 2010	John Quick		 0 Comments
A R wrapper for Google Prediction API	https://www.r-bloggers.com/2010/11/a-r-wrapper-for-google-prediction-api/	November 8, 2010	Paolo Sonego		 0 Comments
Le Monde puzzle [43]	https://www.r-bloggers.com/2010/11/le-monde-puzzle%c2%a043/	November 7, 2010	xi'an	Here is the puzzle in Le Monde I missed last week: Given a country with 6 airports and a local company with three destinations from each of the six airports, is it possible to find a circular trip with three intermediate stops from one of the airports? From all of the airports? One more airport is opened with the same rules about the three destinations. Is it still possible? And with yet another airport? An R resolution is to run random links between airports and to check whether a trip is possible. Here is my solution The only trick is the matrix product that simplifies the computation of the connectivity graph for the airports linked in four trips. Running the above R code shows that for six and eight airports there are always circuits with 3 stops, but that for seven airports, it is impossible to ensure three destinations (the loop never breaks and tour is never created). This is true for any odd number of airports. 	 0 Comments
Wetbulb Temperature	https://www.r-bloggers.com/2010/11/wetbulb-temperature/	November 7, 2010	Steven Mosher	" This google map display is just one of 230 GHCN stations that is located in the water. After finding  instances of this phenomena over and over, it seemed an easy thing to find and analyze all such cases in GHCN. The issue matters for a two reasons: The process of finding “wet stations” is trivial in the “raster” package of R. All that is needed is high resolution land/sea mask. In my previous work, I used a ¼ degree base map. ¼ degree is roughly 25km at the equator.  I was able to find a 1km land mask used by satellites. That data is read in one line of code, and then it is simple matter to determine which stations are “wet”. Since NCDC is updating the GHCN V3 inventory I have alerted them to the problem and will, of course provide the code. I have yet to write NASA GISS. Since H2010 is already in the publishing process, I’m unsure of the correct path forward. Looking through the 230 cases is not that difficult. It’s just time consuming.  We can identify several types of case: Atolls, Islands, and coastal locations. It’s also possible to put the correct locations in for some stations by referencing either WMO publications or other inventories which have better accuracy than either GHCN or GISS. We can also note that in some cases the “mislocation” may not matter to nightlights.  These are cases where you see no lights whatsover withing the  1/2 degree grid that I show. In the google maps presented below, I’ll show a sampling of all 230. The blue cross shows the GHCN station location and the contour lines show the contour of the nightlights raster. Pitch black locations have no contour. I will also update this with a newer version of Nighlights. A google tour is available for folks who want it. The code is trivial and I can cover that if folks find it interesting. with the exception of the graphing it is as simple as this: Ghcn
 lonLat 
 Nlight 
 extent(Nlight)
 Ghcn
 distCoast 
 Ghcn 
 # for this mask, Water pixels are coded by their distance from land. All land pixels are 0 # make an inventory of just those land stations that appear in the water. wetBulb 0),] writeKml(wetBulb,outfile=”wetBulb”,tourname=”Wetstations”) Some shots from the gallery. The 1km land/water mask is very accurate. You might notice one or two stations actually on land. Nightlights is less accurate, something H2010 does not recognize. Its pixels can be over 1km off true position. The small sample below should show the various cases. No attempt is made to ascertain if this causes an issue for identification of rural/urban categories. As it stands the inaccuracies in Nightlights and station locations suggests more work before that effort is taken up.        "	 0 Comments
Updating meteorological forecasts, part 1	https://www.r-bloggers.com/2010/11/updating-meteorological-forecasts-part-1/	November 7, 2010	arthur charpentier	"As Mark Twain said “the
art of prophecy is very difficult, especially about the future” (well, actually I am not sure Mark Twain was the  first one to say so,
but if you’re interested by that sentence, you can look here). I have been rather surprised to see how Canadians can
be interested in weather, and weather forecasts (see e.g. here for a
first study). For instance, on that website (here),
we can have forecasts of the temperature (but also rain and wind, which
is interesting when you ride a bike) over the next week, almost on an
hourly basis (I show here only GFS forecasts (Global Forecast System,here), which is quite standard, see here or there, but other meteorological models are considered on the website)             (colors are different because of the log scaling but the shape is similar).         "	 0 Comments
R is a cool image editor!	https://www.r-bloggers.com/2010/11/r-is-a-cool-image-editor/	November 7, 2010	Todos Logos		 0 Comments
Installing R packages	https://www.r-bloggers.com/2010/11/installing-r-packages/	November 6, 2010	csgillespie	"Part of the reason R has become so popular is the vast array of packages available at the cran and bioconductor repositories. In the last few years, the number of packages has grown exponentially! This is a short post giving steps on how to actually install R packages. Let’s suppose you want to install the ggplot2 package. Well nothing could be easier. We just fire up an R shell and type:


> install.packages(""ggplot2"") In theory the package should just install, however: First, you need to designate a directory where you will store the downloaded packages. On my machine, I use the directory /data/Rpackages/ After creating a package directory, to install a package we use the command:


> install.packages(""ggplot2"", lib=""/data/Rpackages/"")

> library(ggplot2, lib.loc=""/data/Rpackages/"")

 It’s a bit of a pain having to type /data/Rpackages/ all the time. To avoid this burden,  we create a file .Renviron in our home area, and add the line R_LIBS=/data/Rpackages/ to it. This means that whenever you start R, the directory /data/Rpackages/ is added to the list of places to look for R packages and so: > install.packages(""ggplot2"")

> library(ggplot2) just works! Every time you install a R package, you are asked which repository R should use. To set the repository and avoid having to specify this at every package install, simply: 

cat("".Rprofile: Setting UK repositoryn"")

r = getOption(""repos"")  # hard code the UK repo for CRAN

r[""CRAN""] = ""http://cran.uk.r-project.org""

options(repos = r)

rm(r)

 I found this tip in a stackoverflow answer . "	 0 Comments
Livin’ la Vida Poisson	https://www.r-bloggers.com/2010/11/livin%e2%80%99-la-vida-poisson/	November 5, 2010	Matt Asher	"Yes, I did just mix English, Spanish and French. And no, I living the “fishy” life, popular opinion to the contrary. Here’s the story. As someone who spends the majority of his time working online, with no oversight, I notice that I tend to drift a lot. I don’t play solitaire, or farm for virtual carrots, but I do wander over to Reddit more than I should, or poke around in this or that market in virtual assets to see if anything interesting has shown up. To some extent this can be justified. Many, perhaps all, of my profitable ventures have come from keeping my eyes open, poking around, doing my best to understand the digital world. On the other hand, at times I feel like I’ve been drifting aimlessly, that I’m all drift and no focus. My existing projects are gathering dust while I chase after shiny new things.  That’s the feeling, anyway. What does the evidence say? To keep track of what I was really doing, and perhaps nudge me towards more focus, I set a stopwatch to go off every 15 minutes. When it did, I would stop, write down what I was doing at that moment, and continue on. Perhaps you can see how these set intervals might provide an incentive to, shall we say, cheat? Especially right after the stopwatch chimed, I knew that whatever I did for the next few minutes was “free”, untracked. So I decided that I would have to write down everything I did during those 15 minute intervals, which worked sometimes, othertimes not so well. My current solution? Setup a bell which chimes at random intervals, with an average time between chimes of 15 minutes. To hear what the bell sounds like, Go ahead and try it out, I think you’ll find it makes a nice sound. Go ahead and leave that page open while you read the rest of this post, see how many times it rings.   At any rate, in order to randomize how long the wait was between chimes, I used a little something called a Poisson process. Actually, what I used was the Binomial approximation to the Poisson built from multiple Bernoulli trials, which results in wait times that are Exponential. Wait! Did you get all that? If so, then skip ahead until things look interesting. Otherwise, here’s more detail about how this works: In order to determine the length of time between chimes, my computer generates a random number number between 0 and 1. If this random number is less than 1/15, then the next chime is in just one minute. Otherwise, the computer generates another random number and adds one minute to the time between chimes. On average, it will take 15 tries to get a number below 1/15, so the average time between chimes will be 15 minutes. However, to call 15 minutes the average is somewhat misleading. Here are the frequencies of different wait times (source code in R at the end): 
 As you can see, the most common time between chimes is just one minute. Strange, no? What’s going on here is that each test to see if the random number is below 1/15 is a Bernoulli trial, which is basically Italian for “maybe it succeeds, maybe it fails”. In this case “success” has probability of 1/15, failure happens the other 14 out of 15 times. In cases where probability is small, and you end of doing a lot of trials, the total number of successes over a given time period will have the Poisson distribution. The “Poisson” here is a Frenchman, who may or may not have smelled like his surname, but who certainly understood The Calculus as well as anyone in the early 1800′s. To get an even better approximation of the Poisson, I could have used trails with probability of success of 1/900, then treated each failure as another second of waiting time. That would have made the graph above smoother.  But wait! I didn’t show you a graph of the Poisson. I showed you a graph of something that approximates the exponential distribution. The number of chimes per hour is (roughly) Poisson distributed, but the waiting time between each chime is exponential, which means shorter wait times are more frequent, but no length of time, no matter how long, can be ruled out. In fact, the exponential distribution is the only (continuous) distribution which is “memoryless”. If you have waited 15 minutes for a chime, your expected wait time is still…. 15 minutes. In fact, your expected wait is independent of how long you have waited so far. The exponential distribution is a “maximal entropy” distribution, entropy in this case is related to how much you know. With the exponential, no matter how long you’ve waited, you still don’t know anything more than when you started waiting.  If you’ve been tuning out and scanning this post, now would be a good time to tune back in. I promise new and interesting things ahead! It’s one things to understand the memoryless property of the exponential, even down to the mathematical nitty-gritty. It’s quite another to actually live with the exponential. No matter how well I know the formulas, I can’t shake the felling that the longer I have waited in between bell rings, the sooner the next chime must be coming. Certainly, it should be due any time now! While I “know” that any given minute has exactly the same probably as the next to bring with it the bell, the longer I wait, the nearer I feel the the next chime must be. After all, the back of my mind insists, once the page loads the wait time has been set into stone. However it was distributed before, it’s now a constant. Every minute you wait you are getting closer to the next bell, whenever it might have been set to come. I keep wanting to know more than I did a minute about about when the next bell will arrive. This isn’t the only way in which I find my psyche battling with my intellect. I would also swear that over time the distribution of short waits and long waits evens out. Now, by the law of large numbers, it’s true that the more chimes I sit through, the closer the mean wait time will approach 15 minutes. However, even if you’ve just heard three quick bells in a row, that has absolutely no bearing on how long the wait will be between the next three chimes. The expected wait times going forward are completely independent of the wait times in the past. The mean remains 15 minutes, the median remains 10.4 minutes. Yet that’s not what I feel is happening, and over the past two weeks of experimenting with this I would swear that on days when there are a number of unusually quick intervals, these have been followed, later that very the same day, with unusually long intervals. And vice versa. It feels like things are evening out. It’s possible that when my computer wakes up from a sleep mode, my web browser doesn’t remember where it was in a countdown to refreshing the chime page. So I reload it. Now, in theory, if you “reload” an exponential wait time while in process, this has absolutely no effect on your eventual wait time until the next chime. Yet anytime I reload the page, I have a moment of doubt as to whether I’m “cheating” in some way, to make what would have been a long wait shorter. In this case, the back of my mind says the exact opposite of its previous bias: because I am reloading a page that has been waiting a long time, this means that the wait time would have been really long. By starting the process anew, I’m increasing the chances of a short chime time.  Before you call me a nut, try living for a while with the timer running the background. Keep track of what you are doing if you want (and BTW I’ve found this to be every enlightening and more than a little sad), but mostly keep track about how you feel about the timing. Try reloading the page if you don’t hear a chime for a while. How does that feel? I suspect that in some ways humans were very well hard wired to understand probabilities. Yet I also suspect our wiring hinders how we understand probability, a suspicion backed up by all those gamblers out there waiting for the lucky break that’s well overdue.  CODE: "	 0 Comments
The bms Function Explained	https://www.r-bloggers.com/2010/11/the-bms-function-explained/	November 5, 2010	BMS Add-ons » BMS Add-ons	"This text ‘pedagocially’ explains how the bms function works code-wise and is intended for people who prefer to program customized adjustments of the bms package. bms is the workhorse function to do the sampling part of Bayesian Model Averaging in the BMS package. The bms function code in the package includes many different options, usability checks, and is tweaked for computational speed. Although it can be difficult to understand, it basically relies on object-oriented subfunctions that can be easily re-shuffled. In order to demonstrate this, the following paragraphs introduce code that replicate a very basic BMA sampling operation but stripped of all options and speed features. The most easy-to-build version of BMA is to focus on a specific g-prior (here we use the BRIC prior by Fernández, Ley and Steel), and omit model priors.
To simply things further, let’s only use MCMC frequencies as an approximation to analytical likelihoods. Moreover, fix the start model of the chain at the null model because this needs the least code lines. A BMA sampling function to produced posterior inclusion probabilites and coefficients can then be sketched as in the example below. The comments indicate the use of subfunctions, mainly .fls.samp to sample candidate models, .ols.terms2 to calculate OLS quantities such as residual sum of squares, and .lprop.constg.init to calculate Bayesian posterior statistics for each model. This function can be called via a command such as: Note that the above command can of course be replicated with bms. Just type Approximating Posterior model probabilites on MCMC frequencies is not very popular. Most authors prefer instead to base their posterior statistics on analytical likelihood expressions – which correspond to the term current.lprob used in the code above.
For that purpose, one must renounce on aggregating coefficients on the fly. Instead, one should retain all the accepted models encountered and do the aggregation based on their analytical PMPs after the sampling. For a huge numebr of sampling steps, this is computationally cumbersome or infeasible. Instead the usual approach is to store the best x models encountered that should cover most of the explored model space. In order to achieve this, we use the topmod object (check the help with ?topmod). The previous code basically just needs to altered by to lines: Before the sampling loop the following is added to initialize the object In the model acceptance part (after the pipvec adding-up, the following line needs to be added: tmo then keeps track of the best nmodel models by likelihood. After the sampling chain, code needs to be altered somewhat to process the information. The entire, augmented function basicbms_wtop is in the code fragment below. A call to basicbms_wtop now produces output basic on exact analytical likelihoods: The result prints output based on the best 10 models encountered by the sampling chain. (The topmod object can be further inspected by the command print(bres3) Note that the building blocks above are enough to reproduce the Matlab code chapter11.m by Gary Koop. His code uses both frequency- and analytical-based inference, and starts from a more custom staring model. Therefore the function basicbms_wtop needs to be slightly adjusted to produce basicbms_koop: Calling basicbms_koop now prodcues the same output as his matlab function. Note that the actual numerical results can differ a lot, as the very few iterations Kopp suggests are not a good approximation. Moreover, note that Koop’s dataset is differently ordered from datafls, therefore necessitating a re-ordering. Note the Koop output function can be easily reproduced with basic BMS commands as well: The above examples show how the building blocks of BMS might be used to construct basic BMA sampling functions. In particular vital are the functions topmod, .ols.terms2 as well as the sampling functions (here .fls.samp) and the likelihood functions (here .lprob.constg.init). For more about these functions check out their source code in the BMS.RData which includes code comments. "	 0 Comments
Because it’s Friday: Epidemiology in 1632	https://www.r-bloggers.com/2010/11/because-its-friday-epidemiology-in-1632/	November 5, 2010	David Smith	I first got interested in epidemiology when I saw the famous John Snow chart (in a Tufte book, I think?) which pinpointed the pump which caused the 1854 cholera outbreak in London. For some reason I'd gotten the impression that this was essentially the birth of epidemiology as a discipline, but it's actually been around a lot longer than that. 200 years before that time, John Graunt developed one of the first systems for recording and reporting human casualties from disease. His annually-published tables (some of which are available in Google Books) offer a fascinating peek into life in 17th-century London. Here's just one page, from 1632:   Some of those diseases might look unfamiliar, but this list of old disease names can help with the translation. Many have since been cured, and some are still with us. And we'll always have Grief, I guess.   	 0 Comments
ACM Data Mining Camp 3	https://www.r-bloggers.com/2010/11/acm-data-mining-camp-3/	November 5, 2010	David Smith	The San Francisco Bay Area chapter of the ACM is will hold its third data mining camp next Saturday (November 13) at the Ebay campus in San José. Like the previous camps, this will be a one-day “unconference”-style event, with an agenda developed ad-hoc on the day according to the interests of the attendee. With data scientists from the likes of LinkedIn, Pacific Bioscience, Forbes, Intel, Sony and Apple already signed up, there's sure to be some great talks on data analysis and data mining. Revolution Analytics is proud to be sponsoring and attending this event. For more details and registration info, check the link below. San Francisco Bay Area Chapter: ACM Data Mining Camp, November 13, 2010 	 0 Comments
Pretty R code in the blog	https://www.r-bloggers.com/2010/11/pretty-r-code-in-the-blog/	November 5, 2010	arthur charpentier	David Smith (alias @revodavid, see also on the Revolutions blog, here) pointed out that my R code was not easy to read (not only due to my computing skills, but mainly because of the typography I use). He suggested that I use the Pretty R tool (here). And I will… So, just to answer quickly to a question I received by email (a few weeks ago, sorry for the delay), here is the code to get the following nice plot  	 0 Comments
New England R Users Group Meeting	https://www.r-bloggers.com/2010/11/new-england-r-users-group-meeting/	November 5, 2010	Josh Paulson	Attended and thoroughly enjoyed Tuesday night’s New England R Users Group. We meet monthly in the Boston area to discuss the various ways in which people use and interact with the R programming language. Not surprisingly, we have a variety of industries represented. One of us is using R to recognize patterns in  tissue samples to indicate the formation of tumors. Another is  looking into how super computers can be used with R to enhance data  analysis. Others, from the finance world, use tick data (over 80 GB in size) to make inferences about different securities in the stock market. The list goes on. Clearly we have a very diverse group! One common issue many of our member’s face is how to deal with big data sets. Obviously there are various packages available related to this issue such as ff and bigmemory. However, most of us have limited knowledge on how to actually utilize these resources. Thus, our next meeting will have some short presentations providing a general overview of the various approaches to dealing with big data sets. I am looking forward to it and am sure it will be insightful. 	 0 Comments
Splines: opening the (black) box…	https://www.r-bloggers.com/2010/11/splines-opening-the-black-box/	November 4, 2010	arthur charpentier	            	 0 Comments
CrossValidated launched!	https://www.r-bloggers.com/2010/11/crossvalidated-launched/	November 4, 2010	Rob J Hyndman	The CrossValidated Q&A site is now out of beta and the new design and site name is live.  The new design looks great, thanks to Jin Yang, our designer-in-residence. Note the normal density icon for accepted answers and the site icon depicting a 5-fold cross-validation (light green for the test set and dark green for the training set). There is a faint background graphic in the header and footer from a program that tracks and plots a person’s mouse movement. This gives the suggestion of randomness as well as the idea of data visualization (another topic covered on the site). The URL crossvalidated.com will work, but re-directs to stats.stackexchange.com. The StackExchange team (who host the site and provide all the architecture) wanted the site to be a subdomain of stackexchange.com. However, at least we got the name CrossValidated. The site is intended for use by statisticians, data miners, and anyone else doing data analysis. It covers questions about The inclusion of data mining and machine learning along with statistics and probability was a deliberate attempt to get these two communities to talk. We work on similar problems, but often with different tools and different perspectives. I hope the site comes to be widely used within both communities. In fact, I hope that we can eventually stop talking about two communities and just refer to the “data science community”. My original idea was that this would be helpful to researchers struggling with data analysis issues but have no statistician to ask for help. University-based statisticians are often inundated with requests for help from researchers in other disciplines who have no quantitative training but need to do apply some statistical techniques. For those who haven’t been reading this blog, I proposed this site on 15 April 2010. The scope of the site was determined via a community process, then we went through a phase of building a sufficient community. The beta site was launched on 19 July 2010 with the first question on “Eliciting priors from experts”. The site was officially launched today (5 November 2010). So it took just over 200 days from proposal to launch — I had no idea what I was starting, but I’m glad it worked out! There are now 1048 questions and 1763 users which is a great start. But there must be hundreds of thousands of people doing data analysis and who would really benefit from a site like this. So please spread the word about CrossValidated.com. 	 0 Comments
Dress your R code for the Web with Pretty R	https://www.r-bloggers.com/2010/11/dress-your-r-code-for-the-web-with-pretty-r/	November 4, 2010	David Smith	If you have some R code to include in a document, especially a Web-based document like a blog post, the new “Pretty R” feature on inside-R.org can help you make it look its best. Given some raw R code, it will create a HTML version of the code, adding syntax highlighting elements and links. Functions, strings, comments and literals are all color-coded to make reading easier, and function names are linked to the corresponding R help pages in the Language Reference from inside-R.org. For example, the Freakonometrics blog had an excellent post the other day on using time series methods to forecast pageviews reported on Google Analytics. (It's a great blog if you haven't checked it out before. I also like it because it gives me a chance to brush up on my French. But I digress.) The author, Arthur Charpentier, did include the R code, but I found it a little hard to read. Here's a screenshot:  I cut-and-pasted the code into the Pretty R tool (after removing the > and + prompts, which I recommend as good practice for posting R code — it makes it easier for others to paste it into a script), and then just pasted the resulting HTML into this blog post. Here's the result: The Pretty R tool is open to everyone, you don't need to set up an account to use it. I hope it's useful to all R authors and bloggers. inside-R.org: Pretty R Syntax Highlighter 	 0 Comments
R is Hot: Part 5	https://www.r-bloggers.com/2010/11/r-is-hot-part-5/	November 4, 2010	David Smith	This the final installment of a five-part article series. You can download the complete article from the Revolution Analytics website.     The value of R to business is borne out by the experiences of John Lucker and his team of advanced analytics professionals at Deloitte Consulting LLP. John is a Deloitte Consulting Principal and leads the firm’s Advanced Analytics and Modeling (AAM) practice, one of the leading analytics groups in the professional services industry. When the group was launched fourteen years ago, its main focus was solving vexing business problems for clients in the insurance industry. One of the challenges facing the industry was the lack of robust analytic processes for supporting critical underwriting decisions. This challenge was particularly acute in the rapidly growing commercial insurance industry. “The commercial insurance underwriting process was rigorous but also quite subjective and based on intuition,” says John.  Convincing experienced underwriting management to change their time-honored traditions was not an easy task.  That’s where R proved exceptionally helpful in recent years. “R enables us to communicate our analytic results in appealing and innovative ways to non-technical audiences through rapid development lifecycles,” says John. “Sometimes people know they have a problem, but they don’t know how to fix it. And sometimes they don’t even know they have a problem. R helps us show our clients how they can improve their processes and effectiveness by enabling our consultants to conduct analyses efficiently.” The group’s success with clients in the insurance industry became a blueprint for expanding into new markets. Today, the Deloitte Consulting Advanced Analytics and Modeling practice also serves clients in healthcare, banking and financial services, retail, consumer products, telecomm, automotive, media, hospitality, public/state/federal sector and other major industries. R played an important role in growing the practice by allowing it to offer robust analytics addressing the specific needs of clients in a variety of markets. “I find the diversity of R solutions and add-ons very appealing,” says John. “R has served as a catalyst in the marketplace. It forces everyone to raise their game, and it incents software developers to enhance their offerings. Everybody becomes more competitive and users of analytic tools benefit.”    With thousands of contributors and two million users worldwide, R is a truly global phenomenon. Unlike traditional commercial software for data analysis, R is both flexible and extensible. Supported by an active community of users and developers, R is constantly changing to meet the changing needs of our rapidly shifting global economy.  The popularity of R is no fluke or fad. R has become the common language of data analysis because it was designed – from the ground up – as a practical system for handling the real-world challenges of complex data sets. R-based programs are applied routinely to solve problems in real-time trading, finance, risk assessment, forecasting, biotechnology, drug development, social networking and more. But the wide acceptance of R as the lingua franca of statistics is based on its unique ability to change, to transform and to evolve. When new techniques in statistical analysis are discovered, they tend to emerge as R packages first – years before those innovations are incorporated in traditional enterprise software products. Thanks to its open-source roots, R has spread virally across the map. It has become both ubiquitous and indispensable. The R community supports development, innovation and continuous improvement. New players are welcome and encouraged. The R eco-system has become a fertile breeding ground for novel ideas and original ways of thinking about numbers. No one can foretell the future of quantitative analytics, but it’s safe to wager that a good deal of it will be written in R.  Read all parts of this series 	 0 Comments
The dead of Juarez	https://www.r-bloggers.com/2010/11/the-dead-of-juarez/	November 4, 2010	Diego Valle-Jones		 0 Comments
The Answer Depends on the Question	https://www.r-bloggers.com/2010/11/the-answer-depends-on-the-question/	November 3, 2010	John Myles White	"
To quote from the preface to the first edition in Jeffreys (1961): ‘It is sometimes considered a paradox that the answer depends not only on the observations but on the question; it should be a platitude.’1
 "	 0 Comments
iPhone App Store Acceptance Time / Download Results	https://www.r-bloggers.com/2010/11/iphone-app-store-acceptance-time-download-results/	November 3, 2010	C		 0 Comments
RMongo: Accessing MongoDB in R	https://www.r-bloggers.com/2010/11/rmongo-accessing-mongodb-in-r/	November 3, 2010	tommy	"I recently created RMongo, a database access layer to MongoDB in R as an R package. To install RMongo, download it from https://github.com/quid/RMongo/downloads
Run:
R CMD install RMongo_0.0.17.tar.gz I tried to mimic the RMySQL commands in RMongo. Below are some example commands.
library(RMongo)
#ask for help

?RMongo
#connect to a database

mongo <- mongoDbConnect(""eat2treat_development"")
#show the collections

dbShowCollections(mongo)

[1] ""conditions""            ""users""                 ""nutrient_metadatas""    ""system.indexes""        ""user_food_preferences"" ""food_metadatas""

[7] ""food_group_ratings
# perform an 'all' query with a document limit of 2 and offset of 0.

# the results is a data.frame object. Nested documents are not supported at the moment. They will just be the string output.

> results <- dbGetQuery(mongo, ""nutrient_metadatas"", ""{}"", 0, 2)

> names(results)

[1] ""X_id""                   ""name""                   ""nutrient_definition_id"" ""description""

> results

                      X_id              name nutrient_definition_id

1 4cd0f8e31e627d4e6600000e  Adjusted Protein                    257

2 4cd0f9061e627d4e6600001a            Sodium                    307
> results <- dbGetQuery(mongo, ""nutrient_metadatas"", '{""nutrient_definition_id"": 307}')

> results

                      X_id   name nutrient_definition_id

1 4cd0f9061e627d4e6600001a Sodium                    307
> dbDisconnect(mongo)
 RMongo is very alpha at this point. I built it as a quick way to prototype algorithms with data from mongoDB in R. Most of RMongo uses the mongo-java-driver to perform json-formatted queries. The R code in the package uses rJava to communicate with the mongo-java-driver.  Please report any bugs or necessary improvements. Or better yet, send in pull requests via the RMongo github project page! "	 0 Comments
Keeping up with election results, with R	https://www.r-bloggers.com/2010/11/keeping-up-with-election-results-with-r/	November 3, 2010	David Smith	"Yesterday's US election is pretty much over now: most of the results are in, the pundits have offered their political analysis, and there's even been a bit of mathematical analysis of the results, too. But last night as the results were flowing in, R user Brock Tibert just wanted to track the results of the Massachusetts governor's race. The Boston Globe was providing regular updates to the precinct level counts, but it's hard to visualize the horse-race from the raw numbers. So with a little hackery and a ingenuity, Brock wrote some R code to scrape the results table from the Boston Globe webpage and visualize the results as a barplot. Here's an excerpt of his code (from his gist page):  Brock used the XML package to extract the table from the webpage, the stringr package to process the text results, and (after arranging the data), the barplot function to visualize the results. And here's the resulting bar plot (which I ran today, so it now represents the final tallies):  Brock said in this tweet: ""Not elegant, but works."". Perhaps true, but that tweet making the code available was sent at 9PM East Coast time, not long after the polls had closed and as the returns were coming in. It may not be elegant, but it's a really impressive way to visualize the results in real time. "	 0 Comments
My residuals look weird… aren’t they ?	https://www.r-bloggers.com/2010/11/my-residuals-look-weird-arent-they/	November 3, 2010	arthur charpentier	"Since I got the same question twice, let us look at it quickly….
 Some students show me a graph (from a Poisson regression) which looks
like that,    "	 0 Comments
Looping through a set of graphics in odfWeave	https://www.r-bloggers.com/2010/11/looping-through-a-set-of-graphics-in-odfweave/	November 3, 2010	Social data blog	"
 At proMENTE social research we often use the odfWeave and Sweave
packages for the amazing statistics program R for automating the
production of graphics and reports. odfWeave and Sweave are for the
OpenOffice and lyx (www.lyx.org) word processors respectively. One problem with this approach arises when you have to produce and
insert into your document a whole bunch of graphics. For any other bunch
of things such as tables it is easy enough to make different kinds of
loops to produce the sets of tables you want. But graphics are a bit
more tricky because each graphic requires its own frame. One way to do this which we have developed in odfWeave is to write a
loop to produce an intermediate source document which contains the code
for producing or inserting the actual graphics. Then you run odfWeave on
this intermediate document to produce the final document with the
graphics. So you do your weaving twice. Assume you have a dataframe df of variables. Assume you have already
produced a folder full of graphics, named with the colnames of the
dataframe (if you have got this far, you are versed enough to know how
to do this already). We will use the following function in the source document, source.odt,
to loop through the dataframe and produce an intermediate document
called intermediate.odt with one piece of odfInsertPlot code for each
graphic. Then the second odfWeave command produces the final document
from intermediate.odt. This is the function you need to have defined previously And this is the loop you need to put in the source.doc. So you run odfWeave(“source.odt”,“intermediate.odt”) in R and then run
odfWeave(“intermediate.odt”,“final.odt”) again in R to produce your
final doc. Permalink 

	| Leave a comment  »
 "	 0 Comments
inline 0.3.7	https://www.r-bloggers.com/2010/11/inline-0-3-7/	November 3, 2010	Thinking inside the box	"

It fixes a minor bug: when package.skeleton() was called to
convert one or more functions created with this package into a package, the
corner case of just a single submitted function failed. This is now corrected.
Otherwise this release is unchanged from the previous release 0.3.6 from
August.

 "	 0 Comments
Rcpp 0.8.8	https://www.r-bloggers.com/2010/11/rcpp-0-8-8/	November 2, 2010	Thinking inside the box	"
This release follows on the heels of 0.8.7, but contains fixes for a few
small things Romain and
I had noticed over the last two weeks since releasing 0.8.7 and contains only
a small number of new tweaks. The NEWS entry follows below:
 
As always, even fuller details are on the 
Rcpp Changelog page and the 
Rcpp page which also
leads to the downloads, the 
browseable
doxygen docs and zip files of doxygen output for the standard formats.
A local directory  has
source and documentation too. 
Questions, comments etc should go to the
rcpp-devel mailing list
off the R-Forge page

 "	 0 Comments
SAS vs Open Source, ctd	https://www.r-bloggers.com/2010/11/sas-vs-open-source-ctd/	November 2, 2010	David Smith	Following up on the story from last week, where SAS CEO Jim Goodnight said he “hadn't noticed” competition from open-source alternatives, open-source BI vendor Pentaho's “Chief Geek” James Dixon responds: What this means is that SAS has moved from the Igorance phase to the Ridicule phase of battling open source, they only have Fighting and Losing to go. There are some interesting parallels with Microsoft's reaction to the insurgency of Linux 10 years ago, as James analyzes in the full post linked below. James Dixon’s Blog: SAS under pressure from Pentaho 	 0 Comments
Comments on probabilities	https://www.r-bloggers.com/2010/11/comments-on-probabilities/	November 2, 2010	arthur charpentier	"
The only thing I remember from courses I had in probability a
few years ago is that we also have to clearly defined the
event we want to calculate the probability. On the Freakonomics blog,
last week, the Israeli lottery was mentioned (here, see also there
where I mentioned that, and odds facts from the French lottery),     "	 0 Comments
Installing rgdal on Mac OS X	https://www.r-bloggers.com/2010/11/installing-rgdal-on-mac-os-x-2/	November 2, 2010	James	"After running a spatial data analysis with R session today, it became apparent that there are one or two teething problems installing the important rgdal package on Mac OS X operating systems. The usual install.packages(“rgdal”) won’t work. My colleague Jon Reades did some digging around to find this solution. I have tested it and it seems to work fine. [Note that you’ll need to be comfortable with the Terminal. If you’re not, then find someone who is.] 1. Download the GDAL OS X install from kyngchaos
– http://www.kyngchaos.com/files/software/unixport/GDAL_Complete-1.7.dmg
(Looks like the basic page [for updates after 1.7 if you’re reading this ages from now] is http://www.kyngchaos.com/software/frameworks)
– Install as per usual OS X install system
– Fire up the Terminal, then pico (or vi[m]) the .bash_login file
– Modify the PATH environment so that it reads:
export PATH=”/Library/Frameworks/GDAL.framework/Programs:$PATH”
[This is what enables the subsequent steps to find gdal-config] 2. Download and install proj4 from source
– http://trac.osgeo.org/proj/wiki/WikiStart#Download
– Download source code version proj-4.7.0.tar.gz
– Fire up the Terminal
> cd ~/Downloads/
> tar -xzvf proj-4.7.0.tar.gz
> cd proj-4.7.0
> ./configure
> make && make test
> sudo make install
[ should install to /usr/local/lib by default] 3. Download and install rgdal from source
– http://cran.r-project.org/src/contrib/rgdal_0.6-28.tar.gz
– Fire up the Terminal
> cd ~/Downloads/
> sudo R CMD INSTALL –configure-args=’–with-proj-include=/usr/local/lib’ rgdal_0.6-28.tar.gz After all of this mucking about I was able to say: > require(sp)
> require(rgdal) And get a message indicating that GDAL was loaded successfully. He also posted his solution on the Computational Mathematics Blog. If there is a better way I would be interested in hearing about it for future classes. "	 0 Comments
Names of villages, in France	https://www.r-bloggers.com/2010/11/names-of-villages-in-france/	November 2, 2010	arthur charpentier	                   	 0 Comments
intergraph+network: no hacking necessary	https://www.r-bloggers.com/2010/11/intergraphnetwork-no-hacking-necessary/	November 2, 2010	Michał	A short update on network+intergraph R packages story: Couple of days ago Carter Butts released a new version of the ‘network’ package (ver. 1.5-1). It has a namespace now. Consequently, the ‘intergraph’ package should work out-of-the-box. There is no need to install my hacked version of the ‘network’ package anymore. 	 0 Comments
How to simulate wind speed time series with R	https://www.r-bloggers.com/2010/11/how-to-simulate-wind-speed-time-series-with%c2%a0r/	November 2, 2010	procomun	If you need to generate synthetic wind speed time series, you may find useful the procedure described in  “A Markov method for simulating non-gaussian wind speed time series” by G.M. McNerney and P.S. Veers (Sandia Laboratories, 1985), and “Estimation of extreme wind speeds with very long return periods” by M.D.G Dukes and J.P. Palutikof (Journal of applied meteorology, 1994). This procedure is internally implemented in the software Hybrid2 and Homer. I have implemented this procedure with R. The script is available here and is described below: First, we have to define the parameters of the wind speed distribution: Now, we build a vector of  wind speeds centered around the mean value of each state and obtain the density function defined by Shape and Scale: The correlation between the different categories of wind speed is defined by a matrix constructed with a decreasing function: Then, an iterative procedure is needed for the calculation of the matrix of initial probability (P matrix): Let’s combine the p vector and P and G matrices for obtaining a Markov Transition Matrix: With this cumulated MTM we are able to simulate a wind speed time series: A result of this procedure is displayed in the next figure: We can check that the distribution of this wind speed series is correct: 	 0 Comments
ROC – plot	https://www.r-bloggers.com/2010/11/roc-plot/	November 1, 2010	kariert	There are many implantation in R already of ROC plots (e.g. in the packages PresenceAbsence, ROCR). I just wrote my own very simple script just to get a better understanding of it. 	 0 Comments
Programming with R – Returning Information as a List	https://www.r-bloggers.com/2010/11/programming-with-r-%e2%80%93-returning-information-as-a-list/	November 1, 2010	Ralph	In previous posts (here and here) we created a simple function that returns a single numeric value. In some situations it may be more useful to return a more flexible data type, such as a list object, to provide more information about the calculations that have been performed. We can extend our previous function by changing the return value to a list including the height and width supplied by the user. The last line of the function is changed to: This creates a list with three elements, which are given very obvious names. The function in full is: We can call this function using a simple example: The output from this function is a list with three slots as discussed above. This approach is ideally suitable to statistical applications where we might have a model with a large amount of supplementary information that should be returned after it has been applied to a set of data. 	 0 Comments
Google TechTalk on integrating R	https://www.r-bloggers.com/2010/11/google-techtalk-on-integrating-r/	November 1, 2010	David Smith	"As noted on the Google Open Source Blog last week, R package authors Dirk Eddelbuettel and Romain Francois recently gave a presentation on R at the Googleplex, on various topic related to “bridging” R into other systems. Their 90-minute talk is available for replay on YouTube (as part of the Google TechTalks series), and you can download the slides from Dirk's blog. For the Google audience, I'm sure it prompted many ideas on how to further marry R into analytical systems there. The first part of the talk is all about how to speed up R by recoding tight loops in C++ and calling them from R via the .Call function. Without help it could be tricky to whip up a C++ function to call from R, but Dirk and Romain have made the process hassle-free by providing the Rcpp package, which provides a high-level mapping between R and C++ object types. Next, they give examples of the inline package by Oleg Skylar et al, which makes it easy to embed C++ code in R functions, and automates the process of compiling and linking the C++ code so you don't have to worry about it. They also discussed their Rinside package, which makes it possible to call R directly from a C++ program (and of course share objects and data via Rcpp). Finally, they gave examples of integrating R with protocol buffers (a widely used mechanism used at Google for serializing data for exchange between different systems) with the RProtoBuf package. The second part of the talk focused on the various ways possible to build data structures in R: S3 classes, S4 classes and now the new reference classes available in R 2.12. These reference classes (which they call R5 classes) are objects with components which are not copied when the object is copied (they are passed by reference, not by value). There's a detailed overview of reference classes in the “methods” package of the R documentation, under ReferenceClasses. The audience of the talk included John Chambers who designed R (and S's) class systems. He asked the first question during the discussion period, so be sure to listen through to the end. 





 Open Source at Google: Integrating R with C++: Rcpp, RInside, and RProtobuf "	 0 Comments
An analysis of the Stackoverflow Beta sites	https://www.r-bloggers.com/2010/11/an-analysis-of-the-stackoverflow-beta-sites/	November 1, 2010	csgillespie	"In the last six months or so, the behemoth of Q & A sites stackoverflow, decided to change tack and launch a number of other non-computing-language sites. To launch a site in the stackoverflow family, sites have to spend time gathering followers in Area51. Once a site has gained a critical mass, a new StackExchange (SE) site is born. At present there are around twenty-one SE beta sites. Being rather bored this weekend, I decided to see how these sites are similar/different. For a first pass, I did something rather basic, but useful none the less. First, we need to use the stackoverflow api to download some summary statistics for each site: For each of the twenty-one sites, we now have information on the: An easy “starter for ten” in terms of analysis, is to do some quick principle components: This gives the following plot:
Main features: In case anyone is interested, the weightings  you get from the PCA are: #PC1 is a simple average

> round(PC.cor$rotation, 2)

PC1     PC2     PC3     PC4

votes   0.54   -0.31   -0.17   -0.77

users   0.51    0.18    0.83    0.10

views   0.50   -0.53   -0.27    0.63

qs      0.44    0.77   -0.45    0.10 "	 0 Comments
Interesting packages of new BioC2.7	https://www.r-bloggers.com/2010/11/interesting-packages-of-new-bioc2-7/	November 1, 2010	Xianjun	NuPoP  Nucleosome positioning prediction Mulcom  Differential expression and false discovery rate calculation through  multiple comparison ontoCAT  Ontology parsing BHC  Bayesian Hierarchical Clustering iSeq  Bayesian Hierarchical Modeling of ChIP-seq Data Through Hidden Ising  Models 	 0 Comments
Example 8.12: Bike ride plot, part 1	https://www.r-bloggers.com/2010/11/example-8-12-bike-ride-plot-part-1/	November 1, 2010	Ken Kleinman		 0 Comments
ggplot2 change axis labels	https://www.r-bloggers.com/2010/11/ggplot2-change-axis-labels/	November 1, 2010	kariert	Today, I will try to change the labels of the x and y axis. 	 0 Comments
Modeling sound pressure level of a rifle shot	https://www.r-bloggers.com/2010/11/modeling-sound-pressure-level-of-a-rifle-shot/	November 1, 2010	danganothererror	"Noise can be classified as pollution and lawmakers often (always?) treat it as such. Noise can have different origin points, point source being among the simplest to model. Because noise has broader health implications, being able to understand its propagation, a simple model can further our understanding in toning down or preventing excessive noise burden on the environment and its inhabitants. In this work, I will focus on firing range noise and the propagation of sound to the surrounding area.
Small scale firing ranges can be considered as point origin of noise. To make a simple predictive model, a number of assumptions and generalization are made. The reader should realize that this makes the model a bit less realistic. When talking to experienced people, they will tell you that the distance between a firing range and the first house should be roughly 200 m. While there is no explicit mention of this number in Slovenian laws (yes, I’ve checked), there is a threshold of sound pressure level (SPL) of 75 dB. So, knowing the SPL of the rifle and we know the legal threshold, we can use a simple model to estimate approximate distance at which the SPL will fall to or below the aforementioned legal threshold. A rifle shot produces a sound pressure level of about 170 dB, which is roughly the sound of a jet engine at a 30 m distance (see here). Noise propagates and dissipates through the air with roughly (source) which gives us where L_2 = sound level at  measured distance
L_1 = sound level at reference distance
r_1 = reference distance from source of the sound
r_2 = measured distance from the source Using this model, we have accepted all sorts of assumptions, like calm weather, even terrain, even air pressure, no air resistance… Come to think of it, this model would be best suited for a desert in lovely weather. Nonetheless, it gives us a starting point. I would be interested to hear from more knowledgeable readers on any potential mistakes and how to improve the model with regards to at least  above assumptions. Modeling this equation in R is trivial. Let’s write a function that will calculate L_2 for a sequence of r_2 values. Result of the plotting is

This tells us that the sound pressure level at roughly 113 m away from the rifle will be 75 dB (the legal threshold). Based on these results, a 200 m buffer around a firing range gives an estimate with a margin of around 100 m buffer. As already mentioned, I would be happy to hear your comments on errors and how to improve the above model. "	 0 Comments
Choropleth Maps of Presidential Voting	https://www.r-bloggers.com/2010/11/choropleth-maps-of-presidential-voting/	November 1, 2010	d sparks	Having always appreciated the red and blue cartograms and cartographs of geographic electoral preferences, such as those made available by Mark Newman, I sought to produce similar maps, but include information about support for non-“state-sponsored” parties, and to extend the coverage back in time. I was able to find county-level presidential election returns going as far back as 1920, thanks to the CQ Press Voting and Elections Collection (gated). I converted the proportion of the vote garnered by Democratic, Republican, and “Other” parties’ candidates to coordinates in three-dimensional RGB color space, and used shapefiles from the mapdata package to plot these results as choropleth maps with ggplot. Click to view slideshow. It is interesting to observe these maps in a series, which gives historical context to the Red State/Blue State narrative. Most obviously, there is a significant shift in the geographic center of Democratic support, from a concentration in the southeast to the present equilibrium, localized on each coast and near the Great Lakes. Among these 23 elections, landslide victories, such as Roosevelt over Landon in 1936, Johnson over Goldwater in 1964, Nixon over McGovern in 1972, and Reagan over Mondale in 1984, tend to stand out for their monochromaticity. Also intriguing are the elections featuring substantial support for third-party candidates. Most of these are individuals who were had a strong support base in a specific region of the country, such as La Follette in the northwest, and Thurmond and Wallace in the deep south. Ross Perot’s run in 1992 is unique here, as his relatively broad geographic base of support results in a map that runs the gamut to a greater degree than any others. Click on the image below to see a full screen version of the slideshow above, or to download any of the individual maps as PNGs.  Click for slideshow/download 	 0 Comments
"Rearranging the office to find a fresh perspective.
Hasn’t…"	https://www.r-bloggers.com/2010/12/rearranging-the-office-to-find-a-fresh-perspective-hasn%e2%80%99t/	December 31, 2010	awaiting assimilation	Rearranging the office to find a fresh perspective. Hasn’t been done in seven years. I feel better already. Gonna hang some pictures. Gonna set some goals. I’m not afraid to fail. Saw “True Grit” last night. I liked it. 	 0 Comments
History makes Stat. Science!	https://www.r-bloggers.com/2010/12/history-makes-stat-science/	December 31, 2010	xi'an	While the above heading sounds like a title in reverse, its words are in the “correct” order in that our paper with George Casella, A Short History of Markov Chain Monte Carlo, has been accepted for publication by Statistical Science. This publication may sound weird when considering that the paper is also scheduled to appear in the Handbook of Markov Chain Monte Carlo: Methods and Applications, edited by Steve Brooks,         Andrew Gelman, Galin Jones, and         Xiao-Li Meng… However, George noticed we were allowed to keep our copyright ownership when publishing a chapter of this handbook, hence the possibility to submit the chapter to a journal as well. After a smooth editorial process, the paper has thus been accepted, which makes it my last acceptance for 2010, a rather productive year. I wish (myself!) that the extra-time brought by my appointment at Institut Universitaire de France will see an even more productive 2011 year! And very best wishes to all readers of the ‘Og, in terms of personal and professional accomplishments!!! 	 0 Comments
R-Chart: Year End Wrap Up	https://www.r-bloggers.com/2010/12/r-chart-year-end-wrap-up/	December 31, 2010	C		 0 Comments
Video of Joy of Stats by Hans Rosling	https://www.r-bloggers.com/2010/12/video-of-joy-of-stats-by-hans-rosling/	December 31, 2010	Larry D'Agostino		 0 Comments
R / Finance 2011 Call for Papers: Updated and expanded	https://www.r-bloggers.com/2010/12/r-finance-2011-call-for-papers-updated-and-expanded/	December 31, 2010	Thinking inside the box	"
One week ago, I sent the updated announcement below to the 
r-sig-finance list;
this was kindly blogged about by fellow committee member
Josh 
and by our pal 
Dave @ REvo.
By now. I also updated the
R / Finance conference website.
So to round things off, a quick post here is in order as well. It may even get a
few of the esteemed reader to make a New Year’s resolution about submitting a
paper 🙂
 
The preparations for
R/Finance 2011 are progressing, and due to favourable
responses from the different sponsors we contacted, we are now able to offer

 
More details are below in the updated Call for Papers. Please feel free to
re-circulate this Call for Papers with collegues, students and other
associations.
 
Cheers, and Season’s Greeting,
 
Dirk (on behalf of the organizing / program committee)




 
The third annual R/Finance
conference for applied finance using R will be
held this spring in Chicago, IL, USA on April 29 and 30, 2011.  The two-day
conference will cover topics including portfolio management, time series
analysis, advanced risk tools, high-performance computing, market
microstructure and econometrics. All will be discussed within the context of
using R as a primary tool for financial risk management, portfolio
construction, and trading.

 
Complete papers or one-page abstracts (in txt or pdf format) are invited to
be submitted for consideration. Academic and practitioner proposals related
to R are encouraged. We welcome submissions for full talks, abbreviated
lightning talks, and for a limited number of pre-conference (longer)
seminar sessions.

 
Presenters are strongly encouraged to provide working R code to accompany the
presentation/paper.  Data sets should also be made public for the purposes of
reproducibility (though we realize this may be limited due to contracts with
data vendors). Preference may be given to presenters who have released R
packages.

 
The conference will award two $1000 prizes for best paper: one for best
practitioner-oriented paper and one for best academic-oriented paper.
Further, to defray costs for graduate students, two travel and expense grants
of up to $500 each will be awarded to graduate students whose papers are
accepted.  To be eligible, a submission must be a full paper; extended
abstracts are not eligible.

 
Please send submissions to: committee at RinFinance.com

 
The submission deadline is February 15th, 2011.  Early submissions may
receive early acceptance and scheduling.  The graduate student grant winners
will be notified by February 23rd, 2011.

 
Submissions will be evaluated and submitters notified via email on a rolling
basis. Determination of whether a presentation will be a long presentation or
a lightning talk will be made once the full list of presenters is known.

 
R/Finance
2009
and
2010
included attendees from around the world and featured
keynote presentations from prominent academics and practitioners. 2009-2010
presenters names and presentations are online at the conference website. We
anticipate another exciting line-up for 2011—including keynote
presentations from
John Bollinger,
Mebane Faber,
Stefano Iacus, and
Louis Kates.
Additional details will be announced via the
conference website
as they become available.

 
For the program committee:

 "	 0 Comments
The R Journal, Vol.2 Issue 2 is out	https://www.r-bloggers.com/2010/12/the-r-journal-vol-2-issue-2-is-out/	December 31, 2010	Tal Galili	The second issue of the second volume of The R Journal is now available . Download complete issue Refereed articles may be downloaded individually using the links below. [Bibliography of refereed articles] 	 0 Comments
Revolutions blog: 2010 statistics	https://www.r-bloggers.com/2010/12/revolutions-blog-2010-statistics/	December 31, 2010	David Smith	Since it's the end of the year, and since this is a statistics blog, I thought I'd pull some data from the blog server and run some number on the blog itself. Overall, the blog has doubled the average number of daily visitors and pageviews compared to 2009. The number of pageviews varies quite a lot, as you can see from the kernel density estimate of daily values below. The median daily pageview count was just shy of 1400. I couldn't find an easy way to count the number of posts published in 2010, but there's been at least one every weekday (and often more than one).  By the way, some hints on exporting data from Google Analytics for analysis in R R. It's best to export using the TSV (tab-separated format) for ease of import. You'll need to delete or skip the first 9 rows of the export file to remove the comments Google Analytics prepends to the data. And finally, you'll need to read in the pageview data as text and then strip the commas from the numbers (which are exported as “1,234” instead of “1234”) before you convert them to numeric in R. I used the following commands: The remaining stats I pulled directly from Google Analytics. Top 10 Posts of 2010 The top 10 posts, as measured by direct traffic to each page, are listed below: This doesn't include data from posts syndicated at other sites (such as r-bloggers.com), and views to the blog homepage (which is how many people read the blog, rather than post-by-post). Interestingly, the #3 post from 2009, How to choose a random number in R, would have been the #1 post in 2010 -- thanks to its Google Search traffic -- if I hadn't limited the contenders to posts published this year.  Top 5 Browsers of 2010 It's interesting to look at the top Web browsers used to visit this blog, as measured by the number of pages server to each browser: Chrome is the big change here: it represented just 8.4% of traffic in 2009. In the Web as a whole, Internet Explorer represents a nearly 50% share of Web traffic, so clearly the readers of this blog prefer Firefox disproportionately. I use both Chrome and Firefox myself. Top 5 Operating Systems of 2010 Google Analytics also reports the operating system used to visit the blog, and the top five are: This makes Firefox's dominance in the browsers even more remarkable, given IE's built-in advantage on Windows. Regarding the operating systems themselves, the order is unchanged from 2009, and the ratios are about the same too (Windows was 59% in 2009, Mac at 28%). The only change is that the iPad replaced the iPod for the #5 slot. Looking back... Well, that's about it for 2010 - it's been a great year, and a lot of fun writing the blog and getting the word out about all the awesome things the community is doing with R. Thanks to everyone who sent in suggestions for articles, made comments, or just read the blog in 2010. We'll be back again next year with more news about R, statistics and the world of open source. Happy New Year to all of our readers, and we'll see you in 2011! 	 0 Comments
Le Monde puzzle [52]	https://www.r-bloggers.com/2010/12/le-monde-puzzle-52/	December 31, 2010	xi'an	"The last puzzle of the year in Le Monde reads as follows (as far as I understand its wording!): Iter(n,x,y) is the function Find the seven-digit number z such that
Iter(6,1,z)=12, Iter(6,2,z)=19, Iter(6,3,z)=29,
and Iter(6,-1,z)=Iter(6,-2,z)=Iter(6,-3,z)=0. Obviously, the brute-force solution of listing all 90 million seven digit numbers until the six constraints are met is feasible (especially around New Year since the mainframe computer is completely at rest!). However, this sounds like the last resort solution and I thus tried first a simulated annealing approach already tested for the sudoku problem a few years ago… (This puzzle is actually of the same nature as the sudoku problem,  in particular because we do know when we find the solution, except that checking for the six conditions to hold is apparently not so straightforward. For us if not for the computer.) I thus wrote the following R code: where the second argument in Zearch is the scale for the starting temperature… Note that this is not a blind simulated annealing scheme in that we compare all possible moves once a digit has been randomly chosen. As the temperature decreases we are thus more and more likely to pick the most interesting digit (in terms of the criterion). Running the code for 100,000 iterations and a starting scale of 1000 produced a “solution” 9,552,774 that only differed from the targeted value by 1, since Iter(6,2,9552774)=20. And again for 106 iterations with another “solution”, 6,097,917. Obviously, just as for the sudoku problem, this “close solution” has a priori no connection with the exact solution! It is also interesting to compare the simulated annealing solution with a deterministic search, which always gets stuck in local minima: "	 0 Comments
R Journal 2/2	https://www.r-bloggers.com/2010/12/r-journal-22/	December 31, 2010	Paolo Sonego		 0 Comments
Empirical Bayes Estimation of On Base Percentage	https://www.r-bloggers.com/2010/12/empirical-bayes-estimation-of-on-base-percentage/	December 30, 2010	Andrew Landgraf		 0 Comments
R Packages for Social Search	https://www.r-bloggers.com/2010/12/r-packages-for-social-search/	December 30, 2010	David Smith	Jesse Bridgewater works on “social search awesomeness” for the Bing search engine, and is setting up his dev environment with the necessary tools including python, vim, and R. Jesse has shared a handy script he uses to install all the specialty packages he uses for his data analysis. This is a handy script to modify for your own purposes, but it's also interesting to review the packages Jesse lists as part of his standard toolkit: Literate Programming: R2HTML, sweave, Rpad Data Visualization: ggplot2, YaleToolkit Data Mining / Machine Learning / Natural Language Processing: ElemStatLearn, gbm, bayesm, RWeka, lsa, tm  Graphs and Networks: igraph Statistics: survival, Hmisc, ICSNP, zipfR What R packages do you consider to be part of your standard toolkit? Jesse S.A. Bridgewater: My Favorite R Packages (Installed With One Command) 	 0 Comments
Blog year 2010 in review	https://www.r-bloggers.com/2010/12/blog-year-2010-in-review/	December 30, 2010	Pat	The blog year started in August and consists of 30-something posts.  Here is a summary. A performance step beyond “Economists’ Hubris” points out that random portfolios are a more powerful method of performance measurement than the method that is suggested in the “Economists’ Hubris” paper (though that method is probably pretty good). The volatility puzzle solved? suggests that perhaps the reason that low volatility stocks have a higher expected return than high volatility stocks is because hardly anyone pays attention to volatility when selecting stocks. The decision between active and passive investment is explored in Freeloading turnstile jumpers. Elevated stock correlations is a short discussion of fund manager opportunity and its changes through time. Deflation, inflation and blown tires points to an interesting analogy regarding inflation. Anomalies meet volatility discusses a paper by Bernd Scherer about low volatility portfolios.  I wonder if the French/Fama anomalies found to be associated with low volatility portfolios are caused by the inattention to volatility (as in The volatility puzzle solved?). Were stock returns really better in 2007 than 2008? suggests the surprising proposition that stock returns might have been as good in 2008 as in 2007. This same theme is continued in Bear hunting which tries to find periods that were bear markets. Primitive stock markets is an invitation to find better ways of structuring markets. The ARORA guessing game points to a fun game where you decide which of two series is market data.  It also includes a hypothesis of why we can tell the difference. Are momentum strategies antisocial? The title asks the question.  I’m not sure of the answer. The good side of inside trading points to an article that shows  inside trading to be a more ambiguous subject than we would expect. Psychic fund management unfortunately exists. Making science happen is about the Blackawton bee study.  If 8 year-olds can do science, why can’t the fund management industry do more of it? The book reviews were: Table 1 provides a quick view of my recommendations. Table 1: Recommendations of reviewed books. R is a name you need to know declares Forbes magazine.  In this instance I believe them to be correct.  Apparently R appears in the hardcopy magazine published in late December. Some quibbles about “The R Book” by Michael Crawley If you have the book, I suggest that you have a glance at this post. Ideas for World Statistics Day includes some ideas concerning R. Bear hunting shows some R code and also points to R code you can download.  One function in particular may be useful — a function that nicely plots data over years or decades. A tale of two returns has R code for computing returns and for creating a particular plot. The following posts include minor amounts of R code: Posts related to statistics were: Feeding a greedy algorithm explains the idea of greedy algorithms and their use. Clever versus simple risk management discusses why risk modeling is hard. Most read is not at all the same as most important.  Here is my list of the posts that have been given less attention than they deserve: American TV does cointegration is about cointegration.  It is also about the Fringe television series.  I would have thought that Fringe geeks would relish this post which explains an otherwise unintuitive aspect of the show.  Apparently not. I’d like to thank those who have read the blog over the past few months.  Special thanks go to those who (However, spammers are free to resist commenting.) All the best in the coming year. 	 0 Comments
Analysis of Facebook status updates	https://www.r-bloggers.com/2010/12/analysis-of-facebook-status-updates/	December 29, 2010	David Smith	The Facebook Data Team has published an analysis of the status updates of Facebook users, by categorizing words according to the 68 categories of the Linguistic Inquiry and Word Count Dictionary, and tabulating the frequencies of their use. It's fairly interesting to see this kind of analysis applied to Facebook, but unfortunately doesn't reveal much in the way of “A-ha!” insights. For example, this chart of the frequency of six categories of words expressed throughout the day shows pretty much what you'd expect:  (I do wonder whether “Hour of Day” is in local time or all mapped to the same timezone, though: I'd expect the variation to be more pronounced if the analysis were based on local time, and if not I'd have to assume this represents US-based Facebook users only.) Notable for R users is that all of the charts in the Facebook post were clearly created with Hadley Wickham's ggplot2 package. If you haven't tried ggplot2 yet, check out the ggplot2 website for resources for creating beautiful graphics with R. If you're already a ggplot2 user, you might have missed that it was updated just before Christmas with improvements to legends and axes and several bug fixes. Read the update announcement for the full details. Facebook Data Team: What’s on your mind?    	 0 Comments
Temporal Trends in Soil Science Jargon — via Google Ngram Viewer	https://www.r-bloggers.com/2010/12/temporal-trends-in-soil-science-jargon-via-google-ngram-viewer/	December 29, 2010	dylan	"

 read more "	 0 Comments
More typos in Chapter 5	https://www.r-bloggers.com/2010/12/more-typos-in-chapter-5/	December 29, 2010	xi'an	Following Ashley’s latest comments on Chapter 5 of Introducing Monte Carlo Methods with R, I realised Example 5.5 was totally off-the-mark! Not only the representation of the likelihood should have used prod instead of mean, not only the constant should call the val argument of integrate, not only integrate  uses lower and upper rather than from and to, but also the approximation is missing a scale factor of 10, squared root of the sample size… The corrected R code is thus and the corrected Figure 5.5 is therefore as follows. Note that the fit by the t distribution is not as perfect as before. A normal approximation would do better. This mistake is most embarrassing and I cannot fathom how I came with this unoperating program! (The more embarrassing as Cauchy‘s house is about 1k away from mine…) I am thus quite grateful to Ashley for her detailed study of this example. 	 0 Comments
Book Review: A Beginner’s Guide to R	https://www.r-bloggers.com/2010/12/book-review-a-beginner%e2%80%99s-guide-to-r/	December 29, 2010	Luke Miller		 0 Comments
Book Review: R in a Nutshell	https://www.r-bloggers.com/2010/12/book-review-r-in-a-nutshell/	December 29, 2010	Luke Miller		 0 Comments
Converting a String to a Variable Name On-The-Fly and Vice-versa in R	https://www.r-bloggers.com/2010/12/converting-a-string-to-a-variable-name-on-the-fly-and-vice-versa-in-r/	December 28, 2010	ramhiser	"
Recently, I had a professor ask me how to take a string and convert it to an R variable name on-the-fly.  One possible way is:
 
Now, suppose we want to go the other way.  The trick is just as simple:
 "	 0 Comments
Generating stress scenarios: null correlation is not enough	https://www.r-bloggers.com/2010/12/generating-stress-scenarios-null-correlation-is-not-enough/	December 28, 2010	arthur charpentier	In a recent post (here, by @teramonagi), Teramonagi mentioned the use of PCA to model yield curve, i.e. to obtain the three factor, “parallel shift“, “twist” and “butterfly“. As in Nelson & Siegel, if m is maturity,  is the yield of the curve at maturity m, assume that  Consider the following sample  Hence, with PCA, we have two components, orthogonal, with a triangular distribution, so if we generate them independently, we obtain  which is quite different, compared with the original sample. On the other hand, with ICA,we obtain factors that are really independent....  	 0 Comments
nlm [unused argument(s) (iter = 1)]	https://www.r-bloggers.com/2010/12/nlm-unused-arguments-iter-1/	December 28, 2010	xi'an	Ashley put the following comment on Chapter 5 of Introducing Monte Carlo Methods with R”: I am reading chapter 5. I try to reproduced the result on page 128. The R  codes don’t work on my laptop. When I try to run the following codes on  page 128 I always get the error message It seems that the nlm function doesn’t accept the argument iter. I don’t  know how to deal with it. I am in US. I guess the nlm version available  to US R users is different from the version in EU. Please help. And indeed with the most recent versions of R, like 2.12.1 on my own machine, calling nlm with the abbreviated argument iter instead of iterlim produces the above error message. This means the full syntax iterlim=i should now be used. In addition, the function nlm produces the minimum of the first argument f and like should thus be defined as to end up with local maxima as on Figure 5.2. (Note: I do not think there are US versus EU versions of R…) 	 0 Comments
Travel grants and prizes for R/Finance 2011	https://www.r-bloggers.com/2010/12/travel-grants-and-prizes-for-rfinance-2011/	December 28, 2010	David Smith	If you've been thinking about heading to Chicago in April for the R/Finance conference, here's another reason to go: posting for the committee, Dirk Eddelbuettel announced last week that thanks to a favourable response from sponsors[*], the conference organizers can now offer: Full details are in the updated call-for-papers. Note that the submission deadline is February 15, 2011. [*] Revolution Analytics is a proud sponsor of R/Finance 2011. R/Finance 2011: Call For Papers 	 0 Comments
Automatic Simulation Queueing in R	https://www.r-bloggers.com/2010/12/automatic-simulation-queueing-in-r/	December 28, 2010	ramhiser	"
I spend much of my time writing R code for simulations to compare the supervised classification methods that I have developed with similar classifiers from the literature.  A large challenge is to determine which datasets (whether artificial/simulated or real) are interesting comparisons.  Even if we restricted ourselves to multivariate Gaussian data, there are a large number of covariance matrix configurations that we could use to simulate the data.  In other words, there are too many possibilities to consider all of them.  However, it is often desirable to consider as many as possible.
 
Parallel processing certainly has reduced the runtime for simulations.  In fact, most of my simulations are ridiculously parallelizeable, so I can run multiple simulations side-by-side.
 
I have been searching for ways to automate a lot of what I do, so I can spend less time on the mundane portions of simulation and focus on classification improvement.  As a first attempt, I have written some R code that generates a Bash script that can be queued on my university’s high-performance computer.  The code to create a Bash script is create.shell.file(), which is given here:
 
To actually queue the simulation, we make a call to queue.sim():
 
Let’s look at an example to see what is actually happening.  Suppose that we have a simulation file called “gaussian-sim.r” that generates N observations from two different p-dimensional Gaussian distributions each having the identity covariance matrix. Of course, this is a boring example, but it’s a start.  One interesting question that always arises is: “Does classification performance degrade for small values of N and (extremely) large values of p?”  We may wish to answer this question with a simulation study by looking at many values of N and many values of p and see if we can find a cutoff where classification performance declines. Let’s further suppose that for each configuration that we will repeat the experiment B times. (As a note, I’m not going to actually examine the gaussian-sim.r file or its contents here. I may return to this example later and extend it, but for now I’m going to focus on the automated queueing.)  We can queue the simulation for each of several configurations the following code:
 
This will create a Bash script with a descriptive name. For example, with the above code, a file called “gaussian-sim-N10-p1000-B1000.sh” is created.  Here are its contents:
 
A note about the shell file created.  The actual call to R can be customized, but this call has worked well for me. I certainly could call R in batch mode, but I never do without any specific reason. Perhaps one is more efficient than the other? I’m not sure about this. Next, for each *.sh file created, the following command is executed to queue the R script using the above configuration.
 
The scasub command is used for my university’s HPC.  I know that there are other systems out there, but you can always alter my code to suit your needs.  Of course, your R script needs to take advantage of the commandArgs() function in R to use the above code. "	 0 Comments
Tools to tidy up R code	https://www.r-bloggers.com/2010/12/tools-to-tidy-up-r-code/	December 28, 2010	Nick Horton		 0 Comments
High readings of VIX index during 2 days	https://www.r-bloggers.com/2010/12/high-readings-of-vix-index-during-2-days/	December 28, 2010	Dzidorius Martinaitis	"During last two sessions (December 23th and 27th), VIX index posted returns (close to close) above 6 %. My question is – what return can we expect next day after such event? As you can see from the graph above, expected return is positive. During 1995-2010 were 53 such events and mean return was 1.02 % and median 0.6% and win rate 65%.
What can be the explanation for such consistency in returns? It is known, that volatility is mean reverting process and the value of VIX index tends to return to its mean.  Worth to note, that despite VIX index spike, S&P 500 index was very very still during the last days. "	 0 Comments
Phylogenetic meta-analysis in R using Phylometa	https://www.r-bloggers.com/2010/12/phylogenetic-meta-analysis-in-r-using-phylometa/	December 28, 2010	Scott Chamberlain		 0 Comments
Poster at MCMSki III	https://www.r-bloggers.com/2010/12/poster-at-mcmski-iii/	December 28, 2010	xi'an	Here is the poster presented at MCMSki III next week by Pierre Jacob about our joint paper on parallelisation:  	 0 Comments
A new blog about using R for ecology and evolution	https://www.r-bloggers.com/2010/12/a-new-blog-about-using-r-for-ecology-and-evolution/	December 27, 2010	Scott Chamberlain		 0 Comments
The tightrope of the random walk	https://www.r-bloggers.com/2010/12/the-tightrope-of-the-random-walk/	December 27, 2010	Pat	"We’re really interested in markets, but we’ll start with a series of coin tosses.  If the coin lands heads, then we go up one; if it lands tails, we go down one. Figure 1: A coin toss path.Figure 1 is the result of one thousand coin flips.  It is a random walk. The R command that created Figure 1 was: > plot(cumsum(sign(rnorm(1000))), type=""l"") You can do it multiple times and see the variety of paths that is produced. Random walks are on a tightrope between mean reversion and momentum. Suppose there were mean reversion in our coin toss.  Then it would have a greater chance of tails if the process were above zero, and a greater chance of heads if the process were below zero. With mean reversion it is as if there is gravity pulling the process back to zero — either down from high or up from low. Momentum is the opposite of mean reversion.  With momentum the probability of a head is greater when there have already been more heads. With momentum it is as if the coin likes doing what it has already done — the process is pushed away from zero. We are really bad at distinguishing these three cases. When people create a series of heads and tails “by hand”, they tend to create a mean-reverting series.  We think long runs of heads or tails should be more rare than they are.  When we create a series, we remember what has happened before.  A random walk completely forgets the past. The first 300 or so flips of Figure 1 are heavily dominated by heads.  I find it hard to think of that as a random walk rather than an exhibition of momentum.  Except I know how it was created and I have quite a lot of faith in the random generation in R.  (That was the first path created by the way — there was no selection bias (actually, there might be — I might have redone it if I didn’t find the path “interesting”).) Humans try to find meaning everywhere.  Even when there is none. The Efficient Market Hypothesis implies that markets are always on the tightrope of the random walk.  The Efficient Market Hypothesis is wrong — it’s just a model and all models are wrong.  In fact it is self-contradictory: it says that no one can make extra money because someone has already made extra money. But it is a pretty good model for a lot of purposes.  (See Perception switching.)  My suspicion is that the market’s tightrope act is one of leaning one way or the other (mean reversion or momentum) and righting itself before it falls completely off.  It’s not going to be easy for us to know which state it is in.  If we didn’t know the generating mechanism of Figure 1, would random walk be our best guess for the first part of the series? A related blog post is: Are momentum strategies antisocial? Photo by _gee_ via everystockphoto.com "	 0 Comments
R/Finance 2011 Call for Papers	https://www.r-bloggers.com/2010/12/rfinance-2011-call-for-papers-2/	December 26, 2010	Joshua Ulrich		 0 Comments
Graphics *and* Statistics: The Facebook Map	https://www.r-bloggers.com/2010/12/graphics-and-statistics-the-facebook-map/	December 26, 2010	martin	There is this beautiful graph created by the facebook intern Paul Butler showing all (?) connections between facebook accounts:  Paul’s article is called “Visualizing Friendships“, which I would more call “Visualizing connections between facebook accounts”, but that is probably a different matter. Although this is a beautiful piece of artwork, from a statistical point of view it is not really giving us a great deal of insights. Sure, there are certain “white spots” on the map, where either there is a competitor of facebook more successful or people don’t want to, or can not use this kind of “social” contacts. Obvious examples are Russia or China. But this is info more on a meta level, i.e., not really part of the info shown. What would be more interesting are things like a comparison between the expected link intensity based on either population, broadband connections or actual facebook accounts and the data Paul compiled. Looking at Germany, e.g., we see the former eastern part being less connected, which is based on both, smaller population density as well as a poorer development of broadband connections. A visualization of these connection intensities should be hierarchic, starting with continents with the ability to drill down into countries, states and cities. That would certainly mean some development and could not be done in R (yes, this map was created in R!) so easily – maybe a case for iplots. 	 0 Comments
Age and happiness:  The pattern isn’t as clear as you might think	https://www.r-bloggers.com/2010/12/age-and-happiness-the-pattern-isnt-as-clear-as-you-might-think/	December 26, 2010	Andrew Gelman		 0 Comments
Autocorrelation Matrix in R	https://www.r-bloggers.com/2010/12/autocorrelation-matrix-in-r/	December 25, 2010	ramhiser	I have been simulating a lot of data lately  with various covariance (correlation) structures, and one that I have been using is the autocorrelation (or autoregressive) structure, where there is a “lag” between variables. The matrix is a v-dimension matrix of the form $$\begin{bmatrix} 1 & \rho & \rho^2 & \dots & \rho^{v-1}\\ \rho & 1& \ddots & \dots & \rho^{v-2}\\ \vdots & \ddots & \ddots & \ddots & \vdots\\ \rho^{v-2} & \dots & \ddots & \ddots & \rho\\ \rho^{v-1} & \rho^{v-2} & \dots & \rho & 1 \end{bmatrix}$$, where \(\rho \in [-1, 1]\) is the lag.  Notice that the lag decays to 0 as v increases. My goal was to make the construction of such a matrix simple and easy in R.  The method that I used explored a function I have not used yet in R called “lower.tri” for the lower triangular part of the matrix.  The upper triangular part is referenced with “upper.tri.” My code is as follows: I really liked it because I feel that it is simple, but then I found Professor Peter Dalgaard’s method, which I have slightly modified.  It is far better than mine, easy to understand, and slick. Oh so slick. Here it is: Professor Dalgaard’s method puts mine to shame. It is quite obvious how to do it once it is seen, but I certainly wasn’t thinking along those lines. 	 0 Comments
Has the seed that gets software development out of the stone-age been sown?	https://www.r-bloggers.com/2010/12/has-the-seed-that-gets-software-development-out-of-the-stone-age-been-sown/	December 25, 2010	Derek-Jones	A big puzzle for archaeologists is why stone age culture lasted as long as it did (from approximately 2.5 millions years ago until the start of the copper age around 6.3 thousand years ago).  Given the range of innovation rates seen in various cultures through-out human history a much shorter stone age is to be expected.  A recent paper proposes that low population density is what maintained the stone age status quo; there was not enough contact between different hunter gather groups for widespread take up of innovations.  Life was tough and the viable lifetime of individual groups of people may not have been long enough for them to be likely to pass on innovations (either their own on ones encountered through contact with other groups). Software development is often done by small groups that don’t communicate with other groups and regularly die out (well there is a high turn-over, with many of the more experienced people moving on to non-software roles).  There are sufficient parallels between hunter gathers and software developers to suggest both were/are kept in a stone age for the same reason, lack of a method that enables people to obtain information about innovations and how worthwhile these might be within a given environment. A huge barrier to the development of better software development practices is the almost complete lack of significant quantities of reliable empirical data that can be used to judge whether a claimed innovation is really worthwhile.  Companies rarely make their detailed fault databases and product development history public; who wants to risk negative publicity and law suits just so academics have some data to work with. At the start of this decade public source code repositories like SourceForge and public software fault repositories like Bugzilla started to spring up.  These repositories contain a huge amount of information about the characteristics of the software development process.  Questions that can be asked of this data include: what are common patterns of development and which ones result in fewer faults, how does software evolve and how well do the techniques used to manage it work. Empirical software engineering researchers are now setting up repositories, like Promise, containing the raw data from their analysis of Open Source (and some closed source) projects.  By making this raw data available they are reducing the effort needed by other researchers to investigate their own alternative ideas (I have just started a book on empirical software engineering using the R statistical language that uses examples based on this raw data). One of the side effects of Open Source development could be the creation of software development practices that have been shown to be better (including showing that some existing practices make things worse).  The source of these practices not being what the software developers themselves do or how they do it, but the footsteps they have left behind in the sand.   	 0 Comments
Rcpp 0.9.0 announcement	https://www.r-bloggers.com/2010/12/rcpp-0-9-0-announcement-2/	December 25, 2010	Thinking inside the box	"
The text below went out as a post to the
r-packages list a few days ago, but I thought it would make sense to post it
on the blog too. So with a little html markup…
 
 
 
 
 
 
 
The RcppGSL
package permits easy use of the GNU Scientific Library (GSL), a
collection of numerical routines for scientifc computing. It is particularly
useful for C and C++ programs as it provides a standard C interface to a wide
range of mathematical routines such as special functions, permutations,
combinations, fast fourier transforms, eigensystems, random numbers,
quadrature, random distributions, quasi-random sequences, Monte Carlo
integration, N-tuples, differential equations, simulated annealing, numerical
differentiation, interpolation, series acceleration, Chebyshev
approximations, root-finding, discrete Hankel transforms physical constants,
basis splines and wavelets.  There are over 1000 functions in total with an
extensive test suite.  The RcppGSL package provides an easy-to-use interface
between GSL data structures and R using concepts from Rcpp. The RcppGSL
package also contains a vignette with more documentation.


 
 
 
 
 
 "	 0 Comments
one-dimensional integrals	https://www.r-bloggers.com/2010/12/one-dimensional-integrals/	December 25, 2010	R on Guangchuang Yu	"The foundamental idea of numerical integration is to estimate the area of the region in the xy-plane bounded by the graph of function f(x). The integral was esimated by divide x to small intervals, then add all the small approximations to give a total approximation. Numerical integration can be done by trapezoidal rule, simpson’s rule and quadrature rules. R has a built-in function integrate, which performs adaptive quadrature. Trapezoidal rule works by approximating the region under the graph f(x) as a trapezoid and calculating its area. Simpson’s rule subdivides the interval [a,b] into n subintervals, where n is even, then on each consecutive pairs of subintervals, it approximates the behaviour of f(x) by a parabola (polynomial of degree 2) rather than by the straight lines used in the trapezoidal rule.  To calculate an integrate over infinite interval, one way is to transform it into an integral over a finite interval as introduce in wiki. Simpson’s rule is more accuracy than trapezoidal rule. To compare the accuracy between simpson’s rule and trapezoidal rule, I estimated  = -log(0.01) for a sequence of increasing values of n. 
The plot showed that log(error) against log(n) appears to have a slope of -1.90 and -3.28 for trapezoidal rule and simpson’s rule respectively. Another way to compare their accuracy is to calculate how large of the partition size n for reaching a specific tolerance. As show above, simpson’s rule converge much faster than trapezoidal rule. Quadrature rule is more efficient than traditional algorithms. In adaptive quadrature, the subinterval width h is not constant over the interval [a,b], but instead adapts to the function.
Here, I presented the adaptive simpson’s method. Quadrature rule is effective when f(x) is steep. Reference:
1. Robinson A., O. Jones, and R. Maillardet. 2009. Introduction to Scientific Programming and Simulation Using R. Chapman and Hall.
2. http://en.wikipedia.org/wiki/Numerical_integration
3. http://en.wikipedia.org/wiki/Trapezoidal_rule
4. http://en.wikipedia.org/wiki/Simpson%27s_rule
5. http://en.wikipedia.org/wiki/Adaptive_quadrature "	 0 Comments
Chromosome bias in R, my notebook	https://www.r-bloggers.com/2010/12/chromosome-bias-in-r-my-notebook/	December 23, 2010	Jeremy Leipzig		 0 Comments
Did you feel that?	https://www.r-bloggers.com/2010/12/did-you-feel-that/	December 23, 2010	David Smith	There was a small earthquake in northern England on Tuesday. Barry Rowlingson felt the quake (it rattled the photographs on his wall), but didn't know how big of a quake it was because he didn't know how close he was to the epicentre. The British Geological Survey hadn't yet announced the quake, but did give access to seismograph readings, which indeed featured spikes marking the earthquake. So Barry read the times of the spikes from the charts, did a little triangulation based on the speed of wave propagation, crunched the numbers in R, and came up with this location for the epicentre (in blue):  The actual epicentre, as calculated by the USGS, is marked in red. Professional seismologists also use R to measure and locate earthquakes, but this is pretty good for a self-proclaimed quick-and-dirty calculation! Barry Rowlingson's GeoSpatial Blog: Mapping The Kendal Mint Quake 	 0 Comments
Citizen Data Journalism: Mexico Homicides	https://www.r-bloggers.com/2010/12/citizen-data-journalism-mexico-homicides/	December 23, 2010	David Smith	I've recently praised some mainstream media outlets like the New York Times and New Scientist for leading the charge on data journalism. But you don't need to be a large organization to find news in data. With open data sources, and open-source data analysis tools, individuals can make newsworthy discoveries. Diego Valle-Jones has been investigating the impact of the Drug War in Mexico for a couple of years now, by using R to analyze the homicide statistics reported by the local municipalities. But Diego has noticed some anomalies in the data: many murders are not reported as homicides at all, but instead as accidental deaths. For example, data including the Acteal Massacre of 1997 (where 45 Tzotzil Indigenous people were killed by paramilitaries) shows a spike in accidents, not homicides:  (Diego provides the R code that generated this plot.) A data error, or falsification? Further investigation is needed to decide, but without the availability of open data for data journalism, and the possibility of citizen data journalism such as that by Diego, we might never know. Diego Valle-Jones: Some problems with the Mexican mortality database 	 0 Comments
R function to convert degrees to radians	https://www.r-bloggers.com/2010/12/r-function-to-convert-degrees-to-radians/	December 23, 2010	fabiomarroni	I would have never imagined that I would have to go back to high school concepts and do strange trigonometric calculations. However, it happened to me that I needed to convert GPS coordinates of a large data set to radians. It’s a trivial task, if you know how to do it. The function takes as input two numbers: degrees and minutes (no seconds for the moment, sorry!), converts it in decimal form of degrees and then return a numeric value, which represents your coordinate in radiants. As, usual, comments welcome. 	 0 Comments
Project Euler — Problem 187	https://www.r-bloggers.com/2010/12/project-euler-%e2%80%94-problem-187/	December 23, 2010	R on Guangchuang Yu	"http://projecteuler.net/index.php?section=problems&id=187 A composite is a number containing at least two prime factors. For example, 15 = 3 × 5; 9 = 3 × 3; 12 = 2 × 2 × 3. There are ten composites below thirty containing precisely two, not necessarily distinct, prime factors: 4, 6, 9, 10, 14, 15, 21, 22, 25, 26. How many composite integers, n < 10^(8), have precisely two, not necessarily distinct, prime factors? —-
[1] 17427258
   user  system elapsed
 449.67   72.40  522.87  Not fast enough… "	 0 Comments
Some problems with the Mexican mortality database	https://www.r-bloggers.com/2010/12/some-problems-with-the-mexican-mortality-database/	December 22, 2010	Diego Valle-Jones		 0 Comments
A plea for consistent style!	https://www.r-bloggers.com/2010/12/a-plea-for-consistent-style/	December 22, 2010	Nick Horton		 0 Comments
Forbes: R is a name you need to know in 2011	https://www.r-bloggers.com/2010/12/forbes-r-is-a-name-you-need-to-know-in-2011/	December 22, 2010	David Smith	The December 20 issue of Forbes magazine, on newsstands now, includes a column about R on page 128 as part of the “Name You Need to Know in 2011” feature. It's basically an excerpt from this blog post by Steve McNally and its comments, and includes quotes from Norman Nie of Revolution Analytics, Bill Alpert of Barron's, and Brandon Witcher. There's no online version of the column, but it's worth checking out in the printed magazine. 	 0 Comments
A Special Graphics Device in R: the Null Device	https://www.r-bloggers.com/2010/12/a-special-graphics-device-in-r-the-null-device/	December 22, 2010	Yihui Xie	It is well-known that R has several graphics devices — either the screen devices (X11(), windows(), …) or the off-screen devices (pdf(), png(), …). We can query the default graphics device in options(): In a non-interactive session, the default device is pdf(). This is why Sweave has to create a file named Rplots.pdf no matter if you want it or not when you run Sweave on an Rnw file which has code chunks creating plots. Such a behaviour is annoying to me — the PDF file is not only unnecessary, but also time-consuming (creating this PDF file is completely a waste of time). Is there a way to set a “null” device? (like the /dev/null for *nix users) The answer is yes, but not so obvious. I have not found the device below documented anywhere: This device can speed up Sweave a lot when there are many plots to draw. Here is a comparison: One thing I don’t understand in Sweave is that it evaluates the code chunk twice if its Sweave options contain fig=TRUE. I think this might be a waste of time as well, and this is why I like pgfSweave, which has both the mechanism of caching R objects (using cacheSweave) and a smart way to cache graphics (using pgf). 	 0 Comments
Oil – Natural Gas Cointegration – turning point?	https://www.r-bloggers.com/2010/12/oil-natural-gas-cointegration-turning-point/	December 22, 2010	Lloyd Spencer		 0 Comments
Data-driven arterial input functions	https://www.r-bloggers.com/2010/12/data-driven-arterial-input-functions/	December 22, 2010	Brandon Whitcher		 0 Comments
Stacked histogram with ggplot2	https://www.r-bloggers.com/2010/12/stacked-histogram-with-ggplot2/	December 22, 2010	kariert	With ggplot2 there is a possibility to create divide bars of a histogram into different categories: 	 0 Comments
RcppExamples 0.1.2	https://www.r-bloggers.com/2010/12/rcppexamples-0-1-2-2/	December 22, 2010	Thinking inside the box	"
RcppExamples 
contains a few illustrations of how to use 
Rcpp. It grew out
of documentation for the classic API (now in its own package RcppClassic) and
we added more functions documenting how to do the same with the new API we
have been focusing on for the last year or so.  One of the things I added in
the last few days was the example below showing how to use
Rcpp::List with lookups to replace use of the old and deprecated
RcppParams. It also show how to return values to
R rather easily

 

#include <Rcpp.h>

RcppExport SEXP newRcppParamsExample(SEXP params) {

    try {                                       // or use BEGIN_RCPP macro

        Rcpp::List rparam(params);              // Get parameters in params.
        std::string method   = Rcpp::as<std::string>(rparam[""method""]);
        double tolerance     = Rcpp::as<double>(rparam[""tolerance""]);
        int    maxIter       = Rcpp::as<int>(rparam[""maxIter""]);
        Rcpp::Date startDate = Rcpp::Date(Rcpp::as<int>(rparam[""startDate""])); // ctor from int
        
        Rprintf(""\nIn C++, seeing the following value\n"");
        Rprintf(""Method argument    : %s\n"", method.c_str());
        Rprintf(""Tolerance argument : %f\n"", tolerance);
        Rprintf(""MaxIter argument   : %d\n"", maxIter);
        Rprintf(""Start date argument: %04d-%02d-%02d\n"", 
                startDate.getYear(), startDate.getMonth(), startDate.getDay());

        return Rcpp::List::create(Rcpp::Named(""method"", method),
                                  Rcpp::Named(""tolerance"", tolerance),
                                  Rcpp::Named(""maxIter"", maxIter),
                                  Rcpp::Named(""startDate"", startDate),
                                  Rcpp::Named(""params"", params));  // or use rparam

    } catch( std::exception &ex ) {             // or use END_RCPP macro
        forward_exception_to_r( ex );
    } catch(...) { 
        ::Rf_error( ""c++ exception (unknown reason)"" ); 
    }
    return R_NilValue; // -Wall
}


 
The package is work-in-progress and needs way more general usage
examples for
Rcpp
and particularly the new API. But it’s a start.

 
A few more details on the page are on the
RcppExamples page.


 "	 0 Comments
CrossValidated Journal Club	https://www.r-bloggers.com/2010/12/crossvalidated-journal-club/	December 21, 2010	Rob J Hyndman	Journal Clubs are a great way to learn new research ideas and to keep up with the literature. The idea is that a group of people get together every week or so to discuss a paper of joint interest. This can happen within your own research group or department, or virtually online. There is now a virtual journal club operating in conjunction with CrossValidated.com. The first paper discussed was on text data mining. It appears that the next paper may be on collaborative filtering. The emphasis is on Open Access papers, preferably with associated software that is freely available. Some of the discussion tends to centre on how to implement the ideas in R. For those of us in Australia, the timing is tricky. The first discussion took place at 3am local time! If you can’t make the CrossValidated Journal Club chats, why not start your own local club? 	 0 Comments
Questions on the parallel Rao-Blackwellisation	https://www.r-bloggers.com/2010/12/questions-on-the-parallel-rao-blackwellisation/	December 21, 2010	xi'an	Pierre Jacob and I got this email from a student about our parallel Rao-Blackwellisation paper. Here are some parts of the questions and our answer: Although I understand how the strategy proposed in the paper helps in variance reduction, I do not understand why you set b=1 (mentioned in Section 3.2) and why it plays no role in the comparison. If b=1 and p=4 for example, every chains (out of the 4 chains of the block) has a length of 4 samples. And there are no additional blocks. How is this length enough for the sampler to converge and sample the distribution adequately? You mention that you do 10000 independent runs (Which means you do the above procedure 10000 times independently) but aren’t all these runs equally short and inadequate? Even for p=100, aren’t the chains too short? Indeed setting b = 1 and p to be “small” leads to very short chains, so the approximations based on these short chains would clearly be very poor! However we were interested in the comparison between the variances computed using standard IMH and block IMH. Our point there is that, to compare the methods, no need to use b > 1. As a matter of fact, Figure 6 shows for various b (1, 10, 100, 1000) that the variance reduction of  compared to  is pretty much the same (always around 30% in this case). This simply allowed us to produce the results very quickly. Using 10,000 independent replicates allows to get a good precision on the variance. In every practical situation where we would like to use the chains to get some actual estimate, we would use a much bigger b (typically, so that b * p = T would be more than 100,000). And we wouldn’t necessarily compute 10,000 independent replicates either, depending on what kind of results we want to produce. I would like to ask which applications-models that use MCMC as a simulation tool are in greater need of acceleration and what are the characteristics of these problems that make them difficult (e.g. strongly correlated components, multimodality). Also, which sampler is preferred in each case? I have found some work from Lee, Yau, Giles, Doucet and Holmes, in which they accelerate Population MCMC and SMC and use it in Mixture Models inference as a representative example of multimodal distributions (difficulty in exploring all modes). Could you suggest any other cases where you think hardware acceleration of MCMC would have an impact and some representative problems I could work on? As in the previous answer, you have to realise that our paper is a formal post-processing of MCMC output, not a new way of running MCMC, nor even a way to speed it up!! We are simply reconsidering the way the MCMC output (the chain) is used for approximation purposes. And showing that reprogramming through the abilities offered by parallel processors or even GPU’s brings a free improvement or even a decrease in computing time when the acceptance probability is the most expensive block in the computation. Thus, the paper does not advise about choices of kernels, strategies for better convergence or even for convergence checking. Once again this is about post-processing… As to which application would better benefit from acceleration, I do not see any that would not. Large scale problems and complex likelihoods (eg in population genetics, graphical models, cosmology) cry out for faster implementation. In the last paragraph of your text you also note that independent chains could be used to initialize an MCMC algorithm from several well dispersed starting points. First, is this approach actually used in real applications to reduce variance by collecting more samples or is it only used to detect convergence (Gelman-Rubin)? Does inference usually come from one long chain? Second, do we know that all of the chains are mixing with the same rate if they start them from different positions? For example, could one chain sample properly and fast from the whole space and another one slowly or get stuck? Would this mean that if we take all samples from all chains into account they would not be representative of the actual distribution (because some chains sample the space slowly or even partially)? Will this change if we start all chains from the same position? Parallel multiple chains versus single Markov chains is a debate that has somehow died out in the field. Mathematically, it is very hard to assess parallel chains (except as a whole) because of the dependence on the starting measure and all that. In practice, there is no loss in running parallel chains if you have the parallel abilities and averaging across chains is harmless in the huge majority of cases. Another instance of a free lunch in my opinion. Starting MCMC algorithms from various starting points is common practice. Both to assess convergence and to get estimates of the variance of the estimator of interest. However if all the independent chains converge, then after the burn-in time, you consider your Markov chains to be in their stationary / invariant distribution, and therefore 10 times 10,000 iterations is equivalent to 100,000 iterations. It’s obviously faster to run 10 times 10,000 iterations with parallel processors and stuff. However if it takes 50,000 iterations to converge, ie the burn-in time is around 50,000, then it’s clearly better to run one chain for 100,000 iterations!! There cannot be any absolute rule or number in this game, it all depends on the problem at hand. If your chains don’t seem to mix, then you have identified an issue. It can be because of multimodality in the target distribution. Then you cannot use the chains to get some estimates since you cannot consider your values to be drawn from the target distribution. So, to answer your question precisely, if you start various chains, with the same transition kernel, from various starting points, you cannot have one chain mixing well on the whole space and another one mixing poorly and getting stuck; it goes against their Markov property. However observing that the chains get stuck is pretty common with multimodal targets. Imagine one chain that looks like it mixes well around one mode, then you run another chain and it looks like it mixes well around another mode. Then you do not know how many modes there are, there could be more modes that you didn’t find yet; so using both chains is better than using only one, but it’s still a very poor idea. Starting various chains from the same starting point does not seem like a useful idea to me, I cannot see any advantages of doing that over starting from various points or running a longer chain. Overall, there is not much theoretical difference between starting chains from a single point or from a single measure. Ergodicity tells us the chains should eventually free themselves from the starting item. At which speed is a problem-dependent issue.   	 0 Comments
How Orbitz uses Hadoop and R to optimize hotel search	https://www.r-bloggers.com/2010/12/how-orbitz-uses-hadoop-and-r-to-optimize-hotel-search/	December 21, 2010	David Smith	Positional bias — the tendency for users to preferentially select results in the first few positions of a search — is a big issue for all kinds of search engines. But for online travel site Orbitz the stakes are higher than for a traditional Web search engine: if a customer chooses the first-listed hotel in a search for accommodations, but will be dissatisfied with their stay, that means Orbitz will soon have an unhappy customer. So for Orbitz, a key problem was to optimize their hotel search results for customer satisfaction. As Orbitz's Jonathan Seidman (Lead Engineer on the Intelligent Marketplace/Machine Learning Team) and Ramesh Venkataramaiah (Principal Engineer on the Operations and Engineering Team) revealed in presentations to the WindyCityDB and Hadoop World NYC conferences, Orbitz solves this problem by using R to perform statistical analysis on data stored in Hadoop and extracted with Hive.   After extracting data including customer hotel booking records and user ratings of hotels from Hive, the Orbitz team used statistical analysis to identify the best hotel to promote to the top of the list for each new booking. Ramesh reports that the statistical techniques included liner filtering of time series (via the filter function) and applied moving averages with equal weights. These models even allowed for seasonal trends to be incorporated into the recommendations — for example, the fact that longer hotel stays tend to be booked in the summer months, as shown by the red days in this calendar heat map:  This is another great example of applying advanced statistical and visualization techniques in R to large and complex data sets stored in a Hadoop environment. See the full slide deck for other analyses employed by the Orbitz team, including hexagonal binning charts to identify positional bias and kernel density estimation to model hotel ratings. As Ramesh says in the presentation, R has a “steep learning curve, but worth it!”.   Slideshare.net: Using Hadoop and Hive to Optimize Travel Search 	 0 Comments
My favorite R packages  (installed with one command)	https://www.r-bloggers.com/2010/12/my-favorite-r-packages-installed-with-one-command/	December 21, 2010	bridgewater		 0 Comments
NppToR 2.5.2 Improves startup	https://www.r-bloggers.com/2010/12/npptor-2-5-2-improves-startup/	December 21, 2010	R Blog	I’ve been getting lots of feedback that there are problems starting NppToR with some of the latest version.  I took to the task of looking at that yesterday on the train home.  I have made improvements to the way NppToR finds the RHome directory, not relying entirely on the windows registry.  I also removed the error that pops up when R cannot be found.  That should not have happened since NppToR only needs to know the Rhome directory to spawn new R processes, but is not critical to the passing commands.  A warning is now issued instead of the error that terminates NppToR. The new files can be downloaded from sourceforge http://npptor.sf.net 	 0 Comments
R programming books	https://www.r-bloggers.com/2010/12/r-programming-books/	December 21, 2010	csgillespie	  My sabbatical is rapidly coming to an end, and I have to start thinking more and more about teaching. Glancing over my module description for the introductory computational statistics course I teach, I noticed that it’s a bit light on recommend/background reading. In fact it has only two books: What other good R books could I recommend? In particular, I’m looking for books that: Suggestions  welcome (needed!) 	 0 Comments
A Very Data Christmas	https://www.r-bloggers.com/2010/12/a-very-data-christmas/	December 21, 2010	Drew Conway	"This week Google announced its Ngram Viewer, which allows you to explore the use of words in thousands of texts overtime, going back two hundred years.  Given the relatively long time period covered by this massive data set, it is fun to explore how language has changed overtime. Some texts, however, seem to transcend time.  One great example of such texts are Christmas carols, many of which have origins dating back many hundreds of years.  As a holiday gift to ZIA readers I thought it would be fun to explore the lyrics of Christmas carols, and see how the word usage in these songs compares with today’s lexicon.  To do so I needed two things: first, Christmas carol texts; and second, a way to compare the usage of words in those songs to that of today. A simple Google search for Christmas carol lyrics yielded this site, which I downloaded into a single text file.  Then, I used the R tm package to create a clean word corpus from this text, stripping out English stopwords, punctuation and case.  This left me with 755 words to explore. 
  The above figure shows the frequency of the most popular words from the 25 carols in the corpus.  In this case, “most popular” is defined as a frequency of 10 or more, which amounts to 42 words.   Perhaps unsurprisingly, words related to the birth of Jesus Christ are by far the most popular, and then those with a general celestial theme the second most.  While this is interesting, it is only have of the exploration.  To put these lyrics in context I used the Infochimps API to get the usage statistics for all of these words on Twitter.  For consistency, I have extracted the top 42 most frequently used Carol words on Twitter from the full data set.  In keeping with the spirit of Christmas, “love” is the most popular word on Twitter.  The others seem to make sense, as they include several temporal words, but I must admit I did get a chuckle seeing the popularity of “ass” given this context.  Finally, to compare the usage in the carols to peoples’ tweets I generated a simple scatter plot.  On the x-axis is the Twitter usage in log-scale, the y-axis the frequency of words in carols, and each word is colored from red to green based on the normalized number of unique users that have used the term—with green corresponding to more users.  To reduce clutter I restricted this plot to only words that appeared in carols five or more times. What’s most interesting (to me) is that there is roughly a linear relationship for some words, starting with “excelsis” in the lower-left, and moving up through “christ,” “born,” “king,” and “night.”  There are, however, many more words that do not fit this pattern, and in fact, the most popular words from the sample on Twitter are relatively infrequent in the carols, such as “love” and “day.” The code and data for all of this are available on my github, and I hope you can take some time out from singing to your neighbors to play around with it. Happy holidays to all, and a very merry (data) Christmas.  See you in 2011! "	 0 Comments
oro.nifti 0.2.4	https://www.r-bloggers.com/2010/12/oro-nifti-0-2-4/	December 21, 2010	Brandon Whitcher		 0 Comments
Back from Philly	https://www.r-bloggers.com/2010/12/back-from-philly/	December 20, 2010	xi'an	 The conference in honour of Larry Brown was quite exciting, with lots of old friends gathered in Philadelphia and lots of great talks either recollecting major works of Larry and coauthors or presenting fairly interesting new works. Unsurprisingly, a large chunk of the talks was about admissibility and minimaxity, with John Hartigan starting the day re-reading Larry masterpiece 1971 paper linking admissibility and recurrence of associated processes, a paper I always had trouble studying because of both its depth and its breadth! Bill Strawderman presented a new if classical minimaxity result on matrix estimation and Anirban DasGupta some large dimension consistency results where the choice of the distance (total variation versus Kullback deviance) was irrelevant. Ed George and Susie Bayarri both presented their recent work on g-priors and their generalisation, which directly relate to our recent paper on that topic. On the afternoon, Holger Dette showed some impressive mathematics based on Elfving’s representation and used in building optimal designs. I particularly appreciated the results of a joint work with Larry presented by Robert Wolpert where they classified all Markov stationary infinitely divisible time-reversible integer-valued processes. It produced a surprisingly small list of four cases, two being trivial.. The final talk of the day was about homology, which sounded a priori rebutting, but Robert Adler made it extremely entertaining, so much that I even failed to resent the powerpoint tricks! The next morning, Mark Low gave a very emotional but also quite illuminating about the first results he got during his PhD thesis at Cornell (completing the thesis when I was using Larry’s office!). Brenda McGibbon went back to the three truncated Poisson papers she wrote with Ian Johnstone (via gruesome 13 hour bus rides from Montréal to Ithaca!) and produced an illuminating explanation of the maths at work for moving from the Gaussian to the Poisson case in a most pedagogical and enjoyable fashion. Larry Wasserman explained the concepts at work behind the lasso for graphs, entertaining us with witty acronyms on the side!, and leaving out about 3/4 of his slides! (The research group involved in this project produced an R package called huge.) Joe Eaton ended up the morning with a very interesting result showing that using the right Haar measure as a prior leads to a matching prior, then showing why the consequences of the result are limited by invariance itself. Unfortunately, it was then time for me to leave and I will miss (in both meanings of the term) the other half of the talks. Especially missing Steve Fienberg’s talk for the third time in three weeks! Again, what I appreciated most during those two days (besides the fact that we were all reunited on the very day of Larry’s birthday!) was the pain most speakers went to to expose older results in a most synthetic and intuitive manner… I also got new ideas about generalising our parallel computing paper for random walk Metropolis-Hastings algorithms and for optimising across permutation transforms. 	 0 Comments
Michael Kane on Bigmemory	https://www.r-bloggers.com/2010/12/michael-kane-on-bigmemory/	December 20, 2010	Josh Paulson	Handling big data sets has always been a concern for R users. Once the size of the data set reaches above 50% of RAM, it is considered “massive” and can literally become impossible to work with on a standard machine. The bigmemory project, by Michael Kane and Jay Emerson, is one approach to dealing with this class of data set. Last Monday, December 13th, the New England R Users Group warmly welcomed Michael Kane to talk about bigmemory and R. Bigmemory is one package (of 5 in the bigmemory project) which is designed to extend R to better handle large data sets. The core data structures are written in C++ and allow R users to create matrix-like data objects (called big.matrix). These big.matrix objects are compatible with standard R matrices, allowing them to be used wherever a standard matrix can.  The backing store for a big.matrix is a memory-mapped file, allowing it to take on sizes much larger than available RAM.  Mike discussed the use of bigmemory on the well-known Airline on-time data which includes over 20 years of data on roughly 120 million commercial US airline flights. The data set is roughly 12 GB in size and considering that read.table() recommends the maximum data size to be 10%-20% of RAM, it is nearly impossible to work with on a standard machine. However, bigmemory allows you to read in and analyze the data without problems. Mike also showed how bigmemory can be used with the MapReduce (or split-apply-combine) method to greatly reduce the time required by many statistical calculations. For example, if one were trying to determine if older planes suffer greater delays, you need to know how old each of the 13,000 planes are. This calculation, running on a standard 1 core system is estimated to require nearly 9 hours to compute. Even when running in parallel on 4 cores, it can take nearly 2 hours. However, using bigmemory and the split-apply-combine method, the computation takes a little over one minute! The bigmemory project was recently awarded the 2010 John M. Chambers Statistical Software Award and was presented to Mike at the 2010 Joint Statistical Meetings held in August. 	 0 Comments
Rcpp 0.9.0 and RcppClassic 0.9.0	https://www.r-bloggers.com/2010/12/rcpp-0-9-0-and-rcppclassic-0-9-0-3/	December 20, 2010	Thinking inside the box	"
With this release, the older API which we have been referring to as the
classic Rcpp API has been split off into its own new package
RcppClassic to ensure backwards compatibility.
Rcpp will now
contain only the new API.

 
We also fixes a number a minor bugs and applied a few contributed patches
which extended functionality or documentation as detailed below in the NEWS entry:
 
As always, even fuller details are on the 
Rcpp Changelog page and the 
Rcpp page which also
leads to the downloads, the 
browseable
doxygen docs and zip files of doxygen output for the standard formats.
A local directory  has
source and documentation too. 
Questions, comments etc should go to the
rcpp-devel mailing list
off the R-Forge page

 "	 0 Comments
MATLAB / R Reference	https://www.r-bloggers.com/2010/12/matlab-r-reference/	December 20, 2010	Dan Knoepfle's Blog	Anyone with a MATLAB background interested in transitioning to R is advised to check out this MATLAB / R Reference by Professor David Hiebeler of the University of Maine. 	 0 Comments
Google Insights and RCurl	https://www.r-bloggers.com/2010/12/google-insights-and-rcurl/	December 20, 2010	Dan Knoepfle's Blog	Google Insights is nifty.  If you’re logged in to your Google account, you can download the results as a CSV file.  This is straightforward if you’re using a browser; if you’re trying to retrieve the results of queries using R, however, things get more complicated. The following code retrieves the results of a Google Insights search for “Sarah Palin” as a data.frame.  It uses the RCurl package to do all of the hard work. download ‘Google Insights.R’ from gist.github.com I don’t have much else to say about this, but I hope that it will be helpful to someone. You can change the query to incorporate geographic restrictions or such by adding the parameters that appear in the URL when you change your search through the Google Insights web search; for instance, a basic search for “QUERY” gives URL http://www.google.com/insights/search/#q=QUERY&cmpt=q whereas the same search restricted to the state of New York has URL http://www.google.com/insights/search/#q=QUERY&geo=US-NY&cmpt=q; the added parameter is “geo=US-NY”.  To incorporate this into the script, change to have the additional parameter in the .params list: [Updated 2012-04-24] 	 0 Comments
How to buy a used car with R (part 2)	https://www.r-bloggers.com/2010/12/how-to-buy-a-used-car-with-r-part-2/	December 20, 2010	Dan Knoepfle's Blog	"Continued from Part 1. The only thing better than a bit of data is a lot of data.  Now that we can grab KBB values for a given trim of a given model in a given year, we set our ambitions higher:  automating the collection of these values for all trims of a model over a set of years.  To do so, let’s back up and recall how we got to the KBB results page: Let’s suppose we’re still set on the Honda Accord and are considering the last ten model years.  Going with “Search by: Year, Make & Model”, we get to the following self-explanatory screen:  Choosing (2005, Honda, Accord) pushes us to the following address: http://www.kbb.com/used-cars/honda/accord/2005/.  There, we are reminded that the KBB reports different values for retail, certified retail, private sellers, and trade-ins:  Let’s go with “Private Party Value” for now; we end up at http://www.kbb.com/used-cars/honda/accord/2005/private-party-value.  We’re now presented with a plethora of different trims, enough to make us nostalgic for Henry Ford:  Start with the “DX Sedan 4D”.  We arrive at http://www.kbb.com/used-cars/honda/accord/2005/private-party-value/equipment?id=846.  If the previous screen didn’t freak us out, this one definitely should—-but if we ignore the options at the bottom (which are set to their standard values for the given model year and trim), we’re left with the important parameters:  the choice of automatic or manual transmission and the mileage (and the ZIP code, which I’ll discuss later). I can’t drive stick, so I’m not particularly worried about changing the transmission from its default of Automatic.  But if you wanted to, note that choosing Automatic with default options and 10,000 miles pushes you to http://www.kbb.com/used-cars/honda/accord/2005/private-party-value/condition?id=846&mileage=10000 whereas choosing Manual, 5-Spd with the same options and mileage gives http://www.kbb.com/used-cars/honda/accord/2005/private-party-value/condition?id=846&equipment=35014|true&mileage=10000.  Either way, we end up at a completely pointless page: no matter what you select, the results page gives values for all conditions.  Say we select “Good”.  The results page for the Automatic is located at http://www.kbb.com/used-cars/honda/accord/2005/private-party-value/pricing-report?condition=good&id=846&mileage=10000 and the results page for the Manual, 5-Spd is located at http://www.kbb.com/used-cars/honda/accord/2005/private-party-value/pricing-report?condition=good&id=846&equipment=35014|true&mileage=10000.  If we want, we can tear off the “condition” field, in which case the default condition, Excellent, is highlighted. So, if we want to grab results for a bunch of different years and trims, we need to figure out the id=846 part of the URL (and possibly the equipment=35014|true part if we’re after a manual transmission).  Again, it’s time for Firebug.  Back up to the trim selection page at http://www.kbb.com/used-cars/honda/accord/2005/private-party-value and load up Firebug.  If we examine the links for the various trims, we see that the links for the available trims are contained within a div with id='UCPathTrim'.  The next step is to write some R code to parse the trim selection page and pull out the available trims and their corresponding id values.  This will make use of some of the core functionality of the XML package. In the last post, we used the function readHTMLTable from the XML package to read the results from a webpage into an R data.frame.  At the time, there was little mention of the technical details; now, we’re moving beyond convenient functions and into the great unknown. The XML package, written by Professor Duncan Temple Lang of UC Davis, is a wrapper for libxml2.  The package website, hosted by The Omega Project for Statistical Computing, is at http://www.omegahat.org/RSXML/, and the package listing on CRAN is located at http://cran.r-project.org/web/packages/XML/index.html. At its core, the XML package is meant for parsing XML and HTML documents into tree structures and selecting and extracting or otherwise manipulating branches or nodes of the trees.  Take a look at the HTML tab of Firebug again (on http://www.kbb.com/used-cars/honda/accord/2005/private-party-value), and note that the webpage consists of a tree of HTML tags.  At its root, there’s a html node, with children head and body; within the body branch are nodes defining the structure of the document, including a branch descending from a div node () containing a branch descending from a span node () with leaf nodes like  Accord DX Sedan 4D.

Now, moving to R, we’ll look at the tree produced by the XML package for this document.  The first section of code should be fairly straightforward:
## download the webpage
kbbHTML <- readLines(""http://www.kbb.com/used-cars/honda/accord/2005/private-party-value"")

## load the XML package and parse the downloaded document
require(XML)
kbbTree <- htmlTreeParse(kbbHTML, asText = TRUE)

## get the root ('html') node
kbbRoot <- xmlRoot(kbbTree)

Each node object (class XMLNode) is also a list containing its immediate children as node objects.
> ## print the child nodes ('head' and 'body')
> print(summary(kbbRoot))
     Length Class   Mode
head 14     XMLNode list
body 19     XMLNode list

Thus, we can get the body of the document:
## select the 'body' child node using the usual R list element extraction syntax
kbbBody <- kbbRoot[[""body""]]

Within the body, there’s a bunch of child nodes (the same ones we see in Firebug, of course):
> ## print the child nodes of the 'body'
> print(summary(kbbBody))
         Length Class          Mode
script   1      XMLNode        list
script   1      XMLNode        list
div      4      XMLNode        list
comment  0      XMLCommentNode list
script   0      XMLNode        list
script   1      XMLNode        list
script   1      XMLNode        list
script   1      XMLNode        list
noscript 1      XMLNode        list
comment  0      XMLCommentNode list
comment  0      XMLCommentNode list
script   0      XMLNode        list
div      2      XMLNode        list
script   0      XMLNode        list
script   1      XMLNode        list
comment  0      XMLCommentNode list
script   1      XMLNode        list
noscript 1      XMLNode        list
comment  0      XMLCommentNode list

Either by looking at the tree in Firebug or using summaries of the tree in R, we can identify the div node we’re looking for and access the corresponding node object in R:
## select our 'div id=""UCPathTrim""...' node; instead of using node
## names (like 'div'), which aren't necessarily unique here, we use
## indices (we want the first child of the first child of the second
## child of the second child of the third child of 'body')
divUCPathTrim <- kbbBody[[3]][[2]][[2]][[1]][[1]]


> ## print the child nodes
> print(summary(divUCPathTrim))
     Length Class       Mode
h2   1      XMLNode     list
text 0      XMLTextNode list
span 9      XMLNode     list

We can then access the trim links, which are the leaf nodes of the span node under divUCPathTrim.  Printing an XMLNode object outputs the raw HTML.
> ## print the HTML of the first of the link leaf nodes (children of the 'span' node)
> print(divUCPathTrim[[""span""]][[1]])
<a href=""/used-cars/honda/accord/2005/private-party-value/equipment?id=846"" class=""link_circle_arrow_blue"">Accord DX Sedan 4D</a>

To get the node contents (here, the trim label), we use the xmlValue function:
> ## print the *contents* of this leaf node
> print(xmlValue(divUCPathTrim[[""span""]][[1]]))
[1] ""Accord DX Sedan 4D""

To get the link target (the ‘href’ attribute), we use the xmlAttrs function:
> ## print the 'href' attribute of this leaf node
> print(xmlAttrs(divUCPathTrim[[""span""]][[1]])[[""href""]])
[1] ""/used-cars/honda/accord/2005/private-party-value/equipment?id=846""

There’s an easier way to select a set of nodes and apply functions over this set.  To do so, we must learn a bit of XPath.
XPath
XPath is a query language for selecting sets of nodes from XML or XML-like documents (like HTML webpages).  A nice quick introduction to XPath syntax is the w3schools.com article XPath Syntax.  Open it in a tab, read it, and come back.
Done?  Good.  If we’re super lazy, we can use Firebug to generate an XPath expression to select a given node—just right click on the node and choose “Copy XPath”.  Here’s the XPath expression for the second of the nine trim links:
/html/body/div/div[2]/div[2]/div/div/span/a[2]

To select all of the nine trim links, we simply chop off the “[2]” on the end (match all a nodes that are children of that span):
/html/body/div/div[2]/div[2]/div/div/span/a

If we want a short XPath expression, we can instead use something like this:
//div[@id = 'UCPathTrim']//a

That is, we select all a nodes that descend from any div node with attribute id='UCPathTrim'.  In XPath syntax, “//nodename” selects descendant nodes named nodename while “/nodename” selects child nodes named nodename (immediate descendants).  Using double forward slashes allows us to skip specifying intermediate nodes.  Expressions within brackets are conditions, evaluated to booleans, specifying whether a node should or should not be included.
Is there any advantage to using one expression over the other?  So long as the structure of the webpage doesn’t change, both will work; however, if the order of the nodes in the document changes, the former expression will fail, but the latter will continue to work (it selects on the div id attribute rather than its position in the document).  Similarly, if the div id changes but the document structure otherwise remains unchanged (this is unlikely, but might happen if they messed around with their CSS styling or something), the former would continue working but the latter would fail.
We can create a fancier XPath expression using XPath functions that will continue to work so long as the KBB URL scheme stays the same.  Since the rest of the code will depend on this remaining constant, our XPath expression should only fail at the same time as the rest of our code.  A list of XPath functions can be found here.  We’ll use the function contains(x, y), which returns true if string x contains string y (else false).  Our XPath expression is:
//a[contains(@href, 'used-cars/honda/accord/2005/private-party-value/equipment')]

This selects all links with target URLs containing ‘used-cars/honda/accord/2005/private-party-value/equipment’.
getNodeSet and xpathApply
To use XPath with the XML package, we need to parse the document a little differently.  You see, the XML package can either parse the document into a tree structure of R objects (as we did above, using htmlTreeParse) or into a tree structure of pointers to C-level objects.  In the latter case, the parsed structure is maintained as lower-level objects in memory, and is not immediately accessible in R.  Indeed, incorrectly accessing the parsed document object can cause R to crash.  However, parsing the document into this C-level structure internal to libxml2 permits the use of XPath expressions.  For more, do help(""xmlParse"").
In practice, using XPath expressions with the XML package is fairly simple.  We parse the document with htmlParse instead of htmlTreeParse, and select sets of nodes corresponding to XPath expressions using getNodeSet.  We can then lapply or sapply over the resulting nodeset.  If we only need to apply a single function, we can instead use xpathApply to apply a function to an XPath-defined set directly.
## parse the downloaded document to an XMLInternalDocument
kbbInternalTree <- htmlParse(kbbHTML, asText = TRUE)

## select nodes matching our XPath expression
xpath.expression <- ""//a[contains(@href,'/used-cars/honda/accord/2005/private-party-value/equipment')]""
trim.nodes <- getNodeSet(doc = kbbInternalTree,
                         path = xpath.expression)


> ## the result is of class ""XMLNodeSet"", a list of 9 externalptr
> ## objects of class ""XMLInternalElementNode""
> print(summary(trim.nodes))
      Length Class                  Mode       
 [1,] 1      XMLInternalElementNode externalptr
 [2,] 1      XMLInternalElementNode externalptr
 [3,] 1      XMLInternalElementNode externalptr
 [4,] 1      XMLInternalElementNode externalptr
 [5,] 1      XMLInternalElementNode externalptr
 [6,] 1      XMLInternalElementNode externalptr
 [7,] 1      XMLInternalElementNode externalptr
 [8,] 1      XMLInternalElementNode externalptr
 [9,] 1      XMLInternalElementNode externalptr


> ## we can now lapply or sapply over this list object
> print(lapply(trim.nodes, function(x) c(xmlValue(x), xmlAttrs(x)[[""href""]])))
[[1]]
[1] "" Accord DX Sedan 4D""                                              
[2] ""/used-cars/honda/accord/2005/private-party-value/equipment?id=846""

[[2]]
[1] "" Accord EX Coupe 2D""                                              
[2] ""/used-cars/honda/accord/2005/private-party-value/equipment?id=863""

[[3]]
[1] "" Accord EX Sedan 4D""                                              
[2] ""/used-cars/honda/accord/2005/private-party-value/equipment?id=859""

[[4]]
[1] "" Accord EX-L Coupe 2D""                                               
[2] ""/used-cars/honda/accord/2005/private-party-value/equipment?id=263736""

[[5]]
[1] "" Accord EX-L Sedan 4D""                                               
[2] ""/used-cars/honda/accord/2005/private-party-value/equipment?id=263737""

[[6]]
[1] "" Accord Hybrid Sedan 4D""                                          
[2] ""/used-cars/honda/accord/2005/private-party-value/equipment?id=868""

[[7]]
[1] "" Accord LX Coupe 2D""                                              
[2] ""/used-cars/honda/accord/2005/private-party-value/equipment?id=856""

[[8]]
[1] "" Accord LX Sedan 4D""                                              
[2] ""/used-cars/honda/accord/2005/private-party-value/equipment?id=850""

[[9]]
[1] "" Accord LX Special Edition Coupe 2D""                              
[2] ""/used-cars/honda/accord/2005/private-party-value/equipment?id=867""

Putting it all together
I’m getting tired, so let’s jump ahead to a complete function that retrieves all of the trims for a given year.  If you’ve read and understood everything above, you should be able to figure out how the function works without much trouble (with the possible exception of the XPath expression, which needlessly uses regular expressions).  Go wild with help(...) until it all makes sense.
getKBBYearTrims <- function(prefix, year, type = ""private-party-value"") {
  require(XML)

  kbbTrimPageURL <- sprintf(""%s%i/%s"", prefix, year, type)
  cat(""Loading"", kbbTrimPageURL, ""\n"")

  x <- readLines(kbbTrimPageURL)
  g <- htmlParse(x, asText=TRUE)

  xpath <- gsub(""([http:/w.]+kbb\\.com/)(.*)"", ""//a[contains(@href, '\\2/equipment')]"", kbbTrimPageURL)
  cat(""XPath expression is:"", xpath, ""\n"")

  trims <- getNodeSet(doc = g, path = xpath)
  trimlabels <- sapply(trims, xmlValue)
  trimids <- sapply(trims, function(node) sub("".*id=([[:digit:]]+)$"", ""\\1"", xmlAttrs(node)[[""href""]]))

  trimtable <- data.frame(year = year,
                          trim = trimlabels,
                          id = trimids,
                          stringsAsFactors = FALSE)
  return(trimtable)
}

The function works great for 2005 Accords:
> ## print trims and ids for 2005 Honda Accords
> print(getKBBYearTrims(prefix = ""http://www.kbb.com/used-cars/honda/accord/"", year = 2005))
Loading http://www.kbb.com/used-cars/honda/accord/2005/private-party-value 
XPath expression is: //a[contains(@href, 'used-cars/honda/accord/2005/private-party-value/equipment')] 
  year                                trim     id
1 2005                  Accord DX Sedan 4D    846
2 2005                  Accord EX Coupe 2D    863
3 2005                  Accord EX Sedan 4D    859
4 2005                Accord EX-L Coupe 2D 263736
5 2005                Accord EX-L Sedan 4D 263737
6 2005              Accord Hybrid Sedan 4D    868
7 2005                  Accord LX Coupe 2D    856
8 2005                  Accord LX Sedan 4D    850
9 2005  Accord LX Special Edition Coupe 2D    867

The following function wraps getKBBYearTrims to return a data.frame of trims for a set of model years.
getKBBTrims <- function(prefix, years, type = ""private-party-value"") {

  kbbTrimList <- lapply(years, function(year) getKBBYearTrims(prefix, year))
  kbbTrims <- do.call('rbind', kbbTrimList)

  return(kbbTrims)
}

Using it, we can try getting the trims for a series of model years:
> ## print trims and ids for years 2003 to 2007
> accord.trims <- getKBBTrims(prefix = ""http://www.kbb.com/used-cars/honda/accord/"", years = 2003:2007)
Loading http://www.kbb.com/used-cars/honda/accord/2003/private-party-value 
XPath expression is: //a[contains(@href, 'used-cars/honda/accord/2003/private-party-value/equipment')] 
Loading http://www.kbb.com/used-cars/honda/accord/2004/private-party-value 
XPath expression is: //a[contains(@href, 'used-cars/honda/accord/2004/private-party-value/equipment')] 
Loading http://www.kbb.com/used-cars/honda/accord/2005/private-party-value 
XPath expression is: //a[contains(@href, 'used-cars/honda/accord/2005/private-party-value/equipment')] 
Loading http://www.kbb.com/used-cars/honda/accord/2006/private-party-value 
XPath expression is: //a[contains(@href, 'used-cars/honda/accord/2006/private-party-value/equipment')] 
Loading http://www.kbb.com/used-cars/honda/accord/2007/private-party-value 
XPath expression is: //a[contains(@href, 'used-cars/honda/accord/2007/private-party-value/equipment')]
> print(accord.trims)
   year                                trim     id
1  2003                  Accord DX Sedan 4D   2488
2  2003                  Accord EX Coupe 2D   2496
3  2003                  Accord EX Sedan 4D   2498
4  2003                Accord EX-L Coupe 2D 263731
5  2003                Accord EX-L Sedan 4D 263730
6  2003                  Accord LX Coupe 2D   2495
7  2003                  Accord LX Sedan 4D   2492
8  2004                  Accord DX Sedan 4D   2664
9  2004                  Accord EX Coupe 2D   2671
10 2004                  Accord EX Sedan 4D   2676
11 2004                Accord EX-L Coupe 2D 263735
12 2004                Accord EX-L Sedan 4D 263734
13 2004                  Accord LX Coupe 2D   2669
14 2004                  Accord LX Sedan 4D   2663
15 2005                  Accord DX Sedan 4D    846
16 2005                  Accord EX Coupe 2D    863
17 2005                  Accord EX Sedan 4D    859
18 2005                Accord EX-L Coupe 2D 263736
19 2005                Accord EX-L Sedan 4D 263737
20 2005              Accord Hybrid Sedan 4D    868
21 2005                  Accord LX Coupe 2D    856
22 2005                  Accord LX Sedan 4D    850
23 2005  Accord LX Special Edition Coupe 2D    867
24 2006                  Accord EX Coupe 2D    741
25 2006                  Accord EX Sedan 4D    739
26 2006                Accord EX-L Coupe 2D 263727
27 2006                Accord EX-L Sedan 4D 263726
28 2006              Accord Hybrid Sedan 4D    744
29 2006                  Accord LX Coupe 2D    736
30 2006                  Accord LX Sedan 4D    734
31 2006                  Accord SE Sedan 4D    738
32 2006                  Accord VP Sedan 4D    737
33 2007                  Accord EX Coupe 2D  83835
34 2007                  Accord EX Sedan 4D  83834
35 2007                Accord EX-L Coupe 2D 263674
36 2007                Accord EX-L Sedan 4D 263675
37 2007              Accord Hybrid Sedan 4D  83836
38 2007                  Accord LX Coupe 2D  83833
39 2007                  Accord LX Sedan 4D  83829
40 2007                  Accord SE Sedan 4D  83832
41 2007                  Accord VP Sedan 4D  83827

Everything works great.  What a shock.


Related
ShareTweet




To leave a comment for the author, please follow the link and comment on their blog:  Dan Knoepfle's Blog.

R-bloggers.com offers daily e-mail updates about R news and tutorials about learning R and many other topics. Click here if you're looking to post or find an R/data-science job.

Want to share your content on R-bloggers? click here if you have a blog, or  here if you don't.
 


← Previous post

Next post →

 Now, moving to R, we’ll look at the tree produced by the XML package for this document.  The first section of code should be fairly straightforward: Each node object (class XMLNode) is also a list containing its immediate children as node objects. Thus, we can get the body of the document: Within the body, there’s a bunch of child nodes (the same ones we see in Firebug, of course): Either by looking at the tree in Firebug or using summaries of the tree in R, we can identify the div node we’re looking for and access the corresponding node object in R: We can then access the trim links, which are the leaf nodes of the span node under divUCPathTrim.  Printing an XMLNode object outputs the raw HTML. To get the node contents (here, the trim label), we use the xmlValue function: To get the link target (the ‘href’ attribute), we use the xmlAttrs function: There’s an easier way to select a set of nodes and apply functions over this set.  To do so, we must learn a bit of XPath. XPath is a query language for selecting sets of nodes from XML or XML-like documents (like HTML webpages).  A nice quick introduction to XPath syntax is the w3schools.com article XPath Syntax.  Open it in a tab, read it, and come back. Done?  Good.  If we’re super lazy, we can use Firebug to generate an XPath expression to select a given node—just right click on the node and choose “Copy XPath”.  Here’s the XPath expression for the second of the nine trim links: To select all of the nine trim links, we simply chop off the “[2]” on the end (match all a nodes that are children of that span): If we want a short XPath expression, we can instead use something like this: That is, we select all a nodes that descend from any div node with attribute id='UCPathTrim'.  In XPath syntax, “//nodename” selects descendant nodes named nodename while “/nodename” selects child nodes named nodename (immediate descendants).  Using double forward slashes allows us to skip specifying intermediate nodes.  Expressions within brackets are conditions, evaluated to booleans, specifying whether a node should or should not be included. Is there any advantage to using one expression over the other?  So long as the structure of the webpage doesn’t change, both will work; however, if the order of the nodes in the document changes, the former expression will fail, but the latter will continue to work (it selects on the div id attribute rather than its position in the document).  Similarly, if the div id changes but the document structure otherwise remains unchanged (this is unlikely, but might happen if they messed around with their CSS styling or something), the former would continue working but the latter would fail. We can create a fancier XPath expression using XPath functions that will continue to work so long as the KBB URL scheme stays the same.  Since the rest of the code will depend on this remaining constant, our XPath expression should only fail at the same time as the rest of our code.  A list of XPath functions can be found here.  We’ll use the function contains(x, y), which returns true if string x contains string y (else false).  Our XPath expression is: This selects all links with target URLs containing ‘used-cars/honda/accord/2005/private-party-value/equipment’. To use XPath with the XML package, we need to parse the document a little differently.  You see, the XML package can either parse the document into a tree structure of R objects (as we did above, using htmlTreeParse) or into a tree structure of pointers to C-level objects.  In the latter case, the parsed structure is maintained as lower-level objects in memory, and is not immediately accessible in R.  Indeed, incorrectly accessing the parsed document object can cause R to crash.  However, parsing the document into this C-level structure internal to libxml2 permits the use of XPath expressions.  For more, do help(""xmlParse""). In practice, using XPath expressions with the XML package is fairly simple.  We parse the document with htmlParse instead of htmlTreeParse, and select sets of nodes corresponding to XPath expressions using getNodeSet.  We can then lapply or sapply over the resulting nodeset.  If we only need to apply a single function, we can instead use xpathApply to apply a function to an XPath-defined set directly. I’m getting tired, so let’s jump ahead to a complete function that retrieves all of the trims for a given year.  If you’ve read and understood everything above, you should be able to figure out how the function works without much trouble (with the possible exception of the XPath expression, which needlessly uses regular expressions).  Go wild with help(...) until it all makes sense. The function works great for 2005 Accords: The following function wraps getKBBYearTrims to return a data.frame of trims for a set of model years. Using it, we can try getting the trims for a series of model years: Everything works great.  What a shock. "	 0 Comments
Vim plugin for R	https://www.r-bloggers.com/2010/12/vim-plugin-for-r-2/	December 19, 2010	Martin Scharm	"Just found a very nice plugin for using R in Vim. It’s unbelievable comfortable! 
There are at least two ways to install it. If your are working on a Debian based distro you can use the .deb package provided by the author Jakson Alves de Aquino itself. On this site you’ll also find some smart screen shots.
Another way is the installation by hand (I did so). Download the package, in your download directory type something like this: Yes, that’s it!
To start an R session just open a R-file (.R or .Rnw or .Rd) with vim and type \rf. To close the session type \rq (not saving hist) or \rw (saving history).
The handling needs to getting used to.. Here is a list of common commands from the documentation (might want to print them as a cheat sheet!?) : Vim-R-plugin But if you got used to, it’s very handy! At the start-up it opens a new R-console (just close it, doesn’t matter) and you can send single lines, a block or a whole file to R (see the documentation). Every thing I tried worked really fine! A small example in action is presented in the image. In an earlier post I explained how to produce such a title consisting of R objects and Greek letters. I’ve attached the documentation of this plugin, first and foremost for me for cheating, but of course you’re allowed to use it also 😉 "	 0 Comments
Principal component analysis to yield curve change	https://www.r-bloggers.com/2010/12/principal-component-analysis-to-yield-curve-change/	December 19, 2010	teramonagi		 0 Comments
White Bumblebee Implemented in R	https://www.r-bloggers.com/2010/12/white-bumblebee-implemented-in-r/	December 18, 2010	Milk Trader		 0 Comments
AMIS revised & resubmitted	https://www.r-bloggers.com/2010/12/amis-revised-%c2%a0resubmitted/	December 18, 2010	xi'an	After a thorough revision that removed most of the theoretical attempts at improving our understanding of AMIS convergence, we have now resubmitted the AMIS paper to Scandinavian Journal of Statistics and arXived the new version as well. (I remind the reader that AMIS stands for adaptive mixture importance sampling and that it implements an adaptive version of Owen and Zhou’s (2000, JASA) stabilisation mixture technique, using this correction on the past and present importance weights, at each iteration of this iterative algorithm.) The AMIS method starts being used in population genetics, including an on-going work by Jean-Marie Cornuet and a published paper in Molecular Biology and Evolution by Sirén, Marttinen and Corander. The challenge of properly demonstrating AMIS convergence remains open! 	 0 Comments
R Workflow: Melbourne R Users Dec 1st 2010	https://www.r-bloggers.com/2010/12/r-workflow-melbourne-r-users-dec-1st-2010/	December 18, 2010	jeromyanglim	Melbourne R Users Group December 1st 2010 Meeting (Meetup page). The other talk from the session was by Geoff Robinson who discussed several useful strategies for working with R. Video is embedded below (requires Flash and may not be viewable in RSS Readers)  or go here . Video is embedded below:   or go here Many thanks to Pedro Olaya for filming and Drew Conway for posting and hosting the videos. 	 0 Comments
Visualizing Facebook Friends: Eye Candy in R	https://www.r-bloggers.com/2010/12/visualizing-facebook-friends-eye-candy-in-r/	December 18, 2010	Paul Butler	Earlier this week I published a data visualization on the Facebook Engineering blog which, to my surprise, has received a lot of media covereage.  I’ve received a lot comments about the image, many asking for more details on how I created it. When I tell people I used R, the reaction I get is roughly what I would expect if I told them I made it with a Microsoft Paint and a bottle of Jägermeister. Some people even questioned whether it was actually done in R. The truth is, aside from the addition of the logo and date text, the image was produced entirely with about 150 lines of R code with no external dependencies. In the process I learned a few things about creating nice-looking graphs in R. Transparency and Faking It My first attempt at plotting the data involved plotting very transparent lines. Unfortunately there was just too much data to get a meaningful plot — even at very low opacity, there were enough lines to make the entire image just a bright blob. When I increased the transparency more, the opacity was rounded down to zero by my graphics device and the result was that nothing was drawn. The solution was to manipulate the drawing order of the lines. I used a simple loop over my data to draw the lines, so it was easy to control which lines are drawn first using order(). I created an ordering based on the length of the lines, so that longer lines were drawn “behind” the shorter, more local lines. Then I used colorRampPalette() to generate a color palette from black to blue to white, and colored the lines according to order they were drawn. Great Circles I wrote my own code to draw the great circle arcs, although I later found a CRAN package called geosphere that would have done it for me (albeit with rougher lines near the poles). I drew the great circle arcs in a way that was easy to derive but slow to compute. I bisected the lines recursively, finding their great circle midpoint, until they were short enough to resemble an arc. To find the great circle midpoint, I converted from spherical coordinates to Cartesian, found the midpoint, then converted back to spherical coordinates and extended the radius. Euclidean Distance Several observent commenters called me out on using Euclidean distance on the projection for the ordering function. Having the ordering function depend on the distance on the projection seems counterintuitive, as Eucliden distance is wildly distorted near the poles. I accepted this drawback because the exact drawing order wasn’t important, as long as very long lines were drawn below very short ones. 	 0 Comments
ASReml-R: Storing A inverse as a sparse matrix	https://www.r-bloggers.com/2010/12/asreml-r-storing-a-inverse-as-a-sparse-matrix/	December 18, 2010	Gregor Gorjanc		 0 Comments
In The Wild: rApache 1.1.13 in Ubuntu and Source	https://www.r-bloggers.com/2010/12/in-the-wild-rapache-1-1-13-in-ubuntu-and-source/	December 18, 2010	awaiting assimilation	 I rolled a new released eight days ago, but tumblr was down and I forgot to post. Then yesterday I remembered again, but then tumblr was down again. Doh! You can get rApache 1.1.13 from here in source form or learn about how to get it here in binary form for Ubuntu lucid. For those of you who have previously installed the binary package, note that the name has changed to libapache2-mod-r-base, so don’t forget to uninstall libapache2-mod-rstats (lame name, I know) first. The photo? Oh, that’s an rApache ninja doing a kick flip on a skateboard in Central park. They do exist! 	 0 Comments
Introduction to ESS: talk and slides	https://www.r-bloggers.com/2010/12/introduction-to-ess-talk-and-slides/	December 17, 2010	Thinking inside the box	"
The user group meetings have a meme of showing how to use
R
with different editors, UIs, IDEs,…  It started with a presentation on
Eclipse and its
StatET plugin.  So a while
ago I had offered to present on ESS, the wonderful
Emacs mode for R (and as well as SAS, Stata, BUGS, JAGS, …).  And now I owe a big
thanks to the ESS Core team for keeping all their documentation, talks,
papers etc in their SVN archive,
and particularly to Stephen Eglen
for putting the source code to Tony Rossini’s tutorial from
useR! 2006 in Vienna
there.  This allowed me to quickly whip up 
a few slides
though a good part of the presentation did involve a live demo missing from
the slides. Again, big thanks to Tony for the old slides and to Stephen for
making them accessible when I mentioned the idea of this talk a while back —
it allowed to put this together on short notice.

 
And for those going to
useR! 2011 in
Warwick next summer,
Stephen will present a
full three-hour
ESS tutorial 
which will cover
ESS in much more detail.

 "	 0 Comments
Joe West vs. Bruce Froemming: A Crude Umpire LHB/RHB Bias Comparison	https://www.r-bloggers.com/2010/12/joe-west-vs-bruce-froemming-a-crude-umpire-lhbrhb-bias-comparison/	December 17, 2010	Millsy		 0 Comments
Programming languages, ranked by popularity	https://www.r-bloggers.com/2010/12/programming-languages-ranked-by-popularity/	December 17, 2010	David Smith	In a presentation to the Chicago R User Group last night, Drew Conway used his new Infochimps package in R to assess the relative popularity of programming languages. Drew used the word.stats function in the Infochimps package to count the frequency of common computer languages mentioned in Twitter messages, and displayed the results in this bar chart:  It's not perfect: languages like C and C++ are excluded because they're impossible to search for, “ada” is excluded because it's ambiguous (and otherwise that niche language would be ranked most popular), and R is measured by the frequency of its community twitter hashtag #rstats and not the letter R. But it's interesting nonetheless. There's lots more info about Infochimps in general, and how this chart in particular was created, in the slides downloadable from Drew's blog. Another way to look at programming language popularity is the frequency of mentions on two popular programmer's resource sites. In a post at the Dataists blog, Drew Conway (again) and John Myles White used R and the XML package to extract the number of questions on stackoverflow.com and number of projects on github.com for about 50 programming languages, and plotted the results in this scatterplot:  As you can see, R tanks higher than the median for github projects and quite a lot higher for stackoverflow questions. So R is doing quite well amongst programming languages in general. As a specialized statistics language, a more relevant comparison may come from looking at tags at the statistical question-and-answer site stats.stackexchange.com, where R currently has 260 questions compared to 6 for SAS and 22 for SPSS. 	 0 Comments
World Bank data plots – Take 2	https://www.r-bloggers.com/2010/12/world-bank-data-plots-take-2/	December 17, 2010	prasoonsharma		 0 Comments
Why use R	https://www.r-bloggers.com/2010/12/why-use-r-2/	December 16, 2010	Daniel Hocking		 0 Comments
Disas-tea-R at dawn	https://www.r-bloggers.com/2010/12/disas-tea-r-at-dawn/	December 16, 2010	xi'an	This was bound to happen sooner or later, given my addiction to tea and sleepless nights, so I eventually managed to spill a cup of tea over my Mac… I had been working for a few hours in my hotel room in Philadelphia, completing an ABC paper with Jean-Michel Marin and Robin Ryder. We had been running experiments in R with Jean-Michel over the past days and I wanted to check some details with him about some graphs he sent me, so we got into a Skype conversation. At the same time, I was running an alternative R code to compare with his, and starting a new ‘Og entry about a current campaign against road accidents. And drinking my fifth or sixth cuppa of the morning. I suddenly realised dawn had come and stood up to raise the blinds next to my desk. Completely forgetting about the earphones on my head. The obvious then occurred: the earphone cord stretched, pulling the teacup over and I turned back to see the keyboard covered with tea… Disaster! I tipped the computer over and grabbed the hotel hairdryer to try to dry it as quickly as possible (apologies to the next room neighbour!). Contrary to all advices, I did not turn the Mac off but kept running the R program instead to add to the heat of the dryer. In retrospect this was quite silly and I am lucky to get out with only two keys not working! 	 0 Comments
Issues of R Client Library For The Google Prediction API	https://www.r-bloggers.com/2010/12/issues-of-r-client-library-for-the-google-prediction-api/	December 16, 2010	Quantitative Finance Collector		 0 Comments
R 2.12.1 is out	https://www.r-bloggers.com/2010/12/r-2-12-1-is-out/	December 16, 2010	David Smith	As promised, the latest patch to R is out with the release of R 2.12.1, as announced today by the R Core Team. If you build R yourself, sources are available now at your local CRAN mirror, and binaries for Windows, Mac and Linux will be available in the next few days. There are a few new features: In addition, several minor bugs have been fixed — you can see a list in the NEWS file or the announcement linked below. r-announce mailing list: R 2.12.1 is released 	 0 Comments
Tennis and risk management	https://www.r-bloggers.com/2010/12/tennis-and-risk-management/	December 16, 2010	arthur charpentier	As mentioned already here, while we were going to Québec City for the workshop, we had interesting discussions in the car, and Maciej mentioned an article recently published in The Actuary, Hence, I wanted to discuss (extremely) rare event probabilities in tennis. The story is simple: in June 2010, at Wimbledon, Nicolas Mahut and John Isner have played the longest match ever. 980 points, 11 But first of all, we need a dataset. Thanks to Duncan Murdoch, I have been able to run a short code to build up a dataset: Here I consider only tournaments where players have to win 3 sets (and actually more tournaments than those in the code above), and I have something like a bit more than 72,000 matches, so, if we look briefly at matches over 35 years, we have the following boxplot (one boxplot per year), The red line being the epic Isner-Mahut match in June 2010 (4-6, 6-3, 7-6, 6-7, 70-68, i.e. 183 games, here for the score card). If we study theory (e.g. from Paul Newton and Kamran Aslam), a lot of results can be obtained for the expected value of the number of games, but if we want to study extremely rare events, we should generate Markov chains (with a lot of generation since the probability should be extremely small). But how many ? Consider below matches with more than 50 games, The tail plot (over 50), i.e. the log-log Pareto plot indicates that it will be difficult to study tails, and similarly with the Hill plot (assuming that tails are Pareto type….) Anyway, if we want to study tails, we should consider a threshold high enough. For instance, with a threshold at 68 (we keep only 24 match), we have I.e. the probability that one match last more than 183 games is 1 chance over a billion… With, say, 2500 match per year, that gives us a return period of 400 years. So yes, we might say that this way a rare event… So perhaps, generating several billions of chains, it should be possible to get a more precise estimation of the probability to play 183 games in a single match… 	 0 Comments
Where to find good data sets	https://www.r-bloggers.com/2010/12/where-to-find-good-data-sets/	December 16, 2010	Larry D'Agostino		 0 Comments
Area plots unmasked	https://www.r-bloggers.com/2010/12/area-plots-unmasked/	December 15, 2010	dan	"RESULTS OF THE GREAT AREA PLOT QUIZ  If you are the type of reader who remembers things from last week, you may remember the great area plot quiz we had running. This week, we are excited to announce that the results are in. The plot above shows answers to the four questions. The correct answers are indicated with the green lines. Remember, in each question, the big circle was area 1000 and readers had to guess the areas of the second and third biggest circles. As the above plot shows, when the circles are 8% to 20% of the size of the biggest (questions 1 and 3), people exhibit a great deal of variation in their area estimates, but the responses benefit from some “wisdom of crowds” magic and approximate the truth. When the circles are 5% or 1% of the biggest, people tend to underestimate the area. It is also interesting to note that 1) the biggest variation in response is in the question with the biggest circle; this was a somehing surprise, since one would think it would be easier to visualize putting a biggish circle inside a little one, however floor effects can account for some of it 2) While the circles in questions 1 and 4 weren’t that different in area, people treated them somewhat differently. It seems as if in question 4, the fact the circle was third largest caused people to underestimate its size. Perhaps if it were second largest, it may have been spot on. The mean absolute deviations from the correct answer in Questions 1 – 4 were 38.6, 9.4,  73.6, and 31.2 respectively. The following plot, which shows the difference between the responses and the correct answers, is also informative (and frankly, we couldn’t decide which one to lead with). It makes the underestimation apparent.  Hadley of ggplot2-authoring fame asked if we used “scale_area” to make our plots. Yes, we did. p <- ggplot(plot.data, aes(num.contacts.sales.part1,response))

p <- p + geom_point(aes(size=count,alpha=.8)) + geom_line(size=.25)

p <- p + scale_area(to=get.range(plot.data$count)) where
get.range <- function(counts) {

dist <- counts/sum(counts)

my.range <- c(sqrt(min(dist)*100),sqrt(max(dist)*100))

my.range <- round(my.range,1)

} Naturally, at this point, many R-hounds will want to play with the data. There are many things to try, such as computing the accuracy of the third circles on the assumption that the areas of the second circles are all correct. Far be it from us to stand in the way of such tinkering. Just paste the following into an R session to reproduce the data frame “df” with the responses. df=structure(list(variable=structure(c(1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,

1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,

1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,

1L,1L,1L,1L,1L,1L,1L,1L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,

2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,

2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,

2L,2L,2L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,

3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,

3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,4L,4L,

4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,

4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,

4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L),.Label=c(""q1"",""q2"",

""q3"",""q4""),class=""factor""),value=c(50,60,70,50,10,100,40,50,50,50,100,

100,100,50,50,111,100,150,10,250,70,65,100,200,100,100,100,40,100,20,

50,200,100,100,50,100,125,100,100,100,50,100,100,10,100,200,100,100,

63,100,100,100,80,10,50,80,50,125,50,300,100,50,150,50,5,5,7,5,2,10,1,

25,8,5,10,10,20,5,1,7,10,50,1,100,8,5,10,50,10,10,10,8,10,2,5,50,15,10,

2,5,16,10,25,10,5,10,10,1,10,25,10,25,6,10,10,10,12,1,10,10,5,30,5,100,

10,5,20,3,100,200,200,100,200,250,200,100,90,50,150,300,200,100,100,

250,250,300,100,400,120,120,250,300,250,200,250,200,200,40,100,400,130,

200,100,200,250,300,200,200,100,150,200,40,250,450,250,200,169,100,1,

250,200,50,200,160,200,250,100,400,300,100,300,100,10,50,40,25,20,125,

40,25,15,5,20,150,100,25,20,28,50,100,10,200,15,25,25,100,60,20,125,40,

40,4,10,100,25,50,10,20,63,30,50,50,10,50,50,10,60,200,50,50,42,10,0.1,

62,40,5,50,25,50,125,20,100,30,50,60,20)),.Names=c(""variable"",""value""),

class=""data.frame"",row.names=c(NA,-256L))


If you want the correct answers (what we in JDM call the “normative” answers), just paste this, too. df$norm=c(rep(78.4 ,nrow(df)/4),

rep( 11.2,nrow(df)/4),

rep(193.1,nrow(df)/4),

rep(50.9,nrow(df)/4)) "	 0 Comments
Value of an R object in an expression	https://www.r-bloggers.com/2010/12/value-of-an-r-object-in-an-expression/	December 15, 2010	Martin Scharm	"Just wanted to create an expression, existing of some mathematical annotation and a value of an R object. Wasn’t that intuitive! 
Each single goal is easy to reach, for example to combine a value of an R object with text just use paste: To have a plot title with an  you can use expression: But to let the the title of a plot contain objects and Greek letters isn’t that easy. Those of you who think it’s just about combining paste and expression might try it on their own and come back head-ached after few minutes of unsuccessful testings. The problem is, that expression interprets chars as expression and not as object identifier, of course, how should it know whether you mean the var alpha or the Greek letter!?  The solution is called substitute! With substitute you can replace objects inline, here is a small example: You see, substitute got a list what to substitute and replaces the v in paste with the content of var. Run eval to evaluate to result: Now it’s easy to create a more complex plot title: Go out and produce imposing graphs! (-; "	 0 Comments
Data Driven Journalism	https://www.r-bloggers.com/2010/12/data-driven-journalism/	December 15, 2010	David Smith	Last night at the Bay Area UseR Group meeting, Peter Aldhous, San Francisco Bureau Chief of New Scientist Magazine, gave an inspiring presentation about Data Driven Journalism. Even though the newspaper industry is faltering as a business model, there's a beacon of light: journalists can be the driving force behind bringing the meaning in the huge data sets that are now available to a wider audience. In an age of data, this is what journalism could become. Even the inventor of the World Wide Web, Time Berners-Lee, says that analyzing data is the future for journalists. In his talk, Peter gave several examples of data journalism in print and online media. I was surprised to learn that the pioneering example comes from not the last few years but the last few decades: the Pulitzer-prize winning reportage on the 1976 Detroit riot by Philip Meyer. There, a well-designed followup survey dispelled some myths about the rioters, for example that college graduates were as likely to have rioted as high-school dropouts. Some more recent examples presented by Peter included a Guardian investigation of the Afghanistan Wikileaks data dump, and a Seattle Times investigation into the relationship between clear-cutting and devastating landslides. Several examples were implemented using R, at least in part. Through conversations with New York Times Graphics Editor Amanda Cox, Peter confirmed that indeed several of the interactive graphics featured in the presentation by Amanda we noted last week involved R, including the Michael Jackson billboard rankings chart, the Mariano Rivera baseball story, and a decision tree (created using the rpart package) on primary voters in the 2008 Obama-Clinton race. In Barron's in 2007, a feature article revealing that the only way to profit from the advice of CNBC financial “guru” Jim Cramer was to short his recommendations was based on financial analysis in R, as described by journalist Bill Alpert in an R News article (see p34) and by his statistical adviser Patrick Burns (of R Inferno fame). Peter also described his own use of R for data journalism in New Scientist, which we reviewed in an earlier post. Peter has graciously made his slides available for download — check them out and follow the links to many of the examples mentioned above. Video of Peter's presentation will also be available soon (check back here for an update with the link).   Bay Area UseR Group: Data Driven Journalism           http://datajournalism.stanford.edu/ 	 0 Comments
Databases (SQL, noSQL); Interfacing R with Excel	https://www.r-bloggers.com/2010/12/databases-sql-nosql-interfacing-r-with-excel/	December 15, 2010	Szilard	Los Angeles R users group Dec. 14 2010 meeting (see meetup info here): 1. A SQL primer for R users – Neal Fultz Video and slides will be available soon 2. R Database Access – Shrikrishna Bhogaonker  3. NoSQL data stores – Scott Gonyea  4. Interfacing R with Excel – Eric Kostello  	 0 Comments
I really need to find hot (and sexy) topics	https://www.r-bloggers.com/2010/12/i-really-need-to-find-hot-and-sexy-topics/	December 15, 2010	arthur charpentier	50 days ago (here), I was supposed to be very optimistic about the probability that I could reach a million viewed pages on that blog (over a bit more than two years). Unfortunately, the wind has changed and today, the probability is quite low… 	 0 Comments
Customizing Maps in R: spplot() and latticeExtra functions	https://www.r-bloggers.com/2010/12/customizing-maps-in-r-spplot-and-latticeextra-functions/	December 15, 2010	dylan	I recently noticed the new latticeExtra page on R-forge, which contains many very interesting demos of new lattice-related functionality. There are strong opinions about the “best” graphics system in R (base graphics, grid graphics, lattice, ggplot, etc.)– I tend to use base graphics for simple figures and lattice for depicting multivariate or structured data. The sp package defines classes for storing spatial data in R, and contains several useful plotting methods such as the lattice-based spplot(). This function, and back-end helper functions, provide a generalized framework for plotting many kinds of spatial data. However, sometimes with great abstraction comes great ambiguity– many of the arguments that would otherwise allow fine tuning of the figure are buried in documentation for lattice functions. Examples are more fun than links to documentation, so I put together a couple of them below. They describe several strategies for placing and adjusting map legends– either automatically, or manually added with the update() function. The last example demonstrates an approach for over-plotting 2 rasters. All of the examples are based on the meuse data set, from the gstat package. Extended spplot() examples read more 	 0 Comments
Examples for infochimps Package, and Intro Slides	https://www.r-bloggers.com/2010/12/examples-for-infochimps-package-and-intro-slides/	December 15, 2010	Drew Conway	Tomorrow I am headed out of town for a few weeks, so posting will be lighter than the usual lightness.  My first stop is Chicago, and tomorrow night I will be speaking about my R package for the infochimps API at the Chicago R Users Group.  If you are in the Chicagoland area and are interested in coming out I am told there is still time to RSVP. For those not in the area tomorrow night, I have made the slides and example code available on the get your own API key.  	 0 Comments
Why Use R?	https://www.r-bloggers.com/2010/12/why-use-r/	December 14, 2010	Joshua Ulrich		 0 Comments
Facebook’s Social Network Graph	https://www.r-bloggers.com/2010/12/facebooks-social-network-graph/	December 14, 2010	David Smith	Paul Butler, an intern on Facebook’s data infrastructure engineering team, was interested in visualizing the “locality of friendship”. Luckily, he has some great data to work with: Facebook's social network of the friendships between its 500 million members. But visualizing that much data can be a challenge in its own right — it takes skill to draw meaning from what could easily be an incomprehensible mess of data. After drawing a sample of 10 million friend pairs from the Hive interface to Facebook's Hadoop-based database, Paul set to using R to solve this visualization problem.  As Paul describes in a post on his Facebook page, initial attempts to visualize the data resulted in a “big white blob” roughly resembling the outline of the continents. But when Paul switched from plotting every friend pair to instead plotting every city pair with a great-circle line whose transparency was determined by the number of friend-pairs in those cities, something beautiful emerges: a clear image of the world, with friendship bonds flowing between the continents:  (Click to enlarge, or visit Paul's post to download a super hi-res version.)  This is a beautiful image, and a testament to Paul's visualization skills (with a little help from the graphical prowess of R). Not only can you see the population centers in bright white (from the density of intra-city friendships), you can also see clear country outlines: look how visible India is, floating in the dark void of China and Russia. You can also see cultural relationships: Hawaii to the continental US; Australia to New Zealand; India to the UK. (The latter's a bit hard to see, though — it would be fascinating to see this in a 3-D globe form, with relationships flowing through the middle of the globe.) Paul sums up the impact of this visualization best in his own words: After a few minutes of rendering, the new plot appeared, and I was a bit taken aback by what I saw. The blob had turned into a surprisingly detailed map of the world. Not only were continents visible, certain international borders were apparent as well. What really struck me, though, was knowing that the lines didn't represent coasts or rivers or political borders, but real human relationships. Each line might represent a friendship made while travelling, a family member abroad, or an old college friend pulled away by the various forces of life. Mashable also has a nice review of this chart … with 4,780 Facebook Likes at the time of writing. It was also featured on FlowingData and ReadWriteWeb. Paul Butler: Visualizing Friendships   	 0 Comments
Logical operators in R	https://www.r-bloggers.com/2010/12/logical-operators-in-r/	December 14, 2010	csgillespie	"A logical operator In R, the operators “|” and “&” indicate the logical operations OR and AND. For example, to test if x equals 1 and y equals 2 we do the following:  > x = 1; y = 2

> (x  == 1) & (y == 2)

[1] TRUE However, if you are used to programming in C you may be tempted to write


#Gives the same answer as above (in this example...)

> (x  == 1) && (y == 2)

[1] TRUE


At this point you could be lulled into a false sense of security and believe that they could be used interchangeably. Big mistake. Let’s consider another example, this time a vector comparison:


> z = 1:6

> (z > 2) & (z < 5)

[1] FALSE FALSE  TRUE  TRUE FALSE FALSE

> z[(z>2) & (z<5)]

[1] 3 4


but the double “&&” gives


> (z > 2) && (z < 5)

[1] FALSE

> z[(z > 2) && (z < 5)]

integer(0)#Probably not what you want


It’s all gone a bit pear shaped! In fact it could have been worse:


> (z > 2) && (z < 5)

[1] TRUE

> z[(z > 0) && (z < 5)]

[1] 1 2 3 4 5 6


Now you’ve the wrong answer and something that would be very tricky to spot. This is because R recylces the TRUE variable. Well from the R help page: “The longer form evaluates left to right examining only the first element of each vector” where the longer form refers to “&&”.  So


> (z > 2) && (z < 5)

[1] FALSE


is equivalent to:


> (z[1] > 2) & (z[1] < 5)

[1] FALSE


The same concept applies to the OR operator, “|”. To be honest, I’m not sure. I can think of a few contrived situations, but nothing really useful. The R help page isn’t that enlightening either. If anyone has suggestions please feel free to leave a comment and I’ll update this section. Note: if you read this post through R-bloggers, then you won’t get any of the updates/comments. You can either subscribe directly to the RSS feed for this blog, or occasionally check the web page . "	 0 Comments
My first Reproducible Research Compendium	https://www.r-bloggers.com/2010/12/my-first-reproducible-research-compendium/	December 14, 2010	Social data blog	"
 I have just completed my first Reproducible Research Compendium “Analysis of the combined survey datasets from the American Red Cross Tsunami Recovery Program Psycho-Social Project (adult community respondents)”. It is basically all the reports and data from all the work I did on evaluation psychosocial projects for the American Red Cross, bundled up. But one of the subfolders also contains the scripts and everything necessary to generate the final pdf report from the original datasets from scratch, in the spirit of transparency and “reproducible research”.  So there is no copying-and-pasting of graphics from one program into another. It is easy to make small but significant changes to the analysis – for instance, to exclude one of the constituent surveys by changing a line near the start of the script – and rerun the whole thing and produce a new corresponding version of the report. No more hunting about to find how you produced some particular graphic or table.               Permalink 

	| Leave a comment  »
 "	 0 Comments
Is it that stupid to make extremely long term forecast when studying mortality ?	https://www.r-bloggers.com/2010/12/is-it-that-stupid-to-make-extremely-long-term-forecast-when-studying-mortality/	December 14, 2010	arthur charpentier	"I received recently a comment by FCA (here) who raised an important question, about forecast in dynamic mortality models. (S)he mentioned that from his(her) point of view, the econometric models I considered were “good to predict for the next, say, 3 or 4 years. Not for the next 50
years…“. Which was the message I tried to stress last year in a conference about retirement in France (here). But from a quantitative point of view, how inconsistent were forecasts made 35 years ago, or 60 years ago ? Consider here the Lee Carter model, obtained on the periods 1816-1950 (in black below), 1816-1975 (in red) and 1816-2000 (in blue), unfortunately, it is difficult to compare ‘s since we have identifiability problems here. Nevertheless, we if consider affine transformation so that  ‘s are equal in 1900 and 1950 (say), we obtain On that graph, we considered an ETS (AAN) forecast. If we do not consider the entire series for forecasting, but only observations following WWI (1945), we obtain For sketches of the R code, Actually, it is not that bad.... even if it is only a qualitative intuition. Again, I am not a demographer, and my interest is more on actuarial science... so if we look at the estimation of annuities (still the same insurance contract, as here) for some insured of age 40 in 2000, we get the following graph (where forecasts  's were obtained on the complete series, i.e. from 1816 until the year we consider), (here it means that in 1900, I had to forecast mortality for someone of age 40 in 2000... so we had to forecast mortality with a 150 year horizon). Obviously, even if we are able to forecast improvement of mortality rates, it is not enough since it looks like, each year, improvement are alway higher than what what expected. Note that if we run it twice (since there might be problem with initial values in the econometric procedure) we obtain something similar, So, the output is consistent. And if we change the way we predict future values, e.g. on focusing only on the past 50 years, i.e.
 "	 0 Comments
RcppDE 0.1.0	https://www.r-bloggers.com/2010/12/rcppde-0-1-0/	December 13, 2010	Thinking inside the box	"
I worked on this on for a few evenings and weekends in October and November
and then spent a few more evenings writing a
paper / vignette
(which is finished as a very first draft now) 
about it. This was an interesting and captivating problem as I had worked on
genetic algorithms going back quite some time to the beginning and then again
the end of graduate school (and traces of that early work are near the bottom of my
presentations page).
So what got me started?
DEoptim is a really
nice package, but it is implemented in old-school
C.
There is nothing wrong with that per se, but at the same time that I was
wrestling with GAs, I also taught myself
C++ which, to put it
simply, offers a few more choices to the programmer. I like having those choices.

 
And with all the work that
Romain and I have put into
Rcpp, I was curious
how far I could push this cart if I were to move it along. 
I made a bet with myself starting from the old saw shorter, easier,
faster: pick any two.  Would it be possible to achieve all three of
these goals?

 
DEoptim, and I take
version 2.0-7 as my reference point here, is pretty efficiently yet
verbosely coded.  Copying a vector takes a loop with an assignment for each
element, copying a matrix does the same using two loops.  Replacing that with
a single statement in C++ is pretty easy.
We also have a few little optimisations behind the scenes here and there in
Rcpp: would all
that be enough to move the needle in terms of performance?
And the same time, 
DEoptim is also full
of the uses of the old R API which we often point to in the
Rcpp documentation
so fixing readibility should be a relatively low-hanging fruit.

 
To cut a long story short, I was able to reduce code size quite
easily by using a combination of 
C++ and
Rcpp idioms.  I was
also able to get to faster: the 
paper / vignette
demostrates consistent speed improvements on all setups that I tested (three
standard functions on three small and three larger parameter vectors).  More
important speed gains were achieved by allowing use of objective functions
that are written in C++
which again is both possible and easy thanks to 
Rcpp.

 
That leaves easier to prove: adding compiled objective functions is
one indication; further proof could be provided by, say, moving the inner
loop to parallel execution thanks to Open MP
which I may attempt over the next few months. So far I’d like to give myself about
half a point here. So not quite yet shorter, easier, faster: pick any three, but working on it.

 
Over the next few days I may try to follow up with a blog post or two
contrasting some code examples and maybe showing a chart from the vignette.




 "	 0 Comments
Adap’skiii [latest]	https://www.r-bloggers.com/2010/12/adap%e2%80%99skiii-latest/	December 13, 2010	xi'an	Just to point out there still is room for more participants to the Adap’skiii workshop! We have now reached 60 participants for this Utah workshop and would welcome more, quite obviously! All participants are also free to present a poster on the evening of the 4th, in the bar. 	 0 Comments
Machine Learning and Data Mining with R	https://www.r-bloggers.com/2010/12/machine-learning-and-data-mining-with-r/	December 13, 2010	David Smith	The San Francisco Bay Area ACM runs several courses on data mining and machine learning with R. Machine Learning 101 deals primarily with supervised learning problems, and Machine Learning 102 covers unsupervised learning and fault detection. Machine Learning 101 & 102 were most recently presented by Mike Bowles & Tricia Hoffman in September, and the lecture notes and class exercises are available for download. If you'd like to attend these classes in person, they'll run again at the Hacker's Dojo near San Francisco starting on January 22. The cost is $150 per person, and you can register here for Machine Learning 101/102. Machine Learning 101, deals primarily with supervised learning problems. Machine Learning 102 will cover unsupervised learning and fault detection. Both 101 and 102 begin at the level of elementary probability and statistics and from that background survey a broad array of machine learning techniques. The classes will give participants a working knowledge of these techniques and will leave them prepared to apply those techniques to real problems. To get the most out of the class, participants will need to work through the homework assignments. You can also register for Machine Learning 201/202 (starting on January 12) for a more in-depth exploration of these topics, as described below: Machine Learning 201 and 202 cover topics in greater depth than 101 and 102. Participants in the class should come away able to read the current literature and apply what they read to their own work. Machine Learning 201 and 202 can be taken in any order. Machine Learning 201 begins with ordinary least squares regression and extends this basic tool in a number of directions. We'll consider various regularization approaches. We'll introduce logistic regression and we'll learn how to code categorical inputs and outputs. We'll look at feature space expansions. These will lead naturally to generalizations of linear regression, known as the “generalized linear model” and the “generalized additive model”. Hacker Dojo Machine Learning 101 & 102: Fall 2010 Course Materials 	 0 Comments
Example 8.18: A Monte Carlo experiment	https://www.r-bloggers.com/2010/12/example-8-18-a-monte-carlo-experiment/	December 13, 2010	Ken Kleinman		 0 Comments
Video of Reproducible Research with R: Melbourne R Users 1st Dec 2010	https://www.r-bloggers.com/2010/12/video-of-reproducible-research-with-r-melbourne-r-users-1st-dec-2010/	December 13, 2010	Jeromy Anglim	I’d like to thank Pedro Olaya for filming the session, and Drew Conwayfor preparing, uploading, and hosting the video along with the many other great existing videos from R User Groups from around the world. If you follow Drew’s Blog ZIA you may have already seen the video. If you don’t follow his blog, you should definitely check it out. Here are three of my favourite posts by Drew: For those who did not catch Drew’s original post, the videos are shown below. For a copy of the slides, R Code, and more see the previous post.  The video is embedded below (If you are viewing this post in a feed reader,  and the embedded video is not displaying click hereto go to the original post):  The other talk from the session was by Geoff Robinson  who discussed several useful strategies for working with R. The slidesand R Scriptare available for download. The talk is embedded below:  Yuval Marom kindly asked me to become a co-organiser of the Melbourne R user Group (MelbURN). While most people who read this blog are from overseas, if you are in Melbourne, feel free to join the group. If you know something about R, are passing through Melbourne,   and would like to give a talk on R,  let Yuval or me know (see the Contact Us Button),  and we’ll see if we can organise something.  	 0 Comments
Some quibbles about “The R Book” by Michael Crawley	https://www.r-bloggers.com/2010/12/some-quibbles-about-%e2%80%9cthe-r-book%e2%80%9d-by-michael-crawley/	December 13, 2010	Pat	"A friend recently bought The R Book and I said I would tell him of problems that I’ve noticed with it.  You can eavesdrop. The word “library” is used instead of “package”.  This (common)  error substantially raises the blood pressure of some people — probably to an unwarranted extent. An R package is a group of functions, data and their documentation.  These are the things that are in repositories like CRAN (where there are over two thousand packages).  A package is installed onto your machine into a library. You are unlikely to call a book a library; don’t call a package a library. Part of the problem is that packages are attached with the library function: > library(fortunes) That is why some instructions have you do the same thing via:   > require(fortunes) Some of the people whose blood pressure is abnormally raised by seeing this mistake are very important to R, so please get this right. An example value is: 3.9+4.5i that is, a complex number.  This is in the chapter called “Essentials of the R Language”.  I’ve been using R and a language not unlike R for a quarter century.  The only time I recall using complex numbers is when documenting them.  Complex numbers don’t match my definition of “essential”. There is a certain amount of irony for a 600-word blog post to take n lines to complain about a 900-page book wasting one line.  However, the complex number is an extreme example of a common occurrence in the chapter.  There is a lot of the chapter that I don’t find particularly essential. My take on “essential” is Some hints for the R beginner. A
B These are two examples of a general feature: while the author’s keyboard seems to work perfectly fine for text, the space-bar is mysteriously broken for R code. It is clearer to write these as:   > A 
> B  The assignment arrow shows up as a separate entity.  Spacesaidunderstanding. The same thing, but this time it’s serious. x<5 really, really should have spaces around the less-than operator. There is no trouble with this particular example, but what if the example were with minus five? x does not give you a logical vector with TRUE values when x is less than minus five.  It changes x to have the single value 5. This and a whole bunch of other  R gotchas are in The R Inferno. The values in the body of a matrix can only be numbers. That is a false statement.  In particular, if x is a numeric matrix, then the result of x < -5 is a matrix of logical values (and is the same dimension as x). This be praise, not quibble. The book uses “explanatory variables” and “response” in the statistical regression context.  It doesn’t enter into the dependent-independent muddle. Amazon has several reviews of The R Book. There is a range of opinions from very positive to quite negative.  A common complaint is that the material is disorganized. The points I have raised are from a quick glance through the book.  Are there other things in the book that should be pointed out to help the unwary? I don’t think there is such a thing as the best book on R.  There can be the best book on R for you as an individual.  Which one is the best will depend on where you are and where you want to go.  A partial list of your choices is Books related to R. "	 0 Comments
Ghcn V3 Metadata improvements	https://www.r-bloggers.com/2010/12/ghcn-v3-metadata-improvements/	December 12, 2010	Steven Mosher	"The Global Historical Climate Network  (GHCN) is in it’s beta stage. On of the stated goals of the project is to improve the metadata that is provided for the station data.  Over the past few months several independent volunteers have been focusing on the issue of station metadata, each with their own focus. Ron Broberg deserves credit for taking the lead with applying GIS tools to the issue and Peter O’neill deserves credit for his station by station review of GISS inventories. A couple other folks are busy at work and I will leave it to them to discuss their efforts when the time is appropriate as the publication process precludes them from talking openly about it. Here, my main focus has been on GHCN and more recently GHCN V3. The goal of the project is to provide a more accurate and more comprehensive inventory of station locations and station metadata.  A short recap of the importance of this. Imagine, if you would, an inventory of 2000 stations. We suspect that some are urban and some are rural. We want to estimate the difference between the urban and the rural with an eye toward assessing the impact of UHI on the record.  Further let’s suppose that the difference is large, suppose that urban warms are 1C per century ( from 1900 to 2010) while rural shows 0C warming. In that case the rule we use to separate urban from rural, while important, is not critical. For example, if we mis identify some urban sites as rural ( say 10%) then some fraction of urban warming will  ”infect” the rural subset. The effect of urbanization will still be clear in our comparison. If our categorization is less accurate, say we get 50% of the urban wrong, then our ability to discriminate the signal will be reduced according. If we also mis label rural sites as urban, the effect will be compounded. We can see then that if the UHI effect is small, the need for a better discrimination function increases.  For example, if we think that the urban stations have warmed .8C over the course of 1900-2009 (+-.05C) while the rural have only warmed, say .6C (+-.05) misidentification will have more impact on our ability to find that UHI signal. One approach would be to take the 2000 stations and divide them into 3 groups. extreme rural, extreme urban, and “mixed”. This would, of course reduce the number of stations and the signal could then be lost in the noise that results from fewer stations. Still, that result would indicate that the UHI  effect is small. That happens to be my position.  Proving that, however, requires a diligent look at the metadata. Metadata Improvements: Data file is in the Box: named ExtV3Metadata.inv, a csv file is also included. here The station information presented here is still in the beta stage. But it’s ready for a public release and some initial comments on what we can tell:  The process of improving the data and extending it is described below. Sources: 1. GHCN V3 Inventory. 2. Updated WMO station locations 3. Nightlights as used by Hansen 2010 4. Improved Nightlights as recommended by Nightlights principle Investigator 5. Nightlight Buffers 6. Gridded population density as provided by GPW 7. Gridded historical population density as provided by Hyde 8. Gridded Population Density as provided by GRUMP 9. Gridded Impervious Surface area. 10. Land masks provided by several sources. Step One: The beta version of the GHCN v3 inventories are read into a R data frame. For the posted file that inventory was the matching inventory for the “adjusted” dataset. This inventory includes only 7279 stations as one appears to be dropped from the unadjusted data.  The data fields read in include Id:  The Id field is the GHCN ID. It’s an 11 digit index of the form cccwwwwwddd. Where ccc indicates a country code, wwwww, indicates a WMO code, and ddd indicates a    IMOD number. There are several things to note. For the US stations, the WMO code does not appear to map to the WMO master list. For example, “42500046506″ is listed as the GHCNID of Orland California. In GHCN v2 the ID for ORLAND is :  42572591004 ORLAND. And for USHCN it is:046506-02 For   For WMO we have no entry for ORLAND. In V2 Orland was listed according the WMO number for nearby Red Bluff. The 004 in the Orland IMOD indicates that Orland is at a different location than the WMO it is reported under. Confusing? You bet. To be accurate the V3 readme will have to be changed to indicate that the new GHCN Id, does not reflect WMO numbers in the middle 5 digits in all cases. In the US, the USHCN ID is used as the last 5 digits. Basically there are USHCN stations that do not have WMO numbers. In V2 they were listed as IMODs of the closest WMO ( redbluff) in V3 they are listed according to their USHCN number. That makes comparing V2 to V3 a bit troublesome. Lat: The latitude of the station is reported in degrees north from -90 to 90. In my inventory   the value  is one of two values: the value found in GHCN V3 or the value found in the recently updated WMO master list. The WMO has required countries to update the precision of the station location data and  that process is underway. It’s not entirely complete. Consequently some of the GHCN V3 station locations remain the same. Those that have been updated by WMO are updated here Lon: Longitude is degrees east, from -180 to 180. As with Latitude this field contains the corrections from the recent WMO updates. Altitude: Altitude in meters from the Ghcn V3 inventory. Corrected altitudes from the WMO master list are not included here. That will come later. Name: The station name from the GHCN V3 inventory. I am in the laborious process of cleaning up the name list to remove the following: country names, state designations, province designations, partial names, punctuation marks. The goal would be to have a list of names as well as alternative names. Countries, states, provinces can be added properly by geocoding and should not be in the station name field. GridEl:  Grid elevation. As  taken from the GHCN inventory data. In meters this represents the average elevation of the grid at .5degrees. Once the position data is improved this could be supplanted with more accurate metadata from DEMs. Rural: A designation R,S,U that indicates whether the station is Rural, Small Town or Urban. This characterization is made based on the population of the nearest town, where R is a town with less than 10K people and Urban is greater than 50,000. This is a dated measure of urbanity. It’s problematic because it does not tell us whether the town is densely populated or spread out. Population: The population of the nearest town in 1000s. Topography: type of topography in the environment surrounding the station, (Flat-FL,Hilly-HI,Mountain Top-MT,Mountainous Valley-MV). Vegetation:type of vegetation in environment of station if station is Ruraland when it is indicated on the Operational Navigation Chart (Desert-DE,Forested-FO,Ice-IC,Marsh-MA). Coastal: An indication if the site is a Coastal location (CO) or near a lake (LA) or more than 30km away from water. DistanceToCoast: In the site is close to water this field indicates the distance in km. Airport: a true false flag for whether the station is at an airport or not. This has not been corrected using WMO data, but there are discrepancies. DistanceToTown: Distance in km for the airport NDVI: Normalized Difference Vegetative index. This field indicates the type of vegetation in the area. Its the original V3 data and should be supplanted with improved data. Light_Code: while the V3 read me does not include or explain this data, it was present in V2. Bascially it is an undocumented description of the sites urbanity Step Two. In the second step the updated WMO master list is merged with GHCN V3. This is not straightforward. First the GHCN list must be reduced to those stations that are not IMODs. Where the GHCN ID is  cccwwwwwddd, the ddd field must be 000. Next the WMO file must be trimmed as well. It has multiple entries for stations. The multiple stations represent “air stations” that are collocated with the ground station. In the WMO index this is indicated by an indexSubNbr = 1. Next the GHCN V3 file is merged with The WMO file based on WMO   ID.  After this is completed distances can be calculated and the names can be checked for consistency. That process results in the following fields: WmoName : The name used by the WMO is recorded. In certain cases the WMO name is spelled differently. In some cases it is entirely different.  WmoLon: The Longitude given by the updated WMO master list. These updates are in progress. Some mistakes remain as memeber nations are delivering partial results. The data is supposed to be accurate to degrees, mintutes and seconds.  WmoLat:the latitude given by the updated WMO master list. These updates are in progress. Some mistakes remain as memeber nations are delivering partial results. The data is supposed to be accurate to degrees, mintutes and seconds. GhcnDistance: The distance between the old loaction given by GHCN V3 and the new location. As calculated by a Haversine distance calculation  NameMatch: A true false flag indicating if the name matched using a rather lax fuzzy name match criteria  GhcnLon: The Legacy longitude. This is the Longitude from the source GHCN V3 file.  GhcnLat: The legacy Latitude Step Three: In step three the corrected inventory is passed to a metadata compilation function.  The lon lat is passed in and metadata associated with those positions is passed out, along with the LON and LAT passed in for consistency checking  Lon  : corrected Longitude same as field 1  Lat : corrected Latitude same as field 2 
 LandWater: The fraction of land in the 1/4 degree grid cell surrounding the station. This includes inland water.  LandOcean :  The fraction of land in the 1/4 degree grid cell surrounding the station. This includes only ocean water.  CoastDistance: The distance the station is from the coast. If the station is over land this should equal 0.  If a station is in the water it returns the distance to the closest coast. This occurs when coastal stations or island stations are misplaced. The accuracy of the coast map is 30 arc seconds. Lights: The value of nightlights using the same file that Hansen2010 uses. It should be noted that this file has been deprecated by the file creators. It represents nightlights at the station in the 1995-97 era.  LightsF16: The value of nightlights using the most recent analysis from 2006.  The raw data in the file has been processed to produce a DN number according to the file readme.  Bright3km: Every LightsF16 field surrounding the station has been processed to extract the brightest pixel within 3km.  Given that Nightlights positional accuracy is ~1-2km, a station with perfect location information may still be mis registered with the image because of positional errors in the nightlights data.  Bright5km: same as above with a 5km radius  Bright10km same as above with a 10km radius  Bright20km same as above with a 20km radius  Isa: Impervious surface percentage. The percentage of impervious surfaces estimated from 0-100% A negative number indicates the station is in the water. ISA is the result of a regression and is based on Nightlights data and Landscan population.
 GpwDensity : population density ( humans per square km) from the GPW source GDensity :population density ( humans per square km) from the GRUMP source The following fields are derived from the HYDE historical population/land use project which is being used for Ar5. The figure is density of humans per sq km. Figures are given for every decade. The data has been  processed from 5 minute data. Pop1850 ,Pop1860,  Pop1870, Pop1880, Pop1890,Pop1900,Pop1910,Pop1920
 Pop1930,Pop1940,Pop1950,Pop1960,Pop1970 ,Pop1980,Pop1990,Pop2000 GrumpUrban: A flag indicating where the site is Urban (2) rural(1) or in water (0) "	 0 Comments
Using R for Introductory Statistics, Chapter 4	https://www.r-bloggers.com/2010/12/using-r-for-introductory-statistics-chapter-4/	December 12, 2010	Christopher Bare	Chapter 4 of Using R for Introductory Statistics gets us started working with multivariate data. The question is: what are the relationships among the variables? One way to go about answering it is by pairwise comparison of variables. Another technique is to divide the data into categories by the values of some variables and analyze the remaining variables within each category. Different facets of the data can be encoded with color, shape and position to create visualizations that show graphically the relationships between several variables. Taking variables one or two at a time, we can rely on our previous experience and apply our toolbox of univariate and bivariate techniques, such as histograms, correlation and linear regression. We can also hold some variables constant and analyze the remaining variables in that context. Often, this involves conditioning on a categorical variable, as we did in Chapter 3 by splitting marathon finishing time into gender and age classes. As another example, the distribution of top speeds of italian sports cars describes a dependent variable, top speed, conditioned on two categorical variables, country of origin (Italy) and category (sports car). Because they’re so familiar, cars make a great example. R comes with a dataset called mtcars based on Motor Trend road tests for 32 cars in the 1973-74 model year. They recorded 11 statistics about each model of car. We can get a quick initial look using the pairs function which plots a thumbnail scatterplot for every pair of variables. Pairs is designed to work on numbers, but they’ve coded categorical values as integers in this data, so it works. Question 4.7 asks us to describe any trends relating weight, fuel efficiency, and number of cylinders. They also make the distinction between American made cars and imports, another categorical value along with cylinders. Let’s make a two-panel plot. First, we’ll make a boxplot comparing the distribution of mileage for imports and domestics. In the second panel, we’ll combine all four variables. Domestics fair worse, but why? For one thing, the most fuel efficient cars are all light imports with 4 cylinder engines. Domestic cars are heavier with bigger engines and get worse milage. Other factors are certainly involved, but weight does a pretty good job of explaining fuel consumption, with a correlation of almost 87%. Of course, fuel economy is not all there is to life on the open road. What about speed? We have quarter mile times, which probably measure acceleration better than top speed. We might hunt for variables that explain quarter mile performance by doing scatterplots and looking for correlation. The single factor that best correlates with qsec is horsepower. But a combination of factors does noticeably better - the power to weight ratio. The numbers above the scatterplots are correlation with qsec. Using position, character, color, multi-part graphs and ratios we've managed to visualize 5 variables, which show increasingly good correlation with the variable we're trying to explain. Here's one theory that might emerge from staring at this data: 4-bangers with automatic transmission are slow. Here's another theory: there's an error in the data. Look at the slow 4 cylinder way at the top. It's quarter mile is nearly three seconds longer than the next slowest car. An outlier like that seems to need an explanation. That car, according to mtcars, is the Mercedes 230. But, the 230 is listed right next to the 240D - D for diesel. The 240D is a solid car. Many are still running. But they're famously slow. What are the odds that the times for these two cars got transposed? We can check if the data supports our theories about how cars work. For example, we might guess that car makers are likely to put bigger engines in heavier cars to maintain adequate performance at the expense of gas mileage. Comparing weight with numbers of cylinders in the scatterplot above supports this idea. Displacement measures the total volume of the cylinders and that must be closely related to the number of cylinders. Try plot(disp ~ as.factor(cyl), data=mtcars). Displacement and carburetors are big determinants of the horsepower of an engine. Statisticians might be horrified, but try this: plot(hp ~ I(disp * carb), data=mtcars). Multiplying displacement by carburetion is a quick and dirty hack, but in this case, it seems to work out well. Chapter 4 introduces parts of R's type system, specifically, lists and data.frames, along with subsetting operations and the apply family of functions. I don't go into it here because that was the first thing I learned about R and if you're a programmer, you'll probably want to do the same. One thing the book doesn't cover at all, so far as I can tell, is clustering. Take a look at the plots above and see if you can't see the cars clustering into familiar categories: the two super-fast sports cars, the three land-yacht luxury cars weighing in at over 5000 pounds a piece, the 8-cylinder muscle cars, and the 4-cylinder econo-beaters. We don't have to do this by eye, because R has several clustering algorithms built in. Hierarchical clustering (hclust) works by repeatedly merging the two most similar items or existing clusters together. Determining 'most similar' requires a measure of distance. In general, coming up with a good distance metric takes careful thought specific to the problem at hand, but let's live dangerously. Not bad at all. The scale function helps out here by putting the columns on a common center and scale so the dist function ends up giving equal weight to each variable. It's easy to analyze the bejeezes out of something you already you already know, like cars. The trick is to get a similar level of insight out of something entirely new. 	 0 Comments
Academic Jargon: Field-Specific Insults	https://www.r-bloggers.com/2010/12/academic-jargon-field-specific-insults/	December 12, 2010	John Myles White	Every academic field seems to develop a set of generic insults based on their intellectual toolkit. Here are two examples I hear often: Do any readers have good examples from other fields? 	 0 Comments
Visualizing Agricultural Subsidies by Kentucky County	https://www.r-bloggers.com/2010/12/visualizing-agricultural-subsidies-by-kentucky-county/	December 12, 2010	Matt Bogard		 0 Comments
R with Vim on Mac OS X	https://www.r-bloggers.com/2010/12/r-with-vim-on-mac-os-x/	December 12, 2010	Jon	"The built-in script editor for the Mac OS X R GUI actually isn’t bad. In fact it is much better than its Window’s counterpart. In particular, it has:  However, when coding in R and pretty much any other language, Vim has always been my goto text editor of choice.  Once you get past the fairly steep learning curve, nothing comes close to it in terms of coding efficiency and navigation except perhaps Emacs, but let’s not go there. Michael Bojanowski wrote a blog post on using R with Vim on an Ubuntu machine.  One commenter asked about getting it to work on Mac OS X. First of all, I highly recommend the MacVim port of Vim (not to be confused with macvim.org).  You can get the latest version (7.3.53 at time of post) at github: https://github.com/b4winckler/macvim/downloads The Vim-R-plugin that Michal mentioned works on OS X with the Screen plugin as well.  However, a simpler method is to use one of the following two OS X specific scripts that rely on AppleScript. R.vim
 R.vim uses AppleScript to send selected lines of code to an R buffer. Simply hit <F3> to run the selected lines in R.  If R is not open, it will automatically open a new session. Note that it opens R.app instead of R64.app so if you need to run the 64-bit version you’ll have to tinker with the script a bit. R-MacOSX
R-MacOSX is also pretty straight forward, although I couldn’t get it working properly. It defaults to <Cmd-E> to run selected lines and <Shift-Cmd-E> to source the entire file. In either case, key mappings are customizable but personally, I have no trouble with <F3> to execute code using R.vim.  Neither are as full-featured as Vim-R-plugin but I find them sufficient as all I really need is to send blocks of code to R. Happy Vimming! "	 0 Comments
R Code Example for Neural Networks	https://www.r-bloggers.com/2010/12/r-code-example-for-neural-networks/	December 12, 2010	Matt Bogard	 	 0 Comments
Load R packages…directly from cran if needed	https://www.r-bloggers.com/2010/12/load-r-packagesdirectly-from-cran-if-needed/	December 12, 2010	bridgewater		 0 Comments
White Bull, An Algorithm in R	https://www.r-bloggers.com/2010/12/white-bull-an-algorithm-in-r/	December 11, 2010	Milk Trader		 0 Comments
Keeping R libraries in sync between different computers using Dropbox	https://www.r-bloggers.com/2010/12/keeping-r-libraries-in-sync-between-different-computers-using-dropbox/	December 11, 2010	Social data blog	"
	We have a few computers including laptops in our network which all use R (r-project.org) for statistics. We use Dropbox to keep all our files in sync and we are all on ubuntu.         Permalink 

	| Leave a comment  »
 "	 0 Comments
socialR: Reproducible Research & Notebook integration with R	https://www.r-bloggers.com/2010/12/socialr-reproducible-research-notebook-integration-with-r/	December 10, 2010	Carl	I’ve created an R package that uses social media tools for reproducible research.  The goal of the package is this: whenever I run a code, output figures are automatically added to my figure repository (Flickr), linked to the timestamped version of the code that produced them in the code repository.  Figures should be tagged by project and be embedded selectively or automatically into this lab notebook.  The basic workflow of the notebook looks like this:[ref]Diagram of my notebook as presented at Science Online, 2011, see other slides in my entry on this.[/ref] To do this, I use a few simple R functions that I wrap around  the system command-line programs git, flickr_upload, and hpc-autotweets to enable monitoring of my simulations through social media. The package has it’s own git repository here.  This is a rather custom development to make for rapid deployment on my own machines, and depends largely on Linux tools external to R, so it may not be easily deployed by others.  See my earlier post, Making R Twitter, for examples and back story. All of these tasks are run by wrapping any plot command with my command “social_plot()” (See link for more detailed instructions) Current program relies entirely on external command-line tools. Probably no easy solution to make this package self-contained and cross platform.  Still, a good bit of functionality can be added: 	 0 Comments
Confidence bands with lattice and R	https://www.r-bloggers.com/2010/12/confidence-bands-with-lattice-and-r/	December 10, 2010	Oscar Perpiñán Lamigueiro	If you use lattice with R, and you need to plot confidence limits in your graphic, then panel.smoother and panel.quantile from latticeExtra will help you with this task. These functions internally calculate the error bounds and use panel.polygon from lattice. If you need to plot your own confidence limits, then you have to define a panel function. A suitable code is proposed here by Deepayan Sarkar, the developer of lattice: Then you can plot your data.frame, named “data” with: where “data$high” and “data$low” are the error bounds of your data. You can also use the glayer function from latticeExtra if you do not feel confortable with panel functions inside xyplot. In this article, I have used this tool to display confidence limits of the wavelet variance calculated with wmtsa, and I got the next figure:  You will find here the R code used for this article. 	 0 Comments
R at Google	https://www.r-bloggers.com/2010/12/r-at-google/	December 10, 2010	Josh Paulson	Last night, Ni Wang and Max Lin from Google gave a talk to the New York R User Group discussing how R is used inside Google. About 150 R developers attended the meeting. Ni and Max said that R is used very widely at Google and is an integral part of the analytics work they do.  One interesting application is the Google Flu Trends project, which uses R to estimate current flu activity based on Google search results. Google Trends aggregates user search queries showing how often a particular word or phrase has been searched. Correlation tests are run on the search results to obtain a manageable data set of potentially relevant variables. Then using R, they massage the data and create models with optimized weights for each search term. From this, they are able to reasonably estimate current flu activity for different regions around the world. When Google uses R in a production environment, they often work with very large data sets. For this, Google integrates R with several internal technologies including gfs, BigTable and ProtoBuf (using the RProtoBuf package). They said their internal system for analyzing large data sets worked in a manner very analogous to the R snow package. Google also announced an R client for the Google Prediction API (a service which accesses Google’s machine learning algorithms to analyze historic data and predict future outcomes). The R client is available here: http://code.google.com/p/google-prediction-api-r-client/  Final note, Google has published an R Style Guide which may be of interest for those seeking a set of standards for R coding: http://google-styleguide.googlecode.com/svn/trunk/google-r-style.html 	 0 Comments
New edition of “R Companion to Applied Regression” – by John Fox and Sandy Weisberg	https://www.r-bloggers.com/2010/12/new-edition-of-r-companion-to-applied-regression-%e2%80%93-by-john-fox-and-sandy-weisberg/	December 10, 2010	Tal Galili	 Just two hours ago, Professor John Fox has announced on the R-help mailing list of a new (second) edition to his book “An R and S Plus Companion to Applied Regression”, now title . “An R Companion to Applied Regression, Second Edition”. John Fox is (very) well known in the R community for many contributions to R, including the car package (which any one who is interested in performing SS type II and III repeated measures anova in R, is sure to come by), the Rcmdr pacakge (one of the two major GUI’s for R, the second one is Deducer), sem (for Structural Equation Models) and more.  These might explain why I think having him release a new edition for his book to be big news for the R community of users. In this new edition, Professor Fox has teamed with Professor Sandy Weisberg, to refresh the original edition so to cover the development gained in the (nearly) 10 years since the first edition was written. Here is what John Fox had to say: Dear all, Sandy Weisberg and I would like to announce the publication of the second edition of An R Companion to Applied Regression (Sage, 2011). As is immediately clear, the book now has two authors and S-PLUS is gone from the title (and the book). The R Companion has also been thoroughly rewritten, covering developments in the nearly 10 years since the first edition was written and expanding coverage of topics such as R graphics and R programming. As before, however, the R Companion provides a general introduction to R in the context of applied regression analysis, broadly construed. It is available from the publisher at (US) or (UK), and from Amazon (see here) The book is augmented by a web site with data sets, appendices on a variety of topics, and more, and it associated with the car package on CRAN, which has recently undergone an overhaul. Regards, John and Sandy  If you are interested in more information about “An R Companion to Applied Regression, Second Edition”, you can head over to the books web page, here’s some of the highlights of what you will find there: 	 0 Comments
LaTeX Typesetting – Document Structure	https://www.r-bloggers.com/2010/12/latex-typesetting-%e2%80%93-document-structure/	December 10, 2010	Ralph	Following on from the initial post about creating a document using LaTeX we need to consider the structure of the document, i.e. headings and page layout. Fast Tube by Casper Document Class The document class is a template that specifies the appearance of different components of a document, e.g. the font and size of headings. The most commonly used classes are article, which corresponds to a typical layout for a journal article, report or book for longer documents and letter. When selecting a document class it is also possible to provide some options, including the size of the font – 10pt, 11pt or 12pt. We could also use the twocolumns option to had two columns of text. The paper size can also be stated in the options – a4paper etc. Page Style There are some basic page styles available in the main document classes. These include plain which has the page number in the footer only or empty where there is nothing in the header or footer. These can be changed during a document, e.g. there might be one page that has a picture but you don’t want the page number to be shown as it might obscur some other useful information. Page Numbering The default option is arabic numerals, but we could also use (upper or lower case) Roman numerals in the front part of the document before reverting to using the Arabic numerals. Parts of a Document A document can be divided into various pieces, known as chapters, sections, subsections and so on. To do this in LaTeX we use the commands chapter, section and subsection with the name of the section in curly brackets. Chapters appear in books and reports but not in the article class, while the letter class is a lot simpler than these other classes. In an article the numbering for sections is 1, 2, 3 and so on and the subsections are 1.1, 1.2, 1.3 and onwards. Other useful resources are provided on the Supplementary Material page. 	 0 Comments
An R interface to the Google Prediction API	https://www.r-bloggers.com/2010/12/an-r-interface-to-the-google-prediction-api/	December 10, 2010	David Smith	An the New York R User Group* last night, 100 R users heard Ni Wang and Max Lin talk explain how “R is one of the important tools used by analysts and engineers at Google for analyzing data”. During the talk, Lin revealed that Google plans to make “R more integrated with internal machine learning algorithms and infrastructure”, and one component of that plan was announced at the meeting: a new library for R to build and score models using the Google Prediction API. The Google Prediction API is a black-box system for building predictive models. Given a set of training data (a set of continuous and/or categorical explanatory variables and a dependent variable), the Google algorithms automatically selects from several available machine learning techniques create a model from the training model. Then later, given a set of explanatory variables, you can predict the value of the dependent variable under this model. Now with the googlepredictionapi R package (which you can download from Google Code), you can create such models based on data stored in a local CSV file or in the Google Storage system. The model is represented as an object in R, which you can then use to make predictions using the standard predict function, as illustrated in the following code: You need to request access to the Google Prediction API to use this package (instructions how to request are here). Anyone tried this out yet? Given that all the standard statistical (as distinct from machine language) models are in R, this package would make it easy to compare the performance of the automated Prediction API with more traditional statistical techniques. [*] The New York R User Group is proudly sponsored by Revolution Analytics. New York R User Group: R at Google (via) 	 0 Comments
Interesting volatility measurement	https://www.r-bloggers.com/2010/12/interesting-volatility-measurement/	December 10, 2010	kafka	Long time ago I stumbled across interesting volatility measurement at  quantifiableedges.blogspot.com. The idea is following: take 3-day historical volatility of S&P 500 index and divide that by 10-day historical volatility. Then mark all points which are less that 0.25 and measure the volatility of 3 following days. On average, the volatility of following 3 days will be 5 times higher. I was tweaking the result to squeeze some profit, but not so much luck. Basically, you need to trade either VIX index derivatives or S&P 500 index options to get direct impact. Before doing that, you need to test historical performance. Unfortunately, I don’t have data for these instruments. What about ETF, like VXX? Nope, because only few data points in the testing sample. Later, I will try to incorporate GARCH model to see if this going to help. Any fresh ideas on this? 	 0 Comments
R: Basic R Skills – Splitting and Plotting	https://www.r-bloggers.com/2010/12/r-basic-r-skills-splitting-and-plotting/	December 10, 2010	Stewart MacArthur		 0 Comments
Once again, chart critics and graph gurus welcome	https://www.r-bloggers.com/2010/12/once-again-chart-critics-and-graph-gurus-welcome/	December 10, 2010	dan	"HOW TO DISPLAY A LINE PLOT WITH COUNT INFORMATION?  In a previously-mentioned paper Sharad and your DSN editor are writing up, there is the above line plot with points. The area of each point shows the count of observations. It’s done in R with ggplot2 (hooray for Hadley). We generally like this type of plot, however, we are concerned about whether it gives people a good sense of the relative counts or not. Ask yourself this:
1) If the area of the big circle represents 1,000 observations, how many observations does the second-biggest circle represent?
2) If the area of the second-biggest circle represents as many observations as you just said, how many observations does the third-biggest circle represent? Write down your answers. There’s a form to enter them in below. Now have a look at this one:  Same two questions: 3) If the area of the big circle represents 1,000 observations, how many observations does the second-biggest circle represent?
4) If the area of the second-biggest circle represents as many observations as you just said, how many observations does the third-biggest circle represent? Kindly Record your answers here or use the embedded form below (if it is visible for you). Loading… Watch this space for the exciting answer! If anyone has good ideas on presenting count information in a chart that relates an ordinal X and a continuous Y, please let us know. "	 0 Comments
Truly random [again]	https://www.r-bloggers.com/2010/12/truly-random-again/	December 9, 2010	xi'an	“The measurement outputs contain at the 99% confidence level 42 new random bits. This is a much stronger statement than passing or not passing statistical tests, which merely indicate that no obvious non-random patterns are present.” arXiv:0911.3427 As often, I bought La Recherche in the station newsagent for the wrong reason! The cover of the December issue was about “God and Science” and I thought this issue would bring some interesting and deep arguments in connection with my math and realism post. The debate is very short, does not go in any depth. reproduces the Hawking’s quote that started the earlier post, and recycles the same graph about cosmology I used last summer in Vancouver! However, there are alternative interesting entries about probabilistic proof checking in Mathematics and truly random numbers… The first part is on an ACM paper on the PCP theorem by Irit Dinur, but is too terse as is (while the theory behind presumably escapes my abilities!). The second part is about a paper in Nature published by Pironio et al. and arXived as well. It is entitled “Random numbers certified by Bell’s Theorem” and also is one of the laureates of the La Recherche prize this year. I was first annoyed by the French coverage of the paper, mentioning that “a number was random with a probability of 99%” (?!) and that “a sequence of numbers is  perfectly random” (re-?!). The original paper is however stating the same thing, hence stressing the different meaning associated to randomness by those physicists, “the unpredictable character of the outcomes” and “universally-composable security”. The above “probability of randomness” is actually a p-value (associated with the null hypothesis that Bell’s inequality is not violated) that is equal to 0.00077. (So the above quote is somehow paradoxical!) The huge apparatus used to produce those random events is not very efficient: on average, 7 binary random numbers are detected per hour… A far cry from the “truly random” generator produced by Intel! Ps-As a concidence, Julien Cornebise pointed out to me that there is a supplement in the journal about “Le Savoir du Corps” which is in fact handled by the pharmaceutical company Servier, currently under investigation for its drug Mediator… A very annoying breach of basic journalistic ethics in my opinion! 	 0 Comments
Illustrating CFAs – Graphviz	https://www.r-bloggers.com/2010/12/illustrating-cfas-graphviz/	December 9, 2010	gerhi	So after yesterdays post you probably ran this fancy new confirmatory factor analysis (CFA) – showed your friends all the cool fit stats and… nothing. As important as doing things right is being able to let others know that. For CFA the method of choice to illustrate the connections between variables are path diagrams these are best generated using graphviz.   The example above is generated by the following code: digraph HSCFA { Factor_1 -> y1 [weight=1000, label=”0.80″]; Factor_1 -> y2 [weight=1000, label=”0.80″]; Factor_1 -> y3 [weight=1000, label=”0.80″]; Factor_1 -> y4 [weight=1000, label=”0.80″]; Factor_2 -> y5 [weight=1000, label=”0.80″]; Factor_2 -> y6 [weight=1000, label=”0.80″]; Factor_2 -> y7 [weight=1000, label=”0.80″]; Factor_2 -> y8 [weight=1000, label=”0.80″]; Factor_1->Factor_2 [dir=both”]; y1 [shape=box,group=”obsvar”]; y2 [shape=box,group=”obsvar”]; y3 [shape=box,group=”obsvar”]; y4 [shape=box,group=”obsvar”]; y5 [shape=box,group=”obsvar”]; y6 [shape=box,group=”obsvar”]; y7 [shape=box,group=”obsvar”]; y8 [shape=box,group=”obsvar”]; { rank = same; y1; y2; y3; y4; y5; y6; y7; y8} { rank = same; Factor_2; Factor_1; } { rank = max; d1; d2; d3; d4; d5; d6; d7; d8} d1 -> y1; d1 [shape=plaintext,label=””]; d2 -> y2; d2 [shape=plaintext,label=””]; d3 -> y3; d3 [shape=plaintext,label=””]; d4 -> y4; d4 [shape=plaintext,label=””]; d5 -> y5; d5 [shape=plaintext,label=””]; d6 -> y6; d6 [shape=plaintext,label=””]; d7 -> y7; d7 [shape=plaintext,label=””]; d8 -> y8; d8 [shape=plaintext,label=””]; } If you are using the sem package to fit the SEM, you can generate the code for graphviz that will plot the model with standardized estimates detailed in this post. 	 0 Comments
Choosing colors for your charts with RColorBrewer	https://www.r-bloggers.com/2010/12/choosing-colors-for-your-charts-with-rcolorbrewer/	December 9, 2010	David Smith	If you're creating a bar chart in R, how do you decide what colors the bars should be? Or if you're creating an image plot, what range of images should you use? The colors you choose can not only affect the viewer's interpretation of the graphic, it can also determine its aesthetic appeal, too. That's where the RColorBrewer package comes in: it helps you choose a selection of visually distinct (as for bar charts) or a continuously varying (as for image plots) range of colors to make your charts more meaningful and more appealing. Stewaer MacArthur of CompBiomeBlog demonstrates (with working R code) how to use RCodeBrewer to generate color scales like this:  and use them in your R charts.  compBiomeBlog: R: Using RColorBrewer to colour your figures in R 	 0 Comments
New version of solaR (0.21)	https://www.r-bloggers.com/2010/12/new-version-of-solar-0-21/	December 9, 2010	procomun	The version 0.21 of the solaR package is now available at CRAN. This package provides a set of calculation methods of solar radiation and performance of photovoltaic systems. The package has been uploaded to CRAN under the GPL-3 license. solaR is now able to calculate from both daily and sub-daily irradiation values. Besides, there are more changes: NEWS. Anyway, you are invited to read the introduction vignette in order to learn the most important funcionalities of solaR.  For example, the Measurement and Instrumentation Data Center of the NREL (NREL-MIDC) provides meteorological data from a variety of stations. We will try the La Ola – Lanai station at Hawaii. We have to change the names of the columns and calculate the horizontal direct irradiation, since only the normal direct irradiation is included in the file. The datalogger program runs using Greenwich Mean Time (GMT), and data is converted to Hawaiian Standard Time (HST) after data collection. With local2Solar we can calculate the Mean Solar Time of the index. Therefore, the  object Meteo is obtained with: With this data, a G0 object can be calculated. First, the direct and diffuse components of the data are used (corr=’none’): If these components were not available, a fd-kt hourly correlation is needed.. For example, we can use the model proposed by Ridley et al.:  Another example: you can compare the losses of a PV system with different configurations. First, we define the irradiation and temperature conditions: Then, we can calculate the energy produced by three different PV systems with a two-axis tracker, an horizontal N-S axis tracker and a fixed system. Last, we calculate the losses of each system and compare them: with this result:  	 0 Comments
All together now – Confirmatory Factor Analysis in R	https://www.r-bloggers.com/2010/12/all-together-now-confirmatory-factor-analysis-in-r/	December 8, 2010	gerhi	Describing multivariate data is not easy. Especially, if you think that statisticians have not developed any new tools after the ANOVA and principal component analysis (PCA). For social and experimental scientists the most important new technique are structural equation models that combine measurement models (that substitute reliability analysis and PCA) and structural models (that substitute ANOVAs or regressions). At present three R-packages provide the functionality to extimate structural equation models.  Today we focus on using structural equation models to fit a measurement model that specifies which items load on which factor. This is similar to what some do with principal component analysis or exploratory factor analysis. If  you already know how the items form the factors you should use CFA, because this gives you several measures of fit and lets you Another advantage is that the SEM-framework provides a framework in which questions of differences between groups can be asked at various levels. Using lavaan a simple model with two latent variables, each measured with four items, can be fit with the following lines of code. 	 0 Comments
Slides from Revolution R: 100% R and More	https://www.r-bloggers.com/2010/12/slides-from-revolution-r-100-r-and-more/	December 8, 2010	David Smith	If you missed today's webcast on Revolution R Enterprise: 100% R and more, the slides from the presentation are now available for download, and a replay of the webcast (in WMV format) will be available at that same link very soon. And if you missed some of the links I mentioned in the presentation, here they are for your convenience: I hope you enjoyed the presentation, and feel free to email me with any questions we didn't get a chance to address in the Q&A session. Revolution Analytics Webinars: Revolution R Enterprise: 100% R and more 	 0 Comments
Interesting Posts at Rational Past Time Related to My Previous Strike Zone Map Post	https://www.r-bloggers.com/2010/12/interesting-posts-at-rational-past-time-related-to-my-previous-strike-zone-map-post/	December 8, 2010	Millsy		 0 Comments
New paper: Survival analysis	https://www.r-bloggers.com/2010/12/new-paper-survival-analysis/	December 8, 2010	csgillespie	Each year I try to carry out some statistical consultancy to give me experience in other areas of statistics and also to provide teaching examples. Last Christmas I was approached by a paediatric consultant from the RVI who wanted to carry out prospective survival analysis. The consultant, Bruce  Jaffray, had performed Nissen fundoplication surgery on 230 children. Many of the children had other medical conditions such as cerebral palsy or low BMI. He was interested in the factors that affected patients’ survival. We fitted a standard cox proportional hazards model. The following covariates were significant: The interaction term was key to getting a good model fit. The figures (one of which is shown below) were constructed using ggplot2 and R. The referees actually commented on the (good) quality statistical work and nice figures! Always nice to read. Unfortunately, there isn’t a nice survival to ggplot2 interface. I had to write some rather hacky R code   The main finding of the paper was the negative effect of cerebral palsy and gastrostomy on survival. Unfortunately, if a child had a gastronomy or had cerebral palsy then survival was dramatically reduced. The interaction effect was necessary, otherwise we would have predicted that all children with a gastronomy and cerebral palsy wouldn’t survive. References:  	 0 Comments
"cumsum ( rnorm(50), lend=""butt"", lwd=12, type=""h"" )
Cumulative…"	https://www.r-bloggers.com/2010/12/cumsum-rnorm50-lendbutt-lwd12-typeh-cumulative/	December 8, 2010	Mathematical Poetics	"cumsum ( rnorm(50), lend=""butt"", lwd=12, type=""h"" ) Cumulative sum of 50 draws from a normal distribution. File this under mysteries of the Central Limit Theorem. "	 0 Comments
Fantasy football (oops, soccer)	https://www.r-bloggers.com/2010/12/fantasy-football-oops-soccer/	December 8, 2010	prasoonsharma		 0 Comments
R: Using RColorBrewer to colour your figures in R	https://www.r-bloggers.com/2010/12/r-using-rcolorbrewer-to-colour-your-figures-in-r/	December 8, 2010	Stewart MacArthur		 0 Comments
Google AI Challenge: Scores/Rank by Language	https://www.r-bloggers.com/2010/12/google-ai-challenge-scoresrank-by-language/	December 8, 2010	C		 0 Comments
inline 0.3.8	https://www.r-bloggers.com/2010/12/inline-0-3-8/	December 7, 2010	Thinking inside the box	"
This version adds an internal performance enhancement which is obtained by
making due with fewer reads. The short NEWS file entry follows:

 "	 0 Comments
Big Data Logistic Regression with R and ODBC	https://www.r-bloggers.com/2010/12/big-data-logistic-regression-with-r-and-odbc/	December 7, 2010	Larry D'Agostino		 0 Comments
R Workflow	https://www.r-bloggers.com/2010/12/r-workflow/	December 7, 2010	Daniel Hocking		 0 Comments
Bayesian model selection	https://www.r-bloggers.com/2010/12/bayesian-model-selection/	December 7, 2010	xi'an	Last week, I received a box of books from the International Statistical Review, for reviewing them. I thus grabbed the one whose title was most appealing to me, namely Bayesian Model Selection and Statistical Modeling by Tomohiro Ando. I am indeed interested in both the nature of testing hypotheses or more accurately of assessing models, as discussed in both my talk at the Seminar of philosophy of mathematics at Université Paris Diderot a few days ago and the post on Murray Aitkin’s alternative, and the computational aspects of the resulting Bayesian procedures, including evidence, the Savage-Dickey paradox, nested sampling, harmonic mean estimators, and more… After reading through the book, I am alas rather disappointed. What I consider to be innovative or at least “novel” parts with comparison with existing books (like Chen, Shao and Ibrahim, 2000, which remains a reference on this topic) is based on papers written by the author over the past five years and it is mostly a sort of asymptotic Bayes analysis that I do not see as particularly Bayesian, because involving the “true” distribution of the data. The coverage of the existing literature on Bayesian model choice is often incomplete and sometimes misses the point, as discussed below. This is especially true for the computational aspects that are generally mistreated or at least not treated in a way from which a newcomer to the field would benefit. The author often takes complex econometric examples for illustration, which is nice; however, he does not pursue the details far enough for the reader to be able to replicate the study without further reading. (An example is given by the coverage of stochastic volatility in Section 4.5.1, pages 83-84.) The few exercises at the end of each chapter are rather unhelpful, often sounding rather like notes than true problems (an extreme case is Exercise 6 pages 196-197 which introduces the Metropolis-Hastings algorithm within the exercise (although it has already been defined on pages 66-67) and then asks to derive the marginal likelihood estimator. Another such exercise on page 164-165 introduces the theory of DNA microarrays and gene expression in ten lines (which are later repeated verbatim on page 227), then asks to identify marker genes responsible for a certain trait.) The overall feeling after reading this book is thus that the contribution to the field of Bayesian Model Selection and Statistical Modeling is too limited and disorganised for the book to be recommended as “helping you choose the right Bayesian model” (backcover). This is rather minor but I find the quality of the editing to be quite poor, with many typos, which makes me wonder if CRC Press is so financially pressed as to be unable to afford a copy-editor. For instance, one section of Chapter 6 covers the Gelfand-Day’s approximation instead of the Gelfand-Dey’s approximation, Gibbs sampling is spelled Gibb’s sampling in Chapter 6, the bibliography is not printed in alphabetical order and contains erroneous entries, like Jacquier, Nicolas and Rossi (2004), instead of Jacquier, Polson and Rossi (2004). Tierney and Kanade (1986) is used instead of Tierney and Kadane (1986), some sentences are not grammatically correct (e.g., the posterior has multimodal, because…, page 55) or meaningful (e.g., the accuracy of this approximation on the tails may not be accurate, page 49)., … While I do not want to discuss about asymptotics, I do not understand the presentation made in the book of priors satisfying  where n is the sample size. Indeed, (a) this would mean priors that depend on the sample size and (b) the opposition between both cases does not seem to be processed in the examples used in Bayesian Model Selection and Statistical Modeling. (I also think the nine assumptions for the “Bayesian central limit theorem” page 48 are missing the definition of the value .) A more important matter is the way improper priors are handled. The author recognises the difficulty with using improper priors in Bayesian model comparison, however he instead resorts to proper priors with very large variances (see e.g. page 37), failing to mention this is a perfect case for the Lindley-Jeffreys paradox. He further considers using Bayes factors to compare “models” that only differ via their prior distributions. I find this use difficult to defend from a Bayesian perspective, since it means picking the prior according to the data (and hence selecting the Dirac mass in the MLE as the optimal choice). “In contrast [to maximum likelihood estimation], a Bayesian treatment of this inference problem relies solely on probability theory.” Bayesian Model Selection and Statistical Modeling, page 205 A rather confusing mistake about the nature of Bayesian testing is found on page 106. When comparing  under a prior covering both subsets, the Bayes factor is given as  instead of  and is thus missing the normalising factors for the prior restricted to each subset… This means that a small null set will never get a chance to achieve a high Bayes factor, which should have warned the author about the mistake. “When selecting among various Bayesian models, the best one is chosen by maximising the posterior mean of the expected log-likelihood.” Bayesian Model Selection and Statistical Modeling, page 200. The most critical part of the book is, in my opinion, related with the computational aspects. From this perspective, I consider the book to be a significant regression from Chen, Shao and Ibrahim, not to mention more recent works on the topic of model choice. In several occurences, it appears that the author is confused about those computational issues. For instance, take the first (true) introduction of the Metropolis-Hastings algorithm on page 66. As this algorithm is presented following the Gibbs sampler, the book applies the Metropolis-Hastings algorithm to the full conditional densities used in a Gibbs sampler, rather than to an arbitrary target, but fails to account for the other components of the parameter in the Metropolis-Hastings acceptance probability for the k-th component,  which makes the whole matter incomprehensible (not to mention the fact that the proposed value is denoted the same way as the next value of the Markov chain)! On page 75, we find the remark that simulated from a truncated normal can be done by simulated from the corresponding untruncated normal and discarding values outside the truncated region. Chapter 6 introduces the worst possible choice for the Gelfand-Day’s (sic!) estimator by considering the harmonic mean version with the sole warning that it “can be unstable in some applications” (page 172). Chib and Jeliazkov’s (2001) estimator is defined with a confusion between numerator and denominator (page 180). The presentation of the bridge sampling estimator in Section 6.5 misses the appeal of the method and concludes with an harmonic mean version, instead of the asymptotically optimal version well-covered by Chen, Shao and Ibrahim. The Savage-Dickey approach unsurprisingly misses the difficulty with the representation (as well as the spelling for Isabella Verdinelli’s last name). The description of Carlin and Chib’s (1995) representation of the product space via pseudo-priors  (pages 190-191) does not put enough emphasis on the difficulty of calibrating those pseudo-priors. A final example of the computational difficulties within Bayesian Model Selection and Statistical Modeling is given by Section 7.1.4 where particle filtering is introduced on pages 205-206 in a very confusing manner with mixed-up indices and is further followed by a return to MCMC for the simulation of the model parameters on page 210, thus negating the whole appeal of running the filter. 	 0 Comments
Build RQuantLib on 32-bit Windows	https://www.r-bloggers.com/2010/12/build-rquantlib-on-32-bit-windows/	December 7, 2010	Joshua Ulrich		 0 Comments
Finding roots of functions in actuarial science	https://www.r-bloggers.com/2010/12/finding-roots-of-functions-in-actuarial-science/	December 7, 2010	arthur charpentier	The following simple code can be used to find roots of functions (based on the secant algorithm), It can be interesting in actuarial science, e.g. to find the actuarial rate so that to present values are equal. For instance, consider the following capital, given only if the insured is still alive (this example was initially considered here). We would like to find the rate so that the probable discounted value is 600, 	 0 Comments
Kendall Rank Coefficient by GPU	https://www.r-bloggers.com/2010/12/kendall-rank-coefficient-by-gpu/	December 7, 2010	rtutor.chiyau	"

The correlation coefficient is a measurement of correlation between two random variables.
While its computation is straightforward, it is not readily applicable to
non-parametric statistics.
 read more "	 0 Comments
Webinar: Revolution R is 100% R and More	https://www.r-bloggers.com/2010/12/webinar-revolution-r-is-100-r-and-more/	December 7, 2010	David Smith	I'll be hosting a webinar tomorrow (Wednesday) aimed at R users who want to know more about how Revolution R Enterprise extends open source R for big data, Web services, multi-core processing, debugging and more. For R users at schools and universities, I'll also explain how you can download and use Revolution R Enterprise free of charge. The full webinar description is after the jump, and you can register to attend at the link below. Revolution Webinars: Revolution R Enterprise, 100% R and More Revolution R Enterprise: 100% R and More R users already know why the R language is the lingua franca of statisticians today: because it's the most powerful statistical language in the world. Revolution Analytics builds on the power of open source R, and adds performance, productivity and integration features to create Revolution R Enterprise. In this webinar, author and blogger David Smith will introduce the additional capabilities of Revolution R Enterprise, including: This webinar will be of value to current R users in industry and government who want to learn more about the additional capabilities of Revolution R Enterprise to enhance the productivity, ease of use, and enterprise readiness of open source R. R users in academia will also find this webinar valuable: we will explain how all members of the academic community can obtain Revolution R Enterprise free of charge. Register here 	 0 Comments
Sequential Line Plots in R	https://www.r-bloggers.com/2010/12/sequential-line-plots-in-r/	December 7, 2010	gjabel	I was trying to create some sequential plots today in R to analyse some MCMC simulations. I found the par(ask=TRUE) command very useful for looking at iterations of individual parameter values. Setting the ask graphical parameter to TRUE (before a for loop) allows you to update plots by clicking on the plotting device (in windows). Here is some example code to show how par(ask=TRUE) works. This will give the following output after each click: If you want to keep previous plotted lines on the plot you can adapt the for loop in such a manner: This will give the following output after each click: 	 0 Comments
Webinar on Revolution R Enterprise	https://www.r-bloggers.com/2010/12/webinar-on-revolution-r-enterprise/	December 7, 2010	Stephen Turner		 0 Comments
Statistique de l’assurance STT6705V, partie 12 bis	https://www.r-bloggers.com/2010/12/statistique-de-lassurance-stt6705v-partie-12-bis/	December 7, 2010	arthur charpentier		 0 Comments
Le Monde puzzle [49]	https://www.r-bloggers.com/2010/12/le-monde-puzzle-49/	December 7, 2010	xi'an	Here is a quick-and-dirty solution to Le Monde puzzle posted a few days ago: the R code counts the number of winning tickets between 1 and N, and stops when there is a proportion of 10% of winning tickets. The (only) solution is therefore N=3500. (I am using this home-made decomposition of a number into its decimal digits, but there must be some function doing that in R already!) 	 0 Comments
highlight 0.2-5	https://www.r-bloggers.com/2010/12/highlight-0-2-5/	December 7, 2010	romain francois	I pushed highlight 0.2-5 on CRAN. This release improves the latex renderer and the sweave driver so that multiple lines character strings are properly rendered.  This example vignette shows it: Once processed with Sweave, e.g. :  we get this result, embedded below with google viewer:  See this question on stack overflow for the tip of using google documents to display pdf files 	 0 Comments
R 2.12.1 scheduled for December 16	https://www.r-bloggers.com/2010/12/r-2-12-1-scheduled-for-december-16/	December 6, 2010	David Smith	The next update to R will be a patch release: R 2.12.1 will be released on December 16, as announced today by the R Core Team. As is typical for a patch release, this version will include some minor bug fixes plus a few new features (from the current build's NEWS file): r-announce mailing list: R 2.12.1 scheduled for December 16 	 0 Comments
Jeromy Anglim on Reproducible Research and R	https://www.r-bloggers.com/2010/12/jeromy-anglim-on-reproducible-research-and-r/	December 6, 2010	Drew Conway	"Jeromy Anglim, fellow social scientist and R aficionado from across the globe, gave a great talk to the Melbourne R Users Group last week on the joys of creating reproducible results. A subject near and dear to me, but not one that is given enough attention in research training.  Jeromy discusses tools for generating reproducible results, best practices; and provides some great SWeave examples.  He also gave a nice plug to John Myles White’s ProjectTemplate library, which is a wonderful framework for any analytical workflow. The video is embedded below and is available in the Rchive, along with a wonderful presentation by Geoff Robison on writing simple and reusable R code.  Enjoy! 
 "	 0 Comments
What my R code looks and feels like (Vanilla)	https://www.r-bloggers.com/2010/12/what-my-r-code-looks-and-feels-like-vanilla/	December 6, 2010	VCASMO - drewconway		 0 Comments
Reproducible Research and R Workflow	https://www.r-bloggers.com/2010/12/reproducible-research-and-r-workflow/	December 6, 2010	VCASMO - drewconway		 0 Comments
3 weak days in a row	https://www.r-bloggers.com/2010/12/3-weak-days-in-a-row/	December 6, 2010	kafka	"Recently, Trading the odds posted one of many flavors of mean reverting strategies and I decided to get my hands dirty by writing R code and testing it. You can find full description of the strategy by following latter link above. Long story short – if SPY shows lower open, high and close 3 days in a row, then buy on the close of third day and sell it 1 days later.
Let’s do simple test: The code above supposed to produce something similar:  Nice curve, isn’t it? But neither commissions nor slippage were taken into account. So, let’s run more complicated test. For that purpose I utilized blotter package. Here’s the code:  Nice curve, but let’s look beyond that. First of all, here’s nice function in PerformanceAnalytics package, AnnulizedReturns:
table.AnnualizedReturns((result-6)/10000)
Gross.Txn.Realized.PL
Annualized Return                        0.0265
Annualized Std Dev                      0.0494
Annualized Sharpe (Rf=0%)          0.5366 Well, Sharpe ratio is not impressive. The profit percentage of this strategy is 57% and mean of profitable return is 111$ against 98$ loss. Profit factor is ~1.55. I think, this strategy can be as one of the parameter or vote in another system, but alone it is weak. "	 0 Comments
JAGS – Bayesian Analysis	https://www.r-bloggers.com/2010/12/jags-bayesian-analysis/	December 6, 2010	Daniel Hocking		 0 Comments
"Using the ""Divide by 4 Rule"" to Interpret Logistic Regression Coefficients"	https://www.r-bloggers.com/2010/12/using-the-divide-by-4-rule-to-interpret-logistic-regression-coefficients/	December 6, 2010	Stephen Turner		 0 Comments
Example 8.17: Logistic regression via MCMC	https://www.r-bloggers.com/2010/12/example-8-17-logistic-regression-via-mcmc/	December 6, 2010	Ken Kleinman		 0 Comments
Electoral Marimekko Plots	https://www.r-bloggers.com/2010/12/electoral-marimekko-plots/	December 6, 2010	d sparks	To be reductive, visual displays of quantitative information might be reasonably categorized on a continuum between “data display” and “statistical graphics.” By statistical graphics, I mean a plot that displays some summary of or relationship amongst several variables, likely having undergone some processing or analysis. This may be as simple as a scatterplot of a primary independent variable and the dependent variable, a boxplot, or a graphical regression table. In this reductive scheme, then, “data displays” present variables in raw form — for use in exploratory data analysis, or perhaps just to offer the viewer access to all of the data. Where “statistical graphics” might be best served by simplicity and minimalism in design, such that a single idea might be conveyed clearly, “data displays” will tend to be inherently complex, and require effort from both the creator and viewer to parse meaning from the available information. Where statistical graphics are ideal for presenting conclusions, data displays are useful for generating ideas, and optimally, permitting the relatively rapid identification of relationships between multiple variables. On top of this, I might add that many of the more well-regarded data displays of recent note offer macro-level insight as well as the opportunity to ascertain specific details (for this, interactivity is often valuable, as in the internet-classic New York Times box office visualization). As several recent posts suggest, I am interested in finding ways to successfully and clearly convey multidimensional data, and have been focusing on political data as it varies across geopolitical units and time. Here I offer an approach which departs from the spatial basis of other recent efforts in favor of allowing the position of graphical objects to convey other variables. County Vote Marimekko Plot, 1992, sorted by votes cast. Click for slideshow. This type of plot is called, variously, a spinogram, a mosaic plot, or a marimekko — and is not dissimilar from a treemap with a different organizational structure (other examples). The utility of this plot type is that it can spatially convey four numeric variables (x position, y position, height, width), and color can be added to incorporate up to three additional variables (R, G, B). Further, there is a straightforward geometric interpretation of each cell: the areas of each (in this case, width/state turnout ×height/county proportion of state turnout) are directly comparable. Unlike a stacked bar plot, the width of each column conveys information, permitting height to convey proportion rather than count. Further, columns and cells within columns can be sorted to express the ordering of variables of interest. In some ways, these can be seen as extreme reinterpretations of (Dorling) cartograms, in which not only the size and shape of political boundaries, but also their position, are distorted by other variables. County Vote Marimekko Plot, 1924, sorted by Democratic share of the two-party vote. Click for slideshow. In the plots above, cells are colored according to the strength of Democratic (blue), Republican (red), and other party (green) support, and counties whose turnout represents greater than 1% of the total turnout in an election are labeled. I present two different layouts for the cells in each plot. The first arrays states left-to-right in order of the number of votes cast in an election, and sorts counties bottom-to-top in the same order. Thus, more populous states are on the right, and more populous counties are at the top of the plot. This arrangement allows the viewer to observe the effects of population density both within and across states, and may better facilitate tracking changes in county or state politics over time. The second layout sorts states left-to-right, and counties bottom-to-top in order of the Democratic share of the two party vote (Dem Votes / (Dem Votes + Rep Votes)). Thus, more Democratic-leaning (relative to Republican) states are on the right, and counties that were more supportive of Democratic candidates are at the top. I believe that this arrangement makes it easier to discern overall trends in partisanship across time, as the total “sum” of red within a diagram is relatively easy to compare to the total “sum” of blue (and green). I have attempted to make my R code fairly general, and it is available for download here, although it will obviously require some modifications for other applications. Our approaches differ, but another instructive example can be found at Learning R. 	 0 Comments
Bear hunting	https://www.r-bloggers.com/2010/12/bear-hunting/	December 6, 2010	Pat	"When were there bear and bull markets in US stocks since 1950? While we’d really like to estimate the expected return at each point in time, finding bear markets is ambitious enough.  The plan starts by smoothing the daily returns through time, as in Figure 1. Figure 1: Smoothed returns with a 4 year window. Note that for each time point we are using data in the future as well as in the past.  This is a much easier task than predicting the return. Figure 2: Smoothed returns with a half-year window.Of course both Figures 1 and 2 wiggle about.  How are we to know what is significant wiggle, and what is just random wiggle?  One thing to do is to smooth randomly permuted data.  This will not have systematic good times and bad times.  An example is in Figure 3. Figure 3: Smoothed permuted returns with a half-year window.There is obviously structure in the returns because it is quite easy to tell which of Figures 2 and 3 is the real data.  However, some of the structure that we see is from changing volatility.  The mid-90′s were notorious for low volatility and this is definitely visible in Figure 2. If there really are bull and bear markets, then those periods should have smooths that are outlying in the distribution of the smooths of randomly permuted data. Figure 4: Densities of quarter-year window smooths of the returns (blue) and randomly permuted returns (green).If we are to believe Figure 4, there were no bull markets during the 60 years and bear markets were rare and fairly subtle.  We can use robust estimation in the smoothing process — Figure 5 shows the resulting densities. Figure 5: Densities of quarter-year window robust smooths of the returns (blue) and randomly permuted returns (green). Figure 5 indicates more extensive bear markets, and also some possibility of bull markets.  (By the way, the green lines in Figures 4 and 5 are much smoother than the blue lines because they are estimated with 100 times more data.) Four different smoothing windows were used: quarter-year, half-year, year, 4 years.  Each of these were used with standard estimation and robust estimation.  We don’t know which of these is best.  But for each point in time we can plot how many of these 8 thought a bear market was on. Figure 6: Count of models in bear market. For each of the 8 models an arbitrary cutoff was established from inspecting the density plots like Figures 4 and 5. This is far from a polished analysis.  I have a couple ideas of how to make it better.  What are your ideas? Is there any way to know how smooth the expected return curve really is? Some days you gets the bear, and some days the bear gets you.  Which is this? The smoother used was loess and the robust smooths used family=""symmetric"". All of the data and functions used in the analysis are available. In particular the function that highlights decades (and alternatively years) is a public domain function called pp.timeplot. You can get the S&P data (spxdf and spxret) into your R session with: > load(url('http://www.portfolioprobe.com/R/blog/bearspx.rda')) You can get the functions (including pp.timeplot and functions to create the figures) with: > source('http://www.portfolioprobe.com/R/blog/bearhunt.R') The commands to recreate the analysis are in bearhunt.Rscript. Once these commands have been done, then you can use the plotting functions (starting with a capital P) to reproduce the figures. "	 0 Comments
Forecasting workshop: Switzerland, June 2011	https://www.r-bloggers.com/2010/12/forecasting-workshop-switzerland-june-2011/	December 6, 2010	Rob J Hyndman	I will be running a workshop on Statistical Forecasting: Principles and Practice in Switzerland, 20-22 June 2011. Check out the venue: Waldhotel Doldenhorn, Kandersteg! So if you fancy a trip to the beautiful Swiss Alps next June, read on… Forecasting is required in many situations: deciding whether to build another power generation plant in the next five years requires forecasts of future demand; scheduling staff in a call centre next week requires forecasts of call volume; stocking an inventory requires forecasts of stock requirements. Forecasts can be required several years in advance (for the case of capital investments), or only a few minutes beforehand (for telecommunication routing). Whatever the circumstances or time horizons involved, forecasting is an important aid in effective and efficient planning. In this workshop, we will explore methods and models for statistical forecasting. Topics to be covered include seasonality and trends, exponential smoothing, ARIMA modelling, dynamic regression and state space models, as well as forecast accuracy methods and forecast evaluation techniques such as cross-validation. Some recent developments in each of these areas will be explored. The workshop will involve a mixture of lectures and practical sessions using R. Examples will be drawn from my consulting experiences Workshop participants will be assumed to be familiar with basic statistical tools such as multiple regression and maximum likelihood estimation, but no knowledge of time series or forecasting will be assumed. Some prior experience in R is desirable but not essential. Please bring a laptop with preinstalled R software in its latest release (see CRAN.R-project.org) and the “forecast” package for R. The workshop is being organized by the Swiss Statistical Society. The workshop brochure provides additional information about costs and registration. The forecasting workshop finishes on Wednesday 22 June, and I will then be flying to Prague for the 31st International Symposium on Forecasting, beginning on Sunday 26 June. The ISF is the leading international forecasting conference and is always well-worth attending for anyone involved with forecasting research. I hope some of the workshop participants will also join me in Prague for the ongoing forecasting fun and festivities! 	 0 Comments
R 2.12.0 and Eclipse with StatET installation	https://www.r-bloggers.com/2010/12/r-2-12-0-and-eclipse-with-statet-installation/	December 5, 2010	Luke Miller		 0 Comments
Bayesian adaptive sampling	https://www.r-bloggers.com/2010/12/bayesian-adaptive-sampling/	December 5, 2010	xi'an	In the continuation of my earlier post on computing evidence, I read a very interesting paper by Merlise Clyde, Joyee Ghosh and Michael Littman, to appear in JCGS. It is called  Bayesian adaptive sampling for variable selection and model averaging. The sound idea at the basis of the paper is that, when one is doing variable selection (i.e. exploring a finite if large state space) in setups where the individual probabilities of the models are known (up to a constant), it is not necessary to return to models that have been already visited. Hence the move to sample models without replacement called BAS (for Bayesian adaptive sampling) in the paper. The first part discusses the way to sample without replacement a state space whose elements and probabilities are defined by a binary tree, i.e.  (The connection with variable selection is that each level of the tree corresponds to the binary choice between including and excluding one of the variables. The tree thus has 2k endpoints/leaves for k potential variables in the model.) The cost in updating the probabilities is actually in O(k) if k is the number of levels, instead of 2k because most of the branches of the tree are unaffected by setting one final branch to probability zero. The second part deals with the adaptive and approximative issues. In a model selection setup, the posterior partial conditional probability of including variable i given the inclusion/exclusion of variables 1,…,i-1 is obviously unknown. The authors suggest to use instead an approximation based on the marginal posterior inclusion probabilities, namely for variable j  the frequency of the inclusion of variable j in the model along past iterations, with a possible shrinkage correction to avoid probability estimates equal to zero. (The motivation is the fundamental paper by Berger and Barbieri, 2004, Annals of Statistics, that shows the optimality of the median posterior probability model.) In this sense, the algorithm is adaptive. (In addition, there is a step to periodically calculate new marginal inclusion probabalities that replace the old sampling distribution at time t=0.) But it also is approximative in that the only convergence result is one by attrition, namely that the posterior partial conditional probabilities are exactly recovered after sampling all models. (The paper is associated with an R package called BAS.) 	 0 Comments
Rethinking ‘loess’ for Binomial-Response Pitch F/X Strike Zone Maps	https://www.r-bloggers.com/2010/12/rethinking-loess-for-binomial-response-pitch-fx-strike-zone-maps/	December 5, 2010	Millsy		 0 Comments
Pareto plot party!	https://www.r-bloggers.com/2010/12/pareto-plot-party/	December 5, 2010	richierocks	"A Pareto plot is an enhanced bar chart. It comes in useful for deciding which bars in your bar chart are important.  To see this, take a look at some made up DVD sales data.  
The “importance” of each bar is given by its height, so we can make the plot easier to interpret by simply sorting the bars from largest to smallest.  Suppose you need to describe what you’ve been selling to your boss.  You don’t want to list every DVD because bosses have short attention spans and get confused easily.  To make it easy for the boss, you can tell him which films make up 90% of sales.  (Or some other percentage — if your catalogue is much bigger then a smaller percentage may be more realistic.) To find 90% of sales, we need a cumulative total of the sales values.  Due to a technicality of ggplot2, since bars use a categorical scale but lines require a numeric scale, we also need to convert the x values to be numeric.  This function fortifies our dataset with the requisite columns.  To see which DVDs constitute 90% of sales, read across from 90% on the y-axis until you hit the cumulative total line.  Now read down until you hit the x-axis, and all the DVDs to the left of that point constitute your “important” set.  In this case, you’d be telling your boss that 90% of sales come from “Urban Coitus 2″, “Fe Man 2″, “Germination” and “The Dusk Saga: Black Out”. And there you have it: a Pareto plot.  These plots are useful whenever you need to reduce the number of categories of data.  As well as these businessy examples, they are great for things like principal component analysis and factor analysis where you need to reduce the number of components/factors. "	 0 Comments
Genetic optimization for Trading Strategies using Rapidminer and R	https://www.r-bloggers.com/2010/12/genetic-optimization-for-trading-strategies-using-rapidminer-and-r/	December 5, 2010	a Physicist		 0 Comments
GLMM and R issues	https://www.r-bloggers.com/2010/12/glmm-and-r-issues/	December 4, 2010	Daniel Hocking		 0 Comments
Comparison of results	https://www.r-bloggers.com/2010/12/comparison-of-results/	December 4, 2010	Shige		 0 Comments
Root finding	https://www.r-bloggers.com/2010/12/root-finding/	December 4, 2010	R on Guangchuang Yu	Numerical root finding methods use iteration, producing a sequence of numbers that hopefully converge towards a limits which is a root. In this post, only focus four basic algorithm on root finding, and covers bisection method, fixed point method, Newton-Raphson method, and secant method. The simplest root finding algorithms is the bisection method. It works when f is a continuous function and it requires previous knowledge of two initial gueeses, u and v, such that f(u) and f(v) have opposite signs. This method is reliable, but converges slowly. For detail, see http://ygc.cwsurf.de/2008/11/25/bisect-to-solve-equation/ . Root finding can be reduced to the problem of finding fixed points of the function g(x) = c*f(x) +x, where c is a non-zero constant. It is clearly that f(a) = 0 if and only if g(a) = a. This is the so called fixed point algorithm.  The fixed point algorithm is not reliable, since it cannot guaranteed to converge. Another disavantage of fixed point method is relatively slow.  Newtom-Raphson method converge more quickly than bisection method and fixed point method. It assumes the function f to have a continuous derivative. For detail, see http://ygc.cwsurf.de/2007/06/02/newton-raphson-method/ . The secant method does not require the computation of a derivative, it only requires that the function f is continuous. The secant method is based on a linear approximation to the function f. The convergence properties of the secant method are similar to those of the Newton-Raphson method. 	 0 Comments
Le Monde puzzle [48: resolution]	https://www.r-bloggers.com/2010/12/le-monde-puzzle-48-resolution/	December 4, 2010	xi'an	The solution to puzzle 48 given in Le Monde this weekend is rather direct (which makes me wonder why the solution for 6 colours is still unavailable..) Here is a quick version of the solution: Consider one column, 1 say. Since 326=5×65+1, there exists one value c with at least 66  equal to c. Among those (at least) 66 rows, if a pair (i,j) satisfies , the problem is over. Otherwise, all  are different from c for those (at least) 66 rows, hence equal to one of the four remaining values. Since 65=4×16+1, for a given row i in this group, there exists d different from c for which at least 17  are equal to d. Again, either there is at least one  in this group of indices, else they all are different from c and d, hence equal to one of the three remaining values. Then 16=3×5+1, and for a given index j within this group there exists e different from c and d for which at least 6 ‘s are equal to e. Again, either there is a triplet or they all take a value different from c,d,e. Since 5=2×2+1, there exists f different from c,d,e, for which at least 3 ‘s are equal to f. Again, either end of the story or they all three take the final value g, but then constitute a triplet… This week puzzle [49]: in a lottery, 999 A manageable problem for R, obviously! 	 0 Comments
A Draft of ProjectTemplate v0.2-1	https://www.r-bloggers.com/2010/12/a-draft-of-projecttemplate-v0-2-1/	December 3, 2010	John Myles White	I’ve just uploaded a new binary of ProjectTemplate to GitHub. This is a draft version of the next release, v0.2-1, which includes some fairly substantial changes and is backwards incompatible in several ways with previous versions of ProjectTemplate. Foremost of the changes is that most of the logic for load.project() is now built into the load.project() function directly, rather than spread out into autogenerated scripts that you can edit by hand. While this makes ProjectTemplate harder for non-experts to modify, the change will make it much easier to make revisions to ProjectTemplate in the future without having to worry about existing projects falling behind because of vestigial code that’s not being automatically updated when you install a new version of ProjectTemplate. Because more system logic is now hardcoded into functions, each project’s configuration is handled through a YAML file in config/global.yaml. Incidentally, this introduces the new directory, config/, where configuration files will go from now on. The data loading system is also more complex than it was before. First, there’s a new hierarchy of data sources: now the system will look for data in a cache/ directory before moving on to the data/ directory. This makes it possible for you to permanently store changes to your data set in cache/ that will allow you to skip loading the raw data set. This is helpful when the original data set is enormous and you only need a radically reduced form of it for your future analyses that you’ll store in cache/. In addition, preprocessing is now handled through a series of ordered scripts in a munge/ directory rather than just a single preprocessing script in the lib/ directory. There’s also a log/ directory, used by the new integrated log4r support, which is off by default, but can be easily set up after installing log4r from CRAN. Finally, there’s a src/ directory where we’re going to encourage users to place their primary analyses, so that the main directory always has the same files and directories across all projects. In addition to all of these changes, many of which were inspired by conversations with Mike Dewar, I’ve incorporated some very helpful patches in this release. Specifically, Diego Valle-Jones fixed a bug in clean.variable.name() that lead to trouble when filenames in the data/ directory began with numbers and Patrick D. Schalk contributed code that adds support for SQLite to ProjectTemplate along with general improvements to the database access codebase. Thanks for all of the support since the last release. Please let me know if there any changes that need to be made before I turn v0.2-1 loose on CRAN. 	 0 Comments
Fun with infochimps: Animated Blog Post Hit Map	https://www.r-bloggers.com/2010/12/fun-with-infochimps-animated-blog-post-hit-map/	December 3, 2010	Drew Conway	In a few weeks I will be visiting Chicago, and JD Long—the organizer of the local R users group—has graciously invited me to give a presentation.  Ostensibly, the presentation will be on my recently released infochimps package, so I thought it was a good time to start actually putting together some examples and documentation for the package.   If you have not visited the site in a week or so you will have missed my previous post on analyzing WikiLeaks data, which from the traffic and comments was at least somewhat controversial.  Given this rare spotlight I thought it would be fun to use the infochimps API to map out the geo-location of everyone that visited the blog post over the last few days.  Unfortunately, after nearly two years with the same web hosting service, only today did I realize that I was not capturing daily log files for my domain. While the issue has been resolved; tragically, all of that data has been lost.  In lieu of analyzing the logs from the last week, I am limited to only visualizing the traffic from today.  Hopefully before my presentation in Chicago I will have another post that strikes a nerve deep within the Internet; but until then, I present an animated map hits from today to my “Why I will Not Analyze The New WikiLeaks Data.”  Animated Blog Post Hit Map from Drew Conway on Vimeo.  The timing of the hits is significantly sped up; each second of the animation representing roughly 9.5 minutes of blog post traffic.  With the IP addresses of visitors to the blog post, I used the infochimps package to collect latitude and longitude coordinates for each hit, and then simply mapped these out in ggplot2 over time and created the animation with ffmpeg.   The sizes of the bubbles represent the number of concurrent hits from the same coordinate at a given second.  As such, you will notice sudden bursts in some locations.  I am far from a DNS expert, so I am sure someone will tell me how I am over counting certain IPs, but it is fun to watch the activity.   I will release the code and instruction for this as part of a general update to the infochimps documentation in the lead up to my trip to Chicago.  In the meantime I am happy to answer any questions about this, or the package more generally. 	 0 Comments
Some ideas on communicating risks to the general public	https://www.r-bloggers.com/2010/12/some-ideas-on-communicating-risks-to-the-general-public/	December 3, 2010	dan	"SOME EMPIRICAL BASES FOR CHOOSING CERTAIN RISK REPRESENTATIONS OVER OTHERS  This week DSN posts some thoughts (largely inspired by the work of former colleagues Stephanie Kurzenhäuser, Ralph Hertwig, Ulrich Hoffrage, and Gerd Gigerenzer) about communicating risks to the general public, providing references and delicious downloads where possible. Statements of the form “The probability that X will happen is Y”, such as “The probability that it will rain on January 1st is 30%” are single-event probability statements. They are problematic not only for philosophical reasons (some say that such statements are meaningless), but also because they are ambiguous: they do not specify if we’re saying this about January first based on other January firsts, or if we’re saying it based on all January firsts at a particular weather station (or an average across many weather stations), or if we’re not even considering the date but basing our prediction on today’s weather, a mathematical model, an average of other people’s forecasts, our intuition, or what. What may seem unambiguous is actually interpreted by different people in different ways. A survey of people in 5 international cities found no agreement on what a 30% chance of rain means. Some thought it means rain on 30% of Tuesday’s minutes, others thought rain in 30% of the land area, and so on [1]. A further problem with the statement is that it gives no information about what it means to rain. Does one drop of rain count as rain? Does a heavy mist? Does one minute of rain count? In addition, when risks are described as probabilities, people tend to overweight small probabilities and underweight large probabilities. This observation shows up in the “probability weighting function” of Tversky & Kahneman’s Prospect Theory, the dominant behavioral model of gamble evaluations. A representation that leads to misperceptions of underlying probabilities is undesirable. Doctors given problems of the type: The probability of colorectal cancer in a certain population is 0.3% [base rate]. If a person has colorectal cancer, the probability that the haemoccult test is positive is 50% [sensitivity]. If a person does not have colorectal cancer, the probability that he still tests positive is 3% [false-positive rate]. What is the probability that a person from the population who tests positive actually has colorectal cancer? give mostly incorrect answers that span the range of possible probabilities. Typical answers include 50% (the “sensitivity”) or 47% (the sensitivity minus the false positive rate). The correct answer is 5%. [2] It seems as if people given conditional probabilities, such as the sensitivity or the false-positive rate, confuse them with the posterior probability they are being asked for. This likely happens because each numerical representation lend themselves to computations that are easy or difficult for that representation. The thing to do with the conditional probabilities listed above is to plug them into Bayes Theorem, which most people do not know. Even if they know the theorem, they have little intuition for it and cannot make good mental estimates. Fortunately, there are other ways to represent information than conditional probabilities that allow even those who do not know Bayes’ theorem to arrive at the correct answer. Relative risk statements speak of risk increasing or decreasing by a percentage, for instance, that mammography in women over 40 reduces the risk of breast cancer by 25%. But all percentages erase the frequencies from which they were derived.  We cannot tell from the relative risk reduction what is the absolute risk reduction: by how much does the risk of breast cancer actually decrease between those who get mammographies and those who do not: the answer is .1% Relative risk information does not give information on how many people need to undergo a treatment before a certain benefit is obtained. In particular, based on the relative risk information, can one say how many women must be screened before a single life is saved? If your intuition tells you 4, you are again far off, as 1000 women must be screened to save the one life.  Relative risk information can cause people to misjudge the effectiveness of treatments [3]. Consider the colorectal cancer example given previously. Only 1 in 24 doctors tested could give the correct answer. The following, mathematically-equivalent, representation of the problem was given to doctors: Out of every 10,000 people, 30 have colorectal cancer. Of these 30, 15 will have a positive haemoccult test. Out of the remaining 9,970 people without colorectal cancer, 300 will still test positive. How many of those who test positive actually have colorectal cancer? Without any training whatsoever, 16 out 24 physicians obtained the correct answer to this version. That is quite a jump from 1 in 24. Statements like 15 out of 30 are “natural frequency” statements. They correspond to the, trial by trial way we experience information in the world. (For example, we’re more likely to encode that 3 of our last 4 trips to JFK airport were met with heavy rush-hour traffic that encoding p of .75, which removes any trace of the sample size). Natural frequency statements lend themselves to simpler computations than does Bayes’ Theorem, and verbal protocols show that given statement like the above many people correctly infer that the probability of cancer would be the number testing positive who have the disease (15) divided by the number who get back positive test results (15 + 300). 15 divided by 315 is 5%, the correct answer. While compact statements of probability such as a “there is a 30% chance of rain on April first” save words, they do not reveal their underlying reference classes. When information is conveyed with statements like “In New York City, 3 out of every 10 April firsts have more than a centimeter of rain” there is no ambiguity as to whether the 30% refers to days, area, or time, and it is more clear what “rain” means. Since probabilities can be translated to frequencies out of 100, 1,000, 10,000 and so on, they can easily be represented visually on grids that allow for visual assessment of area and facilitate counting. Research by Sedlmeier used information grids to teach people how to solve Bayesian reasoning problems (like the original colorectal cancer problem) by converting them into natural frequencies and representing them on a grid. Even six months later, experimental participants who received the information grid instruction were able to solve the problems correctly, while those who were instructed with the classic version of Bayes Theorem did not retain what they learned [4]. Information grids whose squares are embellished with faces showing positive or negative affect have also proven effective in presenting treatment alternatives to patients [5]. The statement that a certain treatment causes a 25% risk reduction, as mentioned, does not disclose the magnitudes of the risks involved. In the case studied, among women receiving mammographies 3 in 1000 died of cancer, while among women not receiving mammographies 4 in 1000 died of this cause. The absolute risk reduction pops out of this formulation, and we see it to be 1 life in 1000. The number needed to treat, which is not computable from the relative risk reduction is now clear:  to save one life, 1000 women must be screened. This formulation not only expresses the difference between alternative actions, but relates absolute magnitudes of risk as well. 
The distribution builder of Goldstein, Johnson and Sharpe While descriptive numerical probability formats leads to overweighting of small probabilities, recent research shows that when people learn probabilities through experience (actually taking draws from a distribution) leads to the opposite tendency: underweighting of large probabilities. An exciting possibility is that when descriptive and experienced probability formats are combined, the effects may cancel each other out. Other research shows that watching animated draws from probability distributions led to the most accurate estimates of the probability of a loss and of upside return of an investment [6]. Decision aids such as the Distribution Builder of Goldstein, Johnson, & Sharpe [7] allow participants to visually observe the magnitude of probabilities (as information grids do), while simulating numerous draws from the distribution to allow people to experience random sampling. We propose to experiment with this format to see if it may lead to calibrated probability weighting. 
The simulator of Haisley, Kaufmann and Weber  Gigerenzer, G. , Hertwig, R., van den Broek, E., Fasolo, B., & Katsikopoulos, K. V. (2005). “A 30% chance of rain tomorrow”: How does the public understand probabilistic weather forecast? Risk Analysis, 25, 623-629. Available online.  Hoffrage, Ulrich & Gigerenzer G. (1998). Using natural frequencies to improve diagnostic inferences. Academic Medicine, 73, 538-540. Available online.  Kurzenhauser, Steffi & Ralph Hertwig (2006). Kurzenhäuser, S., & Hertwig, R. (2006). How to foster citizens’ statistical reasoning: Implications for genetic counseling. Community Genetics, 9, 197-203. Available online.  Sedlmeier, Peter and Gerd Gigerenzer (2001) Teaching Bayesian reasoning in less than two hours. Journal of Experimental Psychology General, 130, 380–400. Available online.  Man-Son-Hing, Malcolm et al (1999) Therapy for Stroke Prevention in Atrial Fibrillation: A Randomized Controlled Trial. Journal of the American Medical Association, 282(8):737-743. Available online.  Haisley, Emily, Christine Kaufmann and Martin Weber (working paper) The Role of Experience Sampling and Graphical Displays on One’s Investment Risk Appetite. Available online.  Goldstein, Daniel G., Johnson, Eric J. & Sharpe, William F. (2008). Choosing Outcomes Versus Choosing Products: Consumer-Focused Retirement Investment Advice. Journal of Consumer Research, 35 (October), 440-456. Available online. "	 0 Comments
Because it’s Friday: The 4th Amendment X-ray T-shirt	https://www.r-bloggers.com/2010/12/because-its-friday-the-4th-amendment-x-ray-t-shirt/	December 3, 2010	David Smith	Here's a clever way to express your objection to the obtrusiveness of the new backscatter X-ray scanners at airports: A T-shirt with the text of the 4th Amendment printed in metallic ink:  Also available in underwear format. Cargo Collective: 4th Amendment Wear 	 0 Comments
Why I love open source!	https://www.r-bloggers.com/2010/12/why-i-love-open-source/	December 3, 2010	ucfagls	Today I had a great reminder of why I love open source software and why I spend  a bit of my time contributing R code to several packages. In the vegan, I had included some code to do an analysis of multivariate dispersions. Bone-headedness on my part meant that if you tried to plot the ordination results behind the model/test for any axes other than the first two, the code failed. A user (Sarah Goslee) spotted this and emailed me with a fix to the code, that I had integrated and added to the vegan source tree on R-Forge within about 30 mins of Sarah’s email landing in my in-box. This particular bug was fixed in the forthcoming 1.17.5 release of vegan. Don’t you just love open source! 	 0 Comments
Data Visualization Practices at the New York Times	https://www.r-bloggers.com/2010/12/data-visualization-practices-at-the-new-york-times/	December 3, 2010	David Smith	Amanda Cox of the New York Times' graphics department recently gave a great presentation to the New Media Days conference in Copenhagen and described how the Times uses data visualizations to reveal patterns, provide context, describe relationships, and even create a sense of wonder about the world. In the video, Amanda demonstrates several of the Times' best interactive visualizations over the past couple of years. The visualizations span all departments: breaking news (for example, the destruction of the Haiti earthquake), news analysis (the spread of the BP oil spill), politics (the US 2010 election), entertainment (Netflix rental habits), and even sports. One visualization I hadn't seen before, but was very cool, showed the distribution of Mariano Rivera's pitches by animating 2000 pitches — all at once. Amanda also revealed that the best interactive graphics are competitive with the most significant news stories, in terms of traffic to the Times website. It's likely that several of the graphics presented in the video used R at some point in their construction. As you might recall, Amanda used R to create the Michael Jackson infographic we featured on the blog last year. And in the article “R is Hot“, Amanda said of R: “R makes it easy to read data, generate lines and points, and place them where you want them. It’s very flexible and super quick. When you’ve only got two or three hours until deadline, R can be brilliant.” I hadn't seen several of the visualizations in the video, but I did learn that I can keep informed of new ones by following @nytgraphics on Twitter. Follow the link below to watch the presentation and see Amanda demonstrate some of the best ones. New Media Days: Amanda Cox 	 0 Comments
formatR update (0.1-5)	https://www.r-bloggers.com/2010/12/formatr-update-0-1-5/	December 3, 2010	Yihui Xie	The formatR package has been silent for quite a few months now. Recently I’ve been moving my old packages from R-Forge to GitHub, and I finally killed several things on my TODO list. In the past, I made an awkward decision to let formatR depend on the animation package, which was ridiculous. Ronggui suggested me remove this dependency long time ago, and now it came true in the new version 0.1-5 (on CRAN now). So formatR is finally a standalone package, although you can choose to use the GUI version after installing the gWidgets package (see the formatR() function). To work with command lines, tidy.source() can be helpful. It takes a file, formats it into a tidy form, and writes the output into a specified file, or simply to the console. When I first saw the Pretty-R tool provided by Revolution Analytics, I was not completely satisfied because it is just not “pretty” enough. If they have R and the formatR package installed on the server, perhaps they can provide another option to help the users format their source code in a more beautiful way, e.g. they could have turned to (You may ask: what the heck is the difference?! In this case, you don’t need the formatR package because you are not “picky” enough…) At the same time, I also removed the function tidy.source() from the animation package. Since this was my first R package, I tended to put everything into a single package, including some functions which are apparently irrelevant. I put them there just for my own use. Now all of them are defunct; see ?'animation-defunct' in the next version of the animation package. 	 0 Comments
Google, The Brew’s On Me	https://www.r-bloggers.com/2010/12/google-the-brews-on-me/	December 3, 2010	awaiting assimilation	 While drinking these fine liquids may up your trip count to the john, they can also make your R reports better. Not because they allow you to reach your Ballmer Peak… but because each of these elixers is the direct result of brewing. Okay, I’m reaching. Brew is my own clever name for my very own clever R package that allows the lucky user to use common template processing techniques to generate textual reports. It’s similar to Sweave, but brew can do something that Sweave cannot. It can utilize control flow statements to loop through repeated text and code chunks or conditionally include them with if/else statements. I could go into detail here. Instead I’ll point to a most excellent resource that does a far better job. Brew was written on a whim, stemming from a conversation I had with my colleague Charles Dupont (Hmisc maintainer). We were discussing the very issue that Sweave was limited because of lack of control-flow. I bet him it would be very easy to come up with a function that could work just like eRuby (he’s a big fan of ruby) or PHP. And a few days later I had it working! All it took was a little hubris, impatience, and laziness! (Look at the code to see how lazy. I dare you.) Which brings us to Google… I don’t know if there are others of you like me out there.  I sometimes stare at the non-existent windows in my drab, colorless office and wonder what the heck I’m doing. Reading blogs, tweeting the interwebs, tinkering rather than working, planning my marathon debut (years away), thinking about software, writing code bits, all the while making sure that the code I do write won’t rot and never get put to use. I wonder if anyone uses my software. Then all of a sudden (more like at the beginning of this year), I get an email from someone at Google. (!)  He says there’s an issue with your Brew package. Nothing big, just an extra file in the tarball that shouldn’t be there. I email a thanks. I update Brew. Then a few months later I’m curious about what they’re up to, why they use brew. He says a few analysts (smart analysts- uber smart analysts- analysts that passed the rigorous interview process- the same interview process I didn’t pass five years ago) are using Brew. I ask why they use it. He puts me in touch with one of the uber smart analysts that passed the rigorous interview process (it was them that asked me to apply! I wasn’t looking for a job). I start an email conversation with him that leads to the following (paraphrased, a little): UberSmartAnalyst: So and So told me you were curious about what we use the brew package for at google. The typical use case of brew is to dynamically generate LaTeX documents, which are converted to pdf or html. Thanks for making this package available, it is very useful and convenient. Me: Excellent! Using brew in place of Sweave was one of my main motivations for writing the package in the first place! UberSmartAnalyst: I actually started with Sweave, and it was incredibly frustrating 🙂 Being able to use R loops and conditionals to repeat/omit blocks of text is so convenient. Well, I guess I have to thank my good friend Stuart Smalley for his encouraging words: I am good enough, I am smart enough, and dog-gone it, Google likes me. Your welcome, Google. 	 0 Comments
Evolution of Rcpp code size	https://www.r-bloggers.com/2010/12/evolution-of-rcpp-code-size/	December 3, 2010	romain francois	"I’ve been contributing to Rcpp for about a year now, initially to add missing bits that were needed for the development of RProtoBuf. This led to a complete redesign of the API, which now goes way beyond the initial code (that we now call classic Rcpp API). This has been quite a journey in terms of development with more than 1500 commits to the svn repository of the project on R-forge, and promotion with presentations at RMetrics 2010, useR 2010, LondonR and at Google, as well as many blog posts about Rcpp and the packages that derive from it. I wanted to take this opportunity to express visually how vibrant the development of Rcpp has been since it was first relaunched in 2008, and since I started to contribute.  The graph below shows the evolution of the number of lines (counting the .h, .cpp, .R, .Rd, .Rnw files) accross released versions of the Rcpp package on CRAN The first thing I need for this is to download the 32 versions of Rcpp that have been released since 0.6.0. Then, all it takes is some processing with R to extract the relevant information (number of lines in files of interest), and present the data in a graph. I’m also taking this opportunity to have some fun with raster images and the png package The code explosion that started around version 0.7.8 marks the beginning of development of two of the most exciting and addictive projects I ever worked on: modules and sugar
 The acceleration between 0.8.8 and the current version 0.8.9 represents many of the improvements that were made in modules. That alone, with more than 8000 new lines of code and documentation represents about 4 times as many lines as the total number of lines in 0.6.0 We still have plenty of ideas, and Rcpp will continue to evolve to deliver a quality interface between R and C++, to the best of the current team’s abilities.  The full code is available below:  "	 0 Comments
Programming with R – Processing Football League Data Part II	https://www.r-bloggers.com/2010/12/programming-with-r-%e2%80%93-processing-football-league-data-part-ii/	December 3, 2010	Ralph	Following on from the previous post about creating a football result processing function for data from the football-data.co.uk website we will add code to the function to generate a league table based on the results to date. To create the league table we need to count various things such as the number of games played, number of wins/draws/losses, goals scored etc. This information is available in the results object that is loaded from a csv file in the function as it stands. To facilitate these calculations we create a data frame with a row for each team in the division and then calculate the statistics required – this was a reason for ordering the factors in the HomeTeam and AwayTeam columns of the results table. The data frame is created with the code below: There are a number of slots that are may be redundant in a league table but are used for intermediate calculations, such as HomeWin and AwayWin that are combined to find the total number of victories for a team. The number of games played by each team home and away are counted using the table command for the two columns respectively. The labels created by the table command are discarded using the as.numeric function to retain only the number of games. The table command is also used to count the number of wins, draws and losses at home and away for each team. The commands are shown here: Note that we subset on the values in the FTR column, which is full-time result, and then count. The subsetting is reversed when looking at the away fixtures because a victory for the team is now an away win rather than a home win. This information is then combined to get total games played, won etc. The total points is calclated by multiplying the number of wins, draws and losses by the number of points awarded for each match outcome. The next set of calculations are to count the number of goals scored, goals conceeded and goal difference. The tapply function is used for these calculations. The tapply function applies the sum to the number of goals scored at home or away, and the number of goals conceeded by each team in the division. These are then combined to create totals home and away: The ifelse statement is used to handle situations where a team hasn’t played a home and/or away fixture yet. The goal difference is easy to calculate: Now that all of the statistics have been calculated we sort the table based on the number of points, goal difference and finally alphabetically. There might be different ways that we can order the teams but this is what we will use for the time being: The ordering might look odd but we want to ranking from highest to lowest points and goal difference but then in ascending alphabetical order for the teams. The whole function is now: There are other functionality that we might want to add to the function. 	 0 Comments
Google AI Challenge: Languages Used by the Best Programmers	https://www.r-bloggers.com/2010/12/google-ai-challenge-languages-used-by-the-best-programmers/	December 2, 2010	C		 0 Comments
Méthodes de Monte-Carlo avec R	https://www.r-bloggers.com/2010/12/methodes-de-monte-carlo-avec-r/	December 2, 2010	xi'an	The translation of the book Introducing  Monte Carlo Methods with R is close to being completed. The copy-editing and page-setting are done, I have received the cover proposal and am happy with it, so it should now go to production and be ready by early January, (earlier than the tentative end of February indicated on amazon) maybe in time for my R class students to get it before the exam. Thanks to the efforts of Pierre-André Cornillon and Eric Matzner (from the Université de Haute-Bretagne in Rennes), the move from the Use R! series format to the Pratique R series format was done seamlessly and effortlessly for me. (Again, thanks to the traductors who did produce their translations in sometimes less than a month!) I am curious to see how much of a market there is for the French translation… The Japanese translation is scheduled for August 2011 at the very least, but I am obviously not involved at all in this translation! 	 0 Comments
pgfSweave 1.1.0 now on CRAN!	https://www.r-bloggers.com/2010/12/pgfsweave-1-1-0-now-on-cran/	December 2, 2010	cameron	The next release of pgfSweave is now on CRAN! It has been a while since I posted about pgfSweave and there have been some significant changes in the past couple of months. The main new features are: And of course bug fixes: See the NEWS file for the complete list of changes and the vignette for information on now to use the new options. 	 0 Comments
Another boring blog	https://www.r-bloggers.com/2010/12/another-boring-blog/	December 2, 2010	Daniel Hocking		 0 Comments
R 101 at TDWI	https://www.r-bloggers.com/2010/12/r-101-at-tdwi/	December 2, 2010	David Smith	Last month TDWI's James Powell interviewed Revolution CEO Norman Nie and published the interview as “R 101“. The article answers some of the basic questions about R, such as: … and much more. Read the full article at TDWI for the answers. TDWI: Q&A: R 101   	 0 Comments
R with Vim	https://www.r-bloggers.com/2010/12/r-with-vim/	December 2, 2010	Michał	For all those who think that Vim is The Editor for text files, and simultaneously think that R is The EnvironmentForStatisticalAnalysisAndGraphics. After trying out various options for intergrating Vim with R I settled on the following configuration: Vim and R using Vim-R-Plugin in action Here is a way how to use both plugins simultaneously for Sweave files. The instruction applies to Ubuntu (so probably any Linux-like system). On Windows the ~/.vim directory corresponds to the ‘vimfiles’ directory, which most likely is something like ‘c:Program FilesVimvimfiles’. So: This will essentially load both plugins one after another. QED. See here how to set it up on Mac 	 0 Comments
Random variable generation (Pt 2 of 3)	https://www.r-bloggers.com/2010/12/random-variable-generation-pt-2-of-3/	December 2, 2010	csgillespie	"This post is based on chapter 1.4 of Advanced Markov Chain Monte Carlo. Another method of generating random variates from distributions is to use acceptance-rejection methods. Basically to generate a random number from , we generate a RN from an envelope distribution , where .  The acceptance-rejection algorithm is as follows: Repeat until we generate a value from step 2: 1. Generate  from  and  from  2. If , return  (as a random deviate from ). This example illustrates how we generate  RNs using the logistic distribution as an envelope distribution. First, note that  On setting , we get . This method is fairly efficient and has an acceptance rate of  since both  and  are normalised densities. This example is straightforward to code: To check the results, we could call myrnorm a few thousand times: 
 Suppose the density  is expensive to evaluate. In this scenario we can employ an easy to compute function , where  .  is called a squeeze function. In this example, we’ll use a simple rectangular function, where  for . This is shown in the following figure: 
The modified algorithm is as follows: Repeat until we generate a value from step 2: 1. Generate  from  and  from  2. If  or , return  (as a random deviate from ). Hence, when  we don’t have to compute . Obviously, in this example  isn’t that difficult to compute. "	 0 Comments
Statistique de l’assurance STT6705V, partie 12	https://www.r-bloggers.com/2010/12/statistique-de-lassurance-stt6705v-partie-12/	December 2, 2010	arthur charpentier	"
 
     

        i.e.   Thus,  Complete expectation of life is then  or its discrete version  Hence, it is possible to write  which can be extended in the dynamic framework as follows  A natural estimator of that quantity is  i.e., with Lee-Carter model  All those quantities can be computed quite simply. But first, I have to work a little bit on the dataset, at least to be able to predict mortality up to 100 years old (with the demography package, we have to stop at 90). One idea can be to mix two estimation techniques: the nonlinear Poisson regression to get  and  up to 99 years old, and then to use Rob Hyndman’s package to estimate and predict the  component. But first, we have to check that  ‘s  and ‘s
with the two techniques are not too different. The code for the first model is
 for the second one. Then, we can compare predictions for  ‘s e.g. (with output from econometric regression in blue, and Rob’s package in red) The second problem is that we have to use a linear transformation, since in the econometric package, we do not use the same constraints as in Rob’s package. Here, it was Then we can compute predicted  for all ages and years, "	 0 Comments
Rd2roxygen: Convert Rd to roxygen documentation	https://www.r-bloggers.com/2010/12/rd2roxygen-convert-rd-to-roxygen-documentation/	December 1, 2010	Yihui Xie	I must admit that I have been tired of maintaining my R packages for a long time, and the main reason is I feel really uncomfortable with writing R documentations (Rd). The required structure of an R package mainly includes two directories R and man — the former for the R source code (typically functions), and the latter for documentation. In the past I usually use package.skeleton() to generate a skeleton of the documentation and fill in the tags one by one. The main headache is to frequently switch between the two files and type the raw Rd commands such as \title{} and \description{}. People told me all kinds of advantages of Emacs+ESS in the past few years, and I tried it for more than ten times, but often ended up with frustration (so I installed and removed Emacs repeatedly for several times). My last attempt a few months ago succeeded finally, and I realized how easy it was to document R functions in Emacs with roxygen. See the 1-minute video below:  In Emacs (with ESS), you just press C-c C-o to insert a documentation template as roxygen comments above your function, and the rest of things to do are to fill the @tags and update them when necessary. These inline comments will be converted to the real Rd files when we run R CMD roxygen your.package or roxygenize('your.package'). For details please read the vignette of the roxygen package. In all, roxygen makes the life of developers much easier. Then a natural problem comes: What if I have already been “stuck” in the raw Rd files? Since R 2.9.x, Duncan Murdoch announced a new way of parsing Rd files (mainly the function tools::parse_Rd()), which is quite useful for converting Rd files to other formats (e.g. Rd2txt(), Rd2latex(), …). This also brings us the possibility of converting Rd files to roxygen comments. I asked the question in R-help a few days ago, and Hadley pointed me to his code snippets on this conversion. I was fairly happy about them, so I sat down and spent one day on improving them. Finally I released this work as an R package Rd2roxygen on CRAN. The main function that the developers may want to use is Rd2roxygen(): given a package root directory, this function can parse all the Rd files under the man directory, convert them into roxygen comments and update all the corresponding R scripts under the R directory. Most of the Rd tags are supported, but we may need some tiny adjustment by hand. Moreover, the function roxygen_and_build() can be of help for building packages. It tries to remove the unnecessary Rd files generated automatically by roxygen, replaces % with \% (because I believe it is very uncommon to write comments with % in Rd files; this replacement is optional, though), and builds the source package. Optionally it also installs or checks the package. After I have done this package, I experimented on my animation package, and the converted package, when roxygenized, can pass R CMD check (I have moved the package to GitHub). This finally gives me new courage to actively maintain my old packages… My brain cannot deal with too many functions without seeing the documentation instantly. This new package might have little bugs, of course. Please let me know if it does not work for you. 	 0 Comments
Recent developments in the drug war	https://www.r-bloggers.com/2010/12/recent-developments-in-the-drug-war/	December 1, 2010	Diego Valle-Jones		 0 Comments
Online Resources for Learning R	https://www.r-bloggers.com/2010/12/online-resources-for-learning-r/	December 1, 2010	Josh Paulson	Online classes are an easy and convenient way to learn more about a topic of interest. Not surprisingly, there are a variety of online resources, free and otherwise, to learn more about R. From online graduate classes, to the more “learn at your own pace” approach, here are some resources I have found useful: Programming R – Beginner to advanced resources for the R programming language Statistics.com – Variety of courses including beginner and advance topics in R University of Washington – Certificate in Computational Finance with R Programming Carnegie Mellon – Open and Free Courses including teaching Statistics with R Penn State – Online Learning through the Department of Statistics 	 0 Comments
Le Monde puzzle [48]	https://www.r-bloggers.com/2010/12/le-monde-puzzle-48/	December 1, 2010	xi'an	This week(end), the Le Monde puzzle can be (re)written as follows (even though it is presented as a graph problem): Given a square 327×327 symmetric matrix A, where each non-diagonal entry is in {1,2,3,4,5} and , does there exist a triplet (i,j,k) such that  Solving this problem in R is very easy. We can create a random matrix A and check whether or not any of the five triple indicator matrices  has a non-zero diagonal entry. Indeed, since  satisfies  there is a non-zero entry iff there exists a triplet (u,v,w) such that the product  is different from zero. Here is the R code: I did run the above and did not find any case where no triplet was sharing the same number. Neither did I for 326. But Robin Ryder told me that this is a well-known problem in graph theory that goes under the name of Ramsey’s problem and that 327 is an upper bound on the number of nodes for the existence of the triplet with 5 colours. So this is another illustration of a case when crude simulation cannot exhibit limiting cases in order to void a general property, because of the number of possible cases. Incidentally, I wonder if there is a faster way to produce a random symmetric matrix than the cumbersome Using the alternative saving on the lower triangular part certainly takes longer… 	 0 Comments
How to create PDF reports with R	https://www.r-bloggers.com/2010/12/how-to-create-pdf-reports-with-r/	December 1, 2010	David Smith	Sweave is a literate programming system included with R. It makes it possible to create a PDF document containing not just text, but also tables and charts generated from R. The process is automated, so once you've created an Sweave document (which includes both LaTeX text markup and R commands), you can create the formatted text and insert the tables and charts (based on the latest data you have) into the PDF at the appropriate places in a single step. Sweave can be a little daunting to get started with, but Jeromy Anglim has created three tutorials to get you going. Tutorial 1 shows how to make a document based on data stored in a database. Tutorial 2 shows how to make a series of polished PDF reports in batch mode. And Tutorial 3 shows how to incorporate user input to create “parameterized” reports. Jeromy provides all the source code to recreate the reports and plenty of practical tips (how to use “make”, for example), so if you follow along you'll soon be able to automate reports containing tables like this:   and embedded graphics like this:  Be aware though that Sweave doesn't solve every problem: see this list of top 10 annoyances with Sweave from Mario Pineda-Krch. Jeromy Anglim: Sweave Tutorial 3: Console Input and Output – Multiple Choice Test Analysis     	 0 Comments
RcppGSL 0.1.0	https://www.r-bloggers.com/2010/12/rcppgsl-0-1-0-2/	December 1, 2010	Thinking inside the box	"
We have now found some time to finish this work for a first release, together
with a nicely detailed eleven page
package vignette. 
As of today, the package is now a
CRAN package, and
Romain already posted a nice 
announcement on his blog
and on
the rcpp-devel list.

 
So what does RcppGSL do?  I gave the package
its own webpage here
as well and listed these points as key features of RcppGSL:

 
Also provided is a simple example which is a simple implementation of a column norm (which we could easily
compute directly in R, but we are simply re-using an example from
Section 8.4.14
of the GSL manual):

 
 
This example function is implemented in an example package contained in the
RcppGSL package itself — so that users have a complete stanza to use in
their packages. This will then build a user package on Linux, OS X and Windows provided the
GSL is installed (and on Windows you have to do all the extra steps of
defining an environment variable pointing to and of course install Rtools to
build in the first place—Linux and OS X are so much easier for
development).

 
Another complete example is in the package itself and provides a faster
(compiled) alternative to the standard lm() function in R; this
example is the continuation of the same example I had in several versions of
my Intro to HPC with R tutorials and in the
Rcpp 
package itself as an early example.

 
We will try to touch base with CRAN package authors using both GSL and Rcpp to see
how this can help them.  The API in our package may well be incomplete, but
we are always happy to try to respond to requests for additional features
brought to our attention, preferably via the rcpp-devel list.


 
More information is on the 
RcppGSL page.
Questions, comments etc should go to the
rcpp-devel mailing list
off the R-Forge page.

 "	 0 Comments
R Workflow: Slides from a Talk at Melbourne R Users (1st Dec 2010)	https://www.r-bloggers.com/2010/12/r-workflow-slides-from-a-talk-at-melbourne-r-users-1st-dec-2010/	December 1, 2010	Jeromy Anglim	A PDF of the slides  is available in fullscreen and four-to-a-page handoutformat. Most slides contain links to resources for learning more. Thus, it should be possible to understand the main ideas of the talk just  by just reading the slides and reading the links. A video of both my talk and the previous talk by Geoff Robinson will hopefully be made available in the not too distant future (I’ll post when it is available). If you want to learn more about beamer, LaTeX and make, you can check out the source code used to produce the PDF of the presentation. The repository is available on GitHub: I’d like to thank Yuval Marom for all the work he has put into kickstarting and organising the Melbourne R User Groupand Deloitte for providing such a great venue. 	 0 Comments
bubble chart by using ggplot2	https://www.r-bloggers.com/2010/12/bubble-chart-by-using-ggplot2/	December 1, 2010	R on Guangchuang Yu	The visualization represented by Hans Rosling’s TED talk was very impressive. FlowingData provides a tutorial on making bubble chart in R. I prefer ggplot2 for graphics. Here is what it looks like.  	 0 Comments
Top 10 things that suck about Sweave	https://www.r-bloggers.com/2010/12/top-10-things-that-suck-about-sweave/	December 1, 2010	Mario Pineda-Krch	"People rave about Sweave and the literate programing paradigm and I am guilty as charged. I speak Sweave, I think Sweave, I dream Sweave. As a matter of fact my default mode of operation is Sweave and anything else is an exception. I do all my math and stats in Sweave, I write manuscripts in Sweave, my lab notebook is in Sweave, I manage my personal finances with Sweave, and I do my presentations indulging in a most satisfying threesome with LaTeX, R and Beamer. Being this enamoured with Sweave might suggest that, in my eyes at least, Sweave might be perfect. But like all intimate relationships there are rough patches and when you say tomato and it says tomahto it takes work to iron out the differences. So let’s take a close and brutally honest look at the apple of my eye. Not that I am questioning my relationship, on the contrary, identifying and acknowledging the limitations of Sweave will hopefully help me become more in tune with it and perhaps even find new ways of working with it. An alternative interpretation would be that it is far easier to be the devil’s advocate and enumerate Sweave’s cons rather than its innumerable pros. So here it is, my Top 10 things that suck about Sweave and to keep everyone on the edge of their seats I will not be proposing any solutions (of course I have solutions, but giving those away here would take the fun out of it). And finally, a personal message to Friedrich: my sincere apologies if I came across to harshly. Rest assure that my devotion and admiration for Sweave is unwavering. This is from the “Mario’s Entangled Bank” blog (http://pineda-krch.com) of Mario Pineda-Krch, a theoretical biologist at the University of Alberta.
 "	 0 Comments

title	link	date	author	text	comment_rbloggers
A Handbook of Statistical Analyses Using R – Everitt and Hothorn (2006)	https://www.r-bloggers.com/2009/01/a-handbook-of-statistical-analyses-using-r-everitt-and-hothorn-2006/	January 31, 2009	bryan	 A Handbook of Statistical Analyses Using R addresses a list of several common statistical analyses in great detail.  Over a course of 15 chapters, the handbook takes the reader from an introduction to R through a discussion of statistical inference, to linear and logistic regression, tree analysis, survival analysis, longitudinal analysis, meta-analysis, factoring, scaling, and clustering.  The handbook has a peer-reviewed journal style that will be familiar to academic researchers and each chapter stands on its own.  This approach makes the text exceptionally useful in the academic setting as a professor can distribute and assign the first chapter of the book to her Research Methods 101 course; the final chapters on scaling and dimensionality to her Psychometrics Methods course; the last chapter on clustering to her Marketing Research course; and require the entire book for her graduate methods course.  For custom research shops making the transition to R or who frequently hire new entry level R users, this book will work well as a reference and training manual.   The handbook does show typical first edition flaws.  There are sporadic mistakes in grammar such as misspellings and incorrect words.  The overall organization of the book is strong, but the chapter level organization is less effective.  Each chapter begins with a discussion of all of the datasets used in that chapter and is followed by examples and applications based on those datasets. In chapters where there are several examples, the discussion of the data is too detached from its corresponding example.  When the reader reaches the example based on the first dataset they have likely forgotten the relevant details about that data’s structure.  Grouping the data discussions with the examples they accompanied would have made the example based approach more effective. read more 	 0 Comments
State-of-the-art in parallel computing with R: New paper	https://www.r-bloggers.com/2009/01/state-of-the-art-in-parallel-computing-with-r-new-paper/	January 30, 2009	Thinking inside the box		 0 Comments
Importing Generic Function from Other Package: A Tip on Writing a R Package	https://www.r-bloggers.com/2009/01/importing-generic-function-from-other-package-a-tip-on-writing-a-r-package/	January 30, 2009	Yu-Sung Su		 0 Comments
Visualizing Eigenfactors	https://www.r-bloggers.com/2009/01/visualizing-eigenfactors/	January 30, 2009	John Myles White	These interactive graphics are simply beautiful. And they just so happen to be profoundly informative about the structure of modern science as well. Here’s to the hope that we will see more work from Moritz Stefaner soon that shows how our aesthetic and scientific demands can be met simultaneously. HT to Infosthetics. 	 0 Comments
Runing R in the Command Mode in the Window system	https://www.r-bloggers.com/2009/01/runing-r-in-the-command-mode-in-the-window-system/	January 29, 2009	Yu-Sung Su		 0 Comments
MCMCglmm package for R	https://www.r-bloggers.com/2009/01/mcmcglmm-package-for-r/	January 29, 2009	Gregor Gorjanc		 0 Comments
InDesign: Export Selection to PDF	https://www.r-bloggers.com/2009/01/indesign-export-selection-to-pdf/	January 29, 2009	[email protected]		 0 Comments
Controlling margins and axes with oma and mgp	https://www.r-bloggers.com/2009/01/controlling-margins-and-axes-with-oma-and-mgp/	January 26, 2009	bryan	"When creating graphs, we’re usually most concerned with what happens near the center of our displays, as this is where most of the important information is generally held.  But sometimes, either for aesthetics or clarity, we want to adjust what’s outside of the box – in the margins, labels or tick marks.  The par() function offers several ways to do this and I’ll discuss two that deal primarily with spatial orientation – rather than content – below.

The oma, omd, and omi options read more "	 0 Comments
New CRAN Task View on HPC	https://www.r-bloggers.com/2009/01/new-cran-task-view-on-hpc/	January 24, 2009	Thinking inside the box		 0 Comments
Data Analysis and Graphics Using R – Maindonald and Braun (2003)	https://www.r-bloggers.com/2009/01/data-analysis-and-graphics-using-r-maindonald-and-braun-2003/	January 23, 2009	bryan	 Data Analysis and Graphics Using R (DAAG) covers an exceptionally large range of topics.  Because of the book’s breadth, new and experienced R users alike will find the text helpful as a learning tool and resource, but it will be of most service to those who already have a basic understanding of statistics and the R system. Although the text includes both an Introduction to R section (chapter one) and a discussion of the basics of quantitative data analysis (chapters two through four), these chapters will be most useful as overviews (or reviews for more experienced readers), as they lack the detail required to take a reader from no knowledge of these subjects to a functional understanding.  For example, chapter one discusses importing data in .txt and .csv format, but the foreign package is not discussed until chapter fourteen – the final chapter of the book.  In practice, .txt data structures are not common enough to justify relegating a discussion of the foreign package to the supplemental materials and a researcher stuck with a .sav or .dbf file would not leave chapter one with enough knowledge to import their data into R. read more 	 0 Comments
R Programmer – Calgary	https://www.r-bloggers.com/2009/01/r-programmer-calgary/	January 23, 2009	Programming R	"Originally posted on r-sig-jobs listserv: Hello All, We are in ned of an R programmer for a project in Calgary, Alberta, Canada.  We need the person to be local as the project involves coordination of a series of experts.  The project is only about 150 hours.  The ideal candidate will be an experienced R programmer who is available immediately.  Please contact me for further details or if you know of someone that might be a suitable candidate.  A referral fee of $200 will be given if your referral is chosen. Cheers,
Tyler
[email protected] "	 0 Comments
Install JAGS and rjags in Fedora	https://www.r-bloggers.com/2009/01/install-jags-and-rjags-in-fedora/	January 23, 2009	Yu-Sung Su		 0 Comments
R: Combining vectors or data frames of unequal length into one data frame	https://www.r-bloggers.com/2009/01/r-combining-vectors-or-data-frames-of-unequal-length-into-one-data-frame/	January 23, 2009	markheckmann	Today I will treat a problem I encounter every once in a while. Let’s suppose we have several dataframes or vectors of unequel length but with partly matching column names,  just like the following ones: This for example may occur when fitting several multiple regression models each time using different combination of regressors. Now I would like to combine the results into one data frame.  The merge() as well as the rbind() function do not help here as they require equal lengths. I posted this matter on r-help as my first solution was somewhat awkward and could not be generalized to any data frames or list of data frames. The first solution was posted by Charles C. Berry. myList is a list containing the data frames as elements What he does is to use a nested loop. The inner loop runs for each data frame over each column name. It basically takes each column name and the correponding element [i, j] from the data frame ( myList[[i]] ) and writes it into an empty data frame (dat). Thereby a new column that is named just like the column from the list element data frame is created. The cells that are left out are automatically set NA. Note that the order of the output columns depends on the input order. The list below renders a different order, though it contains the same elements but ordered differently. Another solution was posted by Henrique Dallazuanna. This one has the advantage that it does not use loops. It looks a bit scary at first, so let's examine it starting from the inside. As a next step for each vector with column names all columns are selected leaving those that are not present with NA values. As a last step the vectors having the same columns are combined. The only little flaw in this function is that the column names of the first vector are taken as column names of the developing data frame. Using the second list from above, gives the following. Thus, in a last step we need change the column names of the data frame. Well this works but it would be much more convenient to get this done in one single function and well, since october 2008 there is one. It can be found in the plyr package written by Hadley Wickham. So the solution is as easy as: The results: Now, this is nice! It is really worthwhile having a look at Hadley Wickhams plyr package as it provides a lot of functions that make life a lot easier when it comes to splitting list or data frames, doing a calculation or not and merge them afterwards again. More on that another day. Cheers, Mark 	 0 Comments
Interesting tip about multicolor title of a plot	https://www.r-bloggers.com/2009/01/interesting-tip-about-multicolor-title-of-a-plot/	January 23, 2009	Paolo		 0 Comments
EMBL Heidelberg seeks a Bioinformatician / Data Analyst	https://www.r-bloggers.com/2009/01/embl-heidelberg-seeks-a-bioinformatician-data-analyst/	January 21, 2009	Programming R	Originally posted on r-sig-jobs listserv: Hello, Might be interesting for some of you: Bioinformatician / Data Analyst Grade: 5 or 6, depending on experience EMBL site: EMBL Heidelberg Commencing date: As sooon as possible, after closing date (28 February 2009) read more 	 0 Comments
Vacancy postdoc computational systems biology – Amsterdam	https://www.r-bloggers.com/2009/01/vacancy-postdoc-computational-systems-biology-amsterdam/	January 21, 2009	Programming R	"Originally posted on r-sig-jobs listserv: *Postdoc Position Computational Systems Biology ‘Systems Bioinformatics: Computational Modelling Methods’ f/m
* *
VU University Amsterdam, the Netherlands
* *Research project * read more "	 0 Comments
Sweave.sh plays with weaver	https://www.r-bloggers.com/2009/01/sweave-sh-plays-with-weaver/	January 21, 2009	Gregor Gorjanc		 0 Comments
Customizing R: startup script	https://www.r-bloggers.com/2009/01/customizing-r-startup-script/	January 19, 2009	[email protected]		 0 Comments
Maximum likelihood estimation in R	https://www.r-bloggers.com/2009/01/maximum-likelihood-estimation-in-r/	January 18, 2009	Quantitative Finance Collector		 0 Comments
R vs. SAS	https://www.r-bloggers.com/2009/01/r-vs-sas/	January 17, 2009	martin	Everything started with the article in the NYT talking about R – and of course – did mention SAS. Andrew Gelman picked up the article and posted his take on the matter. Maybe it are sentences like Andrew’s “And it’s good to hear that SAS is in trouble” and Anne H. Milley, director of technology product marketing at SAS: “We have customers who build engines for aircraft. I am happy they are not using freeware when I get on a jet.”, which did stir the readers up.   	 0 Comments
IBrokers Featured on Quantitative Trading	https://www.r-bloggers.com/2009/01/ibrokers-featured-on-quantitative-trading/	January 17, 2009	Joshua Ulrich	"
 "	 0 Comments
Versions of Sweave.sh	https://www.r-bloggers.com/2009/01/versions-of-sweave-sh/	January 16, 2009	Gregor Gorjanc		 0 Comments
Use of include and input in Sweave documents	https://www.r-bloggers.com/2009/01/use-of-include-and-input-in-sweave-documents/	January 16, 2009	Gregor Gorjanc		 0 Comments
Creating R Packages: A Tutorial	https://www.r-bloggers.com/2009/01/creating-r-packages-a-tutorial/	January 16, 2009	Gregor Gorjanc		 0 Comments
littler 0.1.2	https://www.r-bloggers.com/2009/01/littler-0-1-2/	January 14, 2009	Thinking inside the box	"
This version adds two new command-line switches:
 

As usual, our code in our
svn archive, on my r page, and in the
local directory here. A fresh package
is in Debian’s incoming queue, and  Jeff’s
littler page at Vanderbilt should reflect the new release soon too.



 "	 0 Comments
Analyzing Nike+ using R	https://www.r-bloggers.com/2009/01/analyzing-nike-using-r/	January 14, 2009	[email protected]		 0 Comments
Error Bars in R (& other R stuff)	https://www.r-bloggers.com/2009/01/error-bars-in-r-other-r-stuff/	January 14, 2009	Paul Gribble		 0 Comments
R resources for psychologists	https://www.r-bloggers.com/2009/01/r-resources-for-psychologists/	January 12, 2009	Thom Baguley		 0 Comments
R gets some -E-S-P-E-C-T	https://www.r-bloggers.com/2009/01/r-gets-some-e-s-p-e-c-t/	January 12, 2009	dan	NEW YORK TIMES STORY ON THE APPEAL OF R  (click to view movie) It is no secret that Decision Science News is crazy about the R language for statistical computing. Find out why R is so great in this New York Times article. Then start to teach yourself R with our short series of video tutorials. Addendum: Check out the hordes of R supporters in this comments of this Freakonomics blog post, correcting an assumption by Ayers that happens to be 180 degrees in the wrong direction. Animation credit: Friend of Decision Science News Yihui Xie. 	 0 Comments
TextWrangler and R	https://www.r-bloggers.com/2009/01/textwrangler-and-r/	January 10, 2009	[email protected]		 0 Comments
Rcpp 0.6.3	https://www.r-bloggers.com/2009/01/rcpp-0-6-3/	January 9, 2009	Thinking inside the box	" 
This version adds a fix to the OS X installation (thanks to Simon Urbanek),
adds some ‘view-only’ classes for R vectors, matrices and string vectors
(kindly suggested/provided by David Reiss) as well two shorter helper
functions to derive compilation and linker flags for packages using Rcpp.

 "	 0 Comments
Review of R in NYT and GDAT	https://www.r-bloggers.com/2009/01/review-of-r-in-nyt-and-gdat/	January 8, 2009	Neil Gunther		 0 Comments
R in the New York Times (updated)	https://www.r-bloggers.com/2009/01/r-in-the-new-york-times-updated/	January 8, 2009	Paul Gribble		 0 Comments
R-Sessions 30: Visualizing missing values	https://www.r-bloggers.com/2009/01/r-sessions-30-visualizing-missing-values/	January 8, 2009	Rense Nieuwenhuis		 0 Comments
Google Summer of Code 2009	https://www.r-bloggers.com/2009/01/google-summer-of-code-2009/	January 7, 2009	Thinking inside the box		 0 Comments
R featured in New York Times article	https://www.r-bloggers.com/2009/01/r-featured-in-new-york-times-article/	January 7, 2009	Thinking inside the box	"
That’s silly on so many levels. A concise and rather appropriate follow-up came in early from
Frank Harrell,
a long-time S and R advocate:
 
Achim already added this (and two more posts
from the aforementioned threads) to the 
fortunes package that collects such choice quotes.

 
R in Finance (the topic of
our upcoming conference)
gets mentioned as well. Now, as editor of the
Finance task view, I find that second half of
 "	 0 Comments
Forecasting Presidential Elections	https://www.r-bloggers.com/2009/01/forecasting-presidential-elections/	January 7, 2009	John Myles White	Because of Andrew Gelman’s strong, repeated recommendations, I’ve been reading “Forecasting Presidential Elections” by Steven J. Rosenstone for the last two days. It’s quite a remarkable book and complex enough that I’m sure I’ll return to it many times after I’ve finished it. I was particularly intrigued by a table in the first chapter noting the performance of the Gallup poll over the years. The very first Gallup polls were conducted well in advance of the elections, and it seems that Gallup thought that this had been a substantial source of error in his predictions. A review of the data in the aforementioned table, though, makes clear that there is essentially no meaningful relationship between the number of days in advance of the election when the poll was begun and the accuracy of that poll. If one assumes that the Gallup polls were accurate measures of the state of the public’s opinion on the dates when they were conducted, the data suggests that there are years when the public’s electoral decision is decided well in advance of the election and there are other years when the public’s decision is decided almost immediately before the election. To make all of this clear, I graphed the data describing the accuracy of all of the Gallup electoral polls from 1936 to 1980 as a function of how far ahead of the election the poll was conducted. As you can see, there’s virtually no pattern: the light blue line in the chart is the least squares line and the gray line is simply a horizontal line plotted at 0. The least squares line reflects a statistically insignificant correlation of 0.20. Another point that struck me while reading Rosenstone’s book was his discussion of the non-response error of polls. Non-response error is normally explained as the errors in telephone polls caused by those who don’t pick up their phones, those who refuse to be surveyed and those who simply don’t own phones. The first two problems always make sense to me, but I always find myself wondering about the third: how many Americans actually vote in presidential elections who don’t own phones? I’d love to have an answer to that question if anyone has relevant data. 	 0 Comments
New York Times on R	https://www.r-bloggers.com/2009/01/new-york-times-on-r/	January 7, 2009	Gregor Gorjanc		 0 Comments
Combining R and LaTeX with Sweave	https://www.r-bloggers.com/2009/01/combining-r-and-latex-with-sweave/	January 6, 2009	:)-k		 0 Comments
Multiseat setup via Userful	https://www.r-bloggers.com/2009/01/multiseat-setup-via-userful/	January 3, 2009	Thinking inside the box	"
Shortly after Christmas, that computer suffered a catastrophic disk failure (and as an aside, I hate LVM when that
happens…).  So I reinstalled, this time using the Ubuntu rather Kubuntu variant.  This should allow use of
Userful Multiplier
— a commercial multiseat solution with 
free two-seat licenses.
The base package even comes via the Ubuntu repos.

 
I still had a couple of minor issues. One was possibly related to the Radeon card (as in: don’t drive one
monitor in dvi mode and one in analog mode but rather use both in analog mode via a dvi/analog dongle) so the
live cdrom offered by Userful just went into a perpetual ‘reconfigure, reboot, reconfigure, reboot, …’ loop.
Another hitch was that their license manager no longer wanted to use the license key I had requested in
November when I tried in vain to use Userful with KDE. And of course I wouldn’t a new key as the home ip
address hadn’t changed…  Now, with a newly requested key from another IP address, things appear to work at
last using the default Gnome setup — and the kids are back in proper ‘parallel’ use of their workstation.

 
All in all, Userful Multiplier is a nice and useful product especially as long as stock XFree does them the
favour of no longer competing in the basic two-seat case.



 "	 0 Comments
RWinEdt and Windows Vista/Window 7	https://www.r-bloggers.com/2009/01/rwinedt-and-windows-vistawindow-7/	January 1, 2009	Yu-Sung Su		 0 Comments
R/Finance conference in Chicago in April: Call for Papers	https://www.r-bloggers.com/2009/01/rfinance-conference-in-chicago-in-april-call-for-papers/	January 1, 2009	Thinking inside the box	"
 
      The Finance Department of the University of Illinois at Chicago (UIC),
      the International Center for Futures and Derivatives at UIC, and
      members of the R finance community are pleased to announce
       
      R/Finance 2009: Applied Finance with R
       
      on April 24 and 25, 2009, in Chicago, IL, USA
       
      Confirmed keynote speakers include:
       
      Presenters are strongly encouraged to provide working R code to accompany
      the presentation/paper.  Datasets need not be made public.
       
      Please send submissions to [email protected]
      The submission deadline is January 31st, 2009.
      Submissions will be evaluated and submitters notified via email
      on a rolling basis.
       
      Additional details about the conference will be announced as available.
       
      For the program committee:
       "	 0 Comments
Plotting PDQ Output with R	https://www.r-bloggers.com/2009/02/plotting-pdq-output-with-r/	February 27, 2009	Neil Gunther		 0 Comments
R in The Windy City	https://www.r-bloggers.com/2009/02/r-in-the-windy-city/	February 27, 2009	JD Long	"In honor of me moving to Chicago, the powers who abide have decided to hold the first annual “R/Finance conference for applied finance using R” conference in Chicago this year. The dates are April 24-25, 2009. R/Finance 2009: Applied Finance with R To those who made the decision on location, I’m pleased but slightly embarrassed that you let my relocation decision have such a profound impact on your venue choice. And to the three readers that feedburner tells me regularly read this blog (hi Mom), if you are attending this conference please let me know and I’ll buy you a beer or three. -JD 
 "	 0 Comments
Data Analysis Workflow… Part 1 of Infinity	https://www.r-bloggers.com/2009/02/data-analysis-workflow%e2%80%a6-part-1-of-infinity/	February 26, 2009	JD Long	One of the many things that I sit around pondering when I should be doing productive things is the idea of analytical workflow. I have only worked with one analytical guru who I felt really gave thought and structure to workflow and its impact on analyist productivity. When I talk about workflow I mean the whole process from the time the analytical guy thinks, “Hey, I need to understand the velocity of new purchases between different types of sales campaigns.” until he writes down his findings in a presentation or even just a notebook. In the middle I assume this guy extracts some data from a warehouse or live system, does some work on said data, tests some theories, does more stuff, goes and gets coffee, comes back and plays some flash games, goes home and does it again the next day. Today I was reading over at Data Evolution about a presentation on how Google and Facebook use R. The following was a summary of what Bo Cowgill of Google said about his workflow: The typical workflow that Bo thus described for using R was: (i) pulling data with some external tool, (ii) loading it into R, (iii) performing analysis and modeling within R, (iv) implementing a resulting model in Python or C++ for a production environment. I found this interesting as I have been masticating on the idea of learning Python for some time. I have run into situations where R was slow, but generally I have solved those through rethinking my algorithm. I’m not really a good programmer in R (or any other language for that matter), but I do want/need/like the statistical functions and ease of plotting in R. If I do learn Python I’ll certainly use it to call R… but maybe I should just stick to R. This has nothing to do with workflow, but the most thought provoking insights in the article above came from Itamar Rosenn at Facebook: Itamar’s team used recursive partitioning (via the rpart package) to infer that just two data points are significantly predictive of whether a user remains on Facebook: (i) having more than one session as a new user, and (ii) entering basic profile information. … [they also] found that activity at three months was predicted by variables related to three classes of behavior: (i) how often a user was reached out to by others, (ii) frequency of third party application use, and (iii) what Itamar termed “receptiveness” — related to how forthcoming a user was on the site. So Facebook really wants new users to put more info into FB, use it more, and play with third party apps. I guess that logic is why LinkedIn is always telling me I am only 90% complete on my profile and I would be 95% if I would just, yada yada yada… The more info I put into their walled garden, the more I will play there. And the more ads I will see. Makes sense to me. I guess I follow the same model when I try to get my clients to use my services more and more… I want to be sticky too. But not in a bad way. 	 0 Comments
Review of ‘Applied Econometrics in R’ in JSS	https://www.r-bloggers.com/2009/02/review-of-applied-econometrics-in-r-in-jss/	February 25, 2009	Thinking inside the box		 0 Comments
Absolutely great resource	https://www.r-bloggers.com/2009/02/absolutely-great-resource/	February 25, 2009	:)-k		 0 Comments
R/Finance conference in Chicago in April: Registration now open	https://www.r-bloggers.com/2009/02/rfinance-conference-in-chicago-in-april-registration-now-open/	February 23, 2009	Thinking inside the box	"
See you in Chicago in April!

 "	 0 Comments
Sorry, you said you want a stats revolution?	https://www.r-bloggers.com/2009/02/sorry-you-said-you-want-a-stats-revolution/	February 23, 2009	dan	ALL ABOUT REVOLUTION COMPUTING’S R DISTRIBUTION  Decision Science News was intrigued by a company called REvolution Computing that got some attention of late for spinning their own mix of the R language for statistical computing and giving it away for free. So DSN asked to interview them to see what it’s all about Decision Science News: So who are you guys and what is your scientific background? REvolution: Well, at this point our team has grown and we have about 30 employees with diverse backgrounds, from bioinformatics to finance to core statistics and software engineering.  However when we got started with REvolution, we were a group that had tremendous experience with high performance computing and building production software.  Our first application with R was something called ParallelR , which enables users of R to seamlessly benefit from optimized performance by automatically running on multiple cores, servers, and clusters (we even have a cloud-based deployment).  Our team today is a combination of employees and an extended community, from R community participants, to package developers, to researchers and related consultants. Decision Science News: how did you get started working with R professionally? REvolution: Traditionally our customers came to us for parallel computing solutions based in languages like C, Fortran, or Java.  More and more we started to see pull from our customers toward scripting languages, and R in particular.  Some of our pharma partners particularly were compelled by the proposition of optimizing the performance of R, and many of our first references are related to those applications (gene expression, classification, etc. [case study]) Decision Science News: What’s so great about your R compared to the regular download? REvolution: Well, we’re not competing with the “regular” download – we actively collaborate with the core team, and utilize the codebase.  What we have done is on several fronts.  First, we have added capability and functionality related to optimization and high performance.  Second, we are adding specific support for the 64-bit Windows platforms (and other more obscure OS distributions).  Third, we are actively working on an IDE, large data handling, and other interesting capabilities (stay tuned!). In addition to these aspects, we have packaged REvolution R into a commercially supported distribution around which we also provide training and consulting services.  It’s a fully supported product in the same spirit as, say, RedHat Linux. Decision Science News: Is there any risk that getting ‘locked in’ to your distribution of R? What if your distribution goes away, will our code still run on vanilla R? REvolution: We prefer to say “mandatory customer loyalty” than lock-in.  (KIDDING!)  Of course, “open source” is a big part of “commercial open source,” and users of REvolution R can run their codebase on vanilla R. 	 0 Comments
PDQ-R Lives!	https://www.r-bloggers.com/2009/02/pdq-r-lives/	February 22, 2009	Neil Gunther		 0 Comments
R graphics: margins are way to large	https://www.r-bloggers.com/2009/02/r-graphics-margins-are-way-to-large/	February 22, 2009	Gregor Gorjanc		 0 Comments
Illinois long-term selection experiment for oil and protein in corn	https://www.r-bloggers.com/2009/02/illinois-long-term-selection-experiment-for-oil-and-protein-in-corn/	February 22, 2009	Gregor Gorjanc		 0 Comments
How Facebook and Google use R	https://www.r-bloggers.com/2009/02/how-facebook-and-google-use-r/	February 21, 2009	i82much	"How Facebook and Google use R
 Interesting read.  rpart comes in handy again. "	 0 Comments
Registration for R/Finance 2009 is Open!	https://www.r-bloggers.com/2009/02/registration-for-rfinance-2009-is-open/	February 20, 2009	Joshua Ulrich	"
 "	 0 Comments
People who love scatter plots & connecting dots	https://www.r-bloggers.com/2009/02/people-who-love-scatter-plots-connecting-dots/	February 20, 2009	mike	"
We hosted the first Dataviz Salon SF on Tuesday night, with lightning talks by boredom cop  Shane Booth, dataviz wiz  Lee Byron , computational journalist Brad Stenger, data wrangler  Pete Skomoroch , and any/all data enthusiast  Brendan O’Connor . I was going to blog all about it — but Tom Carden of Stamen Design already has a great write-up. … Dataspora invited a few people to a Dataviz Salon yesterday evening. Mike and I went along and huddled in a brick-built basement in SoMa to listen to the following: . "	 0 Comments
R: Good practice – adding footnotes to graphics	https://www.r-bloggers.com/2009/02/r-good-practice-%e2%80%93-adding-footnotes-to-graphics/	February 17, 2009	markheckmann	In some statistical programs there is the option available to attach a footnote to the graphical output that is created. This footnote may contain the name of the script or the file that produced the graphic, the author’s name and the date of creation. In SAS for example there is a footnote command to achieve this. Ever since I realized that this makes life a lot easier, I wrote a simple three-lines function in R which I use at the end of the construction of any graphic. I suppose, that this is what my professors meant with “good practice”. The nice thing about implementing this in the grid graphics system is that you can produce multiple graphics [e.g. by par(mfrow=c(2, 2))] and still the footnote will be positioned correctly.  Here an example of a footnote added to the graphical output. Correlation matrix with footnote Cheers, Mark 	 0 Comments
Pearson vs. Spearman Correlation Coefficients	https://www.r-bloggers.com/2009/02/pearson-vs-spearman-correlation-coefficients/	February 17, 2009	John Myles White	One of the misuses of statistical terminology that annoys me most is the use of the word “correlation” to describe any variable that increases as another variable increases. This monotonic trend seems worth looking for, but it plainly is not what most people discover when they use standard correlation coefficients. This is because the Pearson product moment correlation coefficient, which is usually the only correlation coefficient students learn to calculate, is strongly biased towards linear trends: those in which a variable y is a noisy linear function of a variable x. Only the Spearman correlation coefficient, which is usually not taught to students, actually detects a general monotonic trend. You can see this for yourself easily by seeing what the correlation coefficient is between x and  progressively higher-degree polynomials in x. If the Pearson correlation coefficient actually detected monotonic trends, it wouldn’t plunge to zero as the degree of the polynomial in x increases. This is precisely what the Spearman correlation coefficient does. I hope that we can reconcile our intuitive thinking and our statistical practice by ending the self-contradiction in which the word “correlation” is used in discourse to describe the behavior of an ideal Spearman correlation coefficient, while in practice correlations are computed using Pearson’s formula. 	 0 Comments
TTR_0.2 on CRAN	https://www.r-bloggers.com/2009/02/ttr_0-2-on-cran/	February 15, 2009	Joshua Ulrich	"
 "	 0 Comments
Single Letter Frequencies in English	https://www.r-bloggers.com/2009/02/single-letter-frequencies-in-english/	February 15, 2009	John Myles White	Every time that I read a paper that discusses the frequencies of single letters in English, I feel like I should sit down and calculate them for myself from a sample of English text. Today, I finally did. Here are the probabilities and negative log probabilities of the characters in English over the corpus of Shakespeare’s plays: And, for those who care, here’s the code to generate the data from the plays, which I downloaded from Project Gutenberg: 	 0 Comments
R in SAS	https://www.r-bloggers.com/2009/02/r-in-sas/	February 15, 2009	Gregor Gorjanc		 0 Comments
R-Sessions 32: Forward.lmer: Basic stepwise function for mixed effects in R	https://www.r-bloggers.com/2009/02/r-sessions-32-forward-lmer-basic-stepwise-function-for-mixed-effects-in-r/	February 13, 2009	Rense Nieuwenhuis		 0 Comments
New project: RInside	https://www.r-bloggers.com/2009/02/new-project-rinside/	February 12, 2009	Thinking inside the box	"
RInside 
makes it easy to embed R into your own
C++ application by hiding the nitty gritty of initializing an R interpreter
behind a simple abstraction. More information is at a (currently pretty simple)
RInside page,
and you may want to look at the related 
Rcpp and possibly
littler
projects. The former is helpful for data exchange, and the latter provided my
first real use of R embedding which in some ways also lead to 
RInside.

 "	 0 Comments
Fitting Legendre (orthogonal) polynomials in R	https://www.r-bloggers.com/2009/02/fitting-legendre-orthogonal-polynomials-in-r/	February 10, 2009	Gregor Gorjanc		 0 Comments
Positioning charts with fig and fin	https://www.r-bloggers.com/2009/02/positioning-charts-with-fig-and-fin/	February 9, 2009	bryan	"R offers several ways to spatially orient multiple graphs in a single graphing space.  The layout() function and mfrow/mfcol parameter settings are adequate solutions for many tasks and allow the graphing space to be broken up into tabular or matrix-based arrangements.  For more fine grained manipulation, the fig and fin parameter settings are available.  This article illustrates the capabilities and use of fig and fin.

First we’ll create some simulation data to work with:  read more "	 0 Comments
Princeton Graduate Student Housing	https://www.r-bloggers.com/2009/02/princeton-graduate-student-housing/	February 8, 2009	John Myles White	For any Princeton graduate students who are interested, here’s the success rate for graduate students applying for school housing. These charts were built using the data from the 2008-2009 Room Draw Statistics pamphlet provided by the Division of Housing here at Princeton. 	 0 Comments
Our new R package: R2jags	https://www.r-bloggers.com/2009/02/our-new-r-package-r2jags/	February 8, 2009	Yu-Sung Su		 0 Comments
Baby Got Stats!	https://www.r-bloggers.com/2009/02/baby-got-stats/	February 8, 2009	jebyrnes	"I was completely tickled last year with the oh so amusing Statz Rappers.  It kept me and my nerdy stats friends laughing for days.  Rapping.  Stats.  The Internet. Good times. But little did I know that rapping about statistics was really just hitting its stride on youtube.  This is Why We Plot began my trip down the rabbit hole.  Quite a nice effort.  I followed this with that Stats Rap – not bad, and oh so mellow.  Also I love the equations. But the pièce de résistance is Baby Got Stats from Dorry Serev in the biostats department  at JHU.  Oh. Dear. Lord.  I laughed.  I wept. I even put some of the lyrics into my .sig file.  Enjoy.
 (note, mildly nsfw?  maybe?) There are a ton more – just follow the related links.  It’s kind of amazing.  And if anyone gets a yen to start doing some multivariate SEM or Bayesian raps, I want to know! "	 0 Comments
ave	https://www.r-bloggers.com/2009/02/ave/	February 8, 2009	vikasrawal	I discovered a new, very useful, R function yesterday: ave. This is what it does: “Subsets of ‘x[]‘ are averaged, where each subset consist of those observations with the same factor levels.”  But interestingly, you can use any function other than average. The output of that function is set against each observation. I wanted to, for example, stick sub-group ranks to each observation. I can think of several situations where this would be of great value! V.  	 0 Comments
R: Calculating all possible linear regression models for a given set of predictors	https://www.r-bloggers.com/2009/02/r-calculating-all-possible-linear-regression-models-for-a-given-set-of-predictors/	February 6, 2009	markheckmann	"Although the graphic at the left might not seem a 100% appropriate, it gives a hint to what I am about to do. I want to calculate all possible linear regression models with one dependent and several independent variables.  I do not want to address bias and fitting issues or the question if this makes sense from a statistical point of view in this posting. Here I want to emphasize the technical issues only. To solve the task, several approaches are possible. The first one is a step-by-step approach using a lot of code. Another one would be to make use of a specialized package. The packages leaps and meifly would be appropriate for the task but have some slight drawbacks in terms of flexibility. I will not address solutions using these packages here, but I would like to point out that in contrast to the below only a few lines of code would do the job. Let’s suppose we have the following set of four possible regressors. Now we want to construct a formula that contains the first and third regressor. So the paste commmand works vectorwise which helps a lot in this case. Now we add a plus sign between the regressors…  … and add the left side of the equation. The 1 in the formula models the intercept , 0 would be a model without intercept. Now let’s make a formula out of it. So we can construct a formula from each row of a TRUE /FALSE matrix which determines if a regressor is used or not. Now we need a TRUE / FALSE matrix of all the possible regressor combinations. The expand.grid() function produces one (see ?expand.grid). The last line describes a trivial model as it does not contain any regressors (as it contains only FALSE values), thus it is removed. Now we can apply the above way of formula construction to each row of the matrix so we get a list with all the possible models. The last step is to use each list element for the calculation. So basically, here our computation work is done, but as in most cases a lot of work follows to prepare the data in a nice way. So now let’s get all the important information into one dataframe. Let’s say we want a data frame like the following. So  we need to extract all the following information (coefficients, SE etc.) and cast them into one data frame. This used to be one of the nasty tasks in R. Here Hadley Wickhams plyr package really helps a lot. ldply takes a list, applies a function and casts the results into ONE data frame (see ?ldply). As function return value it expects a data frame or a vector. The advantage to return data frames is that the ldply() function uses rbind.fill for combining the results when they are data frames. rbind.fill() allows a different number of columns in each data frame. Here this is the case as a different number of regressors are  used each time. So we have to make sure that the function returns a data frame. Thus we use as.data.frame paying attention to the orientation of the data frame, using t() in case it is outputted as one column. This was really a lot of code. But now we have assembled all the information and indices that were important for my task. To choose what is needed is simple now. And we have the flexibility to add any indices. Next time I will try to extend this example doing a k-fold estimation for each set of regressors.
 Cheers, Mark Heckmann
 
 
 "	 0 Comments
Analysis of Variance (ANOVA) using R	https://www.r-bloggers.com/2009/02/analysis-of-variance-anova-using-r/	February 5, 2009	Paul Gribble		 0 Comments
R-Sessions 31: Combining lmer output in a single table (UPDATED)	https://www.r-bloggers.com/2009/02/r-sessions-31-combining-lmer-output-in-a-single-table-updated/	February 5, 2009	Rense Nieuwenhuis		 0 Comments
If I Had a Text File, I’d Hack Regexes in the Morning	https://www.r-bloggers.com/2009/02/if-i-had-a-text-file-i%e2%80%99d-hack-regexes-in-the-morning/	February 4, 2009	John Myles White	Yesterday the topic of academic citation counts came up, so I decided that I should write up some tools for exploring cite counts. The first thing I did was to build a cheap screenscraper in Ruby for pulling citation count information from Google scholar. You’ll see the ugly hack I produced below. With that in hand, I wrote a simple wrapper to pull information for a list of authors you store in a file called authors.txt from Google Scholar. The wrapper then prints a CSV file to STDOUT that can be redirected to a file for later analysis. Then I coded up a simple barplot in R to give you a sense of the citation count for the first few authors that came to mind. The result is below. Now I think the goal should be to put these tools to a good use. 	 0 Comments
Correct Datetime / POSIXct behaviour for R and kdb+	https://www.r-bloggers.com/2009/02/correct-datetime-posixct-behaviour-for-r-and-kdb/	February 3, 2009	Thinking inside the box	"

Anyway, the reason for this post was that the R / kdb+ glue code works well
… but not for datetimes. I really like to be able to pass date/time objects
natively between systems as easily as, say, numbers or strings (and see
e.g. my Rcpp package
for doing this with R and C++) and I was a bit annoyed when the millisecond
timestamps didn’t move smoothly.  Turns out that the basic converter function in the
code had a number of problems: it converted to integer, only covered a
single scalar rather than vectorised mode, and erroneously reduced a
reference count.  A better version, in my view, is as follows:

 "	 0 Comments
Online R programming resources	https://www.r-bloggers.com/2009/02/online-r-programming-resources/	February 2, 2009	bryan	R can legitimately be called both a programming language and a statistical package.  Many books address both the programming and statistical components of R, but invariably the discussion of statistical topics is more detailed than the discussion of programming capabilities.  As a supplement, I’ve started the list of links below.  Each of these sources deals specifically and almost exclusively with the the programming aspects of R: objects, arrays, loops and conditional statements, custom functions, debugging, and so on.  I’ll add to this list as I become aware of other sites. read more 	 0 Comments
Don’t group Figures in Word	https://www.r-bloggers.com/2009/02/dont-group-figures-in-word/	February 1, 2009	[email protected]		 0 Comments
R tips: Swapping columns in a matrix	https://www.r-bloggers.com/2009/03/r-tips-swapping-columns-in-a-matrix/	March 31, 2009	Allan Engelhardt	"
Using R, the statistical analysis and computing platform, swapping two columns in a matrix is really easy: m[ , c(1,2)]  <- m[ , c(2,1)].
 
Note, however, that this does not swap the column names (if you have any) but only the values.  You could do something like colnames(m)[c(1,2)] <- colnames(m)[c(2,1)] if you need the names changed as well, but better is perhaps just to assign:
 
 "	 0 Comments
Enhanced tidy.source() (Preserve Some Comments)	https://www.r-bloggers.com/2009/03/enhanced-tidy-source-preserve-some-comments/	March 31, 2009	Yihui Xie	After a few hours’ work, I modified the function tidy.source() in the animation package so that it can preserve complete comment lines. See the tidy.source() wiki page for example. Note that inline comments will still be removed. I don’t want to spend more time on dealing with inline comments any more. 	 0 Comments
Multiple plot in a single image using ImageMagick	https://www.r-bloggers.com/2009/03/multiple-plot-in-a-single-image-using-imagemagick/	March 31, 2009	Paolo		 0 Comments
How accurate or reliable are R calculations?	https://www.r-bloggers.com/2009/03/how-accurate-or-reliable-are-r-calculations/	March 28, 2009	markheckmann	On the REvolutions Blog there is a nice posting treating the often raised concern on “How good or reliable R is”. At my university R is hardly used. Sometimes I was asked by lecturers wether the calculations done by R and its packages are accurate. The linked posting treats this matter and tries to clarify this point. 	 0 Comments
R: Zip fastener for two data frames / combining rows or columns of two dataframes in an alternating manner	https://www.r-bloggers.com/2009/03/r-zip-fastener-for-two-data-frames-combining-rows-or-columns-of-two-dataframes-in-an-alternating-manner/	March 27, 2009	markheckmann	Sometimes I find it useful to merge two data frames like the following ones by using zip feeding either along the columns or along the rows of the data frames. The following function acts like a “zip fastener” for combining two dataframes. It takes the first column (or row) of the first data frame and places it next to the first column (or row) of the second data frame and so on. Only one dimension of the data frame has to be equal to do this. E.g. to combine the columns by zip feeding the number of rows must be equal and vice versa. So here comes the code for the zipFastener() function. Actually its only the last few lines (from #zip fastener operations on) that do the job, but as I did not want to restrict the function to equal dimensions there is a little prelude. Here come some examples. I hope you find that useful. Ciao, Mark 	 0 Comments
R tips: Eliminating the “save workspace image” prompt on exit	https://www.r-bloggers.com/2009/03/r-tips-eliminating-the-%e2%80%9csave-workspace-image%e2%80%9d-prompt-on-exit/	March 26, 2009	Allan Engelhardt	"
When using R, the statistical analysis and computing platform, I find it really annoying that it always prompts to save the workspace when I exit.  This is how I turn it off.
 
I wish there was an option to change the default of the q/quit functions.  I start and stop R frequently and so the exit question which I have to answer every time is really annoying:
 
Why is there no R option to disable this prompt?  If I want to save the image, I have already saved it.  And I don’t like the default name anyhow, preferring to give my own with save.image(file=...).  For a while, I had a function defined in my ~/.Rprofile that terminated the session without prompting.
 
While this means I can type exit() and avoid the annoying prompt, in practice I normally type Control-D to end the session which still calls the normal q function with its annoying prompt.
 
So instead I use the alias functionality of my (bash) shell to change the default.  In my ~/.bashrc I now have
 
And finally I am happy.  But I still think R should have an option (accessible through options) to change the default behavior.
 "	 0 Comments
R tips: Keep your packages up-to-date	https://www.r-bloggers.com/2009/03/r-tips-keep-your-packages-up-to-date/	March 25, 2009	Allan Engelhardt	"
In this entry in a small series of tips for the use of the R statistical analysis and computing tool, we look at how to keep your addon packages up-to-date.
 
One of the great strengths of R is the many packages available.  All the new approaches, as well as some of the best implementations of your old favorites are there.  But it can also be a little daunting, and so the CRAN task views are often the best way to get started and download a reasonable “bundle” of packages for your analysis.
 
First we need a place to store the packages.  On Linux (and other Unix-like systems) I use the file ~/.Renviron to set the R_LIBS variable to where I want the files:
 
On Windows, I set the same variable for the user account.  Don’t forget to create the directory.
 
Now your can start R and install the CRAN task view package:
 
Then I have a few things in my ~/.Rprofile startup file.  The previous command probably prompted you for a download mirror which is annoying, so let’s exit R and edit the startup file to contain:
 
Then I define three functions.  The first is to install the views I need.  I like to try new things, so my list is long.  Edit it to suit your needs:
 
Try it out!  Save the file, start R, and type install.myviews() at the prompt.  If your list is as long as mine, then this may take some time and you may get some warnings and errors.  We might add a tip on these later, but the main reason for the errors is probably that you are missing the development files for external libraries (or that R just can’t find it).
 
Now that we have finally got them, we need to make sure they are up-to-date.  I add two functions to ~/.Rprofile:
 
The first allows me to easily update all my locally installed libraries (not just these installed from views).  The second updates my views which is useful when the view definitions change (rarely, but it happens as the recommended packages evolve).
 
Now I can of course update from the R command prompt using update.local() or update.myviews().  But that is not the main benefit.  I can now update directly from the shell command line using commands like:
 
The beauty of this is that I can add it to my crontab(5) and have it run automatically every night or every week as I feel I need it.  This way I always have the latest versions installed.
 "	 0 Comments
Alternative implementations using ggplot2	https://www.r-bloggers.com/2009/03/alternative-implementations-using-ggplot2/	March 25, 2009	Paolo		 0 Comments
Comparison of different circle graphs	https://www.r-bloggers.com/2009/03/comparison-of-different-circle-graphs/	March 24, 2009	Cloud Wei		 0 Comments
Streaming Hadoop Data Into R Scripts	https://www.r-bloggers.com/2009/03/streaming-hadoop-data-into-r-scripts/	March 23, 2009	Neil Gunther		 0 Comments
American Immigration Trends	https://www.r-bloggers.com/2009/03/american-immigration-trends/	March 22, 2009	John Myles White	The New York Times has a beautiful visualization of immigration trends in the United States since 1880. I highly recommend spending a few minutes playing with the interactive display. 	 0 Comments
India Census 2001 – Part 1	https://www.r-bloggers.com/2009/03/india-census-2001-part-1/	March 22, 2009	anandram	I was trying – for the last few weeks – to get the 2001 Indian census data. Alas the census website is under construction. But fortunately the Internet rewind button works! Thankfully the literacy data was online there. The raw data is available here. I cleaned up the data so that it is easy to work with R. I removed the commas in the numbers. Also, under the urban status column I removed the dots and capitalized the status codes. One of the urban status became ‘NA’ and since R treats ‘NA’ as a missing data I changed it to NA1. The cleaned up data is available here. Please download  and rename it as india-census-2001.csv Here goes the R code to explore the data: Lets us print out those places with zero population. Find out the population in all Kachchh districts. Find out the population in all Rajkot districts. Looks as if the data in the Kachchh region was not collected. Wonder why those two Rajkot districts also suffered the unfortunate fate. Maybe they are close to Kachchh region. Anyway let us look at the data which has non-zero population. Let us plot the literacy rate of the city/town (x-axis) against the State (y-axis) Here goes the plot  Looking at the plot, no surprise that Kerala has very high literacy rate in all the towns and the spread is also low. Tamil Nadu has a bigger spread in the literacy rates. The Northeastern states are doing very well in the educational aspect if we evaluate them by their literacy rates. Let us check which city/town has the highest and the lowest literacy in India Well, this is a surprise. The city with the highest literacy is in Jammu & Kashmir (Gulmarg) and the lowest is in Maharashtra (Tarapur). What is shocking is that the literacy rate in Tarapur is less than 1%. I hope that there was mistake in data collection, otherwise it is a damning indictment of a huge administrative failure in that district. This is unacceptable.  In the next few posts, I will concentrate on Tamil Nadu and Coimbatore. It should be pretty easy to modify the code in the coming posts to look at the states and districts of your interest. 	 0 Comments
Play Sliding Puzzles on R	https://www.r-bloggers.com/2009/03/play-sliding-puzzles-on-r/	March 22, 2009	Cloud Wei		 0 Comments
Dianne Reeves at Dominican	https://www.r-bloggers.com/2009/03/dianne-reeves-at-dominican/	March 16, 2009	Thinking inside the box		 0 Comments
R: Monitoring the function progress with a progress bar	https://www.r-bloggers.com/2009/03/r-monitoring-the-function-progress-with-a-progress-bar/	March 16, 2009	markheckmann	Every once in while I have to write a function that contains a loop doing thousands or millions of calculations. To make sure that the function does not get stuck in an endless loop or just to fulfill the human need of control it is useful to monitor the progress. So  first I tried the following: Unfortunately this does not work as the console output to the basic R GUI is buffered. This means that it is printed to the console at once after the loop is finished. The R FAQs (7.1) explains a solution: Either to change the R GUI buffering settings in the Misc menu which can be toggled via  or to tell R explicitly to empty the buffer by flush.console(). So like this it works: Of course it would be even nicer to have a real progress bar. For different progress bars we can use the built-in R.utils package. First a text based progress bar:  To get a GUI progress bar the tkProgressBar() function from the tcltk package can used. Last but not least, a progress bar using the Windows operating system. Ciao, Mark 	 0 Comments
Identify Data Points in Off-Screen R Graphics Devices	https://www.r-bloggers.com/2009/03/identify-data-points-in-off-screen-r-graphics-devices/	March 16, 2009	Yihui Xie	Today Ruya Gokhan Kocer asked me how to use the R function identify() in off-screen graphics devices. Actually it’s pretty easy as long as we obtain the list returned by identify(pos = TRUE). For example, 	 0 Comments
2009 March Madness Half Marathon in Cary	https://www.r-bloggers.com/2009/03/2009-march-madness-half-marathon-in-cary/	March 15, 2009	Thinking inside the box	"
So how did it go?  Well, I had pretty low expectations. Training has been
difficult with too much snow, rain and plain cold weather. So like most of my
running friends, motivation ran pretty low of late. I was really only trying
to set a modest pace, and to hope to hang on to it and run steadily.  That
worked: I didn’t walk a single water stop, and while my legs were getting
really tired and sore I carried through to the end.  Final time was
1:36:08.57 per my stop watch. That’s a tad faster than 
last year,
a lot slower than 
2007,
just a tad slower than 
2006,
and quite a bit faster than 
2005.

 
Next stop: my second Boston Marathon in five weeks and given the
underwhelming training this winter, it will be a challenge. 

 "	 0 Comments
Color:  The Cinderella of dataviz	https://www.r-bloggers.com/2009/03/color-the-cinderella-of-dataviz/	March 13, 2009	mike	"“Avoiding catastrophe becomes the first principle in bringing color to information: Above all, do no harm.”  — Envisioning Information, Edward Tufte, Graphics Press, 1990    Color is one of the most abused and neglected tools in data visualization.  It is abused when we make poor color choices; it is neglected when we rely on poor software defaults.  Yet despite its historically poor treatment at the hands of engineers and end-users alike, if used wisely, color is unrivaled as a visualization tool. Most of us think twice before walking outside in fluorescent red underoos.  If only we were as cautious in choosing colors for infographics.  The difference is that few of us design our own clothes.  But until good palettes (like ColorBrewer) are commonplace, to get colors that fit our purposes, we must be our own tailors. While obsessing about how to implement color on the Dataspora Labs’ PitchFX viewer I began with a basic motivating question: If our data are simple, a single color is sufficient, even preferable.  For example, below is a scatter plot of 287 pitches thrown by the major league pitcher Oscar Villarreal in 2008.  With just two dimensions of data to describe — the x and y location in the strike zone — black and white is sufficient.  In fact, this scatter plot is a perfectly lossless representation of the data set (assuming no data points perfectly overlap). Fig 1. Location of Pitches (Villarreal, HOU, 2008)  But what if we’d like to know more: for instance, what kinds of pitches (curveballs, fastballs) landed where?  Or their speed?  Visualizations live in two dimensions, but the world they describe is rarely so confined. The defining challenge of data visualization is projecting high dimensional data onto a low dimensional canvas. (As a rule, one should never do the reverse: visualize more dimensions than what already exist in the data). Getting back to our pitching example, if we want to layer another dimension of data — pitch type — into our plot, we have several methods at our disposal: Which techniques you employ depend on the nature of the data and the media of your canvas.  I will describe all three by way of example. Fig 2. Location and Pitch Type (Villarreal, HOU, 2008)  In this plot, I’ve layered the categorical dimension of pitch type into our plot by using four different plotting symbols. I consider this visualization an abject failure.  In fact, the prize for my most despised graphs in graduate school goes to  bacterial growth curves rendered this way .  The reason these graphs make our heads hurt is because (i) distinguishing glyphs demands extra attention (versus what academics call ‘pre-attentively processed‘ cues like color), (ii) even after we visually decode the symbols, we have yet another step: mapping symbols to their semantic categories.  (Admittedly this can be improved with Chernoff faces or other iconic symbols, where the categorical mapping is self-evident). Folding additional dimensions into a partitioned canvas has a distinguished pedigree in information graphics.  It has been employed everywhere from  Galileo sunspot illustrations  to William Cleveland’s trellis plots.  And as Scott Mccloud’s unexpected  tour de force on comics  makes clear, panels of pictures possess a narrative power that a single, undivided canvas lacks. In this plot below, the four types of pitches that Oscar throws are splintered horizontally.   By reducing our plot sizes, we’ve given up some resolution in positional information. But in return, patterns that were invisible in our first plot, and obscured in our second (by varied symbols) are now made clear (Oscar throws his fastballs low, but his sliders high). Fig 3:  Location and Pitch Type (Villarreal, HOU, 2008)  Multiplying plots in space works especially well on printed media, which can hold more than ten times as many dots per square inch as a screen.  Both columns and rows can be used to lattice over additional dimensions, the result being a  matrix of scatter plots  (in R, see the ‘splom‘ function). So why bother with color? First, as compared to most print media, computer displays have fewer units of space, but a broader color gamut.  So color is a compensatory strength. For multi-dimensional data, color can convey additional dimensions inside a unit of space — and can do so instantly.  Color differences can be detected within 200 ms, before you’re even conscious of paying attention (the ‘pre-attentive’ concept I mentioned earlier). But the most important reason to use color in multivariate graphics is that color is itself multidimensional.  Our perceptual color space —  however  you  slice  it  — is three-dimensioned. In the example below, I’ve used color as a means of encoding a fourth dimension of our pitching data: the speed of pitches thrown. The palette I’ve chosen is a divergent palette that moves along one dimension (think of it as the ‘redness-blueness’ dimension) in the CIELUV color space, while maintaining a constant level of luminosity. Fig 4. Location, Pitch Type, and Velocity (Villarreal, HOU, 2008)     Holding luminosity constant is important, because luminosity (similar to brightness) determines a color’s visual impact. Bright colors pop, and dark colors recede.  A color ramp that varies luminosity along with hue will highlight data points as an artifact of color choice. I chose only seven gradations of color, so I’m downsampling (in a lossy way) our speed data – but further segmentation of our color ramp is not likely to be perceptible. I’ve also chosen to use filled circles as my plotting symbol, as opposed to the open circles in all my previous plots.  This is done to improve the perception of each pitch’s speed via its color: small patches of color are less perceptible.  But a consequence of this choice — compounded by our choice to work with a series of smaller plots — is that more points overlap.  We’ve further degraded some of our positional information.  However, in our last step, we attempt to recover some of this. Now I’ve finally brought color to bear on this visualization, but I’ve only encoded a single dimension — speed.  Which leads to another question: In theory, yes.  Colin Ware researched this exact question.  In practice, it’s difficult.  It turns out that asking observers to assess the amount of ‘redness’, ‘blueness’, and ‘greenness’ of points is possible, but not intuitive (I suspect it’s somewhat like parsing symbols). Another complicating factor is that a nontrivial fraction of the population has some form of color blindness.  This effectively reduces their color perception to two dimensions. And finally, the truth is that our sensation of color is not equal along all dimensions; it’s thought the closely related ‘red’ and ‘green’ receptors emerged via duplication of the single long wavelength receptor (useful for detecting ripe from unripe fruits, according to one just-so story). Because the high level of dichromacy in the population, and because of the challenge of encoding three dimensions in color, I  feel color is best used to encode no more than two dimensions of data. So, for my last example of our pitching plot data, I will introduce luminosity as a means of encoding the local density of points (using a kernel density estimator).  This allows us to recover some of the data lost by increasing the sizes of our plotting symbols. Fig 5. Location, Pitch Type, Velocity, and Density (Villarreal, HOU, 2008)  
 Here we have effectively employed a two-dimensional color palette, with blueness-redness varying along one axis for speed, and luminosity varying in the other to denote local density. One final point about using luminosity.  Observing colors in a data visualization involves overloading, in the programming sense.  We rely on cognitive functions that were developed for one purpose (perceiving lions) and use them for another (perceiving lines). Since we can overload color any way we want, whenever possible,  we should choose mappings that are natural.  Mapping pitch density to luminosity feels right because the darker shadows in our pitch plots imply depth.  Likewise, when sampling from the color space, we might as well choose colors found in nature.  These are the palettes our eyes were gazing at for the millions of years before #FF0000 showed up. Color, used thoughtfully and responsibly, can be an incredibly valuable tool in visualizing high dimensional data. This discussion has focused on using static graphics in general, and color in particular, as a means of visualizing multivariate data.  I’ve purposely neglected one very powerful tool:  motion. The ability to animate graphics multiplies by several orders of magnitude the amount of information that can be packed into a visualization.   But packing  information into a time-varying data structure has to be done by someone (you or me) and from my view, this remains a significant challenge.  Canonical forms of animated visualizations (equivalent to the histograms, box plots, and scatterplots of the static world) are still a ways off, but frameworks like Processing and Prefuse are a promising start towards their development. The final product of these five-dimensional pitch plots — for all available data for the 2008 season — can be explored via the PitchFX Django-driven web tool at Dataspora labs. All of the visualizations here were developed using R and the Lattice graphics package.  (Of note, Hadley Wickham is developing ggplot2, a bold re-write of the R graphics system based on a grammar of graphics). "	 0 Comments
Visulization of correlation matrix	https://www.r-bloggers.com/2009/03/visulization-of-correlation-matrix/	March 12, 2009	Cloud Wei		 0 Comments
no “Infinities”	https://www.r-bloggers.com/2009/03/no-infinities/	March 12, 2009	Paolo		 0 Comments
Andrews’ Curve And Parallel Coordinate Graph	https://www.r-bloggers.com/2009/03/andrews-curve-and-parallel-coordinate-graph/	March 11, 2009	Cloud Wei		 0 Comments
Scatterplots	https://www.r-bloggers.com/2009/03/scatterplots/	March 11, 2009	Cloud Wei		 0 Comments
Choosing an SQL Engine for Analytics	https://www.r-bloggers.com/2009/03/choosing-an-sql-engine-for-analytics/	March 9, 2009	JD Long	I’ve been struggling for a while on which database to use for my working data. I used to use MS Access quite a lot. The problems with MS Access include but are not limited to: I used Oracle for a few years as a result of my previous employer being an Oracle shop. I then switched to SQL Server when I changed jobs. A full blown client/server DB really does not make a lot of sense for much of what I do. I don’t run a transactional data store. I don’t need to have dozens of users hooked to the DB. And I do sometimes need access to my data when I am not hooked to the mother-ship. So I could run the free version of SQL Server on my laptop or run MySQL on my laptop, but both of these options rub me the wrong way. Why? I do a lot of data analysis in R which is RAM intensive. Running a DB server on my laptop means that some fraction of my RAM is going to be taken up by the db server software which is hanging out waiting for me to throw requests at it. I could manually hack around this by starting the server before I load data and then killing it after the data is loaded. That’s just too big of a pain in my rectum. Oh yeah, one more design requirement: I want to be able to push the whole DB out to a storage blob at Amazon and pound on it using EC2 machines, running Linux. Plus I am cheap and don’t want to pay a lot. I’ll probably end up with a model where I keep some master data sets on a client/server DB and then I will replicate chunks of that to my laptop into my serverless db. I’ll probably also put output from my desktop db back into the server after analytic work is  done. I knew about SQLite because of an interview with its author, Richard Hipp on FLOSS Weekly. There’s also a video of Hipp talking at the Googleplex. I wish that guy was my neighbor. He seems like the type of guy who would shovel your walk for you then apologize for not doing it perfectly by sending over homemade cookies. Unrelated to the cookies, I really like that SQLite is weakly typed.  I’m a free spirit like that. I did some digging for SQLite alternatives and came up with some stuff at StackOverflow. You can read the post but it reminded me of Firebird. I’m immediately drawn to FireBird since their logo looks so dang much like the Ruger logo:   But is Firebird able to be run severless?  If I have to install a server then I would just as well run MySQL. Berkeley DB seems like another option worth investigating, although I am not sure if I can use it without really embedding it in another program the way that I can with SQLite. SQLite gets bonus points for having native R drivers meaning that I don’t have to go through a connector technology like ODBC. This is important enough that I should probably make that a requirement. I think Berekley DB has support in R as well. I know for a fact that writing back to SQL Server through the R ODBC package (RODBC) is like pushing a car with a rope, but only slower. Plus I don’t know how to make ODBC work on Linux. Not rocket science, I am sure, but still one more thing I would have to learn before I do that which I am paid to do. I’m going to do some testing, but it looks like I should test real life performance of SQLlite and Firebird with my data.  More to come on this, I am sure. 	 0 Comments
Repeated Measures ANOVA using R	https://www.r-bloggers.com/2009/03/repeated-measures-anova-using-r/	March 9, 2009	Paul Gribble		 0 Comments
i-Screen, u-Screen, Vee All Screen for Which Screen?	https://www.r-bloggers.com/2009/03/i-screen-u-screen-vee-all-screen-for-which-screen/	March 9, 2009	Neil Gunther		 0 Comments
NREGA and Indian maps in R	https://www.r-bloggers.com/2009/03/nrega-and-indian-maps-in-r/	March 8, 2009	anandram	A few days ago I was reading an article by Jean Drèze and his colleagues on how the first two years of National Rural Employment Guarantee Act (NREGA) has progressed (There was another article by Drèze on NREGA in 2007). The NREGA is empowering the rural people in a radical way: [ …] NREGA programmes visualise a decisive break with the past. Ever since independence, rural development has largely been the monopoly of local contractors, who have emerged as major agents of exploitation of the rural poor, especially women. Almost every aspect of these programmes, including the schedule of rates that is used to measure and value work done, has been tailor-made for local contractors. These people invariably tend to be local power brokers. They implement programmes in a top-down manner, run roughshod over basic human rights, pay workers a pittance and use labour-displacing machinery. NREGA is poised to change all that. It places a ban on contractors and their machines. It mandates payment of statutory minimum wages and provides various legal entitlements to workers. It visualises the involvement of local people in every decision — whether it be the selection of works and work-sites, the implementation of projects or their social audit. After going through the articles I thought about reproducing the color (gray) coded maps. Of course the best tool to do this would be R. It took a few days to figure out how to do this. The rest of the post (hopefully clearly) will be on how to produce an Indian map gray coded with literacy rate of the state. Now on to the exciting part of producing Indian maps: The code is straightforward. I adapted the code from http://r-spatial.sourceforge.net/gallery/ more specifically fig14.R there. Here is the output if everything goes fine. Darker shades means dire literacy levels.  	 0 Comments
Coimbatore Weather and Questioning Amma!	https://www.r-bloggers.com/2009/03/coimbatore-weather-and-questioning-amma/	March 8, 2009	anandram	"A week ago, Amma was telling the weather was getting hot in Coimbatore. I was telling her it is going to get worse in the next two months. She shot back saying that March is the hottest month while April and May are less hotter in Coimbatore. Growing up in India you are thought that your mother knows the best and she is right (almost) always. Well I could not resist the thought of putting the thesis to test and the internet comes to my help. So here goes the perl and R code to  get the temperature data and explore it. The metric of choice would (arbitrarily) the average temperature. In R plots in this post the average is plotted by a big black dot. The month with the highest average temperature will be adjudged the hottest month. There are a lot of metrics to use but this is the simplest and the most intuitive. (1)  perl code to scrap temperature data from web. The wunderground website has data in csv format. I did not do checks like limiting days in June to 30. We will fix that in clean up script which will build a single csv file with data from years 2005 to 2008. You need the CPAN package LWP.You should be able to google and figure out how to install LWP package. Save this file as [dir]/src/get_temperature_files.pl
To run
% cd [dir]/src
% perl get_temperature_files.pl
You will have the data in the directory [dir]/data
The data is for four years from 2005 to 2008.  (2) Clean up the data and construct the data as a single file.
Save this file as [dir]/src/build_csv.pl
To run
% cd [dir]/src
% perl build_csv.pl ../data/ > cbe.csv (3) Explore the data using R Now for answering the question at the top of the post.  Looks as if Amma thesis is probably rejected! Four years worth data shows that highest average temperature is in April. Let us see a split by years and see if there is a year in which March’s average temperature was the highest. Looks like we need to do little clean up of the data. There is a zero in October. Definitely not possible in Coimbatore!  Well it looks like at least in 2005 and 2007 March’s average temperature is the highest. Although April matches March in both those years. I am trying to find something to salvage for my Amma! Here is an another plot which splits by the hour of the day. 
The one surprising thing one was the fact that the lowest temperatures occur between 2:30 AM and 5:30 AM not around midnight. The highest temperatures are around 2:30 PM not noon. Well the zero is definitely an error since it shows up at 11:30 AM. Update 1 (March 16 2009): You can download [~700 KB, rename it as cbe.csv] the big csv file which contains the data for the weather in Coimbatore for years 2005 — 2008. Now you can skip to the Step (3) and use R to analyze the data. "	 0 Comments
Dealing with missing values	https://www.r-bloggers.com/2009/03/dealing-with-missing-values/	March 8, 2009	Paolo		 0 Comments
So here we have our 1st problem…	https://www.r-bloggers.com/2009/03/so-here-we-have-our-1st-problem/	March 7, 2009	the R user...		 0 Comments
Hello everybody	https://www.r-bloggers.com/2009/03/hello-everybody/	March 7, 2009	the R user...		 0 Comments
Causation’s Mistreated Sibling Correlation	https://www.r-bloggers.com/2009/03/causation%e2%80%99s-mistreated-sibling-correlation/	March 6, 2009	John Myles White	This is why I love XKCD, though surely the best part of this strip was the mouseover: “correlation doesn’t imply causation, but it does waggle its eyebrows suggestively and gesture furtively while mouthing, ‘look over there’.” 	 0 Comments
Workflow with Python and R	https://www.r-bloggers.com/2009/03/workflow-with-python-and-r/	March 6, 2009	Abhijit	I seem to be doing more and more with Python for work over and above using it as a generic scripting language. R has been my workhorse for analysis for a long time (15+ years in various incarnations of S+ and R), but it still has some deficiencies. I’m finding Python easier and faster to work with for large data sets. I’m also a bit happier with Python’s graphical capabilities via matplotlib, which allows dynamic updating of graphs a la Matlab, another drawback that R has despite great graphical capabilities. Where am I finding Python useful? Mainly in reading, cleaning and transforming data sets, and a bit of analysis using scipy. Python seems more efficient in reading and working through large data sets than R ever was.  Data cleaning using the string utilities and the re  module and exploration also seems pretty easy. I’ll probably have to right a few utilities, or just pass that stuff into R. I’m more comfortable doing the analysis within R, so I’m using rpy2 quite a bit. Gautier has done a nice upgrade of the old rpy which I used quite a bit. One thing that Python doesn’t have well yet is a literate programming interface. Sweave is one of the strengths of R (and StatWeave looks interesting as an interface with other software like SAS, Stata, etc) which I use almost on a daily basis for report writing. pyreport 0.3 seems promising, and does allow for the report to be written in LaTeX, but I need to play with it some more before I can make a call on it. pyreport does allow the simplicity of reStructured Text for documentation, which I wish Sweave was capable of. I figure this can be easily remedied in R by modifying the RweaveHTML driver written by my old friend Greg Snow. [Addendum, 3/22/09: I recently found a python package for LaTeX (python.tex), which allows one to embed python code in a LaTeX document, then run latex using the –shell-escape flag. This then runs the python code and embeds the results into the LaTeX document. Sort of the opposite of Sweave, but I figure it will be quite useful as well. It should even work within Sweave documents, since the Sweave parser will take out the R/S parts, then running latex will take care of the python parts.] Speaking of report writing, this in another place I use Python a lot in my workflow to convert file formats. I use the Python API for OpenOffice.org to transform formats, both for Writer documents and for spreadsheets. I’ve written small Python scripts in my ~/bin so that I can, on the fly, convert HTML to odt or doc. This is proving quite useful and seems to preserve formats reasonably well. So my reporting workflow is to use Sweave to create a LaTeX document, which I then convert to PDF and HTML, and then transform the HTML to doc using Python. I also create all my graphics as PDF, EPS and SVG formats for subsequent editing by clients. These formats produce the least loss on transformation (the vector formats EPS and SVG have no loss), which is great for large, multicolored heatmaps I produce. I will also create PNG graphics if I have to provide a Word document for the client. 	 0 Comments
Short introduction to R in Finance	https://www.r-bloggers.com/2009/03/short-introduction-to-r-in-finance/	March 5, 2009	Thinking inside the box	" I just posted my slides on my presentations
page. The slides give a brief overview of R, the CRAN network and the by now over 1600
packages, mention the Finance Task
View, briefly present four different packages (or package sets) and of
course beat the drum for our upcoming
R/Finance conference that will take place here in Chicago at the end of
next month.

 "	 0 Comments
Wanderlust	https://www.r-bloggers.com/2009/03/wanderlust/	March 4, 2009	John Myles White	We Americans have a reputation as being unworldly. Given the results of the most recent Pew survey, perhaps we deserve it. Evidently, the majority of us never move out of our home states. 	 0 Comments
Click Tracks and Beat Detection	https://www.r-bloggers.com/2009/03/click-tracks-and-beat-detection/	March 4, 2009	John Myles White	Being a drummer, a programmer and a fan of statistical analysis, this post on the (unnaturally) perfect timing of drum parts recorded to a click track was a real delight to me. Of course, many claims in the post are odd: it seems hard to imagine that a  person recorded the drums for Britney Spears. And it seems as likely that some of the drum parts discussed were edited after the recording to make them line up with a click better. Still, it’s a great experiment. What would be really nice is a time series plot averaging over top ten songs from each year. Maybe I’ll get to work on that tonight before bed. 	 0 Comments
RQuantLib 0.2.11	https://www.r-bloggers.com/2009/03/rquantlib-0-2-11/	March 3, 2009	Thinking inside the box		 0 Comments
Simulate parameters of a tobit model	https://www.r-bloggers.com/2009/03/simulate-parameters-of-a-tobit-model/	March 3, 2009	Yu-Sung Su		 0 Comments
Color Schemes for R Bar Plots	https://www.r-bloggers.com/2009/03/color-schemes-for-r-bar-plots/	March 1, 2009	John Myles White	A recurrent source of irritation for me is the absence of a good default behavior in R for choosing the color scheme for bar plots. A stacked bar plot looks only as good as the color scheme you use. In hope of finding a usable scheme that I could settle on as a personal default, I picked two color schemes, Sunshine over Glacier and Sweet Valentine, from Adobe’s Kuler site that struck me as particularly nice for bar plots. To test them, I plotted data from a recent Gene Expression post on moral beliefs among different religious groups in America. You can see the results below. Sunshine over Glacier Sweet Valentine What do people think of these color schemes? Also, does anyone have ideas about how to incorporate a legend that doesn’t ruin the balance of this plot? At present, it’s impossible to know what these graphs actually mean, but I haven’t been able to figure out how to add a legend to the graphs without substantially expanding the screen area used by them. 	 0 Comments
Your flight is moving …	https://www.r-bloggers.com/2009/03/your-flight-is-moving-%e2%80%a6/	March 1, 2009	dan	THE VALUE OF NOT FOLLOWING INSTRUCTIONS  As Shane Frederick has noted, if you say “A bat and a ball cost $1.10. The bat costs $1 more than the ball. How much is the ball?”, you will notice that the vast majority of your friends will say “10 cents” instead of the correct “5 cents”, because they don’t pay attention to the “more than the ball” bit. They assume you mean that the bat costs a buck. But Decision Science News would like to pause and say a few words in defense of not listening to exactly what is said. What if we took instructions literally? If we did, when we got emails from British Airways saying “Your British Airways flight is moving to Heathrow Terminal 3″ as their subject, we would actually believe that our recently booked British Airways flight (to Geneva) was moving to Heathrow Terminal 3. And we’d be wrong. If we then went on to read that the change only affects flights to Barcelona, Nice, and three other cities where we are NOT going, we would be completely perplexed, thinking “How can my flight be moving and not moving at once? It’s impossible!” We’d panic. We’d call BA. But we don’t do any of that. We just shake our heads and think “Worst … subject line … ever”. As Decision Science News reflects and tries to find cases in which it pays to be perfectly literal, it can’t. Even when dealing with computers, even when reading the output of statistical routines, one needs regularly to think “Ok, SAS (or R or STATA or SPSS) you say that, but we both know you don’t mean that.” 	 0 Comments
Rcpp 0.6.4	https://www.r-bloggers.com/2009/03/rcpp-0-6-4/	March 1, 2009	Thinking inside the box	" 
This version changes how use the std namespace: all usage is now
properly prefixed.  Likewise, we now define R_NO_REMAP to not
let R define utility functions as length() or
error() which occassionally creates trouble with other include
files — so now use the more explicit forms Rf_length(),
Rf_error() etc.  Also, starting from this release, C++ class
documentation created by Doxygen is included; it can also be seen from my box
as both browsable
html and a pdf file.
Lastly, this version also adds a minor correction to the Windows build
(spotted by Uwe and Simon).

 "	 0 Comments
GSoC 2009 Chicago area meeting	https://www.r-bloggers.com/2009/04/gsoc-2009-chicago-area-meeting/	April 30, 2009	Thinking inside the box		 0 Comments
Slides from most recent R and HPC tutorial	https://www.r-bloggers.com/2009/04/slides-from-most-recent-r-and-hpc-tutorial/	April 29, 2009	Thinking inside the box	"
This tutorial was a shorter format of just an hour which did not allow for any
parallel computing with R. However, parallel computing with R via MPI, snow,
nws, … is covered in the slides from
December’s
workshop at the BoC.

 "	 0 Comments
Babylonian confusion or flexibility?	https://www.r-bloggers.com/2009/04/babylonian-confusion-or-flexibility/	April 29, 2009	martin	"As we are used from UNIX. there is not one single suitable solution to solve a problem, but usually a few different ways to do “the same”. Depending on what commands we know (best), we will chose the one or the other solution. Only the absolute expert will be able to choose the most efficient commands. There is a similar situation with R’s graphics (and probably for methods from other domains as well). Let’s look at a simple example like the histogram (I use the Old Faithful Geyser Data here as example). 
 I don’t claim that this list is complete, but I think it nicely shows “the problem”. Of course, a “real R programmer” can make any of the plots look like one of the others … The question is more, which of the implementations beyond hist from the base library graphics really adds value to what we already had? The second question probably is, how R will ever resolve the backwards compatibility spiral that might make it look like a real legacy project some time – not to mention the package quality issue? "	 0 Comments
Rails, Scalability and Sexism	https://www.r-bloggers.com/2009/04/rails-scalability-and-sexism/	April 29, 2009	Ed Borasky		 0 Comments
screen in ubuntu 9.04	https://www.r-bloggers.com/2009/04/screen-in-ubuntu-9-04/	April 29, 2009	Paolo		 0 Comments
Introducing Influence.ME: Tools for detecting influential data in mixed models	https://www.r-bloggers.com/2009/04/introducing-influence-me-tools-for-detecting-influential-data-in-mixed-models/	April 29, 2009	Rense Nieuwenhuis		 0 Comments
Google Summer of Code 2009: R / Quantlib	https://www.r-bloggers.com/2009/04/google-summer-of-code-2009-r-quantlib/	April 28, 2009	Thinking inside the box	"
We had twenty-two applications to review for the
R project, including three for the
RQuantLib topic I had proposed.  Khanh’s application was clearly among the
best, and I look forward to helping him do cool stuff over the summer.  He
already posted two short emails on the r-sig-finance
and the quantlib-user
lists soliciting suggestions and comments. So if you have comments regarding R
and QuantLib, please get in touch with him or me!

 "	 0 Comments
2009 Chambers award winner announced	https://www.r-bloggers.com/2009/04/2009-chambers-award-winner-announced/	April 28, 2009	Hadley Wickham		 0 Comments
R/Finance 2009 Overview	https://www.r-bloggers.com/2009/04/rfinance-2009-overview/	April 28, 2009	Joshua Ulrich	"
 "	 0 Comments
Tips from the R-help list : shadow text in a plot and bumps charts	https://www.r-bloggers.com/2009/04/tips-from-the-r-help-list-shadow-text-in-a-plot-and-bumps-charts/	April 28, 2009	Paolo		 0 Comments
Review of ‘Analysis of Integrated and Cointegrated Time Series with R (2nd ed)’ in JSS	https://www.r-bloggers.com/2009/04/review-of-analysis-of-integrated-and-cointegrated-time-series-with-r-2nd-ed-in-jss/	April 27, 2009	Thinking inside the box		 0 Comments
Interesting R Packages	https://www.r-bloggers.com/2009/04/interesting-r-packages/	April 27, 2009	dylan	" 
CHNOSZ Chemical thermodynamics library and activity diagram software   read more "	 0 Comments
R / Finance 2009	https://www.r-bloggers.com/2009/04/r-finance-2009/	April 26, 2009	Thinking inside the box	"
We were fortunate to get seven outstanding invited keynote speakers, as well as
eleven excellent presentations. This was preceded by four short tutorials (and
I’ll post slides from my Introduction to High-Performance Computing with
R soon). With about 150 registered participants, plus keynoters,
presenters, committee members, representatives from the sponsors (a quick
shout of Thanks! to them), some folks from UIC (especially Holly
without whom few things would have happened), we were probably around 200 people gathered at
UIC. And then there was an extended social program at
Jaks which is rather appropriate as we had numerous
important committee meetings there over the preceding months.  All in all it
seems like a successful event.  We may even do it again.  

 "	 0 Comments
Colors in the R terminal	https://www.r-bloggers.com/2009/04/colors-in-the-r-terminal/	April 24, 2009	Paolo		 0 Comments
Real nice Boston Marathon writeup	https://www.r-bloggers.com/2009/04/real-nice-boston-marathon-writeup/	April 23, 2009	Thinking inside the box	"
And so it is with regrets that I have to decline Christian’s
invitation to run Cologne with him on October 4. Another time, hopefully.


 "	 0 Comments
useR! 2009 acceptance: presenting influence.ME	https://www.r-bloggers.com/2009/04/user-2009-acceptance-presenting-influence-me/	April 23, 2009	Rense Nieuwenhuis		 0 Comments
How to get ESS style indentation in textmate	https://www.r-bloggers.com/2009/04/how-to-get-ess-style-indentation-in-textmate/	April 23, 2009	Shravan Vasishth		 0 Comments
Boston Marathon 2009	https://www.r-bloggers.com/2009/04/boston-marathon-2009/	April 22, 2009	Thinking inside the box	"

The race itself was challenging. Having done it once before, and having come
off a
really decent last marathon, 
I may have underestimated the impact of the famous hills. This really is a
wonderful but challenging course. Combined with the poor
training conditions during this last Chicago winter which forced us indoors for quite a few long runs,
as well as a somewhat upset stomach which forced a two-minute break, I came
up short and posted an underwhelming second half. The head wind was also a factor that was
mentioned in a few reports on the comparatively slow times of the elite
runners. So when all was said and done, I ended up with a time of 3:30:13 and
8:01 min/miles which is a little slower than last time.
 

All in all a really great marathon weekend.  As my time from 
Berlin
qualifies me for Boston 2010, I may well be back next year.

 "	 0 Comments
Visulization of correlation matrix 2	https://www.r-bloggers.com/2009/04/visulization-of-correlation-matrix-2/	April 22, 2009	Cloud Wei		 0 Comments
R 2.9.0 is Out!	https://www.r-bloggers.com/2009/04/r-2-9-0-is-out/	April 21, 2009	Paolo		 0 Comments
Getting at the internals of R functions	https://www.r-bloggers.com/2009/04/getting-at-the-internals-of-r-functions/	April 20, 2009	R Tips	I got asked the question the other day about how to get to the internals of a function.  I realized that this was not something that I had covered yet on my tips blog. read more 	 0 Comments
opentick is no more	https://www.r-bloggers.com/2009/04/opentick-is-no-more/	April 17, 2009	Joshua Ulrich	"
 "	 0 Comments
Barry3-Apdex Also Lives in R	https://www.r-bloggers.com/2009/04/barry3-apdex-also-lives-in-r/	April 17, 2009	Neil Gunther		 0 Comments
Webscraping using readLines and RCurl	https://www.r-bloggers.com/2009/04/webscraping-using-readlines-and-rcurl/	April 14, 2009	bryan	There is a massive amount of data available on the web.  Some of it is in the form of precompiled, downloadable datasets which are easy to access.  But the majority of online data exists as web content such as blogs, news stories and cooking recipes.  With precompiled files, accessing the data is fairly straightforward; just download the file, unzip if necessary, and import into R.  For “wild” data however, getting the data into an analyzeable format is more difficult.  Accessing online data of this sort is sometimes reffered to as “webscraping”. read more 	 0 Comments
Testing RSI(2) with R, First Steps	https://www.r-bloggers.com/2009/04/testing-rsi2-with-r-first-steps/	April 13, 2009	Joshua Ulrich	"
 "	 0 Comments
Using partial name matching and match.arg	https://www.r-bloggers.com/2009/04/using-partial-name-matching-and-match-arg/	April 13, 2009	R Tips	Sorry it has been so long to write an R tip.  I’ve been really busy with my research, which you would think would produce lots of tips, but apparently not.  Anyway here is a little tip to look at. read more 	 0 Comments
What’s better than R for exploratory data plots?	https://www.r-bloggers.com/2009/04/whats-better-than-r-for-exploratory-data-plots/	April 13, 2009	Phil		 0 Comments
Labeled outliers in R boxplot	https://www.r-bloggers.com/2009/04/labeled-outliers-in-r-boxplot/	April 13, 2009	[email protected]		 0 Comments
GGobi and Choreography	https://www.r-bloggers.com/2009/04/ggobi-and-choreography/	April 12, 2009	Hadley Wickham		 0 Comments
New Garmin Forerunner	https://www.r-bloggers.com/2009/04/new-garmin-forerunner/	April 10, 2009	Thinking inside the box	"

But then a few days prior I had followed fellow Debian
marathoner Christian and used my birthday at the end of this month (as
well as the upcoming Boston
Marathon) as an excuse for conspicuous comsumption.  After some price
comparison, I ordered a factory reconditioned Garmin Forerunner 405 from this web discounter at a nice rebate to
the regular price.  It arrived this afternoon, seemingly shining new and I
have been fiddling with it for the last little while.
 

This device features wireless data transfer to a usbstick.  This meant
booting the laptop in windoze for the first time in years to load the ‘client
software’ after which data transfer proceeded.  The Garmin Connect site has very slick
presentation and aggregation of the data.  The trouble is of course how to
get the data there when running Linux… Christian had mentioned the garmin-forerunner-tools
package. Unfortunately, this seems to really be written for the Forerunner
305 models as it doesn’t see the device at all.  Some more googling lead to
this
page and the gant
tarball.  All still fairly raw, but with some prodding in the settings of
the 405 (‘pairing’ set to ‘on’, ‘force send’ set to ‘yes’; which may have to be
reset each time ?) I got my two xml files off the gps watch. Yay. We’ll see
what mode I will settle one. With the 201 and its ancient serial port, I
basically just dropped the run and training histories which their fairly
limited data collections.   

Last but not least, fellow Oak Park runner Peter Sagal had a humorous
Runner’s World column on the whole GPS geekyness. If he’d only known how
to pair it with programming geekyness…

 "	 0 Comments
Inference for R	https://www.r-bloggers.com/2009/04/inference-for-r/	April 8, 2009	Gregor Gorjanc		 0 Comments
Marriage and Happiness	https://www.r-bloggers.com/2009/04/marriage-and-happiness/	April 7, 2009	John Myles White	The Pew Research Center just published a piece reviewing their finding that people who are married report significantly greater levels of happiness than those who are unmarried. I always enjoy this result, particularly because of contemporary Western culture’s uneasy relationship with the institution of marriage. What I would really like to see, though, is a graph that plots the relative difference in happiness between people who are married and unmarried as a function of the number of years of marriage. I’m entirely willing to concede that the difference is always positive, but I have the suspicion that it’s also decreasing on at least some intervals. It would be fascinating if the opposite were the case — assuming, of course, that one had controlled for the obvious selection effect that the couples who will stay married longer are almost by definition the couples that are happier, so that it’s a foregone conclusion that the level of happiness will increase with the length of the marriage. 	 0 Comments
R: Building functions – using default settings that can be modified via the dot-dot-dot / three point argument	https://www.r-bloggers.com/2009/04/r-building-functions-%e2%80%93-using-default-settings-that-can-be-modified-via-the-dot-dot-dot-three-point-argument/	April 7, 2009	markheckmann	"Before you read this post, please have a look at Enrique’s comment below. He pointed out that the built-in R function modifyList() already does what I wanted to describe in this post. Well, I live to learn 🙂 I was wondering how I could write a function that uses default settings but accepts a list to overwrite the default settings via the dot-dot-dot / three-point argument. Here comes my solution.
  "	 0 Comments
Helpful statistical references	https://www.r-bloggers.com/2009/04/helpful-statistical-references/	April 7, 2009	bryan	In a previous article I provided a list of R programming resources.  As a complement to that post, I’ve compiled a list of statistically oriented websites that colleagues and I have found useful below.  For the most part, these sites focus on statistics and quantitative research methods rather than programming.   read more 	 0 Comments
PDQ 5.0 is on the Launch Pad	https://www.r-bloggers.com/2009/04/pdq-5-0-is-on-the-launch-pad/	April 4, 2009	Neil Gunther		 0 Comments
Modern Microprocessor MIPS	https://www.r-bloggers.com/2009/04/modern-microprocessor-mips/	April 2, 2009	Neil Gunther		 0 Comments
Rcpp 0.6.5	https://www.r-bloggers.com/2009/04/rcpp-0-6-5/	April 1, 2009	Thinking inside the box		 0 Comments
SNA with R: Loading your network data	https://www.r-bloggers.com/2009/04/sna-with-r-loading-your-network-data/	April 1, 2009	Allan Engelhardt	"
We are interested in Social Network Analysis using the statistical analysis and computing platform R.  As usual with R, the documentation is pretty bad, so this series collects our notes as we learn more about the available packages and how they work.  We use here the statnet group of packages, which seems to be the most comprehensive and most actively maintained network analysis packages.
 
The first task which we consider in this post is to load our data into a network object, which is how all the statnet packages represent a network.  Typically for R, the documentation is voluminous but not always as helpful as one could want.
 
We will assume that the raw data for our analysis is in a transactional format that is typical at least in the Telecommunications and Finance industries.  In the former the terminology is Call Detail Record (CDR) and an extract may look a little like the following:
 
Here a record indicates that the customer identified as src called (type=call) the customer dest at the given time start and the call lasted duration seconds.  In general, there will be (many) more attributes describing the transaction which are represented by the ….  In a Financial Services example, the records may be money transfers between accounts.
 
In the naive implementation of this data as a network, we would have the sources and destinations (broadly speaking: people) as vertices and the calls as edges.  That broadly seems to make sense: people are connected by the calls they make, and that is the social relationship we wish to model.
 
In the terminology of the network class, that means that our network will be directed (calls and money transfers have a direction from one person to another) and will need to allow multiple edges between the same endpoints (because any one person can, and indeed usually will, make several calls to the same other person).
 
We could consider dropping the multiple attribute of the network and instead represent the fact that A has called B with a single edge and perhaps have the number of calls and their total duration as an edge attribute.  We will investigate this another time, but it is surely a less faithful representation of the data that we have (and we would need to drop information like the time of call).
 
One thing they seem to forget to tell you in the documentation is that when you import your data your vertex identifiers (which in our case is customer or account numbers) must be changed to number the vertices and that this numbering must be sequential and start from 1.  Being used to an environment where the vertex identifiers are arbitrary (and arrays usually start from 0), this one had me puzzled for a while.  The error message that tells you your vertex numbering is not what the package expected is spectacularly unhelpful:
 
For the discussion that follows, we will assume that you have changed your identifies externally to R.
 
The good news is that our data is essentially in a format that the network package calls edge list and which it can import directly.
 
I say “essentially” because for some strange reason the package expects the destination to come before the source which seems ass-backwards to me.  But assume we have our data in a file cdr.csv like this (we only have calls here):
 
Then we can load the data into R easily:
 
OK, that's a lot of warnings, but it basically worked.  We have figured out how to load our network data into the network package in R.
 
We can’t do an exhaustive performance review now, but let us at least make sure we can load medium-sized networks.  We change our CDR simulator to emit the desitnation before the source just like network likes it and let it run.
 
The first file has 2,645,288 (simulated) CDR lines from 100k customers and it loads OK on our small development workstation even with the default settings:
 
The size of the saved network object is 373MB (only 27MB compressed).
 
We can potentially save some time and memory by not explicitly not performing the edge check (again: the documentation frustrates us and is silent on what the defaults are for the network call we used above) so we try this for our next file with 51,316,641 lines of CDR data (again for 100k customers) which also saves us some column swapping:
 
Our attempted optimization did not seem to matter and this network is too big for the machine and the network package.  Building the network was painful as I was working on the workstation at the same time.  The machine has 16GB installed RAM, but it was clearly running out and swapping extensively.
 
51 million might be a reasonable size data set for some Financial Services applications but it is clearly a trivial number of records for Telecommunications.  I’ll need to do some more digging around.
 
Does anybody have any SNA benchmarks?  I like the KXEN implementation for its simplicity and speed so I might get a copy and try it out.  Any R performance experts who could make suggestions in the comments?  How big are your networks?
 "	 0 Comments
Ubuntu Developer Summit in Barcelona	https://www.r-bloggers.com/2009/05/ubuntu-developer-summit-in-barcelona/	May 31, 2009	Thinking inside the box	"
All told, a well-organised conference in a nice setting — two stone throws
from the legendary Camp
Nou. Unfortunately, I had to leave by Wednesday so I missed what was
undoubtedly quite a scene in Barcelona following

Barca’s dismantling of Man U in this year’s Champions League final. 

 "	 0 Comments
Nice Interview	https://www.r-bloggers.com/2009/05/nice-interview/	May 31, 2009	Paolo		 0 Comments
R used by KDD 2009 cup winner of slow challenge	https://www.r-bloggers.com/2009/05/r-used-by-kdd-2009-cup-winner-of-slow-challenge/	May 31, 2009	Allan Engelhardt	"
The results from the KDD Cup 2009 challenge (which we wrote about before) are in, and the winner of the slow challenge used the R statistical computing and analysis platform for their winning submission.
 
The write up (username/password may be required) from Hugh Miller and team at the University of Melbourne includes these points:
 
Impressive hardware selection!  Well done R.  Weka was another popular tool among the top entrants.  Key for all of them were clever data preparation and variable substitution.  The fast track winners from IBM document this in some detail:
 
We normalized the numerical variables by range, keeping the sparsity. For the categorical variables, we coded them using at most 11 binary columns for each variable. For each categorical variable, we generated a binary feature for each of the ten most common values, encoding whether the instance had this value or not. The eleventh column encoded whether the instance had a value that was not among the top ten most common values. We removed constant attributes, as well as duplicate attributes.
 
We replaced the missing values by mean for numerical attributes, and coded them as a separate value for discrete attributes. We also added a separate column for each numeric attribute with missing values, indicating wether the value was missing or not. We also tried another approach for imputing missing values based on KNN.
 
On the large data set we discretized the 100 numerical variables that had the highest mutual information with the target into 10 bins, and added them as extra features.
 
We tried PCA on the large data set, but it did not seem to help.
 
Because we noticed that some of the most predictive attributes were not linearly correlated with the targets, we build shallow decision trees (2-4 levels deep) using single numerical attributes and used their predictions as extra features. We also build shallow decision trees using two features at a time and used their prediction as an extra feature in the hope of capturing some non-additive interactions among features.
 Jump to comments. 

How to win the KDD Cup Challenge with R and gbm
 Hugh Miller, the team leader of the winner of the KDD Cup 2009 Slow Challenge (which we wrote about recently ) kindly provides more information about how to win this public challenge using the R statistical computing and analysis platform on a laptop (!). "	 0 Comments
Emacs: AucTeX + Rubber + Sweave	https://www.r-bloggers.com/2009/05/emacs-auctex-rubber-sweave/	May 30, 2009	Vinh Nguyen	I got rubber to work with auctex and sweave (Rnw) files with the help of this. Basically, combined with my other stuff, I tweaked my .emacs file to look like: Now, when a *.Rnw file is open, I can do C-c C-c, select Sweave.  Then repeat, select RubberSweave (or LatexSweave). 	 0 Comments
JPM Chase Corporate Challenge 2009	https://www.r-bloggers.com/2009/05/jpm-chase-corporate-challenge-2009/	May 30, 2009	Thinking inside the box	"
We fielded a small but spirited
team of nine runners. I finished with a decent (hand-stopped) time of 22
minutes and 27.93 seconds for the 3.5 miles — or a 6:25 min/mile pace. That
is among the fasters times but not quite the fastest
compared
to
the
other
six
times
I have run this.

 
Most importantly, everybody seems to have had a blast. And we did set a
record for longer post-race party which sets a nice precedent for 2010. 

 "	 0 Comments
The R Journal, Issue 1 Volume 1	https://www.r-bloggers.com/2009/05/the-r-journal-issue-1-volume-1/	May 29, 2009	Stephen		 0 Comments
R tips: Use read.table instead of strsplit to split a text column into multiple columns	https://www.r-bloggers.com/2009/05/r-tips-use-read-table-instead-of-strsplit-to-split-a-text-column-into-multiple-columns/	May 29, 2009	Allan Engelhardt	"
Someone on the R-help mailing list had a data frame with a column containing IP addresses in quad-dot format (e.g. 1.10.100.200).  He wanted to sort by this column and I proposed a solution involving strsplit.  But Peter Dalgaard comes up with a much nicer method using read.table on a textConnection object:
 
That is very, very neat!  Thank you Peter.
 "	 0 Comments
R Journal 1/1	https://www.r-bloggers.com/2009/05/r-journal-11/	May 29, 2009	Paolo		 0 Comments
Accessing Soil Survey Data via Web-Services	https://www.r-bloggers.com/2009/05/accessing-soil-survey-data-via-web-services/	May 28, 2009	dylan	"Soil Survey Data  
Online Querying of NRCS Soil Survey Data
Sometimes you are only interested in soils data for a single map unit, component, or horizon. In these cases downloading the entire survey from Soil Data Mart is not worth the effort. An online query mechanism would suffice. The NRCS provides a form-based, interactive querying mechanism and a SOAP-based analogue. These services allow soil data lookup from the current snapshot of all data stored in NASIS. read more "	 0 Comments
Making Sense of Large Piles of Soils Information: Soil Taxonomy	https://www.r-bloggers.com/2009/05/making-sense-of-large-piles-of-soils-information-soil-taxonomy/	May 27, 2009	dylan	"Western Fresno Soil Hierarchy: partial view of the hierarchy within the US Soil Taxonomic system  
Soil Data
Field and lab characterization of soil profile data result in the accumulation of a massive, multivariate and three-dimensional data set. Classification is one approach to making sense of a large collection of this type of data. US Soil Taxonomy is the primary soil classification system used in the U.S.A and many other countries. This system is hierarchical in nature, and makes use on the presence or absence  of diagnostic soil features. A comprehensive discussion of Soil Taxonomy is beyond the scope of this post. A detailed review of Soil Taxonomy can be found in Buol, S. W.; Graham, R. C.; McDaniel, P. A. & Southard, R. J. Soil Genesis and Classification Iowa State Press, 2003. read more "	 0 Comments
Embeding fonts in figures produced by R	https://www.r-bloggers.com/2009/05/embeding-fonts-in-figures-produced-by-r/	May 27, 2009	Gregor Gorjanc		 0 Comments
R and data	https://www.r-bloggers.com/2009/05/r-and-data/	May 26, 2009	erehweb	My fellow bloggers John and Scott have posted recently about the free statistical programming language R.  How does it compare to an expensive language like SAS? If you’ve done any statistical analysis, then you’ll know that getting and cleaning the data is a major step in any project.  SAS does a pretty good job at this, and will complain if the data is not in the format you think it is.  As for R, here’s an excerpt from the R FAQ: 7.10 How do I convert factors to numeric? It may happen that when reading numeric data into R (usually, when reading in a file), they come in as factors. If f is such a factor object, you can use as.numeric(as.character(f)) to get the numbers back. More efficient, but harder to remember, is as.numeric(levels(f))[as.integer(f)] In any case, do not call as.numeric() or their likes directly for the task at hand (as as.numeric() or unclass() give the internal codes). As one of my favorite musicals says, “It ain’t no joke, that’s why it’s funny”.  Maybe when you do an uncommon operation like reading in a file, your numbers will be silently converted into factors / categorical variables.  Or maybe not.  Ha ha.   But certainly, don’t do anything silly like thinking as.numeric(f) would convert f into numbers you might want.  Ha ha ha.  Oh, and that “more efficient” way of doing things?  It crashes if f was actually numeric to start with.  Ha ha ha ha.  Stop, you’re killing me!  [or at least, my productivity]. To complete the joke, here’s an excerpt from the R manual: In general, coercion from numeric to character and back again will not be exactly reversible, because of roundoff errors in the character representation. That’s fair enough.  It’s not as if you have a good reason for doing this, except perhaps when you’re reading numbers in from a file. 	 0 Comments
Free one-day R course at Vanderbilt	https://www.r-bloggers.com/2009/05/free-one-day-r-course-at-vanderbilt/	May 26, 2009	Stephen		 0 Comments
More Recursion in R	https://www.r-bloggers.com/2009/05/more-recursion-in-r/	May 26, 2009	R Tips	I found another gem in R today.  Earlier I commented about how R could do recursion, something that I love.  I write some pretty complicated recursion functions in my research, but I also have a bad habit of compulsively reorganizing things. Now I’ve come across the wonderful word Recall.  Sorry no example this time, just read up on it.  Now if they can just get a self keyword for true object orientation, and encapsulation. 	 0 Comments
Simple Approach to Converting GRASS DB-backends	https://www.r-bloggers.com/2009/05/simple-approach-to-converting-grass-db-backends/	May 23, 2009	dylan	" 
Premise:
The current default database back-end used by the GRASS vector model is DBF (as of GRASS 6.5), however this is probably going to be changed (to SQLite) in GRASS 7. The DBF back-end works OK, however it tends to be very sensitive (i.e. breaks) when reserved words occur in column names or portions of a query. Complex UPDATE statements don’t work, and just about anything more complex than a simple SELECT statement usually results in an error. Switching to the SQLite (or Postgresql, etc.) back-end solves most of these problems. Currently GRASS uses a single SQLite (file-based) database per mapset– convenient if you are interested in joining attribute tables between vectors; but not set-in-stone as the final approach that will be used by default in GRASS 7. Regardless, converting the back-end is a fairly simple matter. Finally, taking the time to convert to an SQLite or Postgresql back-end will undoubtably save you time and sanity if you ever find yourself working with vector+attribute data on a regular basis. Having access to a complete implementation of SQL can make extracting, summarizing, joining, and re-formatting (column names, types, etc.) tabular data much simpler than what is available in the DBF back-end. Also, there are several convenient graphical SQLite managers available, such as SQLite manager, SQLite data browser, and SQLite Admin. read more "	 0 Comments
Temporary Debian mail outage	https://www.r-bloggers.com/2009/05/temporary-debian-mail-outage/	May 23, 2009	Thinking inside the box	"

If you happened to have sent me mail to my debian.org address during that time
period, you may have gotten a hard reject (‘550 Administrative prohibition’)
as did a test mail of mine. In this case mail may not be respooled, so please
do send it again.

 

My alternate address, formed by my first name followed by the family name and
the commercial top-level domain, remained functional as a fallback.

 "	 0 Comments
Data.gov	https://www.r-bloggers.com/2009/05/data-gov/	May 21, 2009	Allan Engelhardt	"
I am always on the lookout for useful data sources for training in statistics, so I am excited that Data.gov has opened for business.  The purpose of Data.gov is to increase public access to high value, machine readable datasets generated by the US Government. 
 
This is a great initiative which I look forward to explore when I am not in a tiny airport at 3 am (but hey: they have free wifi) and which I hope other countries will take up.
 
Are there other catalogues of data sets that you use?
 Jump to comments. 

Beautiful Data
 OReillys recent publication Beautiful Data has a chapter by Jeff Jonas which is enough reason in itself for me to recommend it. The chapter, Data Finds Data , is also available as a PDF download. 

When Big Data Matters
 "	 0 Comments
Bootstrapping and the boot package in R	https://www.r-bloggers.com/2009/05/bootstrapping-and-the-boot-package-in-r/	May 21, 2009	Jeromy Anglim		 0 Comments
Baby steps with RSRuby in Rails	https://www.r-bloggers.com/2009/05/baby-steps-with-rsruby-in-rails/	May 20, 2009	nsaunders	"Plotting and charting libraries for Ruby (on Rails) abound.  However, few are sophisticated enough for scientists and many are not actively maintained.  Plotting in R, on the other hand, is about as sophisticated as it comes. Can we bridge Ruby and R?  Yes we can, thanks to Alex Gutteridge’s RSRuby.  The next logical question:  how to plot data using RSRuby in your shiny new Rails application?
Update Jul 22: recently, the code in this post has stopped working for me (unwanted X windows pop up, stack smashing errors) – so use at your discretion or not at all!

First, thanks to Peter Lane over at Ruby for Scientific Research, a relatively-new blog with some excellent introductory RSRuby articles.  Thanks also to Alex for providing RSRuby. My OS is  Ubuntu 9.04 with rubygems 1.3.3, installed from source and Rails 2.3.2.  This works for me: I edited config/environment.rb to include the gem: Let’s call it Plotter.  Usual procedure: We won’t use the database (default sqlite3) in this example, but you may use it later on. RSRuby will fail unless the environment variable R_HOME is set.  To tell Plotter that R_HOME is /usr/lib/R, I put this in the file app/helpers/application_helper.rb: RSRuby works by providing an instance of an “R object”, on which you call R functions as methods.  I provided the object by editing app/controllers/application_controller.rb to include a method named InitR: For testing purposes, I created a controller named Home with a single view named index: I edited config/routes.rb so as the root of the application is the index view for the home controller: Now we can get to work on the file app/controllers/home_controller.rb.  I wrote a sample method, show_image to plot a histogram using R via RSRuby: Not the prettiest code, but you get the idea.  Obviously the owner of the HTTP daemon (www-data on Ubuntu) must have write permission to the location of the PNG file; /tmp in this case. All that remains is to edit app/views/home/index.html.erb so as it displays the image: Which just says “wrap whatever comes out of the method show_image in an IMG SRC tag”.
  RSRuby on Rails histogram plot One drawback of this approach is the need to write a file to disk and so have to worry about naming, cleaning up and so on.  I have not found a way to make R output plots to stdout or as a base64 string.  If you think this is possible, leave a comment. OK – so now you can get serious, write some generic methods, design a database schema to store data for plotting and so on. "	 0 Comments
Create multiple graphics in R without multiple calls to pdf / postscript / jpeg / png	https://www.r-bloggers.com/2009/05/create-multiple-graphics-in-r-without-multiple-calls-to-pdf-postscript-jpeg-png/	May 14, 2009	Vinh Nguyen	To save multiple graphics, e.g, Rplot001.pdf, Rplot002.pdf, …, Rplot050.pdf, we don’t have to call pdf() 50 times (or any similar function). Use “Rplot%03d.pdf” for filename in pdf() and each plot() call will be saved to a new pdf file.  Use dev.off() once at the end to close all devices! check out ?sprintf for more information about %03 and other C-style string formatting. 	 0 Comments
Nonparametric High-Dimensional Time Series Analysis	https://www.r-bloggers.com/2009/05/nonparametric-high-dimensional-time-series-analysis/	May 13, 2009	Quantitative Finance Collector		 0 Comments
Negative Scalability Coefficients in Excel	https://www.r-bloggers.com/2009/05/negative-scalability-coefficients-in-excel/	May 12, 2009	Neil Gunther		 0 Comments
Packages featured with Inference for R	https://www.r-bloggers.com/2009/05/packages-featured-with-inference-for-r/	May 12, 2009	Joshua Ulrich	"
 "	 0 Comments
Analytic Infrastructure – Three Trends	https://www.r-bloggers.com/2009/05/analytic-infrastructure-%e2%80%93-three-trends/	May 11, 2009	Robert Grossman	This is a post about systems, applications, services and architectures for building and deploying analytics.   Sometimes this is called analytic infrastructure.   In this post, we look at several trends impacting analytic infrastructure. Trend 1.  Open source analytics has reached Main Street.  R, which was first released in 1996, is now the most widely deployed open source system for statistical computing.  A recent article in the New York Times estimated that over 250,000 individuals use R regularly.  Dice News has created a video called “What’s Up with R” to inform job hunters using their services about R.  In the language of Geoffrey A. Moore’s book Crossing the Chasm, R has reached “Main Street.” Some companies still either ban the use of open source software or require an elaborate approval process before open source software can be used.  Today, if a company does not allow the use of R, it puts the company at a competitive disadvantage. Trend 2.   The maturing of open, standards based architectures for analytics.  Many of the common applications used today to build statistical models are stand-alone applications designed to be used by a single statistician.  It is usually a challenge to deploy the model produced by the application into operational systems.  Some applications can express statistical models as C++ or SQL, which makes deployment easier, but it can still be a challenge to transform the data into the format expected by the model. The Predictive Model Markup Language (PMML) is an XML language for expressing statistical and data mining models that was developed to provide an application-independent and platform-independent mechanism for importing and exporting models.  PMML has become the dominant standard for statistical and data mining models.   Many applications now support PMML. By using these applications,  it is possible to build an open, modular standards based environment for analytics.  With this type of open analytic environment, it is quicker and less labor-intensive to deploy new analytic models and to refresh currently deployed models. Disclaimer: I’m one of the many people that has been involved in the development of the PMML standard. Trend 3.   Cloud-based data services.  Over the next several years, cloud-based services will begin to impact analytics significantly.   A later post in this series will show simple it is use R in a cloud for example.  Although there are security, compliance and policy issues to work out before it becomes common to use clouds for analytics, I expect that these and related issues will all be worked out over the next several years. Cloud-based services provide several advantages for analytics.  Perhaps the most important is elastic capacity — if 25 processors are needed for one job for a single hour, then these can be used for just the single hour and no more.  This ability of clouds to handle surge capacity is important for many groups that do analytics.  With the appropriate surge capacity provided by clouds, modelers can be more productive, and this can be accomplished in many cases without requiring any capital expense.  (Third party clouds provide computing capacity that is an operating and not a capital expense.) 	 0 Comments
Searching through mailing list archives	https://www.r-bloggers.com/2009/05/searching-through-mailing-list-archives/	May 8, 2009	Paolo		 0 Comments
Region Of Interest analysis of BOLD data	https://www.r-bloggers.com/2009/05/region-of-interest-analysis-of-bold-data/	May 6, 2009	[email protected]	jpbplot experimentDesign.df visual_roi.mat,motor_roi.mat condition1_condition2 1,2 jpbplot experimentDesign.df ACT-R condition1_condition2 1,2 1 EASY HARD 1 /Volumes/fMRI/pp1/functional/smoothed_000011 EASY HARD 2 /Volumes/fMRI/pp1/functional/smoothed_000021 EASY HARD 3 /Volumes/fMRI/pp1/functional/smoothed_000031 EASY EASY 1 /Volumes/fMRI/pp1/functional/smoothed_000041 EASY EASY 2 /Volumes/fMRI/pp1/functional/smoothed_000051 EASY EASY 3 /Volumes/fMRI/pp1/functional/smoothed_000061 HARD HARD 1 /Volumes/fMRI/pp1/functional/smoothed_000071 HARD HARD 2 /Volumes/fMRI/pp1/functional/smoothed_000081 HARD HARD 3 /Volumes/fMRI/pp1/functional/smoothed_000091 EASY HARD 1 /Volumes/fMRI/pp1/functional/smoothed_0000101 EASY HARD 2 /Volumes/fMRI/pp1/functional/smoothed_0000111 EASY HARD 3 /Volumes/fMRI/pp1/functional/smoothed_0000121 HARD EASY 1 /Volumes/fMRI/pp1/functional/smoothed_0000131 HARD EASY 2 /Volumes/fMRI/pp1/functional/smoothed_0000141 HARD EASY 3 /Volumes/fMRI/pp1/functional/smoothed_000015… 	 0 Comments
Resources for Learning R in Iraq?	https://www.r-bloggers.com/2009/05/resources-for-learning-r-in-iraq/	May 6, 2009	Ben Mazzotta	Please comment on this if you know of Arabic and Kurdish language resources for learning R. I have been encouraging the economics faculty here to learn R for econometrics, both on grounds of quality and cost. Here is a short list of resources that can help new users make the transition from SPSS if they choose. These resources are all in English. Does anyone know where to find R manuals or tutorials in Arabic? Update: the R project posts foreign-language tutorials in Chinese, French, German, Italian, and several other languages but no Arabic. 	 0 Comments
SNA with R: Loading large networks using the igraph library	https://www.r-bloggers.com/2009/05/sna-with-r-loading-large-networks-using-the-igraph-library/	May 6, 2009	Allan Engelhardt	"
We are interested in Social Network Analysis using the statistical analysis and computing platform R.  The documentation for R is voluminous but typically not very good, so this entry is part of a series where we document what we learn as we explore the tool and the packages.
 
In our previous post on SNA we gave up on using the statnet package because it was not able to handle our data volumes.  In this entry we have better success with the igraph package.
 
The task we are considering is still how to load the network data into the R package’s internal representation.  We will assume that the raw data for our analysis is in a transactional format that is typical at least in the Telecommunications and Finance industries.  In the former the terminology is Call Detail Record (CDR) and an extract may look a little like the following:
 
Here a record indicates that the customer identified as src called (type=call) the customer dest at the given time start and the call lasted duration seconds.  In general, there will be (many) more attributes describing the transaction which are represented by the ….  In a Financial Services example, the records may be money transfers between accounts.
 
We are able to load the previous test data with 51 million records easily:
 
Time to up the ante!  We have a file with simulated call data records containing over 700 million entries where we suspect the algorithm used is under-estimating nodes with small connections.  Let’s check on the first ½ billion records (which seems to more-or-less fit in our available memory on this workstation):
 
As we suspected, the Monte Carlo algorithm does not provide enough customers with low calling circle sizes.  Fortunately it is very easy to add these separately: the hard part is modelling the larger calling circles.  A mix of these two algorithms provide a reasonably good fit to actual customer behaviour.  (The cut-off at 100 is a parameter to our Monte Carlo simulation program which indeed was 100 for this run.)
 However, it is not all perfect.  When we attempt to add the edge parameters in the obvious way it fails: 
So we are just at the limit.  Probably 100 million records is OK in this environment.  But the core igraph library is accessible from C so better performance can probably be achieved this way and certainly pointers are 8 byte structures on this machine so we should not have the silly limits that R imposes on us.
 "	 0 Comments
R Reference Card (PDF)	https://www.r-bloggers.com/2009/05/r-reference-card-pdf/	May 5, 2009	Stephen		 0 Comments
05.05.09: 6 Steps to call a C function in R	https://www.r-bloggers.com/2009/05/05-05-09-6-steps-to-call-a-c-function-in-r/	May 5, 2009	Jitao David Zhang		 0 Comments
R/Finance 2009 Presentations Online	https://www.r-bloggers.com/2009/05/rfinance-2009-presentations-online/	May 4, 2009	Joshua Ulrich	"
 "	 0 Comments
Colouring a 3D plot according to z-values	https://www.r-bloggers.com/2009/05/colouring-a-3d-plot-according-to-z-values/	May 3, 2009	Timothée Poisot	Here is a script that colors a 3D plot according to the z value (height) of each point. dd 	 0 Comments
An algorithm to find local extrema in a vector	https://www.r-bloggers.com/2009/05/an-algorithm-to-find-local-extrema-in-a-vector/	May 3, 2009	Timothée Poisot	I spend some time looking for an algorithm to find local extrema in a vector (time series). The solution I used is to “walk” through the vector by step larger than 1, in order to retain only one value even when the values are very noisy (see the picture at the end of the post). It goes like this :  	 0 Comments
Brad Mehldau at the CSO	https://www.r-bloggers.com/2009/05/brad-mehldau-at-the-cso/	May 1, 2009	Thinking inside the box	" The first set was performed as a (strictly acoustic) trio with Larry Grenadier on
bass and Jeff Ballard on drums.
After several compositions by Mehldau and a brazilian samba piece, the first
set closed with a rendition of ‘Holland’ from Sufjan Stevens‘ album
Michigan
which was truly beautiful.  The second set had Mehldau performing solo, again
with several compositions of his own as well as one from Neil Young‘s classic ‘The
Needle and the Damage Done’ leading two two pieces from the Sound of Music
including an amazing, yet really different ‘My favourite things’ that just
hushed a piece of the central melody along with a strond rhythmic
element. Lovely. And then 
to cap it all off, four encores.

  Highly recommended.

 "	 0 Comments
RSI(2) with Position Sizing	https://www.r-bloggers.com/2009/05/rsi2-with-position-sizing/	May 1, 2009	Joshua Ulrich	"
 "	 0 Comments
PDQ 5.0 Test Suite or … How I Spent My Weekend	https://www.r-bloggers.com/2009/06/pdq-5-0-test-suite-or-how-i-spent-my-weekend/	June 29, 2009	Neil Gunther		 0 Comments
August Guerrilla Class: Using R for Performance Analysis	https://www.r-bloggers.com/2009/06/august-guerrilla-class-using-r-for-performance-analysis/	June 29, 2009	Neil Gunther		 0 Comments
Time series data	https://www.r-bloggers.com/2009/06/time-series-data/	June 28, 2009	Jim		 0 Comments
RSI(2) Evaluation	https://www.r-bloggers.com/2009/06/rsi2-evaluation/	June 28, 2009	Joshua Ulrich	"
 "	 0 Comments
Conservatism of Congressional delegation and %Bush vote	https://www.r-bloggers.com/2009/06/conservatism-of-congressional-delegation-and-bush-vote/	June 27, 2009	Jim		 0 Comments
R 2.9.1, CRANberries outage, and missing Java support	https://www.r-bloggers.com/2009/06/r-2-9-1-cranberries-outage-and-missing-java-support/	June 27, 2009	Thinking inside the box	"
Speaking of broken, I had neither noticed that this R version now returns an
additional field (for the repository) in the per-package metadata via
available.packages(), nor that this change had broken my oh-so-useful
and increasingly popular
CRANberrries
html and rss summaries of CRAN changes. So with the
usual beta and rc releases
or R 2.9.1 in Debian starting a week prior, CRANberries had been silent for six days
from Friday the 21st to last Thursday.  I rectified it once I noticed, and
changed the code to no longer fall on its nose at that spot. Sorry for the
few days without service.

 "	 0 Comments
Multiple Imputation with Diagnostics (mi) in R: Opening Windows into the Black Box	https://www.r-bloggers.com/2009/06/multiple-imputation-with-diagnostics-mi-in-r-opening-windows-into-the-black-box/	June 26, 2009	Andrew Gelman		 0 Comments
Filtering cases	https://www.r-bloggers.com/2009/06/filtering-cases/	June 26, 2009	Jim		 0 Comments
Development of tikzDevice is underway	https://www.r-bloggers.com/2009/06/development-of-tikzdevice-is-underway/	June 26, 2009	cameron	Development of the R package tikzDevice has been underway for about a month now.  This package allows for the output of R graphics as TikZ commands. Charlie Sharpsteen and I have gotten it into an alpha stage.  There is no real documentation but there is plenty of comments in the code. We have a R-forge project where binary packages can be downloaded.   The project is also tracked by Github here or at my fork. As of now the device handles most of the graphics parameters.  There are still some issues with text placement and UTF8 string support is currently not implemented. Comments, suggestions are welcome. 	 0 Comments
Set the significant digits for each column in a xtable for fancy Sweave output	https://www.r-bloggers.com/2009/06/set-the-significant-digits-for-each-column-in-a-xtable-for-fancy-sweave-output/	June 26, 2009	Paolo		 0 Comments
A bit about linear models	https://www.r-bloggers.com/2009/06/a-bit-about-linear-models/	June 26, 2009	Jim		 0 Comments
Reading data, and a graph	https://www.r-bloggers.com/2009/06/reading-data-and-a-graph/	June 25, 2009	Jim		 0 Comments
Delete a List Component in R	https://www.r-bloggers.com/2009/06/delete-a-list-component-in-r/	June 24, 2009	Yu-Sung Su		 0 Comments
A start	https://www.r-bloggers.com/2009/06/a-start/	June 24, 2009	Jim		 0 Comments
Weekly R Clinic	https://www.r-bloggers.com/2009/06/weekly-r-clinic/	June 24, 2009	Stephen Turner		 0 Comments
Example 7.3: Simple jittered scatterplot with smoother for dichotomous outcomes with continuous predictors	https://www.r-bloggers.com/2009/06/example-7-3-simple-jittered-scatterplot-with-smoother-for-dichotomous-outcomes-with-continuous-predictors/	June 24, 2009	Ken Kleinman		 0 Comments
Book now discounted 33% at Amazon!	https://www.r-bloggers.com/2009/06/book-now-discounted-33-at-amazon/	June 24, 2009	Ken Kleinman		 0 Comments
PDF tutorial from R course (Introduction to R)	https://www.r-bloggers.com/2009/06/pdf-tutorial-from-r-course-introduction-to-r/	June 23, 2009	Stephen Turner		 0 Comments
I had been wondering what impact my friending 200 people from my…	https://www.r-bloggers.com/2009/06/i-had-been-wondering-what-impact-my-friending-200-people-from-my/	June 22, 2009	Quantitative Doodles	I had been wondering what impact my friending 200 people from my  Gmail address book had, so I scraped the dates from the notification  emails. The plot shows notifications of friend requests from other people to  me in black and confirmations of my requests to other people in red.  That sudden and sharp increase at the end of the graph is from when I  friended 200 people at once on May 17. I also took the dates from all of the other types of notifications  that I have Facebook send to my email. I’ll have more cool results from  these data and from different transformations of the data. Interesting  trends should appear when I take derivatives. I’m also thinking about other ways to stratify the data. I could plot  notifications by time of day or person. Maybe I should get the number  of pictures I was tagged in in addition to just whether I was tagged. I  should also plot total notifications of any sort, not stratified. 	 0 Comments
Who’s Tweets Do I Read… Magic R Code Says…	https://www.r-bloggers.com/2009/06/whos-tweets-do-i-read%e2%80%a6-magic-r-code-says%e2%80%a6/	June 22, 2009	JD Long	" So one glace at my user logs shows the truth: no one gives a rat’s rump that I just quit my job; you just love you some Twitter R code. And I’m nothing but an attention whore, so come get some! So in my last ‘Twitter with R’ post I gave you some code I’d written ripped off that allowed you to update your status from R. That’s kinda cool, but really just for annoying your friends, tweeting when your code is finished running or, as Eva pointed out in the comments, maybe Tweeting the outcome of a routine. But R is good for analyzing data, plotting graphics, and cool stuff like that. Seems like under kill to just Tweet from it. So let’s make some pretty pictures and stuff. Or more specifically, let’s plot a histogram of the last 200 Tweets you received and the people who sent them. An example of said histogram is above. If you don’t already have the libraries XML, lattice, and RCurl, you will need them:
install.packages('RCurl')

install.packages('XML')

install.packages('lattice') Then once you get those bad boys, you can run this code: library(""RCurl"")

library(""XML"")

library(""lattice"")

#

#be sure and put your username and passy here

username<-""YourUserName""

passy<-""YourPass""

#

#This sets up the options for curl

#then makes the request from the Twitter API

#the count=200 option pulls the last 200 tweets from your friends

#the twitter api limits you to a max of 200.... yeah, well that's life

#

opts <- curlOptions(header = FALSE, userpwd = paste(username,"":"",passy,sep=""""))

request <- ""http://twitter.com/statuses/friends_timeline.xml?count=200""

timeline <- getURL(request,.opts = opts)

#

#Now let's beat up on the XML like it owes us money

doc <- xmlInternalTreeParse(timeline, useInternalNodes = TRUE)

#

#grab only the screen_names and make a list

xml_names_of_posters <- getNodeSet(doc, ""/statuses/status/user/screen_name"")

text_names_of_posters <- lapply(xml_names_of_posters,xmlValue)

#

#let's take it out of a list... just for kicks

Twitterbaters <- unlist(text_names_of_posters)

#

#and shove it into a data frame... seems like going around my

#ass to get to my elbow, but I want to put it in a table eventually

#table is kinda like a cross tab. It calcs the frequency for me

posters_list_df<-as.data.frame(Twitterbaters)

Tweets = table(posters_list_df)

#

#lets graph this monkey business with lattice

#

NiftyChart<-barchart(~sort(Tweets), main=list(""Who's Tweets Am I Getting?"" ,cex=1),xlab=list(""Number of Tweets"",cex=1))

NiftyChart

update(NiftyChart, col=""brown"")

# EDIT: Look in the comments for a great base graphics solution from Paolo. He makes the same graph without Lattice. Now be sure and change the username and password then run that mofo. So now you have a pretty picture like the one I made above. Pretty slick, no? Special credit goes to @gappy3000 who tipped me off to making this with Lattice instead of ggplot because of the difficulty sorting with ggplot. @HarlanH for helping me know that my struggles with ggplot were not of my own making but were systemic. The Twitter syntax I hacked together is from the Twitter API documentation. Have fun! And come back later for more attenion whoring blogging from your’s truly, @CMastication. BTW, the reason I didn’t structure this as a function is that you should be stepping through this one line at a time to figure out how it works. That’s just harder with a function. So I did this for your own good. One day you’ll thank me. "	 0 Comments
Parallel computing in R: snowfall/snow	https://www.r-bloggers.com/2009/06/parallel-computing-in-r-snowfallsnow/	June 20, 2009	Vinh Nguyen	I finally have time to try parallel computing in R using snowfall/snow thanks to this article in the 1st issue of R journal, which replaces R news.  I didn’t try it before because i didn’t have a good toy example, and it seemed like a steep learning curve (i only guessed what parallel computing was).  So snow/snowfall works when what you want to do is ‘embarrassingly parallel,’ eg, a simulation study, bootstrap, or a cross-validation.  I do simulation studies a lot, eg, assessing the properties of a statistical methodology, so implementing parallel computing will be very useful. I got the toy example to work, but it was parallel on a single computer with multiple cores.  Thanks to Michael Zeller, I got it to work on multiple machines.  If we use multiple nodes, make sure we enable passwordless ssh. Credit for getting snowfall to work on the BDUC servers (uci-nacs) goes to Harry Mangalam. Here is a script, with a few examples: This is a good general reference for snowfall.  Next thing to try is getting rpvm (PVM) to work for snowfall! 	 0 Comments
Network Analysis Software: focus on F/OSS	https://www.r-bloggers.com/2009/06/network-analysis-software-focus-on-foss/	June 20, 2009	Ben Mazzotta	What do you use for network analysis? I found the Wikipedia list of network software entirely overwhelming. I wanted to test out some of the introductory tools, but avoid the trap of sinking my time into a dead-end software project. (Remember learning Minitab in freshman statistics? How often do you use Minitab today for anything other than freshman statistics?) UCINET came very well recommended by several friends who use social network analysis for business and politics. I wasn’t sure whether it would turn out to be a proprietary, one-way data sink like Blackboard or a useful, interoperable tool for analyzing and sharing data. The Journal of Statistical Software published an issue about Statnet in 2008. Esteemed authors in the field wrote a number of great tutorials about Statnet, which is an umbrella package for most of R’s network analysis tools. UC Irvine hosts a wiki on network analysis tools. That wiki describes a few of the F/OSS tools. All the links contain tutorials. You have your choice of tools that are freestanding, part of R, or part of Sage/Python. If you’ve got a good reason to go with one of the other F/OSS tools, such as Guess, Pajek or Networkx, please comment with your thoughts. I learned a bit about networks at the Santa Fe Institute a couple of years ago. Mark Newman gave stellar lectures, but to be honest a lot of the content there was either over my head, or beyond my programming skills, or not closely related to my research interests. Newman writes his own software which is no doubt more efficient than the widely used packages. I am unabashedly in favor of R. Given the choice, I will always try to knock out a solution in R. Not only is R free and open source, it seems to be the industry standard in statistics. It also has a famously steep learning curve and requires users to learn the command line interface. If you’re not ready to write the script out by hand, you probably need a different solution. I created a picture of industry alliances since 2000 in the space/satellite industry yesterday, and learned a lot about the industry (not just about how to use the software) from the processs. Though they sound fuzzy at first, network measures of position, groups, and distance turn out to extremely intuitive and useful. It’s extremely valuable to have some quantitative tools to back up statements such as, “There is a loose consortium of firms that cluster around a couple of key players.” If you’re reasonably comforatable with basic R commands and have a clear idea what network analysis can tell you, Statnet is ready for prime time. And by the way, I forgot to mention the Robert Hanneman book, a free online textbook for social network analysis. Has anyone read this closely? It seems at first glance like a lucid introduction to a branch of mathematics that can sound a bit technical at first. Especially when mathematicians seem to agree that network analysis is NP-hard and sampling techniques are either extremely computationally intensive, or ineffective, or both. 	 0 Comments
Analysis of Iran absentee votes	https://www.r-bloggers.com/2009/06/analysis-of-iran-absentee-votes/	June 20, 2009	mlt		 0 Comments
R: Function to create tables in LaTex or Lyx to display regression model results	https://www.r-bloggers.com/2009/06/r-function-to-create-tables-in-latex-or-lyx-to-display-regression-model-results/	June 19, 2009	markheckmann	Most people using LaTex feel that creating tables is no fun. Some days ago I stumbled across a neat function written by Paul Johnson that produces LaTex code as well as LaTex code that can be used within Lyx. The output can be used for regression models and looks like output from the Stata outreg command. His R function that produces the LaTex code has the same name:  outreg(). The outreg code can be found on his website or in the PDF copy of the code from his website. I took the code, put it into a .rnw file and sweaved it. It worked like a charm and produced beautiful results (see the picture on the left and the PDF). Below you can find the code for the noweb file (.rnw). Latex code is colored grey, R-code is colored blue. Just have a look at all the results as a PDF file. Besides, Paul Johnson has also created a nice list of R-Tips that can be found on his website as well.      	 0 Comments
Iran Election analyzed with R	https://www.r-bloggers.com/2009/06/iran-election-analyzed-with-r/	June 19, 2009	Paolo		 0 Comments
bugsparallel	https://www.r-bloggers.com/2009/06/bugsparallel/	June 18, 2009	Gregor Gorjanc		 0 Comments
open-source campaign finance analysis with R and MySQL	https://www.r-bloggers.com/2009/06/open-source-campaign-finance-analysis-with-r-and-mysql/	June 18, 2009	jjh	"In Part 1 of this tutorial we introduced the fechell library by extracting all itemized contributions from individuals made to the Obama For America campaign in 2007 and 2008. In Part 2 of the tutorial we will summarize that data set by importing it into a MySQL database and aggregating contributions by week and zip code. Next we’ll visualize the contribution data amounts on a map, week by week. Below is a sample image:   This visualization contains two separate measures: the top portion is a map of the continental US, marked with a dot at the geographic center of every zip code that had individuals who made contributions to Obama for America. The dot is colored according to how much money in total was raised from that zip code since the 2nd week of 2007, with the colors going from a dark blue to bright red indicating amounts. The second measure is a vertical bar plot showing how much money was raised, per week, from all zip codes combined. The current week is highlighted and annotated with the amount raised for that week. Taken together these measures can show us where the first monetary support for the Obama for America campaign came from and how it progressed geographically and in volume. We will use the free R statistics and visualization package to producing the weekly image we shown above. Looking at still images week by week is informative but not very exciting. After all the images are created we’ll use MEncoder to string together the images into a movie to demonstrate the growth of individual financial support pledged to Obama for America. To follow along with this exercise you will need to download MySQL 5.1 for your platform. English instructions for installation are available in the MySQL 5.1 Reference Manual. After installation you will also need to create a database user and grant rights to create databases, create indexes and perform LOAD DATA LOCAL functions. All of these administration tasks are covered in the MySQL 5.1 Reference Manual Section 5.5 (MySQL User Account Management). Keep the user name and password of the account you created handy as we’ll use it in the next few steps to access the database.  We will be using two different tables to represent the itemized individual contributions: the transactions table will hold all 2.8 million transaction values, and the transactions_summary_weekly table will hold the sum of all contributions for every combination of year, week, and 5 digit zip code that exists in the data.  To populate these tables, download and save the transaction table creation script create-transactions.sql available in the Resources sub-section below to a directory. Next download the output of the first part of the post into that same directory (obama-data-F3P-2007-2008.csv), available as ZIP archive in Resources sub-section below. If this CSV file isn’t in the same directory as the create-transactions.sql script you’ll need to change line #4 to reference the exact location.  create-transactions.sql looks like this: We are simply creating a new database (‘fechell’), and creating a new MyISAM table called ‘transactions’ on that database. We chose MyISAM since large data imports are dramatically faster with MyISAM than the default InnoDB, at least in the default setup we’re using.  You can execute this script by running the following command in the directory where you saved create-transactions.sql and obama-data-F3P-2007-2008.csv. Replace YOURDBUSERNAME with the user you created during the MySQL installation. Since we are loading in 2.8 million records and then adding an index to the zip code field this command might take quite a while to execute depending on your processor speed. After it is finished you should have a large database of all individual itemized transactions in the fechell.transactions table.  Next we will create and populate the transactions_summary_weekly table, which will contain aggregated itemized individual fund raising totals by zip code by week and year.
The script create-transactions_summary_weekly.sql available in the Resources sub-section below looks like this: You can run the script with the following command: After the script is finished executing you should have two rather large tables populated with the information we’ll need to visualize the data.  Before we can create the visualizations we’ll need to make sure the R toolkit and several add-on packages are installed correctly. If it isn’t installed already, you can download an R installer from the R mirrors list. The R Frequently Asked Questions page contains instructions for Linux, Unix, Mac, and Windows platforms.   After R is installed you will need to install several add-on packages that may or may not be included with your distribution: sp, maps,maptools, and RMySQL. All are available via CRAN and platform-dependent instructions for installing add-on packages fcan be found on the R Wiki under How do I install a package?.  Before you can create the visualization you will need to download the R script to draw the frames, and a support file containing latitude and longitude pairs for every zip-code in the US. Both are available in the References sub-section below as draw.R and zips.zip. Unzip the zips.zip file and move the newly created zips.csv file into the same directory as draw.R. You will need to make 1 change to the draw.R script before you can run it: line 37 creates the connection to the database we created and populated earlier. You’ll need to change the host, user name, and password arguments to match your database setup.  The draw.R file is pretty simple, so we wont walk through the code line by line. Most of the heavy lifting is done by the fantastic sp and RMySQL packages, with a little bit of help from a lat/long database of zip codes.  Once you’ve edited the draw.R to match your configuration you can can create the images with the following command line, assuming the R bin directory has been correctly added to your PATH: Assuming the packages were installed correctly, the zips.csv file was in the same directory, and the database configuration was modified you should now see 96 PNG files in your directory. The files are named o4a_year_week.png.  To create a FLV movie from the weekly images you will need to install MPlayer/MEncoder if it isn’t installed on your system already. You can find download and install instructions on the MPlayer download page. Once the installation is complete you can create a FLV movie with the following command line (some options taken from a page on 
avi-to-flv-conversion"">http://jeremychaman.info/ After the command is complete you will have a FLV file called output.flv. The final file is embedded below, and available in the Resources section:  While this visualization serves our purpose of tracking the geographic spread and growing amount of individual contributions to the Obama for America campaign, it is not perfect. The entire process, from extraction to visualization was created in about half a day, so there is obviously more work that could be done. Also the specific process used in these tutorials was to be a demonstration of several strategies to perform your own analysis, not necessarily to build the best visualization of independent contributions. One problem with the current visualization is the representation of an entire zip code by a dot at its geographic center. This is problematic because zip codes represent regions that are vastly different in size, so using single dots will end up in decisions being skewed. Based on our maps alone, we could conclude Obama for America enjoyed very little support in places like Montana and Colorado since they have very few dots within their borders. In truth the campaign received more than $11M  of support from Colorado and just over $1.2M in itemized donations from Montana, but this is very difficult to compare this to Philadelphia or New York City where the map is a smear of dots. A better way to display information by zip code could involve drawing the ZIP boundaries and filling with a color, instead of just coloring a single point. This would work fine for places like Montana, Idaho, and Colorado where a zip code might refer to a very large area, but would fail in more populous areas. This is especially true in New York City, where a single zip code could represent an area as small as a single square mile and would be almost impossible to view on a reasonably sized graphic. Merging of zip code areas and averaging their fund raising totals could make this procedure more useful for national analysis or local analysis of heavily populated metro areas.  Additionally, the output format of the visualization could be improved by making it interactive. Combining our weekly summary data with a tool like the Google Motion Chart Gadget along with an Animated Timeline would provide a much greater user experience. Finally, the national analysis of Obama for America is interesting by itself but would be much more useful if it included data from other candidates. Being able to overlay zip code summaries from primary challengers like Hillary Clinton for President, and general election opponent McCain-Pailn 2008 Inc would allow a much greater depth of analysis to be performed. Including demographic information, primary dates, and election returns on the same visualization would be the best. I intend to address these topics -as well as analyzing congressional races-  in a future tutorial.  Despite the flaws listed above after viewing the final output of our visualization we can draw several conclusions about the fund raising success of the Obama for America campaign. First, the campaign was truly national and had financial support from across the country by the end of the campaign. But the campaign didn’t start out national – during the critical first two months of the campaign Obama for America drew financial support from several metro areas including Chicago, New York City, and DC/Northern Virginia. After several months of heavy development of the grassroots network the campaign started seeing donations coming in from across the US.  Using the weekly summary data we can try to attribute large spikes in total receipts to campaign events. Looking closely we see the campaign saw a large increase of contributions in the last week of March, which potentially could be attributed to popular support of candidate Obama’s More Perfect Union speech or as a result of the large new donor push at the beginning of that month. We can also look at weeks 2008/4 and 2008/5(beginning of February 2008) and see week to week receipts jump by $5M to around $11M for two full weeks. This spike coincides with the Super Tuesday Democratic primaries where candidate Obama nearly split the day with then-front runner Hillary Clinton.  Using  free disclosure data and a few open-source tools, like Ruby, fechell, R, and MySQL just about anybody can perform their own professional analysis on federal campaign finance data.  Database creation script
Database population script
R script to draw images draw.R
Zip codes
FLV file
CSV extract from Part 1 "	 0 Comments
The Second Coming	https://www.r-bloggers.com/2009/06/the-second-coming/	June 18, 2009	John Myles White	Pew Research has found that 79% of Americans believe in The Second Coming of Jesus. What worries me more is not that 4 out of 5 Americans believe in The Second Coming, but that 1 out of 5 believes it will happen in their lifetime. It seems inevitable that such a belief will grossly warp your priorities: you would have no reason at all to care about long term problems like global warming or antibiotic resistant bacteria, because you believe the world will end long before those concerns become substantial. I wonder: does this 20% of the American people exhibit more delay discounting than American atheists? 	 0 Comments
Influence.ME: don’t specify the intercept	https://www.r-bloggers.com/2009/06/influence-me-dont-specify-the-intercept/	June 18, 2009	Rense Nieuwenhuis	"Just recently, I was contacted by a researcher who wanted to use influence.ME to obtain model estimates from which iteratively some data was deleted. In his case, observations were nested within an area, but there were very unequal numbers of observations in each area. Unfortunately, he wasn’t able to use the influence.ME package on his models. He kindly sent me his data, so I could figure out what went wrong, and it showed to be a little problem with influence.ME. The problem was with how the model was specified: the intercept was explicated, next to several (fixed) variables. It turned out, that such a model specification is not compatible with the internal changes made to the mixed model. Therefore, I advise users of influence.ME not to explicitly specify the intercept in their lme4 regression models. I reproduced the problem with the school23 data, which is available in influence.ME. Compare the two model specifications below: in the first the intercept is specified, in the second it isn’t. The outcomes of both lmer models are identical. However, the first returns a convergence error when used with the estex() function, while the second doesn’t. The input:


mod <- lmer(math ~ 1 + structure + (1 | school.ID), data=school23)

estex.mod <- estex(mod, ""school.ID"")
mod <- lmer(math ~ structure + (1 | school.ID), data=school23)

estex.mod <- estex(mod, ""school.ID"")

 The output:


> mod <- lmer(math ~ 1 + structure + (1 | school.ID), data=school23)

> estex.mod <- estex(mod, ""school.ID"")

Error in mer_finalize(ans) : Downdated X'X is not positive definite, 3.

>

> mod <- lmer(math ~ structure + (1 | school.ID), data=school23)

> estex.mod <- estex(mod, ""school.ID"")

 I will surely investigate whether this can be resolved in a future update, but for now, simply leave the intercept out of your model specification: lmer will add it for you. "	 0 Comments
Not Just Normal… Gaussian	https://www.r-bloggers.com/2009/06/not-just-normal%e2%80%a6-gaussian/	June 16, 2009	JD Long	Pretty Normal Dave, over at The Revolutions Blog, posted about the big ‘ol list of graphs created with R that are over at Wikimedia Commons. As I was scrolling through the list I recognized the standard normal distribution from the Wikipedia article on the same topic. Below is the fairly simple source code with lots of comments. Here’s the source. Run it at home… for fun and profit. 	 0 Comments
NYT: In Simulation Work, the Demand Is Real	https://www.r-bloggers.com/2009/06/nyt-in-simulation-work-the-demand-is-real/	June 16, 2009	Stephen Turner		 0 Comments
One outlier and you’re out: Influential data and racial prejudice	https://www.r-bloggers.com/2009/06/one-outlier-and-youre-out-influential-data-and-racial-prejudice/	June 16, 2009	Rense Nieuwenhuis	"
Currently preparing a presentation on analyzing influential data in mixed effects models myself, my eye fell on an article in which important claims on racial prejudice were refuted. An important aspect of the criticism on existing work, is that in one article the main correlation was completely due to a single observation. Solely based on this single observation, the study’s outcomes showed the Implicit Association Test (IAT) to predict overall interaction quality between White or Black people. Removing that single observation (out of 41) from the data removed the complete effect.  With survey research showing declines in “American’s endorsement of prejudice sentiments” (p.568), the question rose whether such declines actually took place, or that they are an artifact of social desirability determining respondents’ responses to survey questions. Naturally, tests like the  Implicit Association Test (IAT) gained considerable attention, for the attractive claim of such tests is to be able to show levels of prejudice that people themselves are unaware of and which do not show when asked about explicitly (e.g. in a survey).

Blanton et al. (2009) decided to test several of the articles on which the strong claims for the predictive validity of the IAT were based. They re-analyzed the (partial) data of two articles. In one of these analyses, on which 2001 article was based, it was found that one of the main findings was that high scores on the IAT were associated with worse interaction quality with Black experimenters, compared with White experimenters. I’m not completely sure what this interaction quality entails, but based on the re-study I would say that it is a combination of aspects such as ‘forward leaning’, ‘facing the experimenter’, ‘expressiveness’, ‘smiling’ and making ‘eye contact’. How can just a single observation dominate the outcomes of a statistical analysis? Unfortunately, the answer to this question is: quite easily, especially when the analysis is based on a small number of observations. In this case, the refuted correlation was between the participants’ IAT score and the way the participants’ interacted with either Black of White experimenters. While it is known that the IAT score is determined by the participant’s age, one single participant had an exceptionally high age compared to the overall test group, and indeed that participant score very high on the IAT. Also, the quality of the interaction of that participant with the Black experimenter was rated very low. Now the thing is, that in the rest of the observations no association between IAT score and the quality of the interaction was to be observed, this single observation with extreme scores on both variables, completely dominated the outcomes of the study on this aspect. Deletion of this single observation brought the significant correlation of .32 down to non-significance: no association could be inferred between participants’ IAT score and how they interacted with the Black experimenter. Blanton et al. didn’t write a full article based on just this point, and of course this is not the only criticism on the original article. Other aspects include low timing of the measures (respondents were probably aware of being tested for discriminatory behavior, before the ‘actual’ test took place), inter-rater reliability, improper statistical analysis due to recoding of the data (due to which the coding of a single rater influenced the findings). Nevertheless, from my current point of view, I’m especially interested in the bias caused by the influential observation. Of course, McConnell et al. (2009), whose work was criticized, were given the opportunity to respond. Regarding the influence exerted by the outlier, they respond with two arguments. First they state that Blanton et al. did not study the correct outlier, for although this outlier did have an extreme IAT score, another participant did have an even higher score. Their second reaction states that Blanton et al. did focus on only one of the outcome measures, and not on all the various measure used in the original study. In their response, they show that deletion of the outlier found by Blanton et al. does not influence the outcomes of the analyses on the other outcome measures. I find this response curious on two accounts. First, influential data and outliers are not two of a kind. McConnell’s response that the wrong outlier was selected is not necessarily true, for just having an extreme score on one variable is not enough to make an observation influential. Generally, it has to be an outlier, and to have leverage (changing the slope of the regression line). If the other outlier (mentioned by McConnell) did have a more extreme score on the IAT variable, but an average score on the behavior-quality variable, it may very well prove not to (overly) influence the outcomes of the study. Secondly, an observation is only influential relative to the specification of the analysis and the variables used in it. So, simple deletion of this single observation to show that it does not influence other analyses (on other outcome measures), it not much of a defense to the initial argument that the observation influenced the outcomes of a specific analysis. All in all, an interesting debate. There is much more to it in both the articles by Blanton et al. (2009) and McConnell et al. (2009). But still, I find it especially striking to see how careful one should be when analyzing data and making inferences on it. And, of course, I can add a nice example of the impact of influential data to my collection.  Blanton, H., Jaccard, J., Klick, J., Mellers, B., Mitchell, G., & Tetlock, P. (2009). Strong claims and weak evidence: Reassessing the predictive validity of the IAT. Journal of Applied Psychology, 94 (3), 567-582 DOI: 10.1037/a0014665 McConnell, A., & Leibold, J. (2009). Weak criticisms and selective evidence: Reply to Blanton et al. (2009). Journal of Applied Psychology, 94 (3), 583-589 DOI: 10.1037/a0014649 "	 0 Comments
R tips: Determine if function is called from specific package	https://www.r-bloggers.com/2009/06/r-tips-determine-if-function-is-called-from-specific-package/	June 16, 2009	Allan Engelhardt	"
I like the “multicore” library for a particular task.  I can easily write a combination of if(require(""multicore"",...)) that means that my function will automatically use the parallel mclapply() instead of lapply() where it is available.  Which is grand 99% of the time, except when my function is called from mclapply() (or one of the lower level functions) in which case much CPU trashing and grinding of teeth will result.
 
So, I needed a function to determine if my function was called from any function in the “multicore” library.  Here it is.
 
First define a generally useful function:
 
Then we use it for our purpose:
 
Easy when you know how.
 Jump to comments. 

A warning on the R save format
 The save() function in the R platform for statistical computing is very convenient and I suspect many of us use it a lot. But I was recently bitten by a “feature” of the format which meant I could not recover my data. I recommend that you save data in a data format (e.g. CSV or CDF), not using the save() function which is really for objects (data and code). What is your approach? 

R: Eliminating observed values with zero variance
 I needed a fast way of eliminating observed values with zero variance from large data sets using the R statistical computing and analysis platform . In other words, I want to find the columns in a data frame that has zero variance. And as fast as possible… "	 0 Comments
Who wants school vouchers?  Rich whites and poor nonwhites	https://www.r-bloggers.com/2009/06/who-wants-school-vouchers-rich-whites-and-poor-nonwhites/	June 15, 2009	Andrew Gelman		 0 Comments
Geography and Data	https://www.r-bloggers.com/2009/06/geography-and-data/	June 15, 2009	Jared Knowles		 0 Comments
Side by side analyses in Stata, SPSS, SAS, and R	https://www.r-bloggers.com/2009/06/side-by-side-analyses-in-stata-spss-sas-and-r/	June 15, 2009	Stephen Turner		 0 Comments
Replacing 0 with NA – an evergreen from the list	https://www.r-bloggers.com/2009/06/replacing-0-with-na-an-evergreen-from-the-list/	June 15, 2009	Paolo		 0 Comments
Example 7.2: Simulate data from a logistic regression	https://www.r-bloggers.com/2009/06/example-7-2-simulate-data-from-a-logistic-regression/	June 13, 2009	Ken Kleinman		 0 Comments
Example 7.1: Create a Fibonacci sequence	https://www.r-bloggers.com/2009/06/example-7-1-create-a-fibonacci-sequence/	June 12, 2009	Nick Horton		 0 Comments
R tips: Installing Rmpi on Fedora Linux	https://www.r-bloggers.com/2009/06/r-tips-installing-rmpi-on-fedora-linux/	June 12, 2009	Allan Engelhardt	"Somebody on the R-help mailing list asked how to get Rmpi working on his Fedora Linux machine so he could do high-performance computing on a cluster of machines (or a single multicore machine) using the R statistical computing and analysis platform.  Since it is unusually painful to get working, I might as well copy the instructions here.
 
First install the openmpi libraries using:
 
The default installation on Fedora still doesn’t quite work, so you need to execute the following command as root (only once is required, after installation of the package):
 
You are not quite done: for R to work right with the libraries, you need to modify the LD_LIBRARY_PATH environment variable to include the path to the Open MPI libraries.  I have the following in my ~/.bash_profile:
 
Edit your file to contain the same, and execute that line at the command prompt and you are ready to continue.
 
Now that your Open MPI libraries are set up, and what you do next depends on what version of Rmpi you are installing.  Most likely you are installing the latest version in which case the following section applies.  The instructions for older versions are retained in a later section for reference.
 
Make sure you have executed the ldconfig command and set the LD_LIBRARY_PATH environment variables as described in the previous section before you continue.
 
Since at least version 0.5-8 of the Rmpi library you can install it from the R command line after you have fixed the Open MPI install.  At the R prompt do:
 
It should work and install OK.  This is obviously quite a mouthful to remember, but help is at hand through the options() mechanism in R.  In your ~/.Rprofile you can add something like:
 Then you can just type install.packages(""Rmpi"") at the R command prompt to install the package.
 
The problem is the configuration file configure.ac which is, unfortunately, completely brain-damaged with hard-coded assumptions about which subdirectories should contain header and library files and no way of overriding it.
 
Download the latest Rmpi package from CRAN and unpack it using tar zxvf Rmpi_0.5-7.tar.gz.  Go to the new Rmpi directory and replace the file configure.ac with the one below (for a x86_64 system; for 32 bit you probably need to change -64 to -32):
 
The number 1.3.1 may change in future releases of Fedora: see /usr/lib64/pkgconfig/openmpi-*.pc for the current value.
 
Still in the Rmpi directory do the following in your shell:
 Now Rmpi should be working in R: Jump to comments. 

Spreadsheet errors
 For my sins, I have done more than my fair share of analysis in Excel. I am quite capable of building and maintaining 130Mb spreadsheets (I had a dozen of them for one client). Excel is pretty much installed everywhere, so it is sometimes the only way to get started getting commercial value of the data in the organisation. But I don’t like it and let’s have a look at one reason why. In order not to always pick on Microsoft, we use another application, but you get the same results with Excel. 

R code for Chapter 1 of Non-Life Insurance Pricing with GLM
 Insurance pricing is backwards and primitive, harking back to an era before computers. One standard (and good) textbook on the topic is Non-Life Insurance Pricing with Generalized Linear Models by Esbjorn Ohlsson and Born Johansson. We have been doing som… 

R code for Chapter 2 of Non-Life Insurance Pricing with GLM
 We continue working our way through the examples, case studies, and exercises of what is affectionately known here as “the two bears book” (Swedish björn = bear) and more formally as Non-Life Insurance Pricing with Generalized Linear Models by Esbjörn Ohl… 

Excel Tip: Array boolean operator
 I learn something new every day. Thinking I knew pretty much everythging there is to know about Microsofts Excel spreadsheet application, I was surprised to see that you could turn any array into a boolean array depending on a condition by simply writing … 

Faster R through better BLAS
 Can we make our analysis using the R statistical computing and analysis platform run faster? Usually the answer is yes, and the best way is to improve your algorithm and variable selection. But recently David Smith was suggesting that a big benefit of the… "	 0 Comments
Simulation of Burning Fire in R	https://www.r-bloggers.com/2009/06/simulation-of-burning-fire-in-r/	June 11, 2009	Yihui Xie	 Simulation of Burning Fire in R And code here: The speed of drawing animation frames is rather slow in my computer, but it doesn’t matter since we can use the animation package (hey, you are advertising!) to save all the image frames and convert them to a single animation file.  Now the animation is much more smooth than what we saw in R graphics window. Thanks, awesome Linlin. 	 0 Comments
Creating Tag Cloud Using R and Flash / JavaScript (SWFObject)	https://www.r-bloggers.com/2009/06/creating-tag-cloud-using-r-and-flash-javascript-swfobject/	June 10, 2009	Yihui Xie	"I recently saw a graph on television that displayed selected words/phrases in a speech scaled in size according to their frequency. So words/phrases that were often used appeared large and words that were rarely used appeared small. […] Marc Schwartz mentioned that Gorjanc Gregor has done some work years ago using R (in grid graphics). The obstacle of creating tag cloud in R, as Gorjanc wrote, lies in deciding the placement of words, and it would be much easier for other applications such as browsers to arrange the texts. That’s true — there have already been a lot of mature programs to deal with tag cloud. One of them is the wp-cumulus plugin for WordPress, which makes use of a Flash object to generate the tag cloud, and it has fantastic 3D rotation effect of the cloud. Before introducing how to port the plugin into R, I’d like to introduce an R function pointLabel() in maptools package and it can partially solve the problem of arranging text labels in a plot (using simulated annealing or genetic algorithm). Here is a simulated example: Simulated Tag Cloud with R function pointLabel() in maptools  I was fortunate to get a very neat graph with no labels overlapping, but I don’t think this is a good solution, as it doesn’t take care of the initial locations of the words. My rough idea about deciding the initial locations is to sample on circles with radii proportional to the frequency, i.e. let  and  where . In this case, important words will be placed near the center of the plot. The problem becomes quite easy with a Flash movie tagcloud.swf and a JavaScript program swfobject.js. The mechanism, briefly speaking, is that the tag information is passed to the Flash object by JavaScript, and the Flash object will read the variable tagcloud where the sizes, colors and hyperlinks of tags are stored. Finally the tags are visualized like rotating cloud. It’s not difficult to pass the tag information to JavaScript in pure text. Below is the function which will create an HTML page by default with a tag cloud Flash movie inside it: The main argument is tagData which is a data.frame containing at least three columns (tag, link and count) and looks like: Additional columns color and hicolor will be used if they exist (hexadecimal numbers specifying RGB), e.g. Here is an example on visualizing my blog tags. You may need the following swf and js files first if you wish the loading would be faster (by default your browser needs to download these two files from roytanck.com first). The above code will generate an HTML page like this:
 
You can adjust the parameters as you wish. There is still one more step to answer Tony’s original question, namely splitting the speech into single words and computing the frequency. This can be (roughly) done by strsplit(..., split = "" "") and table(). Encoding problems may exist in the above code, but URLencode(tagXML) could be of help. Only Latin characters are supported, but there’s possibility to modify the Flash source file to support other languages. See Roy Tanck’s post for more information. Other R resources I know so far: "	 0 Comments
Hack-at-it 2009	https://www.r-bloggers.com/2009/06/hack-at-it-2009/	June 9, 2009	Di Cook		 0 Comments
Data Mashups in R from O’Reilly	https://www.r-bloggers.com/2009/06/data-mashups-in-r-from-oreilly/	June 9, 2009	Allan Engelhardt	"
O’Reilly has published Data Mashups in R as a $4.99 PDF download in their Short Cut series.  In 27 pages it takes you through an example of how to combine foreclosure information with maps and geographical information to produce plots like the one below.  This is all done with the R statistical computing and analysis platform.
 

 
They show how to:
 "	 0 Comments
Two plot with a common legend – base graphics	https://www.r-bloggers.com/2009/06/two-plot-with-a-common-legend-base-graphics/	June 6, 2009	Paolo		 0 Comments
Learning R for Researchers in Psychology	https://www.r-bloggers.com/2009/06/learning-r-for-researchers-in-psychology/	June 6, 2009	Jeromy Anglim		 0 Comments
Elementary Statistics with R	https://www.r-bloggers.com/2009/06/elementary-statistics-with-r/	June 5, 2009	rtutor.chiyau	"

Ever wonder how to finish your statistics homework real fast? Or you just
want a quick way to verify your tedious calculations in your statistics class
assignment. We provide an answer here by solving statistics exercises with
R.
 read more "	 0 Comments
Getting started with R (for german speakers)	https://www.r-bloggers.com/2009/06/getting-started-with-r-for-german-speakers/	June 5, 2009	markheckmann	Just a little note for german speaking R beginners: There is an introductory course in R (german) available online on the website of the department of methodology and evaluation research at the University of Jena. Dr. Ivailo Partchev holds a seven sessions course on that topic (duration 11.5 hours). 	 0 Comments
Twitter from R… Sure, why not!	https://www.r-bloggers.com/2009/06/twitter-from-r%e2%80%a6-sure-why-not/	June 4, 2009	JD Long	"
So I have started following the #RStats tag in twitter. Prior to a week ago I had never Twitterbated so I thought I would give it a go since I am not one to shy away from new technology… much. I think of Twitter like a call in radio show where I get to cut off callers when they annoy me. Well one of the interesting things I ran across was this tweet that pointed to this page about posting tweets from R.  The code example was missing a couple of things, so here’s the cleaned up version: This is a pretty good, and fairly self explanatory example of how to use RCurl to spew out some data. Don’t be a scriptard and try to run this without changing ‘username’ and ‘password.’ If you do, I’m coming to your house and going to beat you with your own keyboard until you admit that I am, indeed, your daddy. Am I planning on Tweeting from R? You have to be kidding. That’s just silly. But I might need to use RCurl again. Although for long running scripts I may very well put code at the end of my scripts that sends out a Tweet when the code has finished running. Waaaay easier than setting up email and trying to ensure it runs from EC2 as well as my home machine, etc. The more I think about that, the more I am sure I’m a friggin genius. And of course, on every one of my script completion tweets I’m going to use the #RStats tag so everyone in twitterland will know that I am amazing. And a prick. All at the same time. I’m complex like that. Oh yeah, I’m @CMastication on Twitter. "	 0 Comments
Check our PMML article – The R Journal	https://www.r-bloggers.com/2009/06/check-our-pmml-article-the-r-journal/	June 3, 2009	Alex Guazzelli		 0 Comments
Use meaningful color schemes in your figures	https://www.r-bloggers.com/2009/06/use-meaningful-color-schemes-in-your-figures/	June 3, 2009	Stephen Turner		 0 Comments
How to plot a graph in R	https://www.r-bloggers.com/2009/06/how-to-plot-a-graph-in-r/	June 2, 2009	Chris	Here’s a quick tutorial on how to get a nice looking graph out of R (aka the R Project for Statistical Computing). Don’t forget that help for any R command can be displayed by typing the question mark followed by the command. For example, to see help on plot, type ?plot. Let’s start with some data from your friends, the Federal Reserve. The fed keeps lots of interesting economic data and they make it pretty easy to get at. What if we’re curious about the value of the US Dollar? How’s it doing against other major currencies? Let’s have a look. I’ll use the Nominal Major Currencies Dollar Index. The fed gives us the data here: First, download the file and load it into your favorite text editor. Replace ss+ with t to create two tab delimited columns. I think this is probably easier than trying to get R to read data separated by at least 2 spaces, as the source file seems to be. Now, load your data into R. R will show you the structure of an object using the str() command: So far so good. R is all about stats, so why not do this? OK, let’s get to some plotting. First off, let’s try a simple case. That’s OK for quickly looking at some data, but doesn’t look that great. R can make reasonable guesses, but creating a nice looking plot usually involves a series of commands to draw each feature of the plot and control how it’s drawn. I’ve found that it’s usually best to start with a stripped down plot, then gradually add stuff. Start out bare-bones. All this does is draw the plot line itself. Next, let’s add the x-axis nicely formatted. We’ll use par(tcl=-0.2) to create minor tick marks. The first axis command draws those, but doesn’t draw labels. The second axis command draws the major tick marks and labels the years on even decades. Note that there’s an R package called Hmisc, which might have made these tick marks easier if I had figured it out. Next, we’ll be lazy and let R decide how to draw the y-axis. I like a grid that helps line your eye up with the axes. There’s a grid command, which seemed to draw grid lines wherever it felt like. So, I gave up on that and just drew my own lines that matched my major tick marks. The trick here is to pass a sequence in as the argument for v or h (for vertical and horizontal lines). That way, you can draw all the lines with one command. Well, OK, two commands. Let’s throw some labels on with the title command. Finally, let’s bust out a linear regression. The lm() function, which fits a linear model to the data, has some truly bizarre syntax using a ~ character. The docs say, “The tilde operator is used to separate the left- and right-hand sides in model formula. Usage: y ~ model.” I don’t get at all how this is an operator. It seems to mean y is a function of model? …maybe? In any case, this works. I’m taking it as voodoo. Now, we have a pretty nice looking plot. The full set of commands is here, for your cutting-and-pasting pleasure. 	 0 Comments
Run a remote R session in emacs: emacs + ESS + R + ssh	https://www.r-bloggers.com/2009/06/run-a-remote-r-session-in-emacs-emacs-ess-r-ssh/	June 1, 2009	Vinh Nguyen	"I don’t know how, but somehow, i stumbled on how to do this. I’ve always used emacs with ESS to run R, since spring 2006 (did it on windows, switched to linux, for years, and most recently, on my macbook).  I liked this because i get the same usual interface across ALL platforms (well, maybe not on an MVS mainframe, but i’ve never even seen R on one of those things).  Plus, i use emacs for everything computery and/or scientific, like using python or code in C (well, i hope to do more of this (do i really hope to?)). Regarding the same interface across all platforms, i use, for example, emacs + ESS + R whenever i remotely log into the ics servers at uci (well, at least anything computationally intensive).  So i ssh into it, fire up emacs, fire up R.  Been doing this since whenever.  Thing is, I usually always write my code on my local computer, and when i’m ready to run the final code, i either run it as a batch script (‘nohup’, and with ‘&’), through the ‘screen’ command (to keep the session runnning when i log out, see my post on R with unix tools), or through emacs.  These days, i’ve been doing it with screen mainly so disconnects to the server won’t interrupt my script. Well, I just found yet another way to do this: write code on local computer, send code to R session easily in emacs (ie not copy and paste).  Oh I remember now, I ran into this by googling ‘emacs ess multiple R session’. Go here, go to section 3.3, ESS on remote computers.  We will need this file.  This site clarified how to get graphics to show. Instruction is as follows. This is super cool huh?!?!  I like!
Alternatively, we can do the same without the ssh.el file.  In emacs, hit ‘M-x shell’.  In this shell, do your ssh -X -C stuff, then run R.  Do ‘M-x ess-remote’ and everything should still work! Next thing to do, with my local emacs, open files that are remote.  this should be easy i think. "	 0 Comments
How to win the KDD Cup Challenge with R and gbm	https://www.r-bloggers.com/2009/06/how-to-win-the-kdd-cup-challenge-with-r-and-gbm/	June 1, 2009	Allan Engelhardt	"
Hugh Miller, the team leader of the winner of the KDD Cup 2009 Slow Challenge (which we wrote about recently) kindly provides more information about how to win this public challenge using the R statistical computing and analysis platform on a laptop (!).
 
As a reminder of what we wrote before, the challenge provided two anonymized data set each of 50,000 mobile teleco customers and each entry having 15,000 variables.  The task was to find the best churn, up-, and cross-sell models.
 
Hugh summarizes his team’s approach:
 
Feature selection was an important first step [we mentioned before that this is key for all successful data mining projects – AE]. We looked at how effective each individual variable was as a predictor, which also allowed us to reading parts of the data only, as the whole dataset didn’t fit in memory [my emphasis – AE]. The assessment here was homebrew, making a simple predictor on half the data and measuring performance (by the AUC measure) on the other half:
 
From this we came up with a set of about 200 variables for each model, which we continued to tinker with. The main model was a gradient boosted machine which used the “gbm” package in R. This basically fits a series of small decision trees, up-weighting the observations that are predicted poorly at each iteration. We used Bernoulli loss and also up-weighted the “1” response class. A fair amount of time was spent optimising the number of trees, how big they should be etc, but a fit of 5,000 trees only took a bit over an hour to fit. The package itself is quite powerful as it gives some useful diagnostics such as relative variable importance, allowing us to exclude some and include others.
 
We used trees to avoid doing much data cleaning – they automatically allow for extreme results, non-linearity, missing values and handle both categorical and continuous variables. The main adjustment we had to make was to aggregate the smaller categories in the categorical variables, as they tended to distort the fits.
 
They did this on standard Windows laptops (Intel Core 2 Duo 2.66GHz processor, 2GB RAM, 120Gb hard drive) against a competition that had more computing clusters available than Imelda Marcos had shoes.  It is not what you’ve got, it’s how you use it 🙂.
 
Congratulations to Hugh and his team!
 Jump to comments. 

R used by KDD 2009 cup winner of slow challenge
 The results from the KDD Cup 2009 challenge (which we wrote about before) are in, and the winner of the slow challenge used the R statistical computing and analysis platform for their winning submission. "	 0 Comments
Simple visualization of a 11X5 table (for WordPress 2.9 Features Vote Results)	https://www.r-bloggers.com/2009/07/simple-visualization-of-a-11x5-table-for-wordpress-2-9-features-vote-results/	July 31, 2009	Tal Galili	" I guess this is not the number one post I would like to start with on this blog, but I feel the time is right for it (community-wise). I’ll move on to the subject matter in a moment, but first a short intro: This blog is written by Tal Galili. I am an aspiring statistician who also loves to use R for his work. At the same time I am also a WordPress blogger, writing mainly at www.TalGalili.com where I can use my native language (Hebrew) for self expression. This combination of statistics and blogging will lead me to sometimes much less statistical, but more Web/Open-Source oriented posts like this one. So for the statisticians in the audience I extend my apologies and invite you to wait for future posts which will be more fully focused on Statistics and R. And now for the topic at hand. . . *         *         *         *         *
I have just noticed the nice article published on the wordpress development blog titled “2.9 Features Vote Results“. The post exemplifies a wonderful trend in the WordPress community (led by Jane Wells) having to do with connecting between the core team and the WordPress user community. The way Jane does this is by giving surveys to WordPress users,  which in turn offers the WordPress core team an opportunity to understand the community needs. In the post “2.9 Features Vote Results“, Jane presented the results of such survey. The post had tables and barplots, but the barplots were only present for the one dimensional variables. In contrast, more elaborate data, such as that of question 2 (asking to rate each of 11 potential features on a scale of 1 to 5), was shown only with a table, such as this:  The table gives the full information (although I would love it if it was easily downloadable, instead of having to type in the numbers) – but its main limitation is that from a quick look, one can not easily get (let alone understand) anything. For the goal of understanding more of the results with a quick glance, I offer two simple and well-known visualizations for the results. 1) Parallel barplots (click for bigger image)  This plot can be easily implemented in Excel (although I did it in R) and can allow us to compare the different ranking each potential feature received. For example, this shows us that most answers were usually given rank 4 (”would be nice”) for each feature. 2) Mosaic plot (click for bigger image)  I don’t know if this can be done in Excel, but with R it is just a simple line of code. (mosaicplot((DataSet.table), las = 1, col = c(”gray”,”gray”,”blue”,3,”dark green”), main = “”)) The advantage of this plot is that it allows us to compare the different features easily, while not only comparing the top rank, but also combining different rankings for easy comparison (for example, comparing how many rank 4 or 5 each feature received). So for example, the plot shows me that the most ranked with number 5 was the feature “easier embeds” but the most ranked “number 4 or 5″ was the feature “custom image sizes”. The feature “media album” came close to these two, but didn’t top either. Conclusions from this post: p.s. to Jane – why do none of the numbers in this table add up to 3406 (the number of respondants) ? p.p.s.  to Jane and the Dev team – great work people! "	 0 Comments
Kruskal-Wallis one-way analysis of variance	https://www.r-bloggers.com/2009/07/kruskal-wallis-one-way-analysis-of-variance/	July 31, 2009	Todos Logos		 0 Comments
Analysis of variance: ANOVA, for multiple comparisons	https://www.r-bloggers.com/2009/07/analysis-of-variance-anova-for-multiple-comparisons/	July 30, 2009	Todos Logos		 0 Comments
Comparison of two proportions: parametric (Z-test) and non-parametric (chi-squared) methods	https://www.r-bloggers.com/2009/07/comparison-of-two-proportions-parametric-z-test-and-non-parametric-chi-squared-methods/	July 29, 2009	Todos Logos		 0 Comments
Wilcoxon signed rank test	https://www.r-bloggers.com/2009/07/wilcoxon-signed-rank-test/	July 29, 2009	Todos Logos		 0 Comments
Beta Verson of tikzDevice Released!	https://www.r-bloggers.com/2009/07/beta-verson-of-tikzdevice-released/	July 28, 2009	cameron	The tikzDevice package provides a new graphics device for R which enables direct output of graphics in a LaTeX-friendly way. The device output consists of files containing instructions for the TikZ graphics language and may be imported directly into LaTeX documents using the \input{} command. The beta version of tikzDevice is now available here. An additional location for downloading source tarballs and windows binaries is here. There are many significant improvements compared to the alpha version: The device requires a working installation of LaTeX and the TIkZ package in order to function. This is because font metrics are currently calculated through direct calls to the LaTeX compiler. Unfortunately, this results in some significant computational overhead- it may take several seconds to create a plot that contains a lot of text. In an attempt to offset this behavior, the tikzDevice uses the filehash package to store font metrics that it has already computed. Hopefully the more the device is used, the faster it will be. We suggest reviewing the package vignette, especially the section “R Options That Affect Package Behavior ” for more information on how the caching process works. We think the package is quite usable as it is, but there are surely many bugs that we don’t know about. We welcome bug reports at our R-Forge tracker Enjoy! The tikzDevice Team 	 0 Comments
I know it’s been so long…	https://www.r-bloggers.com/2009/07/i-know-its-been-so-long/	July 28, 2009	the R user...		 0 Comments
Corpus Linguistics with R, Day 2	https://www.r-bloggers.com/2009/07/corpus-linguistics-with-r-day-2/	July 28, 2009	cornelius	"R Lesson 2 
text
 # gsub replaces stuff in strings > gsub (“second”, “third”, text)
SEARCH-REPLACE-SUBJECT
[1] “This is a first example sentence.”
[2] “And this is a third example sentence.”
> gsub (“n”, “X”, text)
[1] “This is a first example seXteXce.”
[2] “AXd this is a secoXd example seXteXce.”
> gsub (“is”, “was”, text)
[1] “Thwas was a first example sentence.”
[2] “And thwas was a second example sentence.” — Perl-style regex ^	beginning of str, e.g. “^x”, ***OR*** NOT inside of []
$	end of str, e.g. “x$”
.	any other char
\	escape char – TWO (“\\”) needed
[]	character classes, e.g. [aeiou] vowels, [a-h] is same as [abcdefgh]
{MIN,MAX} number of immediately preceding unit (chacter) examples
lo+l  > grep(“analy[sz]e”, c(“analyze”, “analyse”, “moo”), perl=T, value=T)
[1] “analyze” “analyse” > grep(“(first|second)”, text, perl=T, value=T)
[1] “This is a first example sentence.”
[2] “And this is a second example sentence.”
> grep(“(first|lalala)”, text, perl=T, value=T)
[1] “This is a first example sentence.”
>  > grep(“ab{2}”, z, perl=T, value=T)
[1] “aabbccdd”
> grep(“(ab){2}”, z, perl=T, value=T)
[1] “ababcdcd”
>
>
> gsub(“a (first|second)”, “another”, text, perl=T)
[1] “This is another example sentence.”
[2] “And this is another example sentence.”
>
>
>
>
> gsub(“[abcdefgh]”, “X”, text, perl=T)
[1] “TXis is X Xirst XxXmplX sXntXnXX.”
[2] “AnX tXis is X sXXonX XxXmplX sXntXnXX.” > grep(“forg[eo]t(s|ting|ten)?_v”, a.corpus.file, perl=T, value=T)
all forms of forget *? lazy matching e.g.
gregexpr(“s.*?s”, text[1], perl=T) > gregexpr(“s.*?s”, text[1], perl=T)
[[1]]
[1]  4 14
attr(,”match.length”)
[1]  4 12 # note: things that are matched are consumed and can then not be found again in the same passtext > gsub(“(19|20)[0-9]{2}”, “YEAR”, text)
[1] “They killed 250 people in YEAR.” “No, it was in YEAR.”
> #replaces only 19xx and 20xx — > textfile
Enter file name: corp_gpl_short.txt
Read 9 items
> textfile
> textfile
[1] “the licenses for most software are designed to take away your”
[2] “freedom to share and change it. by contrast, the gnu general public”
[3] “license is intended to guarantee your freedom to share and change free”
[4] “software–to make sure the software is free for all its users. this”
[5] “general public license applies to most of the free software”
[6] “foundation’s software and to any other program whose authors commit to”
[7] “using it. (some other free software foundation software is covered by”
[8] “the gnu library general public license instead.) you can apply it to”
[9] “your programs, too.”
> unlist(strsplit(textfile, “//W”))
[1] “the licenses for most software are designed to take away your”
[2] “freedom to share and change it. by contrast, the gnu general public”
[3] “license is intended to guarantee your freedom to share and change free”
[4] “software–to make sure the software is free for all its users. this”
[5] “general public license applies to most of the free software”
[6] “foundation’s software and to any other program whose authors commit to”
[7] “using it. (some other free software foundation software is covered by”
[8] “the gnu library general public license instead.) you can apply it to”
[9] “your programs, too.”
> text_split
> text_split
[1] “the licenses for most software are designed to take away your”
[2] “freedom to share and change it. by contrast, the gnu general public”
[3] “license is intended to guarantee your freedom to share and change free”
[4] “software–to make sure the software is free for all its users. this”
[5] “general public license applies to most of the free software”
[6] “foundation’s software and to any other program whose authors commit to”
[7] “using it. (some other free software foundation software is covered by”
[8] “the gnu library general public license instead.) you can apply it to”
[9] “your programs, too.”
>
> text_split
> text_split
[1] “the licenses for most software are designed to take away your”
[2] “freedom to share and change it. by contrast, the gnu general public”
[3] “license is intended to guarantee your freedom to share and change free”
[4] “software–to make sure the software is free for all its users. this”
[5] “general public license applies to most of the free software”
[6] “foundation’s software and to any other program whose authors commit to”
[7] “using it. (some other free software foundation software is covered by”
[8] “the gnu library general public license instead.) you can apply it to”
[9] “your programs, too.”
> text_split
 > textfile
Enter file name: corp_gpl_short.txt
Read 9 items
> textfile
> textfile
[1] “the licenses for most software are designed to take away your”
[2] “freedom to share and change it. by contrast, the gnu general public”
[3] “license is intended to guarantee your freedom to share and change free”
[4] “software–to make sure the software is free for all its users. this”
[5] “general public license applies to most of the free software”
[6] “foundation’s software and to any other program whose authors commit to”
[7] “using it. (some other free software foundation software is covered by”
[8] “the gnu library general public license instead.) you can apply it to”
[9] “your programs, too.”
> unlist(strsplit(textfile, “//W”))
[1] “the licenses for most software are designed to take away your”
[2] “freedom to share and change it. by contrast, the gnu general public”
[3] “license is intended to guarantee your freedom to share and change free”
[4] “software–to make sure the software is free for all its users. this”
[5] “general public license applies to most of the free software”
[6] “foundation’s software and to any other program whose authors commit to”
[7] “using it. (some other free software foundation software is covered by”
[8] “the gnu library general public license instead.) you can apply it to”
[9] “your programs, too.”                                                    > text_split
> text_split
[1] “the licenses for most software are designed to take away your”
[2] “freedom to share and change it. by contrast, the gnu general public”
[3] “license is intended to guarantee your freedom to share and change free”
[4] “software–to make sure the software is free for all its users. this”
[5] “general public license applies to most of the free software”
[6] “foundation’s software and to any other program whose authors commit to”
[7] “using it. (some other free software foundation software is covered by”
[8] “the gnu library general public license instead.) you can apply it to”
[9] “your programs, too.”
> sort(table(text_split), decreasing=T)
text_split
                   to   software        the       free        and    general
         9          9          7          5          4          3          3
        is         it    license     public       your         by     change
         3          3          3          3          3          2          2
       for foundation    freedom        gnu       most      other      share
         2          2          2          2          2          2          2
       all        any    applies      apply        are    authors       away
         1          1          1          1          1          1          1
       can     commit   contrast    covered   designed  guarantee    instead
         1          1          1          1          1          1          1
  intended        its    library   licenses       make         of    program
         1          1          1          1          1          1          1
  programs          s       some       sure       take       this        too
         1          1          1          1          1          1          1
     users      using      whose        you
         1          1          1          1
>  > text_freqs
text_split
        to   software        the       free        and    general         is
         9          7          5          4          3          3          3
        it    license     public       your         by     change        for
         3          3          3          3          2          2          2
foundation    freedom        gnu       most      other      share        all
         2          2          2          2          2          2          1
       any    applies      apply        are    authors       away        can
         1          1          1          1          1          1          1
    commit   contrast    covered   designed  guarantee    instead   intended
         1          1          1          1          1          1          1
       its    library   licenses       make         of    program   programs
         1          1          1          1          1          1          1
         s       some       sure       take       this        too      users
         1          1          1          1          1          1          1
     using      whose        you
         1          1          1
> text_freqs[text_freqs>1]
text_split
        to   software        the       free        and    general         is
         9          7          5          4          3          3          3
        it    license     public       your         by     change        for
         3          3          3          3          2          2          2
foundation    freedom        gnu       most      other      share
         2          2          2          2          2          2
>  > !(text_split %in% stop_list)
 [1] FALSE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE
[13]  TRUE  TRUE FALSE  TRUE  TRUE  TRUE  TRUE FALSE  TRUE  TRUE  TRUE  TRUE
[25]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE  TRUE  TRUE  TRUE
[37]  TRUE  TRUE  TRUE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE
[49]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE FALSE  TRUE  TRUE  TRUE  TRUE
[61]  TRUE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE
[73]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE  TRUE  TRUE
[85]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE
> text_stopremoved
> text_stopremoved
 [1] “licenses”   “for”        “most”       “software”   “are”
 [6] “designed”   “to”         “take”       “away”       “your”
[11] “freedom”    “to”         “share”      “change”     “it”
[16] “by”         “contrast”   “gnu”        “general”    “public”
[21] “license”    “is”         “intended”   “to”         “guarantee”
[26] “your”       “freedom”    “to”         “share”      “change”
[31] “free”       “software”   “to”         “make”       “sure”
[36] “software”   “is”         “free”       “for”        “all”
[41] “its”        “users”      “this”       “general”    “public”
[46] “license”    “applies”    “to”         “most”       “free”
[51] “software”   “foundation” “s”          “software”   “to”
[56] “any”        “other”      “program”    “whose”      “authors”
[61] “commit”     “to”         “using”      “it”         “some”
[66] “other”      “free”       “software”   “foundation” “software”
[71] “is”         “covered”    “by”         “gnu”        “library”
[76] “general”    “public”     “license”    “instead”    “you”
[81] “can”        “apply”      “it”         “to”         “your”
[86] “programs”   “too”
>  # LOAD an R file
source(“something.r”)  "	 0 Comments
Corpus Linguistics with R, Day 1	https://www.r-bloggers.com/2009/07/corpus-linguistics-with-r-day-1/	July 28, 2009	cornelius	"(This post documents the first day of a class on R that I took at ESU C&T. I is posted here purely for my own use.) 

R Lesson 1
> 2+3; 2/3; 2^3

[1] 5

[1] 0.6666667

[1] 8
---
Fundamentals - Functions
> log(x=1000, base=10)

[1] 3
---
(Formals describes the syntax of other functions)
formals(sample)
---
Variables
( <- allows you to save something in a data structure (variable) )

> a<-2+3

> a

[1] 5
# is for comments
whitespace doesn't matter
---

# Pick files

file.choose()
# Get working dir

getwd()
# Set working dir

setwd("".."")
# Save

> save(VARIABLE_NAME, file=file.choose())

Fehler in save(test, file = file.choose()) : Objekt ‘test’ nicht gefunden

> save.image(""FILE_NAME"")
---
> setwd(""/home/cornelius/Code/samples/Brown_95perc"")

> getwd()

[1] ""/home/cornelius/Code/samples/Brown_95perc""

> dir()
> my_array <- c(1,2,3,4)

> my_array

[1] 1 2 3 4

> my_array <- c(""lalala"", ""lululu"", ""bla"")

> my_array2 <- c(1,2,3,4)

> c(my_array, my_array2)

[1] ""lalala"" ""lululu"" ""bla""    ""1""      ""2""      ""3""      ""4""

> 
# it is possible to add something to ALL values in a vector, i.e.

my_array2 + 10
# c (conc) makes a list

stuff1<-c(1,2,3,4,5)
---
# sequence starts at 1 (first arg), goes on for 5 (second arg), increments by 1 (third arg)

seq(1, 5, 1)
---
# put a file into a corpus vector

# what=real|char sep=seperator

> my_corpus<-scan(file=file.choose(), what=""char"", sep=""\n"")
# unique elements in my array

unique(array)
# count elements in an array

table(array)
# sort elements in an array

sort(table(array))
---

# this tells me the position of the elements in my text that aren't ""this""

> values<-which(my_little_corpus!=""this"")

> values

 [1]  2  3  4  5  6  7  8  9 11 12 13 14
# this will produce TRUE|FALSE for my condition (is this element ""this"")

> values<-my_little_corpus!=""this""

> values

 [1] FALSE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE  TRUE  TRUE

[13]  TRUE  TRUE
# this will return the array without ""this""

> values<-my_little_corpus[my_little_corpus!=""this""]

> values

 [1] ""is""      ""just""    ""a""       ""little""  ""example"" ""bla""     ""bla""

 [8] ""bla""     ""is""      ""the""     ""third""   ""line""   
...
> cc<-c(""banana"", ""bagel"")

> cc == ""banana""; cc!=""banana"" #

[1]  TRUE FALSE

[1] FALSE  TRUE

> ""banana"" %in% cc

[1] TRUE

> c(""bagel"", ""banana"") %in% cc

[1] TRUE TRUE

> match (""banana"", cc)

[1] 1

> match (c(""bagel"",""banana""), cc)

[1] 2 1
# match looks for a list of tokens and returns their position in the datastructure
---

> cat(bb, sep=""\n"", file=scan(what=""char""), append=F)

# write the contents of bb to a file, ask the user for file
moo<-scan(what=""char"")

# read something the user types into a var
# Clear Mem

> rm(list=ls(all=T))

> 
---
# create vector1 (ordered)

vec1<-c(""a"",""b"",""c"",""d"",""e"",""f,"",g"",""h"",""i"",""j"")
# oder

# > letters[1:10]

# [1] ""a"" ""b"" ""c"" ""d"" ""e"" ""f"" ""g"" ""h"" ""i"" ""j""
# create vector2 (random)

# > vector2<-sample(vector1)
---
length()

# number of elements
nchar()

# number of characters
> aa<-""know""

> nchar(aa)

[1] 4

> aa<-c(""I"",""do"",""not"",""know"")

> nchar(aa)

[1] 1 2 3 4

> lala<-c(""cat"",""gnu"",""hippopotamus"")

> lala

[1] ""cat""          ""gnu""          ""hippopotamus""

> nchar(lala)

[1]  3  3 12
> substr(""hippopotamus"", 0, 5)

[1] ""hippo""

> 
# like explode() / implode()

paste (string, sep=""my_seperator"", collapse=""stuff to put in"")
---
# percentages

x/sum(x)
barplot (1,2,3)
Read in corpus data and build a list of words frequencies

1) scan file

2) strsplit by "" ""

3) unlist to make vector

4) make a table with freqs

5) sort

6) output
#search for strings

grep(""needle"", haystack)
> grep(""is"", text, value=T)

[1] ""This is a first example sentence.""

[2] ""And this is a second example sentence.""

> grep(""And"", text, value=T)

[1] ""And this is a second example sentence.""

> grep(""sentence"", text, value=T)

[1] ""This is a first example sentence.""

[2] ""And this is a second example sentence.""

> 
gregexpr

# alternative to grep, returns a list of vectors
> mat<-gregexpr(""e"", text)

> mat

[[1]]

[1] 17 23 26 29 32

attr(,""match.length"")

[1] 1 1 1 1 1
[[2]]

[1] 16 22 28 31 34 37

attr(,""match.length"")

[1] 1 1 1 1 1 1
> unlist(mat)

 [1] 17 23 26 29 32 16 22 28 31 34 37

> mat<-gregexpr(""sentence"", text)

> sapply (mat, c)

[1] 25 30

 "	 0 Comments
Wilcoxon-Mann-Whitney rank sum test (or test U)	https://www.r-bloggers.com/2009/07/wilcoxon-mann-whitney-rank-sum-test-or-test-u/	July 27, 2009	Todos Logos		 0 Comments
Beautiful Data	https://www.r-bloggers.com/2009/07/beautiful-data/	July 27, 2009	Allan Engelhardt	"
O’Reilly’s recent publication Beautiful Data has a chapter by Jeff Jonas which is enough reason in itself for me to recommend it.  The chapter, Data Finds Data, is also available as a PDF download.
 
I met Jeff a couple of year ago at an ETech conference, and he is easily one of the smartest people I have ever met who is thinking about data.
 Jump to comments. 

Data.gov
 I am always on the lookout for useful data sources for training in statistics, so I am excited that Data.gov has opened for business. The purpose of Data.gov is to increase public access to high value, machine readable datasets generated by the US Government. 

When Big Data Matters
 "	 0 Comments
R Snippet for Sampling from a Dataframe	https://www.r-bloggers.com/2009/07/r-snippet-for-sampling-from-a-dataframe/	July 27, 2009	Will		 0 Comments
biomaRt	https://www.r-bloggers.com/2009/07/biomart/	July 27, 2009	Stewart MacArthur		 0 Comments
Book now shipping from Amazon	https://www.r-bloggers.com/2009/07/book-now-shipping-from-amazon/	July 27, 2009	Ken Kleinman		 0 Comments
Paired Student’s t-test	https://www.r-bloggers.com/2009/07/paired-students-t-test/	July 26, 2009	Todos Logos		 0 Comments
Select operations on R data frames	https://www.r-bloggers.com/2009/07/select-operations-on-r-data-frames/	July 26, 2009	Chris	The R language is weird – particularly for those coming from a typical programmer’s background, which likely includes OO languages in the curly-brace family and relational databases using SQL. A key data structure in R, the data.frame, is used something like a table in a relational database. In terms of R’s somewhat byzantine type system (which is explained nicely here), a data.frame is a list of vectors of varying types. Each vector is a column in the data.frame making this a column-oriented data structure as opposed to the row-oriented nature of relational databases. In spite of this difference, we often want to do the same sorts of things to an R data.frame that we would to a SQL table. The R docs confuse the SQL-savvy by using different terminology, so here is a quick crib-sheet for applying SQL concepts to data.frames. We’re going to use a sample data.frame with the following configuration of columns, or schema, if you prefer: (sequence:factor, strand:factor, start:integer, end:integer, common_name:character, value:double) where the type character is a string and a factor is something like an enum. Well, more accurately, value is a vector of type double and so forth. Anyway, our example is motivated by annotation of genome sequences, but the techniques aren’t particular to any type of data. So, given a data.frame of that schema, how do we do some simple select operations? Selecting columns by name is easy: As is selecting row names, or both: Selecting rows that meet certain criteria is a lot like a SQL where clause: For extra bonus points, let’s find tRNAs. Duplicate row names Row names are not necessarily unique in R, which breaks the method shown above for selecting by row name. Take matrix a: It looks to me like trying to index by the row names just returns the first row of a given name: But this works: More Resources: Help for R, the R language, or the R project is notoriously hard to search for, so I like to stick in a few extra keywords, like R, data frames, data.frames, select subset, subsetting, selecting rows from a data.frame that meet certain criteria, and find. 	 0 Comments
Rosetta Code	https://www.r-bloggers.com/2009/07/rosetta-code/	July 26, 2009	Paolo		 0 Comments
Two sample Student’s t-test #2	https://www.r-bloggers.com/2009/07/two-sample-students-t-test-2/	July 25, 2009	Todos Logos		 0 Comments
Example 7.7: Tabulate binomial probabilities	https://www.r-bloggers.com/2009/07/example-7-7-tabulate-binomial-probabilities/	July 25, 2009	Ken Kleinman		 0 Comments
Two sample Student’s t-test #1	https://www.r-bloggers.com/2009/07/two-sample-students-t-test-1/	July 24, 2009	Todos Logos		 0 Comments
One sample Student’s t-test	https://www.r-bloggers.com/2009/07/one-sample-students-t-test/	July 23, 2009	Todos Logos		 0 Comments
Two sample Z-test	https://www.r-bloggers.com/2009/07/two-sample-z-test/	July 22, 2009	Todos Logos		 0 Comments
Massively parallel database for analytics	https://www.r-bloggers.com/2009/07/massively-parallel-database-for-analytics/	July 22, 2009	Allan Engelhardt	"
This is by far the best description of why traditional parallel databases (like Teradata, Greenplum et al.) is a evolutionary dead end.  But much more than a theoretical discussion, they have built a solution which they call HadoopDB.  It is based on Hadoop, PostgreSQL, and Hive and is completely Open Source.  Alternative, column-based, backends to PostgreSQL are being implemented now.  Read: Announcing release of HadoopDB.
 See also: "	 0 Comments
One sample Z-test	https://www.r-bloggers.com/2009/07/one-sample-z-test/	July 21, 2009	Todos Logos		 0 Comments
RGG#155, 156 and 157	https://www.r-bloggers.com/2009/07/rgg155-156-and-157/	July 21, 2009	romain francois	"I pushed 3 more graphics from Biecek Przemyslaw to the graphics gallery A list of popular names for colors from packages RColorBrewer, colorRamps, grDevices  A set of examples of few graphical low-level parameters lend, ljoin, xpd, adj, legend(), axis, expressions, mtext, srt etc  Examples for different settings of:
     "	 0 Comments
Score with scoring rules	https://www.r-bloggers.com/2009/07/score-with-scoring-rules/	July 21, 2009	dan	"INCENTIVES TO STATE PROBABILITIES OF BELIEF TRUTHFULLY 
 We have all been there. You are running an experiment in which you would like participants to tell you what they believe. In particular, you’d like them to tell you what they believe to be the probability that an event will occur. Normally, you would ask them. But come on, this is 2009. Are you going to leave yourself exposed to the slings and arrows of experimental economists? You need to give your participants an incentive to tell you what they really believe, right? Enter the scoring rule. You pay off the subjects based on the accuracy of the probabilities they state. You do this by observing some outcome (let’s say “rain”) and you pay a lot of money to the people who assigned a high probability to it raining and you pay a little money (or even impose a fine upon) those who assigned a low probability to it raining. A so-called “proper” scoring rule is one in which people will do the best for themselves if they state what they truly believe to be the case. Three popular proper scoring rules are the Spherical, Quadratic, and Logarithmic. Let’s see how they work. Suppose in your experimental task you give people the title of a movie, and they have to guess what year the movie was released.  You tell them at the outset that the movie was released between 1980 and 1999: that’s 20 years. So you have these 20 categories (years) and you want people to assign a probability to each year. Afterwards, you will pay them out based on the actual year the movie was released and the probability they assigned to that year. Let r be the vector of 20 probabilities, and r_1 could be the probability they assign to 1980 being the year of release, and r_2 the probability that it was 1981, so on through r_20 for 1999′s probability. Naturally, all the r’s add up to one, as probabilities like to do. Now, let r_i be the probability they assign to the year which turns out to be correct. Under the Spherical scoring rule, their payout would be r_i / (r*r)^.5 Under the Quadratic scoring rule, the payout would be 2*r_i – r*r Under the Logarithmic scoring rule, the payout would be ln(r_i) In the movie above, the top row shows various sets of probabilities someone might assign to the 20 years. (Imagine the categories along the x-axis are the years 1980 to 1999).  Each bar in the graphs in the bottom three rows shows the person’s payout if that year turns out to be correct, based on the probabilities assigned to each year in the top row. As you can see, when they assign a high probability to a category and it turns out to be correct, their payout is high. When they assign a low payout to a category and it turns out to be correct, their payout is low. You’ll notice that the Logarithmic scoring rule goes right off the bottom of the page. This is because the log of small probabilities are negative numbers far beneath zero, and the log of 0 is negative infinity! While I was at Stanford I heard that decision scientist extraordinaire Ron Howard (no relation) used to make students assign probabilities to the alternatives (A, B, C or D) on the multiple choice items on the final exam. The score for each question was the log of the probability they assigned to the correct answer. This means, of course, that if you assign a probability of 0 to alternative “B” and alternative “B” turns out to be correct, your score on that question is negative infinity. I always wondered if you got a negative infinity on one question if it meant you got negative infinity on the exam, or if there was some mercy clause. But the main reason I am writing this post is because I wonder what experimental economists and psychologists are supposed to do when implementing log scoring rules in the lab. Naturally, you can endow the participant with cash at the beginning of the experiment and have them draw down with each question, but what do you do if they score a negative infinity? Take their life savings? Winkler (1971) decided that he would treat probabilities less than .001 as .001 when it came time to imposing the penalty. Does anyone know of other methods? REFERENCE Robert L. Winkler (1971)  Probabilistic Prediction: Some Experimental Results, Journal of the American Statistical Association, Vol. 66, No. 336.  pp. 675-685. NOTE To make this simulation, I’ve drawn on the top row various beta distributions of differing modes between two fixed endpoints. This is akin to having a min and a max guess for the year of release, then entertaining various years between those two endpoints as most likely. "	 0 Comments
Geometric and harmonic means in R	https://www.r-bloggers.com/2009/07/geometric-and-harmonic-means-in-r/	July 20, 2009	Todos Logos		 0 Comments
Adding a legend to a plot	https://www.r-bloggers.com/2009/07/adding-a-legend-to-a-plot/	July 20, 2009	Jim		 0 Comments
Example 7.6: Find Amazon sales rank for a book	https://www.r-bloggers.com/2009/07/example-7-6-find-amazon-sales-rank-for-a-book/	July 20, 2009	Ken Kleinman		 0 Comments
ggplot2: more wicked-cool plots in R	https://www.r-bloggers.com/2009/07/ggplot2-more-wicked-cool-plots-in-r/	July 20, 2009	Stephen Turner		 0 Comments
Probability exercise: negative binomial distribution	https://www.r-bloggers.com/2009/07/probability-exercise-negative-binomial-distribution/	July 19, 2009	Todos Logos		 0 Comments
New RInside release	https://www.r-bloggers.com/2009/07/new-rinside-release/	July 19, 2009	Thinking inside the box	"
This releases owes a big Thank you! to Miguel Lechón who not only
noticed errant behaviour and occassional segfaults with overly long commands
sent to the embedded R, but even traced it to an oversight of mine in a
simple memory buffer class and provided the one-line fix to reset a pointer!
Much appreciated, especially as I got his 
mail two days before I talked about 
Rcpp and
RInside
at
UseR!
2009 .  The other changes are mostly cleanups, additions of two more test
examples (to replicate the bug report) and some minor additions of a few new
assign() functions to pass data between C++ and the embedded R session.

 "	 0 Comments
David Varadi’s RSI(2) alternative	https://www.r-bloggers.com/2009/07/david-varadis-rsi2-alternative/	July 19, 2009	Joshua Ulrich	"
 "	 0 Comments
A probability exercise on the Bernoulli distribution	https://www.r-bloggers.com/2009/07/a-probability-exercise-on-the-bernoulli-distribution/	July 18, 2009	Todos Logos		 0 Comments
Let us practice with some functions of R	https://www.r-bloggers.com/2009/07/let-us-practice-with-some-functions-of-r/	July 18, 2009	Todos Logos		 0 Comments
Book excerpts now posted	https://www.r-bloggers.com/2009/07/book-excerpts-now-posted/	July 18, 2009	Nick Horton		 0 Comments
Parsing GEO SOFT files with Python and Sqlite	https://www.r-bloggers.com/2009/07/parsing-geo-soft-files-with-python-and-sqlite/	July 17, 2009	Chris	NCBI’s GEO database of gene expression data is a great resource, but its records are very open ended. This lack of rigidity was perhaps necessary to accommodate the variety of measurement technologies, but makes getting data out a little tricky. But, all that flexibility is a curse from the point of view of extracting data. The scripts I end up with are not general parsers for GEO data, but will need to be adapted to the specifics of other datasets. Note: It could be that I’m doing things the hard way. Maybe there’s an easier way. A GEO record consists of a platform, which describes (for example) a microarray and its probes, and series of samples. In this example, we need to do a join between the platform and the sample records to end up with a matrix of the form (seq, strand, start, end, value1, value2, …, valueN) where the value1 column holds measurements from the first sample and so on. If we do that, we’ll have coordinates on the genome and values for each measurement. My goal is to feed data into a genome browser known as HeebieGB with a stop-over in R along the way. Merging on a common key is only slightly complicated, but tiling arrays are big (~244,000 probes in this case). I hesitate to try merging several 244K row tables in memory. Database engines are made for this sort of thing, so I’ll use SQLite to get this done and Python to script the whole process. I like to start python scripts with a template similar to Guido’s main function, except that I prefer optparse to getopt. An –overwrite option will force the user to be conscious of overwriting files. GEO records a bunch of descriptive data about each sample, some of which we want. I've read that storing arbitrary key-value pairs in a relational DB is considered bad by some. But, I'm going to do it anyway. The entity attributes will go in a table called attributes whose schema is (entity_id, key, value). The function parse_platform_table pulls the platform data from a tab-separated section in the SOFT file into a table with a schema something like this: (id, sequence, strand, start, end). There's also a tab-separated section for each of the samples that refers back to its platform, so I extract that in a similar manner in parse_sample_table. It's easiest to start out with each sample in its own table, even though that's not really what we want. The complete script -also available from SVN here- ends up like this: The specific series I'm interested in (GSE12923) has 53 samples. The platform (GPL7255) is a custom array on Agilent's 244k feature microarrays or just short of 13 million individual features. The SOFT file is 708 MB and the script takes a good 5 or 6 minutes to ingest all that data. The next step is merging all the data into a single matrix. This turned out to be harder than I thought. At first, I naively tried to do a big 54 way join between the platform table and all the sample tables, with an order-by to sort by chromosomal location. I let this run for a couple hours, then gave up. Sure, a big join on unindexed tables was bound to be ugly, but it only had to run once. I'm still surprised that this choked, after all, it's not that much data. There are two ways around it. One is to index the sample tables by ID_REF and the platform table by (sequence, strand, start, end). The other is to do the big join then sort into a second table. Either takes several minutes, but it's just a one-off, so that's OK. Now that we've done that, do you ever find data that doesn't need to be cleaned up a little bit? That's about all the data munging I can stand for now. The rest, I'll leave for Part 2. 	 0 Comments
Simple Data Visualization	https://www.r-bloggers.com/2009/07/simple-data-visualization/	July 16, 2009	jebyrnes	OK, so, I know I already raved about one Hadley Wickham project and how it has changed my life last week.  But what can I say, the man is a genius.  And if you are using R (and let’s face it, you should be) and you want simple sexy graphs made quick, the man has you covered once again. I first learned about ggplot2 while scanning through some slides of the LA Area RUG meetings (that I missed – I still haven’t been to any!) by the folks from Michael Driscoll. And I liked what I saw – ggplot2 and lattice (which I admit, I had kind of avoided) seemed more intuitive than I thought.  Then I ran into a series of articles on ggplot2 from the Learning R blog and I was hooked.  Still am.  And why I ask? Let’s consider a task – you have some data split into four groups.  For each group, you want to plot a fitted regression between two covariates.  You want this split into panels, with common scales, and nicely placed axis labels.  Also, you want it to be purty.  While you can do this with the standard graphics package (and, I admit, I sometimes like the austerity of the standard graphics), it would take a for loop, tons of formatting instructions, and a number of steps where you could easily mess the whole thing up.  Not to mention that you might have to change a good bit if you want to add another group. Here is how easy it is with ggplot2.  Note, there are only two lines of actual graphing code.  The rest is just making up the data. All of which yields the following pretty figure:  And that stat_smooth statement can take lots of other arguments – e.g., glms (I’ve tried, and it looks great!)  So check it out – even for just casual data exploration, there’s some real clarity to be found.  And I look forward to trying out other products by Prof. Wickham! 	 0 Comments
Influence.ME: Simple Analysis	https://www.r-bloggers.com/2009/07/influence-me-simple-analysis/	July 16, 2009	Rense Nieuwenhuis	"With the introduction of our new package for influential data influence.ME, I’m currently writing a manual for the package. This manual will address topics for both the experienced, and the inexperienced users.  I will also present much of the content of this manual on my blog. Of course, feel free to comment on it, and readers are encouraged to discuss the content of the manual here. All information will be accessible from the influence.ME website as well. Note that updates to the manual will be made available on that website”, instead of updating this blog post. So, please refer to the influence.ME website for the most up-to-date information. This is the first section on influence.ME, which deals with a very simply analysis of students nested within 23 schools. Only the effect of a single variable measured at the school level is estimated.  The school23 data contains information on a math test performance of 519 students, who are nested within 23 schools. For this example, we will be interested in the relationship between class structure (in this data measured at the school level) and students’ performance on a math test. The research question is: To what extend does the classroom structure determine the students’ math test outcomes?  Initially, we will estimate the effect of class structure on the result of the math performance test, without any further covariates. We do take into account the nesting structure of the data, however, and allow the intercept to be random over schools. This model is estimated using the following syntax, and is assigned to an object we call ‘model’. The call for a summary of the model results in the output shown below. In this summary, the original model formula is shown, as well as the data on which this model was estimated. Both random and fixed effects are summarized. The amount of intercept variance associated with the nesting structure of students within schools is considerably large (23.8 compared with 81.2 + 23.8 = 104 in total). The effect of interest is that of the structure variable, which is -2.343 and statistically insignificant by most reasonable standards (t=-1.609). Building upon the example model estimated in section 2.1, the first step in the procedure of the influence.ME package is to iteratively exclude the influence of the observations nested within each school separately. This is done using the estex() function. The name estex refers to the ESTimates that are returned while EXcluding the influence of each of the grouping levels separately. Thus, in the case of the math test example, in which students are nested in 23 schools, the estex procedure re-estimates the original model 23 times, excluding the influence of a higher level unit (ie school). The function returns the relevant estimates of these 23 re-estimations, which in Figure [fig:Three-steps] is referred to with 'altered estimates'. The estex() function requires the specification of two parameters: a mixed effects model is to be specified, and the grouping factor of which the influence of the nested observations are to be evaluated. In the syntax example below, the original object 'model' is specified, and 'school.ID' is the relevant grouping factor. school.ID is the name of the variable used to indicate the grouping factor when the original model was specified. The estex() function works perfectly when more than a single grouping is present in the model, but only one grouping factor can be addressed at once.  In the example below, the estimates excluding the influence of the respective grouping levels, as returned by the estex() function, are assigned to an object, which in this case is called este.model (the name of this object, however, is to be chosen arbitrarily by the user).  Note that in the case of complex mixed models (i.e. models with large numbers of observations, complex nesting structures, and/or many nesting groups) the execution of estex() may consume considerable amounts of time. The examples offered by the school23 data, should offer no such problems, however. The object estex.model containing the altered estimates can be used to calculate several measures of influential data. To determine the Cook's distance, the ME.cook() function is to be used. In its most basic specification, the ME.cook() function only requires an object to which the altered estimates as returned by the estex() function were assigned:  This basic specification returns a matrix with the rows representing the groups in which the observations are nested, and the single column represents the associated value of Cook's distance. Clearly, these can also be assigned to an object for later modification. The output below shows the result of the syntax above, representing the Cook's distance associated with each school in the school23 data. Based on the output shown above, the Cook's distance of school number 7472 is the largest. This corresponds very well to what was concluded based on Figure [fig:Bivariate-influence-plots]. For those who prefer to evaluate the Cook's distance based on a visual representation, the ME.cook() function can also plot its output. To do so, an additional parameter is required, stating plot=TRUE. Additional parameters are allowed as well, which are passed on to the internal dotplot() function (Deepayan Sarkar, 2008) and are used to format the resulting plot. In this case, the example syntax below also specifies the xlab= and ylab= parameters, labelling the two axes. The resulting plot is shown in the figure below. These kinds of plots can be used to more easily assert the influence a grouped set of observations exert on the outcomes of analyses, relative to the influence excerted by other groups of observations.  In this case, it (again) is clear that the observation of the level of class structure of school number 7472 excerts the highest influence. This is based on the calculated value for Cook's distance, as well as that this influence clearly exceeds that of other schools.  ME.cook(estex.model, plot=TRUE,
    xlab=""Cook's Distance, Class structure"",
    ylab=""School"")  Based on the analyses and graphs shown in the previous sections, there are strong indications that the observations in school number 7472 excert too much influence on the outcomes of the analysis, and thereby unjustifiably determine the outcomes of these analyses. To definitively decide whether or not the influence of these observations indeed is too large, the value of Cook’s distance of this school can be compared with a cut-off value given. Regarding Cook’s distance, it has been argued that observations exceeding a Cook’s distance of  are too influential Belsley et al. (1980), and need to be dealt with. In this formula, ‘p’ refers to the number of predictors on which Cook’s distance was calculated. In the case of mixed effects models, this refers to the number of groups in which the observations are nested.  The Cook’s distance of school number 7472 was determined to be 1.31, which readily exceeds the cut-off value of  = .17. Thus, is can be concluded that the influence school number 7472 needs to be excluded form the analysis, before the results of that analyses are interpreted. This is done using the function exclude.influence(). This function basically has three parameters: first, the model from which the influence of some observations is to be excluded needs to be specified, together with the grouping factor and the specific level of that grouping factor in which the said observations are nested. The function modifies the original model and returns a new model, which can be checked again for possible influential data. In the example below, the influence of school number 7472 is excluded from the orginal regression model, which was assigned to object ‘model’ in section 2.1.  The result of the exclude.influence() function again has the form of a mixed effects model and is here assigned to object model.2 (again, this name is to chosen by the user).  model.2 
summary(model.2) Functions that work with 'normal' mixed effects models estimated with lme4, also work with models that were modified with the exclude.influence() function. So, also a summary of model.2 was requested, which is shown below. A few things are clear from this output. The estimate of the effect of class structure is now much stronger (-4.55) and statistically significant (t=2.95). This corresponds to what may have been expected based on the graphical representation of the data in Figure [fig:Bivariate-influence-plots]. Some other changes have been made to the model as well. The original intercept vector (which originally was indicated by (Intercept)) is now replaced by a variable called intercept.alt. This variable is basically an ordinary intercept vector (thus, with a value of 1 for each observation), except for the observations that are nested in the excluded nesting group. For these observations, the intercept.alt variable has score 0. Also, a new variable called estex.7472 is shown. This variable is a dummy variable, indicating the observations that are nested in school number 7472. One such dummy variable is added to the model for each nesting group the influence of which is excluded. Generally, these modifications of the model ensure that the observations nested within the excluded nesting group do not contribute to the estimation of both the level and the variance of the intercept, and do not alter the higher level estimates unjustifiably.  As is shown in the procedural schematic in Figure [fig:Three-steps], it is advisable to repeat this procedure to the point that the user is satisfied with the stability of the model, for instance when no group of observations exceeds the cut-off value. To do this in this example, the model.2 object is again input to the estex() function, the results of which are stored in a second altered estimates object which we call estex.model.2: Again, ME.cook() is used to calculate the values for Cook's distance, which returns the output shown below. School number 62821 is associated with the largest value for Cook's distance (.39). The cut-off value now differs (slightly) from the previous one, for the number of (effective) groups in which the observations are nested is decreased by 1, for the influence of school number 7472 was excluded. Thus, the cut-off value now is . Based on the output below, it can thus be concluded that school number 62821 is influential as well.  Finally, the call for ME.cook() in the syntax example above shows one more distinguishing characteristic. Again plot=TRUE is specified, together with specifications for labels on both the x and y axes. A plot of the Cook's distances is thus created, shown in Figure [fig:Cook-2]. In addition to this, the cut-off value of .18 is now indicated as well using cutoff=.18. As a result of this, all Cook's distances with a value larger than .18 will be indicated differently in the plot, as is the case in Figure [fig:Cook-2] regarding the two schools numbered 62821 and 7474. Note that the Cook's distance for school number 7472 now equals 0, indeed, indicating that this school now no longer influences the parameter estimates.  Further analysis of this example would thus entail the exclusion of the influence of observations nested within school number 62821, and then to recheck the model by running through the three steps of the procedure again. This is not shown here, to not make this exercise overly lengthy.  "	 0 Comments
Missing data, logistic regression, and a predicted values plot (or two)	https://www.r-bloggers.com/2009/07/missing-data-logistic-regression-and-a-predicted-values-plot-or-two/	July 15, 2009	Jim		 0 Comments
Job grade plot	https://www.r-bloggers.com/2009/07/job-grade-plot/	July 15, 2009	Jim		 0 Comments
Example 7.5: Replicating a prettier jittered scatterplot	https://www.r-bloggers.com/2009/07/example-7-5-replicating-a-prettier-jittered-scatterplot/	July 15, 2009	Ken Kleinman		 0 Comments
Building R packages for Windows	https://www.r-bloggers.com/2009/07/building-r-packages-for-windows/	July 13, 2009	Rob J Hyndman	"To build an R package in Windows, you will need to install some  additional software tools. These are summarized at This is a collection of unix-like tools that can be run from the DOS  command prompt. It also contains the MinGW compilers that are used for  compiling Fortran and C code. Download using You should download and run it, choosing the default “Package authoring  installation” to build add-on packages. MikTeX is used for producing the pdf help files. You can produce an R  package without it, but the package will not contain pdf help files.  Most of you will have this installed anyway. Download from The PATH variable tells Windows where to find the relevant programs. To  add a directory to you PATH on Windows XP select The path variable may have already been fixed in step 1.1. In any case,  you should check that it looks something like this: Information about creating packages is provided in the document  “Writing R extensions” (available under the R Help menu) or at The main items are summarized below to get you started, but you will  almost certainly need to consult the above document if you are to  successfully compile a package. The simplest way to create a package is to first create an R workspace  containing all the relevant functions and data sets that you want to  include in the package. Delete anything from the workspace that you do  not want to include in the package. Make sure the current directory is  set to whereever you want create the package. Use for example. Then, to create a package called ""fred"",  use the R command This will generate a directory fred and several sub-directories in the required structure. A package consists of a directory containing a file ‘DESCRIPTION’ and  usually has the subdirectories R, data and man.  The package directory should be given the same name as the package. The package.skeleton command above will have created these files for you. You now need to  edit them so they contain the right information. The DESCRIPTION file contains basic information about the package in  the following format: The help files for each function and data set are given in “R  documentation” (Rd) files in the man subdirectory. These are in a simple markup language closely resembling  LaTeX, which can be processed into a variety of formats, including  LaTeX, HTML and plain text. As an example, here is the file which  documents the function seasadj in the forecast package. Detailed instructions for writing R documentation are at If your R code calls C or Fortran functions, the source code for these  functions needs to be placed in the subdirectory src under fred. To compile the package into a zip file, go to a DOS prompt in the  directory containing your package. (i.e., the directory ""C:My DocumentsRpackages"" in the above example. Then type This will compile all the necessary information and create a zip file  which should be ready to load in R. To check that the package satisfies the requirements for a CRAN  package, use The checks are quite strict. A package will often work ok even if it  doesn’t pass these tests. But it is good practice to build packages  that do satisfy these tests as it may save problems later. To build a package for something other than a Windows computer, use This creates a tar.gz file which can then be installed on a  non-Windows computer. It can also be uploaded to CRAN provided it  satisfies the above tests. "	 0 Comments
A recommended book	https://www.r-bloggers.com/2009/07/a-recommended-book/	July 13, 2009	Jim		 0 Comments
cran2deb: Would you like 1700+ new Debian / R packages ?	https://www.r-bloggers.com/2009/07/cran2deb-would-you-like-1700-new-debian-r-packages/	July 13, 2009	Thinking inside the box	"

This is essentially a ‘2.0’ version of earlier work with Steffen Moeller and David
Vernazobres which we had presented in 2007.  Then, the approach was top-down
and monolithic which started to show its limits.  This time, the idea was to
borrow the successful bottom-up approach of my
CRANberries feed.

 
The bulk of the work was done by Charles Blundell as part of his Google Summer of Code
2008 project which I had suggested and mentored.  After that project had
concluded, we both felt we should continue with it and bring it to
‘production’.  The CRAN hosts provided us with a (virtual Xen) machine to build
on, and we are now ready to more publically announce the availability of the
repositories for i386 and amd64:
 
A few more details are provided in 
our presentation slides. 
We look forward to hearing from folks using; the r-sig-debian list
may be a good venue for this.  

 "	 0 Comments
Some detail on the last plot	https://www.r-bloggers.com/2009/07/some-detail-on-the-last-plot/	July 13, 2009	Jim		 0 Comments
Obama approval	https://www.r-bloggers.com/2009/07/obama-approval/	July 12, 2009	Jim		 0 Comments
useR 2009 in Rennes: Recap and slides	https://www.r-bloggers.com/2009/07/user-2009-in-rennes-recap-and-slides/	July 12, 2009	Thinking inside the box	"

As last year (and again at the BoC in December), I presented a three-hour
tutorial on high-performance computing with R. This covers profiling,
vectorisation, interfacing compiled code, debugging, parallel computing, as
well as scripting and automation. Slides, and a 2-up version, are now on my
presentations
page.
 

I also gave two regular conference presentations. The first was on my
Rcpp and
RInside packages
which facilitate interfacing R and C++.  The second talk, based on joint work
with Charles Blundell, describes our cran2deb system for creating Debian
packages of essentially all CRAN
packages. I will try to follow up on this with another post.  Slides from
these talks are also on my
presentations
page.



 "	 0 Comments
Causal inference and biostatistics	https://www.r-bloggers.com/2009/07/causal-inference-and-biostatistics/	July 11, 2009	John Johnson		 0 Comments
The Knapsack Problem	https://www.r-bloggers.com/2009/07/the-knapsack-problem/	July 10, 2009	Allan Engelhardt	"
David posts a question about how to solve this knapsack problem  using the R statistical computing and analysis platform.  My reply in the comments seems to have disappeared for a while so here is my proposed solution.  See David’s blog for my earlier proposed solution with a very common error.
 
Brute force works, it just doesn’t scale well.  (Note that 7×2.15 is another solution.)
 Jump to comments. 

Employee productivity as function of number of workers revisited
 We have a mild obsession with employee productivity and how that declines as companies get bigger. We have previously found that when you treble the number of workers, you halve their individual productivity which is mildly scary. We revisit the analysis … "	 0 Comments
Sometimes, you just need to use a plyr	https://www.r-bloggers.com/2009/07/sometimes-you-just-need-to-use-a-plyr/	July 10, 2009	jebyrnes	I haven’t posted anything about R-nerdery in quite some time.  But I have to pause for a moment, and sing the praises of a relatively new package that has made my life exponentially easier.  The plyr package.   R has the capability to apply a single function to a vector or list using apply or mapply, or their various derivatives.  This returns another vector or list. This is great in principal, but in practice, with indexing, odd return objects, and difficulties in using multiple arguments, it can get out of hand for complex functions.  Hence, one often resorts to a for loop. Let me give you an example.  Let’s say I have some data from a simple experiment where I wanted to look at the effect of adding urchins, lobsters, or both on a diverse community of sessile invertebrates – mussels, squirts, etc.  Heck, let’s say, I had one gazillion species whose responses I was interested in.  Now let’s say I want a simple summary table of the change in the density each species – and my data has before and after values.  So, my data would look like this.   So, each row represents the measurement for one species in one replicate either before or after the experiment.  Now, previously, to get an output table with the mean change for each species for each treatment, I would have had to do something like this: Ok, did that exhaust you as much it did me?  Now, here’s how to get the same result using ddply (more on why it’s called ddply in a moment) Um.  Whoah.  Notice a small difference there.  Now, perhaps the earlier code could have been cleaned up a bit with reshape, but still, 6 lines of code versus 18 – and most of that 18 was bookkeeping code, not something doing real useful computation. Soooo…. Check out plyr!  It will make your life a happier, more relaxed place.  Enjoy! (p.s. I’m noticing wordpress is ignoring all of my code indenting – anyone have any quick fixes for that?) 	 0 Comments
Presenting influence.ME at useR!	https://www.r-bloggers.com/2009/07/presenting-influence-me-at-user/	July 10, 2009	Rense Nieuwenhuis	" Today I presented influence.ME at the useR! conference in Rennes. Influence.ME is an R package for detecting influential data in mixed models. I developed this package together with Ben Pelzer and Manfred te Grotenhuis. More information about influence.ME can be found on another section of my website. Below, please find the slides of the presentation.
Presentation Influence.ME at Rennes, useR! 2009 "	 0 Comments
Useful Links	https://www.r-bloggers.com/2009/07/useful-links/	July 9, 2009	Todos Logos		 0 Comments
Computing Statistics from Poorly Formatted Data (plyr and reshape packages for R)	https://www.r-bloggers.com/2009/07/computing-statistics-from-poorly-formatted-data-plyr-and-reshape-packages-for-r/	July 9, 2009	dylan	" 
Premise
I was recently asked to verify the coefficients of a linear model fit to sets of data, where each row of the input file was a “site” and each column contained the dependent variable through time (i.e. column 1 = time step 1, column 2 = time step 2, etc.). This format is cumbersome in that it cannot be directly fed into the R lm() function for linear model fitting. Furthermore, we needed the output formatted with columns containing slope, intercept, and R-squared values for each site (rows). All of the re-formatting, and model fitting can be done by hand, using basic R functions, however this task seemed like a good case study for the use of the reshape and plyr packages for R. The reshape package can be used to convert between “wide” and “long” format– the first step in the example presented below. The plyr package can be used to split a data set into subsets (based on a grouping factor), apply an arbitrary function to the subset, and finally return the combined results in several possible formats. The original input data, desired output, and R code are listed below. read more "	 0 Comments
useR! slides	https://www.r-bloggers.com/2009/07/user-slides/	July 8, 2009	romain francois	I’ve pushed my slides from the presentation I’ve given at useR! a few minutes ago here 	 0 Comments
RGG# 154: demo of atomic functions	https://www.r-bloggers.com/2009/07/rgg-154-demo-of-atomic-functions/	July 7, 2009	romain francois	"Przemyslaw Biecek has submitted this graph (and also others I will add later) to the graphics gallery A list of examples for the atomic functions polygon(), segments(), symbols(), arrows(), curve(), abline(), points(), lines(). 
this figure is taken from the book Przewodnik po pakiecie R "	 0 Comments
Return	https://www.r-bloggers.com/2009/07/return/	July 6, 2009	Jim		 0 Comments
Using R to Create Misc. Patterns [smocking]	https://www.r-bloggers.com/2009/07/using-r-to-create-misc-patterns-smocking/	July 4, 2009	dylan	"Pattern Chunk  
Premise
My wife asked me to come up with some graph paper for creating smocking patterns. After a couple of minutes playing around with R-base graphics functions, it occurred to me that several functions in the sp package would simplify grid-based operations. Some example functions, along with a simple approach to generating “interesting” patterns, are listed below.  read more "	 0 Comments
Summarizing Grouped Data in R	https://www.r-bloggers.com/2009/07/summarizing-grouped-data-in-r/	July 3, 2009	dylan	A colleague of mine recently asked about computing basic summary statistics from grouped data in R. These are a couple examples that I suggested. Additional documentation for the plyr package can be found here. read more 	 0 Comments
Remove files with a specific pattern in R	https://www.r-bloggers.com/2009/07/remove-files-with-a-specific-pattern-in-r/	July 3, 2009	Paolo		 0 Comments
OECD Statistics	https://www.r-bloggers.com/2009/07/oecd-statistics/	July 2, 2009	Allan Engelhardt	"
I am a sucker for good quality data.  I wrote about data.gov, the US Government data site before, and now I find OECD Statistics which has some 300 data sets, many of which seems to be readily accessible (though some may require subscription)
 
Exports in multiple formats, including Excel, CSV, and SDMX.
 "	 0 Comments
Example 7.4: A prettier jittered scatterplot	https://www.r-bloggers.com/2009/07/example-7-4-a-prettier-jittered-scatterplot/	July 2, 2009	Ken Kleinman		 0 Comments
R String processing	https://www.r-bloggers.com/2009/07/r-string-processing/	July 2, 2009	Chris	Here’s a little vignette of data munging using the regular expression facilities of R (aka the R-project for statistical computing). Let’s say I have a vector of strings that looks like this: What I’d like to do is parse these out into a data.frame with a column for each of sequence, strand, start, end. A regex that would do that kind of thing looks like this: (.*)([+-]):(d+)-(d+). R does regular expressions, but it’s missing a few pieces. For example, in python you might say: As far as I’ve found, there doesn’t seem to be an equivalent in R to regex.match, which is a shame. The gsub function supports capturing groups in regular expressions, but isn’t very flexible about what you do with them. One way to solve this problem is to use gsub to pull out each individual column. Not efficient, but it works: It seems strange that R doesn’t have a more direct way of accomplishing this. I’m not an R expert, so maybe it’s there and I’m missing it. I guess it’s not called the R project for string processing, but still… By the way, if you’re ever tempted to name a project with a single letter, consider the poor schmucks trying to google for help. 	 0 Comments
Getting help with R	https://www.r-bloggers.com/2009/07/getting-help-with-r/	July 2, 2009	Stephen Turner		 0 Comments
Example 7.11: Plot an empirical cumulative distribution function from scratch	https://www.r-bloggers.com/2009/08/example-7-11-plot-an-empirical-cumulative-distribution-function-from-scratch/	August 31, 2009	Ken Kleinman		 0 Comments
Formatting Decimals in Texts with R	https://www.r-bloggers.com/2009/08/formatting-decimals-in-texts-with-r/	August 30, 2009	Yihui Xie	Yanping Chen raised a question in the Chinese COS forum on the output of Eviews: how to (re)format the decimal coefficients in equations as text output? For example, we want to round the numbers in CC = 16.5547557654 + 0.0173022117998*PP + 0.216234040485 * PP(-1) + 0.810182697599 * (WP + WG) to the 3rd decimal places. This can be simply done by regular expressions, as decimals always begin with a “.”. The basic steps are: Given a character vector, we can format the decimals with the code below: I used sapply() for 3 times to avoid explicit loops but consequently the code might be difficult to read. The critical part is the regular expression “\.[0-9]+” which means one of more (controlled by “+” after “[0-9]”) digits (“[0-9]” or “[:digit:]”) after a decimal point “.”. As “.” is a metacharacter in regular expressions, we need to use a backslash before it, and again, “” is a special character in R, so we need another backslash to denote a backslash.   	 0 Comments
Counterintuitive Results in Flipping Coins	https://www.r-bloggers.com/2009/08/counterintuitive-results-in-flipping-coins/	August 28, 2009	Yihui Xie	It seems that the two results are equivalent, as H and T occurs with equal probability 0.5, so we naturally believe the average numbers of steps to HTH and HTT are the same, but the fact is not as we imagined. The answer is counterintuitive, isn’t it?  Number of times needed to get HTT and HTH (bold segments are for median; dots denote mean) Well, mathematicians certainly do not like my solution (I guess they even hate such an imprecise approach). I hope some smart guys can give me some hints on working out the probability distribution and hence the expectation. 	 0 Comments
‘R’ programming	https://www.r-bloggers.com/2009/08/r-programming/	August 28, 2009	Shige		 0 Comments
Learning R 2009-08-28 13:11:00	https://www.r-bloggers.com/2009/08/learning-r-2009-08-28-131100/	August 28, 2009	Jim		 0 Comments
Combine R CMD build and junit	https://www.r-bloggers.com/2009/08/combine-r-cmd-build-and-junit/	August 28, 2009	romain francois	This is a post in the series Mixing R CMD build and ant. Previous posts have shown how to compile the java code that lives in the src directory of a package and how to document this code using javadoc.  This post tackles the problem of unit testing of the java functionality that is shipped as part of an R package. Java has several unit test platforms, we will use junit here, but similar things could be done with other systems such as testng, … The helloJavaWorld package now looks like this : We have added the src/lib directory that contains the junit library and the HelloJavaWorld_Test.java that contain a simple class with a unit test And the ant build file has been changed in order to  The package can be downloaded here  Coming next, handling of dependencies between java code that lives in different R packages 	 0 Comments
A Fast Intro to PLYR for R	https://www.r-bloggers.com/2009/08/a-fast-intro-to-plyr-for-r/	August 27, 2009	JD Long	I’m not dead yet! Although it has been rumored that I am. The new job is going great and I’m thrilled to be with a new firm doing interesting work alongside smart people. It makes me seem smarter by simple association. There’s been a lot going on recently in the R user community. There was an R flash mob of Stack Overflow which resulted in a noticeable increase in the number of R questions and answers in SO. I’ve been blown away by the quality of the participants. There has also been increased quality discussions on Twitter which are being tagged with #rstats. These changes in the community have not gone unnoticed. Recently I posted a question about how to do a ‘group by’ in a regression with R. I had a way I had been doing this but I was suspicious there was a better way. One of the answers proposed using the PLYR package. I think I had seen the plyr package a few times but never really understood it. Although I didn’t select this as my top answer, it prompted me to look into PLYR more. What I discovered was really interesting. The PLYR package is a tool for doing split-apply-combine (SAC) procedures. I’m very fluent in SQL so the best analogy for me was the GROUP BY statement in SQL. PLYR adds very little new functionality to R. What it does do is take the process of SAC and make it cleaner, more tidy and easier. I think I’m not the only one who wants a clean and tidy SAC. Here’s a quick example of making some summary stats using PLYR: result: PLYR functions have a neat naming convention. The first two letters of the function tells the input and output data types, respectively. The one I use the most is ddply which takes a data frame in and spits out a data frame.  Let me see if I can explain what ddply is doing. The first argument, dd, is the input data frame. The next argument is the “group by” variables. Since I want to group by two variables I send them as a vector (that’s what the c() bit does). What threw me for a loop initially was the third argument, the function. What I found myself trying (unsuccessfully) was just using mean(v1) as the third argument. If I did that, R would spit at me and bring the marital status of my parents into question. I discovered that the problem was the ddply function was splitting the data by my ‘group by’ variables and then it wanted to pass each of the resulting data frames to a function. So what does it mean to pass a data frame to mean(v1)? Yeah, it means Jack Crap, that’s what it means. So in one of the PLYR examples I saw they were using these inline functions. The idea behind function(df)mean(df$v1) is to create a function to which we can pass a data frame and get out a meaningful result. The subset (or split) of the data gets passed to the function and that subset is then known as df. mean(df$v1) calculates the mean of v1 and returns an answer. ddply holds on to the answers of each split and then reassembles them all in the end. Slick, ey? As with most things in R the idea can be extended to a vector of functions in order to perform many operations on each split: The result looks like this: Pretty nifty. The author of PLYR is Hadley Wickham who is also the man behind GGPLOT2. If you like PLYR or GGPLOT2 then you should immediately buy Hadley’s GGPLOT2 book on Amazon. But be sure and use the link on this site or the link on Hadley’s site so he can get Amazon associate payment. The authors I have talked to told me they get more from the Associate program than they get from publishing royalties. My father is a retired pilot turned crop farmer. He ALWAYS carries a pair of pliers in a nylon pouch on his belt. I can see that Hadley’s PLRY package is going to become my proverbial ‘belt pliers.’ Of course if I wrote an R package I’d have to name it Super RamBar, cause that’s just how I roll. 	 0 Comments
Using R and Bioconductor for sequence analysis	https://www.r-bloggers.com/2009/08/using-r-and-bioconductor-for-sequence-analysis/	August 26, 2009	Chris	Here’s another quick R vignette, in case I pick this up later and need to remind myself where I got stuck. I was trying to use R for a bit of basic sequence analysis, with mixed results. First, install the BSgenome package, which is part of Bioconductor. Get GeneR while you’re at it. Follow the instructions in the document How to forge a BSgenome data package. You’ll need to get fasta files from somewhere such as NCBI’s Entrez Genome. Another nice data source is Regulatory Sequence Analysis Tools. I created a BSgenome package for our favorite model organism Halobacterium salinarum NRC-1, which I named halo for short. Now, I can ask what sequences make up the halo genome and find out how long they are. There are a few things I wanted to do next. First, I wanted to load a list of genes with their coordinates. That should allow me to quickly get the sequence for each gene, or get sequence of upstream regions for regulatory motif finding. Second, if I’m going to find any new protein coding regions, I’d like to have a function that could take a stretch of DNA and find ORFs (open reading frames). As far as I can tell, all there is to ORF finding is searching each reading frame for long stretches that start with a methionine (AUG) and end with a stop codon (UAG, UGA, and UAA ). Maybe there’s more to it than that. This is where I left off. GeneR seems to use an entirely different way of encoding sequence based on buffers. I have to admit to being a little disappointed. I hope it’s just my cluelessness and there’s really a reasonable way to do this kind of thing in R and Bioconductor. 	 0 Comments
ggplot2 Version of Figures in “Lattice: Multivariate Data Visualization with R” (Final Part)	https://www.r-bloggers.com/2009/08/ggplot2-version-of-figures-in-%e2%80%9clattice-multivariate-data-visualization-with-r%e2%80%9d-final-part/	August 25, 2009	learnr	"Over the past weeks I have tried to replicate the figures in Lattice: Multivariate Data Visualization with R using Hadley Wickham’s ggplot2. With the exception of a few graph types (e.g. ggplot2 doesn’t support 3d-graphs, and there were a few other cases), it was possible to create ggplot2 versions of almost all the figures. Sometimes this required data manipulation before plotting in order to get data into a suitable form to feed into ggplot2, but more often than not ggplot2 provided satisfactory out-of-the-box visualisation very closely comparable to that of lattice. I would like to conclude this series with comments on a few keywords that stuck to my mind while preparing all these graphs. Both lattice and ggplot2 are running on top of the grid graphics, however lattice is a lot faster. A lot. Whilst drawing one or two graphs, one might not even notice the difference in speed, but once the number of graphs increases or the datasets get bigger the relative slowness of ggplot2 becomes more clearly recognisable (have a look at the pdf-s linked to at the end of this post for comparative timings). Reader Ben Bolker emailed Hadley Wickham about the issue, and the response he got was “So far I have been completely focused on functionality, and not at all on speed. I would really like to spend some time profiling and optimised ggplot2 (I suspect an order of magnitude speed increase would be possible), but unfortunately my summer is filling up rapidly and I am feeling some pressure to write papers rather than (more) code.” It is good news that speed can be improved, now let’s hope Hadley finds some time to look into this. Almost every element of the output of both packages is highly customisable. lattice has more options to tinker with the finest details of the plot, allowing to make sure that the final graph looks exactly the way one wants. Such fine-tuning requires, though, a very good knowledge of the inner workings of the program, as the available options are not always so obvious. I find fine-tuning  a graph using the ggplot2’s approach a lot easier, as it is clear which element of the plot is being adjusted. Still, as always, there is room for improvement – the ability to better manipulate the heights/widths/aspect ratios of facets (facet_grid has the space=""fixed"" argument, but not facet_wrap); and better control over size and positioning of legends are the two main items that surfaced during this exercise. As already mentioned lattice has a jungle of parameters one can manipulate to achieve the best output possible. Lattice to me is more cluttered with all of its rich options (panel/prepanel functions), and I personally prefer the ggplot2 approach of building up a graph layer by layer using “human-readable” expressions. Compared to the use of various specialised functions in lattice I find this more intuitive and easier to follow. The lattice panel functions in capable hands make it an extremely powerful tool. However, having seen the lattice examples, only now did I come to fully appreciate the power of the ggplot2 equivalents: stat_summary and stat_function. Again, if there was one thing to add to my wish-list, it would be the ability to use formulas/functions (e.g. reorder) as facetting variables – allowing to skip one data pre-processing step. ggplot2 has a very good website with many useful examples (the same information without the rendered graphs is included in the help file), as well as a book with good explanations. Using a combination of all these, one gets a good overview of the available options, and answers to the questions that may arise. I especially like the examples on the website, that often highlight the more intricate features of the program. lattice manual explains all the available options in great detail, sometimes requiring a good amount of concentration and will to go through the instructions. Apart from the book website, one can also make use of R Graphical Manual that includes “a collection of graphics from all R packages”. Some readers requested a pdf-version of the posts – all the chapters have been compiled into one pdf-file that can be downloaded here (6mb). Another version of the same file which also includes the system.time results for most of the print statements used to generate the images is available here (6mb). And yet another version with no images can be downloaded here (800 kb). I will also list the tools I used to create the blog posts as well as the pdf-files: "	 0 Comments
Web-site trend analysis with data from Google Analytics	https://www.r-bloggers.com/2009/08/web-site-trend-analysis-with-data-from-google-analytics/	August 25, 2009	Todos Logos		 0 Comments
Applied Spatial Data Analysis with R	https://www.r-bloggers.com/2009/08/applied-spatial-data-analysis-with-r/	August 25, 2009	James	I have just reviewed the book Applied Spatial Data Analysis with R which has been published in the September2009  issue of the Royal Statistical Society’s Significance magazine.  Applied Spatial Data Analysis with R is an accessible text that demonstrates and explains the handling  of spatial data using the R Software Platform. The text’s authors have  all been key contributors to the R spatial data analysis community,  and the range of their contributions is evident from the comprehensive  coverage of this work. It will appeal to those familiar with R but not  spatial data, and vice versa, as well as those proficient in both and  in search of a reference text. I highly recommend the book to those interested in embarking on spatial data analysis, those proficient in handling spatial data in other software and want to utlise R, and those already using R to manipulate and analyze spatial data.  	 0 Comments
Standardized Velvet Assembly Report	https://www.r-bloggers.com/2009/08/standardized-velvet-assembly-report/	August 25, 2009	Jeremy Leipzig		 0 Comments
packages and CRANtastic	https://www.r-bloggers.com/2009/08/packages-and-crantastic/	August 24, 2009	Nick Horton		 0 Comments
R help on StackOverflow	https://www.r-bloggers.com/2009/08/r-help-on-stackoverflow/	August 23, 2009	Rob J Hyndman	Ever since I began using R about ten years ago, the best place to find R help was on the R-help mailing list. But it is time-consuming searching through the archives trying to find something from a long time ago, and there is no way to sort out the good advice from the bad advice. But now there is a new tool and it is very neat. Head over to the R tag on StackOverflow. StackOverflow is a website for programming questions. It’s much better than a mailing list because it allows easy searching through answers, and voting on answers so that the best answers appear at the top. If you are a registered user (registration is free), you can vote on the answers that you find. The more votes your answers receive, the more privileges you have on the site. This is the web at its best! Here are my answers to date. It would be great if more R users migrated from the old R-help list to StackOverflow. 	 0 Comments
R handy for crunching data	https://www.r-bloggers.com/2009/08/r-handy-for-crunching-data/	August 23, 2009	Shige		 0 Comments
NpptoR: R in Notepad++	https://www.r-bloggers.com/2009/08/npptor-r-in-notepad/	August 22, 2009	Shige		 0 Comments
Test cointegration with R	https://www.r-bloggers.com/2009/08/test-cointegration-with-r/	August 21, 2009	Quantitative Finance Collector		 0 Comments
No success yet, then more animations…	https://www.r-bloggers.com/2009/08/no-success-yet-then-more-animations/	August 20, 2009	the R user...		 0 Comments
Simple logistic regression on qualitative dichotomic variables	https://www.r-bloggers.com/2009/08/simple-logistic-regression-on-qualitative-dichotomic-variables/	August 20, 2009	Todos Logos		 0 Comments
ggplot2 Version of Figures in “Lattice: Multivariate Data Visualization with R” (Part 13)	https://www.r-bloggers.com/2009/08/ggplot2-version-of-figures-in-%e2%80%9clattice-multivariate-data-visualization-with-r%e2%80%9d-part-13/	August 20, 2009	learnr	"This is the 13th post in a series attempting to recreate the figures in Lattice: Multivariate Data Visualization with R (R code available here) with ggplot2. Previous parts in this series: Part 1, Part 2, Part 3, Part 4, Part 5, Part 6, Part 7, Part 8, Part 9, Part 10, Part 11, Part 12. Topics covered:  
 lattice ggplot2       lattice ggplot2       lattice ggplot2    lattice ggplot2       lattice ggplot2       "	 0 Comments
2 Interesting animations…	https://www.r-bloggers.com/2009/08/2-interesting-animations/	August 19, 2009	the R user...		 0 Comments
Have you ever heard about the ‘animation package’?	https://www.r-bloggers.com/2009/08/have-you-ever-heard-about-the-animation-package/	August 18, 2009	the R user...		 0 Comments
ggplot2 Version of Figures in “Lattice: Multivariate Data Visualization with R” (Part 12)	https://www.r-bloggers.com/2009/08/ggplot2-version-of-figures-in-%e2%80%9clattice-multivariate-data-visualization-with-r%e2%80%9d-part-12/	August 18, 2009	learnr	"This is the 12th post in a series attempting to recreate the figures in Lattice: Multivariate Data Visualization with R (R code available here) with ggplot2. Previous parts in this series: Part 1, Part 2, Part 3, Part 4, Part 5, Part 6, Part 7, Part 8, Part 9, Part 10, Part 11. Topics covered:  
 lattice ggplot2 Note       lattice ggplot2       lattice ggplot2       lattice ggplot2       lattice ggplot2    lattice ggplot2    lattice ggplot2    lattice ggplot2       lattice ggplot2    lattice ggplot2       "	 0 Comments
R Coding Style Guide	https://www.r-bloggers.com/2009/08/r-coding-style-guide/	August 18, 2009	Stewart MacArthur		 0 Comments
The density function	https://www.r-bloggers.com/2009/08/the-density-function/	August 18, 2009	the R user...		 0 Comments
Social Network Analysis Resources for R	https://www.r-bloggers.com/2009/08/social-network-analysis-resources-for-r/	August 17, 2009	Jeromy Anglim		 0 Comments
Potential Loss of Arable Land in the Central San Joaquin Valley, CA	https://www.r-bloggers.com/2009/08/potential-loss-of-arable-land-in-the-central-san-joaquin-valley-ca/	August 17, 2009	dylan	Rapid urban and sub-urban expansion in the San Joaquin Valley have resulted in the loss of millions of acres of prime farmland in the last 100 years. Approximately 11% of class 1 (irrigated) land and 7% of class 2 land have already been paved over in the Fresno-Madera region (first image below). Recent projections in the expansion of urban areas in this region suggests that by 2085 an additional 28% of class 1 and 25% of class 2 land will be paved over (second image below). This is a preliminary summary– details to follow. Fresno Area Urban Areas vs Irrigated LCC: grey regions are current urban areas read more 	 0 Comments
High tech, low tech?	https://www.r-bloggers.com/2009/08/high-tech-low-tech/	August 15, 2009	Shige		 0 Comments
Cool articles in the New York Time’s: Statistics + R	https://www.r-bloggers.com/2009/08/cool-articles-in-the-new-york-time%e2%80%99s-statistics-r/	August 14, 2009	Vinh Nguyen	so these articles are ‘old news,’ but here i am to blog it down before i forget.  First article is entitled “For Today’s Graduate, Just One Word – Statistics,” and the second article is entitled “Data Analysts Captivated by R’s Power.” It really does feel re-enforcing and motivating when the NY Times write about your profession AND the tool you use! 	 0 Comments
Everybody loves R	https://www.r-bloggers.com/2009/08/everybody-loves-r/	August 14, 2009	the R user...		 0 Comments
First 15 min. volatility impact on the day’s volatility	https://www.r-bloggers.com/2009/08/first-15-min-volatility-impact-on-the-day%e2%80%99s-volatility/	August 14, 2009	kafka	"One night I was hooked on this post at elitetrader. I spent couple of hours to find, that a guy is cheating a bit and he didn’t reveal his strategy. Not big deal – free lunch is rear. So, I googled about that guy and one thing catch my attention. In one of his post he mentioned about a stock volatility in the first 15-30 min. and its persistent over whole session. By the way, his new strategy relies on some magic numbers, with I suspect is kind of volatility measurement.Let’s test it. The rules:
Get volatility of the first 15 or 30 min (1 min. bars used)
Get volatility of the day. (5 min. bars used)
Regress two results and check the correlation. At the beginning to get volatility I used standard deviation between closing prices. The result was bizarre. So, I used a few neurons in my brain and understood, that time series are definitely non-stationary and I need something else. I decided to use a spread between high of the bar and low of the bar and standard deviation to get volatility. Results: Stock1 
Correlation 0.798184
Residual Standard Error=0.0474
R-Square=0.6371
F-statistic (df=1, 241)=423.0905
p-value=0 Estimate Std.Err t-value Pr(>|t|)
Intercept   0.0042  0.0064  0.6606   0.5095
X           0.9452  0.0460 20.5692   0.0000  Stock2
Correlation 0.8830484
Residual Standard Error=0.0707
R-Square=0.7798
F-statistic (df=1, 262)=927.6893
p-value=0 Estimate Std.Err t-value Pr(>|t|)
Intercept   0.0292  0.0086  3.3704    9e-04
X           0.6868  0.0225 30.4580    0e+00  Summary
The correlation between first 15 min. volatility of the day and daily volatility is high. "	 0 Comments
Example 7.10: Get data from R into SAS	https://www.r-bloggers.com/2009/08/example-7-10-get-data-from-r-into-sas/	August 13, 2009	Ken Kleinman		 0 Comments
ggplot2 Version of Figures in “Lattice: Multivariate Data Visualization with R” (Part 11)	https://www.r-bloggers.com/2009/08/ggplot2-version-of-figures-in-%e2%80%9clattice-multivariate-data-visualization-with-r%e2%80%9d-part-11/	August 13, 2009	learnr	"This is the 11th post in a series attempting to recreate the figures in Lattice: Multivariate Data Visualization with R (R code available here) with ggplot2. Previous parts in this series: Part 1, Part 2, Part 3, Part 4, Part 5, Part 6, Part 7, Part 8, Part 9, Part 10. Topics covered:  
 lattice ggplot2       lattice ggplot2       lattice ggplot2       lattice ggplot2    lattice ggplot2       lattice ggplot2       "	 0 Comments
random 0.2.1	https://www.r-bloggers.com/2009/08/random-0-2-1/	August 12, 2009	Thinking inside the box	"
The new package is available
here and should
appear on CRAN shortly.

 "	 0 Comments
Completion for java objects	https://www.r-bloggers.com/2009/08/completion-for-java-objects/	August 9, 2009	romain francois	As indicated in this thread, completion after the dollar operator can be customized by defining a custom names method. Here I am showing how to take advantage of this to display fields and methods of java references (jobjRef objects from the rJava package) Here it is in action (I hit tab twice after the dollar sign) 	 0 Comments
Example 7.9: Get data from SAS into R	https://www.r-bloggers.com/2009/08/example-7-9-get-data-from-sas-into-r/	August 8, 2009	Ken Kleinman		 0 Comments
Trend Analysis with the Cox-Stuart test in R	https://www.r-bloggers.com/2009/08/trend-analysis-with-the-cox-stuart-test-in-r/	August 8, 2009	Todos Logos		 0 Comments
Two-way analysis of variance: two-way ANOVA in R	https://www.r-bloggers.com/2009/08/two-way-analysis-of-variance-two-way-anova-in-r/	August 7, 2009	Todos Logos		 0 Comments
Additive vs. dominance two-allele genetic model by DIC	https://www.r-bloggers.com/2009/08/additive-vs-dominance-two-allele-genetic-model-by-dic/	August 7, 2009	Gregor Gorjanc		 0 Comments
CacheSweave	https://www.r-bloggers.com/2009/08/cachesweave/	August 6, 2009	Shige		 0 Comments
R 64 bit on Mac OSX with ESS	https://www.r-bloggers.com/2009/08/r-64-bit-on-mac-osx-with-ess/	August 6, 2009	Matti Pastell	"So today I compiled 64 bit R 2.91 on Mac OS X   10.5.7 in order to get maximum speed for my analyses. The speed increase I got was 15-25 %, when I ran some tests with matrix calculation and intensive loops. Note that you can also download optimized binaries from  http://r.research.att.com/, although they are not the latest version. Update: This method also works with Snow Leopard 10.6.2 and R 2.10.0.

It took me some effort and googling to get it done so I’m going to post the instructions here, so that I know how to do it next time and I hope that it also benefits others. After I got the installation done I ran into a problem getting ESS to recognize my 64 bit version, but I managed to solve that as well. So here is how I did it: First I installed the necessary developer tools from http://r.research.att.com/tools/. I already had gcc 4.2 from apple so installed gfortran and libreadline readline-5.2-1-quad.tar.gz. Readline is a precompiled binary and you extract is using sudo tar fvxz readline-5.2-1-quad.tar.gz -C /  After installing the depencies I extracted R-2.9.1.tar.gz downloaded from cran and executed configure in the extracted directory with the following options (a bit modified from http://r.research.att.com/building.html): After running “make” and “make install” I had a working installation that can be started using: R –arch x86_64 Just running R tries to run 32 bit version of R which I don’t have, so I made an alias R=’R–arch=x86_64′ to .profile and R starts form terminal with just the familiar R. Ok, all was well so far but then I discovered that ESS can’t find R anymore, because it is trying to execute the 32 bit version and doesn’t work. So after looking at the ESS source I discovered that you can modify R startup options in the file  “essd-r.el“. I use Carbon Emacs, but this methods should work with other Emacs variants. If you are wondering how to install ESS on OS X have a look at this post. First give yourself a permission to modify the file: chmod u+w essd-r.el Then open it and look for a line that says: inferior-R-args ” ” ; add space just in case And chance it to: inferior-R-args “–arch x86_64 ” ; add space just in case Thats it, you should now have a working ESS set up with a 64 bit R!   "	 0 Comments
RSRuby in the IRB console	https://www.r-bloggers.com/2009/08/rsruby-in-the-irb-console/	August 6, 2009	nsaunders	"
R is terrific, of course, for all your statistical needs.  But those data structures!  “Everything is a list.”  Leading to such wondrous ways to access variables as “p 
 Sometimes, you want something more familiar.  An array, a hash, a hash of arrays.  Or, you may need to access R data in the language of your choice – e.g. as part of a Rails project. In Ruby, IRB is your friend.  On the right, an IRB session in which we invoke RSRuby, load the GEOquery library from Bioconductor, fetch a dataset from the GEO database and examine the metadata that describes the experiment.  Result:  a ruby hash of arrays, where the keys are covariate types (“sample, disease.state, description”) and the values covariate names for each column (sample) in the dataset.  Now easy to access using: "	 0 Comments
Code Snippet : List of CRAN packages	https://www.r-bloggers.com/2009/08/code-snippet-list-of-cran-packages/	August 5, 2009	romain francois	This is a really simple code snippet that shows how to get the list of CRAN packages and their titles from the html page html page (toulouse mirror in this example). … Note that R has the available.packages function, but it does not give the titles of the packages 	 0 Comments
Locate the position of CRAN mirror sites on a map using Google Maps	https://www.r-bloggers.com/2009/08/locate-the-position-of-cran-mirror-sites-on-a-map-using-google-maps/	August 5, 2009	Paolo		 0 Comments
My Procedure for Upgrading R: Windows XP with StatET	https://www.r-bloggers.com/2009/08/my-procedure-for-upgrading-r-windows-xp-with-statet/	August 5, 2009	Jeromy Anglim		 0 Comments
Contingency table and the study of the correlation between qualitative variables: Pearson’s Chi-squared test	https://www.r-bloggers.com/2009/08/contingency-table-and-the-study-of-the-correlation-between-qualitative-variables-pearsons-chi-squared-test/	August 4, 2009	Todos Logos		 0 Comments
State of the Art in Parallel Computing with R: Now published	https://www.r-bloggers.com/2009/08/state-of-the-art-in-parallel-computing-with-r-now-published/	August 4, 2009	Thinking inside the box		 0 Comments
R parser package on CRAN	https://www.r-bloggers.com/2009/08/r-parser-package-on-cran/	August 4, 2009	romain francois	"The parser package has been released to CRAN, the package mainly defines a function parser that is similar to the usual R function parse, with the few following differences:  Here is an example file containing R source code that we are going to parse with parser It is a very simple file, for illustration purpose. Let's look what to do with it with the parser package The parser generates a list of expressions, just like the regular parse function, but the gain is the data attribute. This is a data frame where each token of the parse tree is a line. The id column identifies each line, and the parent column identifies the parent of the current line.  At the moment, only the forthcoming highlight package uses the parser package (actually the parser package has been factored out of highlight), but some anticipated uses of the package include:
 "	 0 Comments
Transfer files through Rserve	https://www.r-bloggers.com/2009/08/transfer-files-through-rserve/	August 4, 2009	romain francois	This post is motivated by this question on R-help. This is a simple java class that sends files through Rserve using the classes RFileInputStream and RFileOutputStream Then we create a simple file on the client machine:  And we are good to go:  Now in the directory /tmp/Rserv/conn6 of the server, there are the files “serverfile.txt” and “file.txt” and on the client there is also the “file.txt”  	 0 Comments
My mi presentation at the useR! 2009	https://www.r-bloggers.com/2009/08/my-mi-presentation-at-the-user-2009/	August 4, 2009	Yu-Sung Su		 0 Comments
Non-parametric methods for the study of the correlation: Spearman’s rank correlation coefficient and Kendall tau rank correlation coefficient	https://www.r-bloggers.com/2009/08/non-parametric-methods-for-the-study-of-the-correlation-spearmans-rank-correlation-coefficient-and-kendall-tau-rank-correlation-coefficient/	August 3, 2009	Todos Logos		 0 Comments
Rcpp 0.6.6	https://www.r-bloggers.com/2009/08/rcpp-0-6-6/	August 3, 2009	Thinking inside the box	"

While the diffstat output that will appear in
cranberries may look impressive,
it stems chiefly from updating the Doxygen documentation of the C++ classes.
Only one small feature was added: one can now test via exists()
whether a named parameter exists in an instance of the RcppParams class.

 "	 0 Comments
R GUI page on the R wiki	https://www.r-bloggers.com/2009/08/r-gui-page-on-the-r-wiki/	August 3, 2009	romain francois	I’ve started the process of moving the content of this page to the R wiki. The motivation is that the content will become dynamic and updated much more often, people can add their own project, we can have use cases of each gui, tutorials, feature comparison, … When we are ready, we will add an entry for the jedit plugin 	 0 Comments
Parametric method for the study of the correlation: the Pearson r-test	https://www.r-bloggers.com/2009/08/parametric-method-for-the-study-of-the-correlation-the-pearson-r-test/	August 3, 2009	Todos Logos		 0 Comments
useR! 2009 conference, Rennes, France	https://www.r-bloggers.com/2009/08/user-2009-conference-rennes-france/	August 2, 2009	Yu-Sung Su		 0 Comments
Example 7.8: Plot two empirical cumulative density functions using available tools	https://www.r-bloggers.com/2009/08/example-7-8-plot-two-empirical-cumulative-density-functions-using-available-tools/	August 1, 2009	Ken Kleinman		 0 Comments
R: The Dummies Package	https://www.r-bloggers.com/2009/09/r-the-dummies-package/	September 30, 2009	Christopher Brown	"R-2.9.2 was released in August.   While R can be considered stable and battle-ready, it is also far from stagnation.  It is humbling to see such an intelligent and vibrant community helping CRAN grow faster than ever.   Every day I see a new package or read a new comment on R-Help gives me pause to think. As much as I like R, on occasion I will find myself lost in some dark corner.  Sometimes, I find light.  Sometimes I am gnashing teeth and wringing hands.  Frustrated.  In a recent foray, I found myself trying to do something that I thought exceedingly trivial: expanding character and factor vectors to dummy variables.  There must be some function, but what?   Trying ?dummy didn’t turn up anything.  Surely some else must have encountered this and provided a package.   I went to the Internet and sure enough the R-wiki was here to save me.  And looking even harder, I found some who had treaded before me on the R-Help archives.  It turns out, it’s simple.  Expanding a variable as a dummy variable can be done like so: 

x <- c(2, 2, 5, 3, 6, 5, NA)

xf <- factor(x, levels = 2:6)

model.matrix( ~ xf - 1)

 Two problems.  The first problem is that without an external source (Google), I would have never stumbled upon what I wanted.  ( Thanks Google!)  I understand it now, but for what I wanted to do, I would never have thought, “oh, model.matrix.” The second problem is the arcane syntax, wtf <- ~ xf - 1.  I get it now, but it took me some time to figure out what was going on.  I get it, but why not just dummy(var)?  This is what I want to do. The solution on the wiki wasn’t quite what I was looking for.  For instance, you can’t say: model.matrix( ~ xf1 + xf2 + xf3- 1) It turns out, you can only expand one variable at a time.  Well, this is not good.  I know that you could solve this with some sapply’s and some tests, but next time I might forgot about how to do it.  So with a couple of spare hours, I decided that the next guy, wouldn’t have to think about it.  He could just use my dummies package. Like the R-wiki solution, the dummies package provides a nice interface for encoding a single variable.  You can pass a variable -or- a variable name with a data frame.  These are equivalent: 

dummy( df$var )

dummy( ""var"", df )

 Moreover, you can choose the style of the dummy names, whether to include unused factor level, to have verbose output, etc. But more than the R-wiki solution, dummy.data.frame offers to something similar to data.frames.  You can specify which columns to expand by name or class and whether to return non-expanded columns. The package dummies-1.04 is available in CRAN.  Comments and questions are always appreciated. "	 0 Comments
Measuring performance of functions in R	https://www.r-bloggers.com/2009/09/measuring-performance-of-functions-in-r/	September 30, 2009	[email protected]		 0 Comments
Examples using R – Analysis of Variance	https://www.r-bloggers.com/2009/09/examples-using-r-analysis-of-variance/	September 30, 2009	R – StudyTrails	> cotton$p15[1] 7 7 15 11 9$p20[1] 12 17 12 18 18$p25[1] 14 18 18 19 23$p30[1] 19 25 22 19 23$p35[1] 7 10 11 15 11A box plot of the data isA histogram of data looks like A multiple scatter plot can sometimes be used if corresponding values of the observations need comparison. The scatter plot for this data is as shown. Analysis of Variance:Lets use analysis of variance in the above example to find out if all means are equal or if any mean is different.The data needs to be transformed for aov > c(cotton_matrix[1,],cotton_matrix[2,],cotton_matrix[3,],cotton_matrix[4,],cotton_matrix[5,])->cotton_data> cotton_datap15 p20 p25 p30 p35 p15 p20 p25 p30 p35 p15 p20 p25 p30 p35 p15 p20 p25 p30 p35 p15 p20 p25 p30 p357 12 14 19 7 7 17 18 25 10 15 12 18 22 11 11 18 19 19 15 9 18 23 23 11Analysis of variance yields > summary(aov(cotton_data~names(cotton_data))) from the F value we reject the null hypothesis and conclude that the means differ. Analysis of variance uses certain assumptions and it is important to check the validity of these assumptions. The first method is to analyse the residuals for each observations. There should be no pattern in the residuals. If residuals either spread out or narrow down as time progresses then this could be an experimental error.Here’s a plot of residuals against time (observation) Another validation is to check the nature of the residuals themselves. One way to do is to plot of curve of residuals versus the fitted values.Here again no pattern should be present The variance for the five sets can be compared using the Bartlett’s test> bartlett.test(cotton_data~factor(names(cotton_data)))Bartlett test of homogeneity of variancesdata: cotton_data by factor(names(cotton_data))Bartlett’s K-squared = 0.2801, df = 4, p-value = 0.991The results show that the null hypothesis cannot be rejected and hence the variance of the five sets is indeed same. We now need to do a pairwise comparison to find out which pair has a difference in mean. we use the Tukey’s test to do so. If the assumption of normality is not met then a test known is Kruskal-Wallis test may be used > kruskal.test(cotton_data~factor(names(cotton_data)))Kruskal-Wallis rank sum testdata: cotton_data by factor(names(cotton_data))Kruskal-Wallis chi-squared = 18.5513, df = 4, p-value = 0.0009626 	 0 Comments
Example 7.14: A simple graphic of sales	https://www.r-bloggers.com/2009/09/example-7-14-a-simple-graphic-of-sales/	September 29, 2009	Ken Kleinman		 0 Comments
RInside release 0.1.1, and a fresh example	https://www.r-bloggers.com/2009/09/rinside-release-0-1-1-and-a-fresh-example/	September 29, 2009	Thinking inside the box	"
However, today I committed a new example to 
SVN archive at R-Forge.
It is based on this
thread on
r-devel. Abhijit Bera tries to do this in C, but to me his questions provide rather clear motivation for showing how much
simpler things can be via C++ and the Rcpp
classes along with RInside. 
Using a small example, the task was to pass a weight vector to a portfolio solver 
from the Rmetrics package fPortfolio
and to then access the computed solution. The original poster struggled with
access from C to the S4 classes used by fPortfolio and could not set the
weights. But when using 
RInside, we
simply pass a C++ vector of weights down to R, solve the problem and pass a
solution vector back using the handy evaluation of R expressions:


 "	 0 Comments
WordPress Blogging with R in 3 Steps	https://www.r-bloggers.com/2009/09/wordpress-blogging-with-r-in-3-steps/	September 29, 2009	learnr	"A few people have emailed me and enquired about the use of tools mentioned at the end of this post to make blogposts with embedded R-commands. Below is a small step-by-step walkthrough of how to accomplish this.  
 The asciidoc file workflow.txt looks like this: Note will render the following entry: First we will display all the letters. And then only the first five letters of the alphabet. "	 0 Comments
Survive R	https://www.r-bloggers.com/2009/09/survive-r/	September 29, 2009	John Mount	"New PDF slides version (presented at the Bay Area R Users Meetup October 13, 2009). We at Win-Vector LLC appear to like R a bit more than some of our, perhaps wiser, colleagues ( see: Choose your weapon: Matlab, R or something else? and R and data ).  While we do like R (see: Exciting Technique #1: The “R” language ) we also understand the need to defend oneself against the abuse regularly dished out by R.  Here we will quickly share a few fighting techniques.
 If you are not already using R the following will not mean much.  If you are using R this may scratch a few itches.    The model is now a harmless list without a bunch of pesky methods hiding the information.
  

# lets make a tricky function
> fe <- function(x) UseMethod(""fe"")
> fe.formula <- function(x) { print('formula')}
> fe.numeric <- function(x) { print('numeric')}


 How will anyone figure out what we have done?   

> class(fe)
[1] ""function""

> methods(fe)
# [1] fe.formula fe.numeric

> getS3method('fe','numeric')
# fe.numeric <- function(x) { print('numeric')}



  

> m <- summary(c(1,2))[4]
> m
Mean
 1.5

 Ah that’s cute: a little “Mean” tag is following the data around.  But what if we try to use this value: 

> m*m
Mean
2.25

 Okay, now the “Mean” tag has outstayed its welcome.  The fix: 

> attributes(m) <- c()
> m*m
[1] 2.25

 MUCH better.
  

> map <- new.env(hash=TRUE,parent=emptyenv())
> assign('dog',7,map)
> ls(map)
[1] ""dog""
> get('dog',envir=map)
[1] 7

 That (nearly) gives you maps with string keys.  For maps with numeric keys we can fake something else up with findInterval().  For maps from generic comparable objects keys- I have no idea how you would trick R into helping.  This is one reason we like to separate out all data-preparation into a pre-processing step implemented in Java or SQL. Note important correction from Eward Ratzer: use “map 
  The overall all point is that while R has some (unnecessarily) sharp edges and pain-points it is a powerful tool worth using.  I would much rather struggle through a minor R-language issue when trying to prepare my data than to do without the many special functions, distributions, fitters and plotters built into the R system. Related posts: "	 0 Comments
Blogs on R | Statistics	https://www.r-bloggers.com/2009/09/blogs-on-r-statistics/	September 28, 2009	Jeromy Anglim		 0 Comments
Adjusting Correlations for Reliability | Attenuation Formula	https://www.r-bloggers.com/2009/09/adjusting-correlations-for-reliability-attenuation-formula/	September 28, 2009	Jeromy Anglim		 0 Comments
Psychology Statistics 101 | R or SPSS	https://www.r-bloggers.com/2009/09/psychology-statistics-101-r-or-spss/	September 28, 2009	Jeromy Anglim		 0 Comments
How to Import MS Excel Data into R	https://www.r-bloggers.com/2009/09/how-to-import-ms-excel-data-into-r/	September 26, 2009	Yihui Xie	"He should have added the last sentence if he were a Windows user in this age. A lot of R users often ask this question: “How to import MS Excel data into R?” Well, my suggestion is, avoid using M$ Excel if you are a statistician (or going to be a statistician) because you just cannot imagine how messy Excel data can be: some cells might be merged, some are colored, some texts are bold, several data tables can be put everywhere (e.g. cell(1,1) to (10,4), and (17,3) to (25,9)), stupid bar plots and pie charts are inserted in the sheets, silly statistical procedures that are wrong forever… If you don’t trust my words (yes, I’m a nobody), just read the examples here: Problems with Excel (collected by Prof Harrell). I know there are reasons for you to continue using Excel. Your boss required you to do so; you don’t have time to learn more about various data formats; everybody is using Excel, and you don’t want to be so cool to use R; or if you finish your tasks too quickly and accurately, your boss will doubt whether you have really spent time on working, hence you will get less money paid (this is a REAL story for me – though I didn’t get less payment, I was indeed doubted when I used R); … A quick solution to the problem is to save your Excel data in a pure text format, e.g. CSV (comma-separated value) or tab-delimited. If you have ever thumbed through Dr Murrell’s book “Introduction to Data Technologies”, you probably know that the CSV format is NOT an Excel-specific format, although Windows users always find the Excel icon is associated with the *.csv files. Pure text is a ridiculously simple data format, but it’s amazing that there are still many people who do not know anything about it. The basic idea is to separate data columns with a delimiter (e.g. “,” or “;”) and rows with a usual line-break symbol (e.g. carriage-return, which can be different in Windows and Linux). In this case, we can identify all data values as we do in the spreadsheet. Here is an example with data in a spreadsheet:  Data in the Spreadsheet If we save this data as a CSV file, and open it with a pure text editor (e.g. Notepad), we will see: Or save as tab-delimited text: Then use read.table() or read.csv() in R to read these pure text files (as data.frames). A hint for lazy users: you can also select all the data cells, copy it (into clipboard) and use read.table(""clipboard"") to get the data into R. In this case, what exists in your clipboard is the tab-delimited pure text. All right, you don’t bother to save the excel sheet into pure text and even don’t want to copy it into clipboard, then you can treat Excel files as databases, although they are indeed bad databases. You must guarantee that the data is “clean” and well-formatted, i.e. observations in each row and variables in each column (no merged cells, better no graphs). We can use the RODBC package to establish a connection to the Excel file, and execute SQL commands in the connection to make queries to data. Functions related to this task are odbcConnectExcel() or odbcConnectExcel2007() (again, Excel is stupid — they always change the standard in order that their products can be inconsistent). This is described in details in the manual R-data (“R Data Import/Export”). As *.xls (or *.xlsx) is a binary format, never try to read.table(""*.xls""). Meanwhile, read.xls() in the gdata package might be what you want if you are looking for the read.*-style R functions. [Thanks, Doug!] In most cases, pure text format suffices to work, although it is ridiculously simple. Take a look at the “source code” and you will know everything. By the way, the extension of a file name is not that important: *.csv does not have to be a comma-separated text file, and *.doc can be something other than a Word document. It’s just a matter of convention. Again, open it and see what on earth is inside. "	 0 Comments
Formatting a Table in Word | R to Tab-Delimited to APA Style	https://www.r-bloggers.com/2009/09/formatting-a-table-in-word-r-to-tab-delimited-to-apa-style/	September 25, 2009	Jeromy Anglim		 0 Comments
Non-parametric Methods	https://www.r-bloggers.com/2009/09/non-parametric-methods/	September 24, 2009	rtutor.chiyau	"

A statistical method is called non-parametric if it makes no assumption on the
population distribution or sample size.
    This is in contrast with most parametric methods in elementary statistics that
assume the data is quantitative, the population has a normal distribution and the
sample size is sufficiently large.
 read more "	 0 Comments
digest 0.4.0 and 0.4.1	https://www.r-bloggers.com/2009/09/digest-0-4-0-and-0-4-1/	September 24, 2009	Thinking inside the box	"
The new package will appear shortly on its 
CRAN page; the 
R-Forge page has
access to the Subversion repo etc and otherwise there’s always my local 
my local digest page.

 
Oh, and 0.3.1 was never blogged about as it was just a maintenance
release that added a single cast and an edit to the DESCRIPTION file.

 "	 0 Comments
Example 7.13: Read a file with two lines per observation	https://www.r-bloggers.com/2009/09/example-7-13-read-a-file-with-two-lines-per-observation/	September 24, 2009	Nick Horton		 0 Comments
R Function of the Day: cut	https://www.r-bloggers.com/2009/09/r-function-of-the-day-cut-2/	September 23, 2009	erik	"
This post originally appeared on my WordPress blog on September 23, 2009. I present it here in its original form.
 
The R Function of the Day series will focus on describing in plain language how certain R functions work, focusing on simple examples that you can apply to gain insight into your own data. 
 
Today, I will discuss the cut function. 
 
In many data analysis settings, it might be useful to break up a continuous variable such as age into a categorical variable.  Or, you might want to classify a categorical variable like year into a larger bin, such as 1990-2000.  There are many reasons not to do this when performing regression analysis, but for simple displays of demographic data in tables, it could make sense.  The cut function in R makes this task simple! 
 
First, we will simulate some data from a hypothetical clinical trial that includes variables for patient ID, age, and year of enrollment. 
 
Now, we will use the cut function to make age a factor, which is what R calls a categorical variable. Our first example calls cut with the breaks argument set to a single number.  This method will cause cut to break up age into 4 intervals.  The default labels use standard mathematical notation for open and closed intervals.
 
Well, the intervals that cut chose by default are not the nicest looking with the age example, although they are fine with the year example, since it was already discrete. Luckily, we can specify the exact intervals we want for age.  Our next example shows how. 
 
That looks pretty good.  There is no reason that the breaks argument has to be equally spaced as I have done above.  It could be any grouping that you want. 
 
Finally, I am going to show you an example of a custom R function to categorize ages.  It uses cut inside of it, but does some preprocessing and uses the labels argument to cut to make the output look nice.  
 
This function categorizes age in a fairly flexible way.  The first assignment to labs inside the function creates a vector of labels. Then, the cut function is called to do the work, with the custom labels as an argument.  Here are some examples using our simulated data from above. I am no longer going to save the results of the function calls to a variable and call table on them, but rather just nest the call to age.cat in a call to table. I previously did a post on the table function. 
 
The cut function is useful for turning continuous variables into factors.  You saw how to specify the number of cutpoints, specify the exact cutpoints, and saw a function built around cut that simplifies  categorizing an age variable and giving it appropriate labels. 
 "	 0 Comments
ggplot2: Back-to-back Bar Charts	https://www.r-bloggers.com/2009/09/ggplot2-back-to-back-bar-charts/	September 23, 2009	learnr	"On the ggplot2 mailing-list the following question was asked: How to create a back-to-back bar chart with ggplot2? For anyone who don’t know what I am talking about, have a look on a recent paper from the EU. I’d like to create plots like the graphs 5,6,18 in the paper. 
 An example graph from the above report is below: Let’s create the same graph in ggplot2.  
 I was not able to find the exact dataset used to plot the graph above, and used instead the Eurostat “EU27 trade by BEC product group since 1999″  dataset which has a very similar data structure. Access the subset used in this post in here. Calculate monthly total trade balances. Convert data from wide format to long format for plotting. Add horizontal line at 0 for easier reading, plot the trade balance line and make the x-axis labels more readable. The legend explanation (too large to fit on the plot): There shouldn’t be any negative numbers on the y-axis, we need a custom formatter to convert these to positive. I have to agree with some of the posters on the mailing list, that this type of chart is not very easy to follow – it is impossible to compare individual values. Another useful way of showing information would be to use facets: Or combine export and import values on one plot showing the comparative trend between the different goods categories. The same graph with varying scales across panels. Update: As suggested in the comments it would be good to have total values added to the last plot. In order to do so the totals are first calculated and then appended to the original dataframe. Plotting the new dataframe is just a question of changing the source of the last plot. "	 0 Comments
High yield spread	https://www.r-bloggers.com/2009/09/high-yield-spread/	September 23, 2009	kafka	"Last week I had opportunity to participate at NYU, where one of the lecturers was Z-score man’s. He spent same time talking about the high yield spread as measure of the macro economy. Then spread is high (junk bonds become very cheap and Treasuries become ‘gold’) –  crisis is right here. Intuitively, it is similar to VIX index, but the way of building it is different.
To build this indicator I took Barclays Capital High Yield Bond ETF and subtracted SPDR Barclays Capital International Treasury Bond ETF to get spread. Keep in mind, that maturities of two ETF are not the same… The result is:  It is useful to keep eye on it, together with VIX indicator. "	 0 Comments
R Sessions 33: Select (nested) observations with equal number of occurences	https://www.r-bloggers.com/2009/09/r-sessions-33-select-nested-observations-with-equal-number-of-occurences/	September 23, 2009	Rense Nieuwenhuis	"Recently, I was contacted with an question about R code. A befriended researcher was working with nested data, which was unbalanced. He was working with data in a ‘long’ format: all observations nested within the same group had the same identification number. But, the number of observations in each of the groups differed (hence: unbalanced data). He asked me for a piece of code that creates a subset of the data that is balanced, i.e. all observations that are nested within equally sized groups. Or, as an alternative, all observations nested within groups with at least a minimum number of observations. I solved it the quick and dirty way, and the solution involves creating additional variables, a new data.frame, and merging. It sure can be done much prettier, but it works.  So, I share it below:
 

id <- c(""a"", ""b"",""b"", ""c"",""c"",""c"", ""d"",""d"",""d"",""d"", ""e"",""e"",""e"")

y <-  c(3,4,3,2,4,5,6,5,6,7,5,4,3)

df <- data.frame(id, y) # setting up original data.frame
tab <- data.frame(id=names(table(df$id)), fre=as.vector(table(df$id))) # table of frequencies
df.new <- merge(df, tab, by=""id"") # merging frequencies-variable
subset(df.new, fre==3) # subsetting

subset(df.new, fre>3)

 "	 0 Comments
RGG #158:161: examples of package IDPmisc	https://www.r-bloggers.com/2009/09/rgg-158161-examples-of-package-idpmisc/	September 23, 2009	romain francois	three new graphs have made their way to the graph gallery, submitted by Reto Burgin Image lag plot matrix Image scatter plot matrix Regular time series 	 0 Comments
R Function of the Day: rle	https://www.r-bloggers.com/2009/09/r-function-of-the-day-rle-2/	September 22, 2009	erik	"
Edit: This post originally appeared on my WordPress blog on September 22, 2009. I present it here in its original form.
 
The R Function of the Day series will focus on describing in plain language how certain R functions work, focusing on simple examples that you can apply to gain insight into your own data. 
 
Today, I will discuss the rle function. 
 
The rle function is named for the acronym of “run length encoding”. What does the term “run length” mean?  Imagine you flip a coin 10 times and record the outcome as “H” if the coin lands showing heads, and “T” if the coin lands showing tails.  You want to know what the longest streak of heads is.  You also want to know the longest streak of tails.  The run length is the length of consecutive types of a flip.  If the outcome of our experiment was “H T T H H H H H T H”, the longest run length of heads would be 5, since there are 5 consecutive heads starting at position 4, and the longest run length for tails would be 2, since there are two consecutive heads starting at position 2. If you just have 10 flips, it is pretty easy to simply eyeball the answer.  But if you had 100 flips, or 100,000, it would not be easy at all.  However, it is very easy with the rle function in R! That function will encode the entire result into its run lengths.  Using the example above, we start with 1 H, then 2 Ts, 5 Hs, 1 T, and finally 1 H.  That is exactly what the rle function computes, as you will see below in the example.
 
First, we will simulate the results of a the coin flipping experiment.  This is trivial in R using the sample function. We simulate flipping a coin 1000 times. 
 
We can see the results of the first 20 tosses by using the head (as in “beginning”, nothing to do with coin tosses) function on our coin vector. 
 
So, our question is, what is the longest run of heads, and longest run of tails?  First, what does the output of the rle function look like? 
 
So in this case the longest run of heads is 9 and the longest run of tails is 8.  The tapply function was discussed in a previous R Function of the Day article. 
 
The rle function performs run length encoding.  Although it is not used terribly often when programming in R, there are certain situations, such as time series and longitudinal data analysis, where knowing how it works can save a lot of time and give you insight into your data. 
 "	 0 Comments
Emacs Key Binding for eval-defun in lisp-mode	https://www.r-bloggers.com/2009/09/emacs-key-binding-for-eval-defun-in-lisp-mode/	September 22, 2009	erikr	"
When I use R in Emacs through the ESS package, C-c C-c in a .R buffer will send a “block” of code to the inferior R process for evaluation.  This was added just a few years ago, but my fingers are now trained to use that key combination for evaluating any block of code.  Since I have been learning Emacs Lisp, I decided that a good idea would be to make C-c C-c a binding to eval-defun.  I really like how it is working out as I have to redefine my Lisp functions many times! 
 
Just put the following in your .emacs file to get this behavior. However, please note the following from the eval-defun help string, “If the current defun is actually a call to `defvar’ or `defcustom’, evaluating it this way resets the variable using its initial value expression even if the variable already has some other value. (Normally `defvar’ and `defcustom’ do not alter the value if there already is one.)”
 "	 0 Comments
SAS: “The query requires remerging summary statistics back with the original data”	https://www.r-bloggers.com/2009/09/sas-%e2%80%9cthe-query-requires-remerging-summary-statistics-back-with-the-original-data%e2%80%9d/	September 22, 2009	heuristicandrew	Coming from a background writing SQL code directly for “real” RDBMS (Microsoft SQL Server, MySQL, and SQLite), I was initially confused when SAS would give me the following ‘note’ for a simple summary PROC SQL query: The same code executes fine in “real” RDBMS.  As is often the case, SAS has its own way of doing things.  SAS expects its special keyword calculated in each summary/grouping statistic like below.  The calculated keyword simply refers to the value calculated in the SELECT section. So how does SAS optimize such a simple “group by” query when sending it to a RDBMS?  It doesn’t!  Below is a simple query to pull monthly phone call volume from CIC 2.4.  Notice in the log I turned on SQL tracing, and SAS shows it pulls individual records instead of letting Microsoft SQL Server produce the summary.  SAS complains about an error, but it doesn’t describe it.  In SAS’s defense, it is probably confused because the date-type is a date-time type instead of a date-only data type, but Microsoft SQL Server doesn’t support date-only data types.  A pass-through query would be faster for large data sets, but then you lose some of the usefulness of PROC SQL. 	 0 Comments
R Function of the Day: table	https://www.r-bloggers.com/2009/09/r-function-of-the-day-table/	September 21, 2009	erikr	"
The R Function of the Day series will focus on describing in plain language how certain R functions work, focusing on simple examples that you can apply to gain insight into your own data.
 
Today, I will discuss the table function.
 
The table function is a very basic, but essential, function to master while performing interactive data analyses.  It simply creates tabular results of categorical variables.  However, when combined with the powers of logical expressions in R, you can gain even more insights into your data, including identifying potential problems.
 
We want to know how many subjects are enrolled at each center in a clinical trial.
 
We want to know how many subjects are under the age of 60 in a clinical trial.
 
Which center has the most subjects with a missing value for age in the clinical trial?
 
The table function simply needs an object that can be interpreted as a categorical variable (called a “factor” in R).
 
Example 1 is the most trivial.  We want to know how many subjects are enrolled at each center, so simply pass in the variable “center” to the table function.
 
For example 2, we need to create a logical vector indicating whether or not a patient is under 60 or not.  We can then pass that into the table function.  Also, since there are missing ages, we might be interested in seeing those in the table also.  It is shown both ways by setting the “useNA” argument to table.
 
Example 3, finding the center that has the most missing values for age, sounds the trickiest, but is once again an extremely simple task with the table function.  You just need to know that the is.na function returns a logical vector that indicates whether an observation is missing or not.
 
Although table is an extremely simple function, its use should not be avoided when exploring a dataset.  These examples have shown you how to use table on variables in a dataset, and on variables created from a logical expression in R. The “useNA” argument was also introduced.
 "	 0 Comments
Comparison of plots using Stata, R base, R lattice, and R ggplot2, Part I: Histograms	https://www.r-bloggers.com/2009/09/comparison-of-plots-using-stata-r-base-r-lattice-and-r-ggplot2-part-i-histograms/	September 21, 2009	Stephen Turner		 0 Comments
Linking text, results, and analyses: Increasing transparency and efficiency	https://www.r-bloggers.com/2009/09/linking-text-results-and-analyses-increasing-transparency-and-efficiency/	September 20, 2009	Jeromy Anglim		 0 Comments
R Function of the Day: tapply	https://www.r-bloggers.com/2009/09/r-function-of-the-day-tapply-2/	September 20, 2009	erik	"
The R Function of the Day series will focus on describing in plain language how certain R functions work, focusing on simple examples that you can apply to gain insight into your own data.
 
Today, I will discuss the tapply function.
 
In statistics, one of the most basic activities we do is computing summaries of variables.  These summaries might be as simple as an average, or more complex. Let’s look at some simple examples.
 
When you read the results of a medical trial, you will see things such as “The average age of subjects in this trial was 55 years in the treatment group, and 54 years in the control group.”
 
As another example, let’s look at one from the world of baseball.
 
Batting Leaders per Team
 
These two examples have a lot in common, even if they don’t appear to when first reading.  In the first example, we have a dataset from a medical trial.  We want to break up the dataset into two groups, treatment and control, and then compute the sample average for age within each group.
 
In the second example, we want to break up the dataset into 30 groups, one for each MLB team, and then compute the maximum batting average within each group.
 
So what is in common?
 
In each case we have
 The following table summarizes the situation.
 
The tapply function can solve both of these problems for us!
 
The tapply function is simple to use.  First, we will generate some data.
 
Now we have some sample data.  Using tapply is now straightforward.  In general, the call to the function will look like the example in the first comment.  Then, actual calls to the function using the data we defined above are shown. 
 
The tapply function is useful when we need to break up a vector into groups defined by some classifying factor, compute a function on the subsets, and return the results in a convenient form.  You can even specify multiple factors as the grouping variable, for example treatment and sex, or team and handedness.  
"	
Welcome to Sigmafield	https://www.r-bloggers.com/2009/09/welcome-to-sigmafield/	September 20, 2009	erik	"
Edit: This post originally appeared on my WordPress blog on September 20, 2009. I present it here in its original form.  
John Tukey’s preface to Exploratory Data Analysis begins with a useful rule, “It is important to understand what you can do before you learn to measure how well you seem to have done it.” When I decided I wanted to start a blog concentrating on statistics, R, and Emacs, I thought I had better learn first what I can do to make the process of generating content easier. Here, as a meta first post, I present how I used Emacs, R, and other technologies to produce the output you are reading here.
 
I use Emacs for everything I can. I started learning it as a graduate student in Statistics in order to use the ESS package. ESS lets you run R within Emacs, which is really nice for developing programs and performing data analysis interactively. I have discovered many Emacs packages through the years, but lately I have been learning the excellent org-mode package. Org-mode is designed so that you can get started with very minimal knowledge of its capabilities.  You can ignore the more complicated aspects of it if you do not need them, and still have a great system for general organizational tasks and note taking. I think it is worth learning Emacs just so you can use org-mode!
 
Since I am so familiar with Emacs, and work in it daily, I really wanted to use it to write posts. Ideally, I would like to write my blog entries in org-mode, and be able to use in-line R code. The text and R code should be output to HTML and published to my blog. This HTML generation is possible because org-mode has a fantastic feature that exports to various formats, including HTML.  This is beneficial since then all of my headlines, lists, tables, and source code markup in org-mode will be carried over to the HTML automatically.
 
Using the tools already available for Emacs, creating the HTML turned out to be much easier than I initially thought.  The vast majority of the time preparing this was spent learning about those tools, and figuring out how to combine them in the right way to accomplish what I wanted to do.
 
To summarize, my goals were to:
 
I really enjoy using R.  Since I plan to write about statistics, and especially statistical computing, I imagine many of my posts will contain R code. R comes with an interesting function called Sweave.  Sweave allows you to incorporate R commands within a document you are writing wrapped in a special syntax, see some demos.  So you might be writing some $latex \LaTeX$ or HTML and interweaving R code. After you run the Sweave function using the source (e.g., HTML) document as the input, a new file is created that replaces the R code with the results of the commands.  This might sound simple, but it leads to a very powerful model for report generation and reproducible research. My goal was to somehow get publishable HTML from a source org-mode file after running Sweave on it. For example, say I want to generate 100 samples from a normal distribution with mean 10, and summarize the results.
 
I do not want to have to actually type any of the summary output, of course, but I also don’t have to even copy and paste it.  What I can type is the R code to produce the output, and wrap it in Sweave syntax, like this: 
 
When I prepare this post for publishing, Sweave will run the R code and insert the output in place of the Sweave commands.  I have wrapped the Sweave syntax in org-mode's special BEGIN SRC construct, so that when I export to HTML, org-mode will properly syntax highlight the output using the htmlize package, as inspired by this post on using the htmlize package with Erlang.  The only elisp I had to write was the following. 
 
All the first function is doing is running an input file (an org-mode file) through Sweave with the RweaveAsciidoc driver found in the R ascii package. It basically just puts in the R output as plain text, as opposed to $latex \LaTeX$ code or HTML.
 
The rest of the elisp function manipulates the output in some trivial ways. The explanation for the replacement of the four dashes is that the RweaveAsciidoc Sweave driver produces the dashed string both before and after the R output as a visual offset, since asciidoc uses that as a markup indicator.  I did not need that, since my output will not be fed through asciidoc, but rather the org-mode HTML exporter, so I simply replace the dashes found at the beginning of a line with the empty string. The last trick I had to do was to replace the R syntax highlighting with R-transcript syntax highlighting, since the results of Sweave are essentially a transcript of the R commands entered, not the actual R code. Finally, the resulting HTML produced by the org exporter has its pre tags modified with custom colors and font size. 
 
There are a few loose ends that I didn't have time to clean up yet. I would like to modify my Lisp function to work on regions if mark is set and transient-mark-mode is enabled.  As of now, it just works on the whole buffer.  The idea would be to have an org-mode file, say blog.org, that would have a top-level headline for each entry. You could then highlight that entry and publish it.  I realize this is fairly trivial to implement, but it is not done yet. 
 
I also want to investigate the publishing feature of org-mode, and see if there is any value add to using weblogger mode in Emacs. Using that, I could have a full Emacs solution to posting new entries, without even having to go into WordPress to paste HTML as I have done now. As an aside, I found longlines-mode in Emacs very useful for writing this post in org-mode, so that there are not newlines in random spots when the HTML is produced.  
 
I have not tested how plotting from R works in this system.  It would be great to be able to generate in-line graphics in my posts using R commands to construct plots.
 
Finally, it looks like there is a really interesting project on worg called org-babel that will allow not only weaving of R, but many other languages including Ruby, elisp, and shell scripts, turning org-mode into a platform for literate programming. I also just saw blorgit, a blogging engine in org-mode that makes use of the git version control system. 
 
My function provided above should work if you have a recent version of org-mode, R, ESS, and the ascii package installed on your system. I have bound the function to F5 on my keyboard, so just hitting F5 in my org-mode buffer will create an HTML buffer and copy its contents to the kill-ring for pasting into WordPress. 
 When I started this process, I assumed I would have to write at least a few elisp functions, and possibly some extensions to org-mode, perhaps even an Sweave driver.  After thoroughly examining the available tools, I ended up only having to write, in effect, one lisp function, and that is only a utility function to automate the combination of solutions I found.  The moral is that while often times you will have to write your own functions to get the exact behavior you are after, it does pay to really research what is out there already.  I found a solution to my problem, org-mode, that allows a much more flexible framework for extensions and has been thoroughly tested. I now get to enjoy the benefits of whatever future enhancements org-mode comes up with, including extensions by other org-mode users.  In particular, the org-babel functionality looks very promising to replace or augment some of my work here. So before you write your own packages, research what others have already done. At the very least, you'll know what value you are adding by doing it your own way, which is the lesson I took away from Tukey's rule.
 "	 0 Comments
Structural Equation Modelling in R	https://www.r-bloggers.com/2009/09/structural-equation-modelling-in-r/	September 20, 2009	Jeromy Anglim		 0 Comments
RInside release 0.1.0 — and now on CRAN	https://www.r-bloggers.com/2009/09/rinside-release-0-1-0-and-now-on-cran/	September 20, 2009	Thinking inside the box	"

This release improves on the build process and should work on any sane
Unix-alike operating system. A few more examples were added or extended.
Details and the full ChangeLog are on my
RInside
page, and there is now a 
RInside
page on CRAN as well.



 "	 0 Comments
Welcome to Blogistic Reflections! (A blog created entirely in Emacs org-mode)	https://www.r-bloggers.com/2009/09/welcome-to-blogistic-reflections-a-blog-created-entirely-in-emacs-org-mode/	September 19, 2009	erikr	"
John Tukey’s preface to Exploratory Data Analysis begins with a useful rule, “It is important to understand what you can do before you learn to measure how well you seem to have done it.” When I decided I wanted to start a blog concentrating on statistics, R, and Emacs, I thought I had better learn first what I can do to make the process of generating content easier. Here, as a meta first post, I present how I used Emacs, R, and other technologies to produce the output you are reading here.
 
I use Emacs for everything I can. I started learning it as a graduate student in Statistics in order to use the ESS package. ESS lets you run R within Emacs, which is really nice for developing programs and performing data analysis interactively. I have discovered many Emacs packages through the years, but lately I have been learning the excellent org-mode package. Org-mode is designed so that you can get started with very minimal knowledge of its capabilities.  You can ignore the more complicated aspects of it if you do not need them, and still have a great system for general organizational tasks and note taking. I think it is worth learning Emacs just so you can use org-mode!
 
Since I am so familiar with Emacs, and work in it daily, I really wanted to use it to write posts. Ideally, I would like to write my blog entries in org-mode, and be able to use in-line R code. The text and R code should be output to HTML and published to my blog. This HTML generation is possible because org-mode has a fantastic feature that exports to various formats, including HTML.  This is beneficial since then all of my headlines, lists, tables, and source code markup in org-mode will be carried over to the HTML automatically.
 
Using the tools already available for Emacs, creating the HTML turned out to be much easier than I initially thought.  The vast majority of the time preparing this was spent learning about those tools, and figuring out how to combine them in the right way to accomplish what I wanted to do.
 
To summarize, my goals were to:
 
I really enjoy using R.  Since I plan to write about statistics, and especially statistical computing, I imagine many of my posts will contain R code. R comes with an interesting function called Sweave.  Sweave allows you to incorporate R commands within a document you are writing wrapped in a special syntax, see some demos.  So you might be writing some  or HTML and interweaving R code. After you run the Sweave function using the source (e.g., HTML) document as the input, a new file is created that replaces the R code with the results of the commands.  This might sound simple, but it leads to a very powerful model for report generation and reproducible research. My goal was to somehow get publishable HTML from a source org-mode file after running Sweave on it. For example, say I want to generate 100 samples from a normal distribution with mean 10, and summarize the results.
 
I do not want to have to actually type any of the summary output, of course, but I also don’t have to even copy and paste it.  What I can type is the R code to produce the output, and wrap it in Sweave syntax, like this:
 
When I prepare this post for publishing, Sweave will run the R code and insert the output in place of the Sweave commands.  I have wrapped the Sweave syntax in org-mode’s special BEGIN SRC construct, so that when I export to HTML, org-mode will properly syntax highlight the output using the htmlize package, as inspired by this post on using the htmlize package with Erlang.  The only elisp I had to write was the following.
 
All the first function is doing is running an input file (an org-mode file) through Sweave with the RweaveAsciidoc driver found in the R ascii package. It basically just puts in the R output as plain text, as opposed to  code or HTML.
 
The rest of the elisp function manipulates the output in some trivial ways. The explanation for the replacement of the four dashes is that the RweaveAsciidoc Sweave driver produces the dashed string both before and after the R output as a visual offset, since asciidoc uses that as a markup indicator.  I did not need that, since my output will not be fed through asciidoc, but rather the org-mode HTML exporter, so I simply replace the dashes found at the beginning of a line with the empty string. The last trick I had to do was to replace the R syntax highlighting with R-transcript syntax highlighting, since the results of Sweave are essentially a transcript of the R commands entered, not the actual R code. Finally, the resulting HTML produced by the org exporter has its pre tags modified with custom colors and font size.
 
There are a few loose ends that I didn’t have time to clean up yet. I would like to modify my Lisp function to work on regions if mark is set and transient-mark-mode is enabled.  As of now, it just works on the whole buffer.  The idea would be to have an org-mode file, say blog.org, that would have a top-level headline for each entry. You could then highlight that entry and publish it.  I realize this is fairly trivial to implement, but it is not done yet.
 
I also want to investigate the publishing feature of org-mode, and see if there is any value add to using weblogger mode in Emacs. Using that, I could have a full Emacs solution to posting new entries, without even having to go into WordPress to paste HTML as I have done now. As an aside, I found longlines-mode in Emacs very useful for writing this post in org-mode, so that there are not newlines in random spots when the HTML is produced.
 
I have not tested how plotting from R works in this system.  It would be great to be able to generate in-line graphics in my posts using R commands to construct plots.
 
Finally, it looks like there is a really interesting project on worg called org-babel that will allow not only weaving of R, but many other languages including Ruby, elisp, and shell scripts, turning org-mode into a platform for literate programming. I also just saw blorgit, a blogging engine in org-mode that makes use of the git version control system.
 
My function provided above should work if you have a recent version of org-mode, R, ESS, and the ascii package installed on your system. I have bound the function to F5 on my keyboard, so just hitting F5 in my org-mode buffer will create an HTML buffer and copy its contents to the kill-ring for pasting into WordPress.
 When I started this process, I assumed I would have to write at least a few elisp functions, and possibly some extensions to org-mode, perhaps even an Sweave driver.  After thoroughly examining the available tools, I ended up only having to write, in effect, one lisp function, and that is only a utility function to automate the combination of solutions I found.  The moral is that while often times you will have to write your own functions to get the exact behavior you are after, it does pay to really research what is out there already.  I found a solution to my problem, org-mode, that allows a much more flexible framework for extensions and has been thoroughly tested. I now get to enjoy the benefits of whatever future enhancements org-mode comes up with, including extensions by other org-mode users.  In particular, the org-babel functionality looks very promising to replace or augment some of my work here. So before you write your own packages, research what others have already done. At the very least, you’ll know what value you are adding by doing it your own way, which is the lesson I took away from Tukey’s rule.
 "	 0 Comments
Power Analysis for mixed-effect models in R	https://www.r-bloggers.com/2009/09/power-analysis-for-mixed-effect-models-in-r/	September 18, 2009	toddjobe	"The power of a statistical test is the probability that a null
hypothesis will be rejected when the alternative hypothesis is true.
In lay terms, power is your ability to refine or “prove” your
expectations from the data you collect.  The most frequent motivation
for estimating the power of a study is to figure out what sample size
will be needed to observe a treatment effect.  Given a set of pilot
data or some other estimate of the variation in a sample, we can use
power analysis to inform how much additional data we should collect.
 
I recently did a power analysis on a set of pilot data for a long-term
monitoring study of the US National Park Service.  I thought I would
share some of the things I learned and a bit of R code for others that
might need to do something like this.  If you aren’t into power
analysis, the code below may still be useful as examples of how to use
the error handling functions in R (withCallingHandlers,
withRestarts), parallel programming using the snow
package, and linear mixed effect regression using nlme.  If you
have any suggestions for improvement or if I got something wrong on
the analysis, I’d love to hear from you.
 The study system was cobblebars along the Cumberland river in Big
South Fork National Park (Kentucky and Tennessee, United States).
Cobblebars are typically dominated by grassy vegetation that include
disjunct tall-grass prairie species.  It is hypothesized that woody
species will encroach onto cobblebars if they are not seasonally
scoured by floods.  The purpose of the NPS sampling was to observe
changes in woody cover through time.  The study design consisted of
two-stages of clustering: the first being cobblebars, and the second
being transects within cobblebars.  The response variable was the
percentage of the transect that was woody vegetation.  Because of
the clustered design, the inferential model for this study design
has mixed-effects.  I used a simple varying intercept
model:
 

 
where y is the percent of each transect in woody vegetation sampled
n times within J cobblebars, each with K transects.  The
parameter of inference for the purpose of monitoring change in woody
vegetation through time is β, the rate at which cover changes as
a function of time.  α, γ, σ2γ, and
σ2y are hyper-parameters that describe the hierarchical
variance structure inherent in the clustered sampling design.
 
Below is the function code used I used to regress the pilot data.  It
should be noted that with this function you can log or logit transform
the response variable (percentage of transect that is woody).  I put
this in because the responses are proportions (0,1) and errors should
technically follow a beta-distribution.  Log and logit transforms with
Gaussian errors could approximate this. I ran all the models with
transformed and untransformed response, and the results did not vary
at all.  So, I stuck with untransformed responses:
 
Here are the results from this regression of the pilot data:
 When I decided upon the inferential model the first thing that
occurred to me was that I never learned in any statistics course I
had taken how to do such a power analysis on a multi-level model.
I've taken more statistics courses than I'd like to count and taught
my own statistics courses for undergrads and graduate students, and
the only exposure to power analysis that I had was in the context of
simple t-tests or ANOVA.  You learn about it in your first 2
statistics courses, then it rarely if ever comes up again until you
actually need it.
 
I was, however, able to find a great resource on power analysis from
a Bayesian perspective in the excellent book ""Data Analysis Using
Regression and Multilevel/Hierarchical Models"" by Andrew Gelman and
Jennifer Hill.  Andrew Gelman has thought and debated about power
analysis and you can get more from his blog.  The approach in the
book is a simulation-based one and I have adopted it for this
analysis.
 For the current analysis we needed to know three things: effect
size, sample size, and estimates of population variance. We set
effect size beforehand.  In this context, the parameter of interest
is the rate of change in woody cover through time β, and
effect size is simply how large or small a value of β you want
to distinguish with a regression.  Sample size is also set a   priori. In the analysis we want to vary sample size by varying the
number of cobblebars, the number of transects per cobblebar or the
number of years the study is conducted.  
 
The population variance cannot be known precisely, and this is where
the pilot data come in.  By regressing the pilot data using the
model we can obtain estimates of all the different components of the
variance (cobblebars, transects within cobblebars, and the residual
variance).  Below is the R function that will return all the
hyperparameters (and β) from the regression:
 
To calculate power we to regress the simulated data in the same way we
did the pilot data, and check for a significant β.  Since
optimization is done using numeric methods there is always the chance
that the optimization will not work.  So, we make sure the regression
on the fake data catches and recovers from all errors.  The solution
for error recovery is to simply try the regression on a new set of
fake data.  This function is a pretty good example of using the R
error handling function withCallingHandlers and
withRestarts.
 
To calculate the power of a particular design we run
fakeModWithRestarts 1000 times and look at the proportion of
significant β values:
 
Finally, we want to perform this analysis on many different sampling
designs.  In my case I did all combinations of set of effect sizes,
cobblebars, transects, and years.  So, I generated the appropriate designs:
 
Once we know our effect sizes, the different sample sizes we want,
and the estimates of population variance we can generate simulated
dataset that are similar to the pilot data.  To calculate power we
simply simulate a large number of dataset and calculate the
proportion of slopes, β that are significantly different from
zero (p-value < 0.05).  This procedure is repeated for all the
effect sizes and sample sizes of interest. Here is the code for
generating a simulated dataset. It also does the work of doing the
inverse transform of the response variables if necessary.
 
Then I performed the power calculations on each of these designs.  This
could take a long time, so I set this procedure to use parallel processing
if needed.  Note that I had to re-~source~ the file with all the
necessary functions for each processor.
 
The output of the powerAnalysis function is a data frame with
columns for the power and all the sample design settings.  So, I wrote
a custom plotting function for this data frame:
 
Below is the figure for the cobblebar power analysis.  I won't go into
detail on what the results mean since I am concerned here with
illustrating the technique and the R code.  Obviously, as the number of
cobblebars and transects per year increase, so does power.  And, as
the effect size increases, observing it with a test is easier.
 

  Author: Todd Jobe

  Date: 2009-09-18 Fri HTML generated by org-mode 6.30trans in emacs 22 "	 0 Comments
Web-Based Multilevel Modeling	https://www.r-bloggers.com/2009/09/web-based-multilevel-modeling/	September 18, 2009	jebyrnes	This is tremendously cool.  A nice intuitive web-based interface for the lme4 package in R (and you neither need to know R or understand the intricacies of the lme4 package) that gives you pdf output and plots.  If you just want to play around and not worry about coding things up, it’s a great little option.  Be sure to check out the demo video. 	 0 Comments
Set your R working directory in TextWrangler	https://www.r-bloggers.com/2009/09/set-your-r-working-directory-in-textwrangler/	September 18, 2009	[email protected]		 0 Comments
R Community in Australia	https://www.r-bloggers.com/2009/09/r-community-in-australia/	September 17, 2009	Jeromy Anglim		 0 Comments
Workflow in R	https://www.r-bloggers.com/2009/09/workflow-in-r/	September 17, 2009	Rob J Hyndman	This came up recently on StackOverflow. One of the answers was particularly helpful and I plan to adopt this for my future work. In fact, it is close to what I already do, but is a little more structured. The idea is to break the code into four files, all stored in your project directory. These four files are to be processed in the following order. It is a good idea to save your workspace after each file is run. There are many advantages to this set up. First, you don’t have to reload the data each time you make a change in a subsequent step. Second, if you come back to an old project, you will be able to work out what was done relatively quickly. It also forces a certain amount of structured thinking in what you are doing, which is helpful. Often there will be bits and pieces of code that you write, but don’t end up using, yet don’t want to delete. These should either be commented out or saved in files with other names. All analysis from reading data to producing the final results should be reproducible by simply source()ing these four files in order with no further user intervention. 	 0 Comments
Example 7.12: Calculate and plot a running average	https://www.r-bloggers.com/2009/09/example-7-12-calculate-and-plot-a-running-average/	September 17, 2009	Nick Horton		 0 Comments
Comments on “Introduction to Scientific Programming and Simulation Using R”	https://www.r-bloggers.com/2009/09/comments-on-introduction-to-scientific-programming-and-simulation-using-r/	September 17, 2009	Jeromy Anglim		 0 Comments
R clinic this week: Regression Modeling Strategies in R	https://www.r-bloggers.com/2009/09/r-clinic-this-week-regression-modeling-strategies-in-r/	September 16, 2009	Stephen Turner		 0 Comments
Multiple Linear Regression	https://www.r-bloggers.com/2009/09/multiple-linear-regression/	September 14, 2009	rtutor.chiyau	"

A multiple linear regression (MLR) model that describes a dependent variable
y by independent variables x1, x2, …, xp (p > 1) is expressed by the equation as
follows, where the numbers α and βk (k = 1, 2, …, p) are the parameters, and ϵ is the
error term.
    read more "	 0 Comments
Find the function you’re looking for in R	https://www.r-bloggers.com/2009/09/find-the-function-youre-looking-for-in-r/	September 14, 2009	Stephen Turner		 0 Comments
Chicago Half Marathon 2009	https://www.r-bloggers.com/2009/09/chicago-half-marathon-2009/	September 13, 2009	Thinking inside the box	"

As for the race itself, I didn’t run it quite as evenly as I had hoped, faded
a little around mile ten and eleven and came in at 1:34:54, or just under
7:15 min/mile.  Not a bad time—I have done better (e.g.
last year)
but also worse. All in all still one my very favourite races, and this year
with a once-again improved course that featured even more running along the 
lake. I will probably be back next year.

 "	 0 Comments
Finding an R function	https://www.r-bloggers.com/2009/09/finding-an-r-function/	September 13, 2009	Rob J Hyndman	"Suppose you want a function to fit a neural network. What’s the best way to find it? Here are three steps that help to find the elusive function relatively quickly. First, use help.search(""neural"") or the shorthand ??neural. This will search the help files of installed packages for the word “neural”. Actually, fuzzy matching is used so it returns pages that have words similar to “neural” such as “natural”. For a stricter search, use help.search(""neural"",agrep=FALSE). The following results were returned for me (using the stricter search). If you want to look through packages that you have not necessarily installed, you could try using the findFn function in the sos package. This function searches the help pages of packages covered by the RSiteSearch archives (which includes all packages on CRAN). For example returns 206 matches (on 13 September 2009) from over 60 packages. These are ordered based on a relevance score, so the top few packages on the list are probably the most useful. (There’s a good discussion of findFn here.) To look even further afield, use RSiteSearch(""neural"") which will send your search to R site search. As well as CRAN packages, this covers the  R-help mailing list archives, help pages, vignettes and task views. Still no luck? Try Rseek.org. It’s a restricted Google search covering on R-related sites. Finally, if all else fails, try asking a question on StackOverflow (make sure your question is tagged with “r”),  or send your question to the R-help mailing list. Both usually solicit replies in under an hour. However, don’t do this without first trying to find the function using one of the other methods. "	 0 Comments
New R package: sos	https://www.r-bloggers.com/2009/09/new-r-package-sos/	September 12, 2009	romain francois	Searching help pages of contributed packages just got easier with the release of the new sos package. This is a replacement for and substantial enhancement of the existing “RSiteSearch” package.  To learn more about it, try vignette(“sos”) We hope you find this as useful as we have. Spencer Graves, Sundar Dorai-Raj, Romain Francois 	 0 Comments
Aggregating SSURGO Data in R	https://www.r-bloggers.com/2009/09/aggregating-ssurgo-data-in-r/	September 10, 2009	dylan	" 
Premise
SSURGO is a digital, high-resolution (1:24,000), soil survey database produced by the USDA-NRCS. It is one of the largest and most complete spatial databases in the world; and is available for nearly the entire USA at no cost. These data are distributed as a combination of geographic and text data, representing soil map units and their associated properties. Unfortunately the text files do not come with column headers, so a template is required to make sense of the data. Alternatively, one can use an MS Access template to attach column names, generate reports, and other such tasks. CSV file can be exported from the MS Access database for further use. A follow-up post with text file headers, and complete PostgreSQL database schema will contain details on implementing a SSURGO database without using MS Access.  If you happen to have some of the SSURGO tabular data that includes column names, the following R code may be of general interest for resolving the 1:many:many hierarchy of relationships required to make a thematic map.  
This is the format we want the data to be in 
 
So we can make a map like this
 read more "	 0 Comments
Machine Learning in R	https://www.r-bloggers.com/2009/09/machine-learning-in-r/	September 10, 2009	Stephen Turner		 0 Comments
brew: Creating Repetitive Reports	https://www.r-bloggers.com/2009/09/brew-creating-repetitive-reports/	September 9, 2009	learnr	"United Nations report World Population Prospects: The 2008 Revision (highlights available here) provides data about the historical and forecasted population of the country. In exploring the future and past population trends it is relatively easy to subset the dataset by your selected variable. Likewise, it is straightforward to produce a 263 page pdf-file that shows the population trend between 1950 – 2005 for all the countries in the dataset. However, if one would like to create a report showing some summary tables and graphs along with some textual description for all the countries, then the process becomes more complicated. This is exactly what the brew() package written by Jeffrey Horner was designed to help the users with. brew “implements a templating framework for mixing text and R code for report generation” and makes it very easy to generate repetitive reports which is of great help for example in performing exploratory analysis on a large dataset with many variables.  
 The report-generation can be split into three parts: Data preparation script is saved in the popreportdata.R file which is later sourced into the brew template. In fact I would like to explore the data by continent rather than by country, so in order to do this the first step is to group the countries by continent with the help of Isocodes package that contains standard country or area codes and geographical regions used by UN. countries dataframe contains now regional classification for each country. Next step is to merge a subset of this information with the population data. In this step the graphs are saved to the disk using ggsave, and a list of lists with four dataframes about each continent is returned. My report template is essentially a latex document which includes an R code loop wrapped in brew syntax. To make it easier to generate the latex statements for inclusion of graphs and tables, a few helper functions are defined first. Brew syntax is really very simple to use. From the help file: The resulting pdf-file can be seen here "	 0 Comments
search the graph gallery from R	https://www.r-bloggers.com/2009/09/search-the-graph-gallery-from-r/	September 8, 2009	romain francois	This is a short code snippet that is motivated by this thread on r-help yesterday. The gallery contains a search engine textbox (top-right) that can be used to search for content in the website using either its internal crude search engine or perform a google search restricted to the gallery.  Here we write a small R function that can be used to take advantage of the search engine, from R 	 0 Comments
new R package : ant	https://www.r-bloggers.com/2009/09/new-r-package-ant/	September 8, 2009	romain francois	The ant package has been released to CRAN yesterday. As discussed in previous posts in this blog (here and here), the ant R package provides an R-aware version of the ant build tool from the apache project.  The package contains an R script that can be used to invoke ant with enough plumbing so that it can use R code during the build process. Calling the script is further simplified with the ant function included in the package.  The simplest way to take advantage of this package is to add it to the Depends list of yours, include a java source tree somewhere in your package tree (most likely somewhere in the inst tree) with a build.xml file, and include a configure and configure.win script at the root of the package that contains something like this:  This will be further illustrated with the demo package helloJavaWorld in future posts 	 0 Comments
Comparison between 15 different cities…	https://www.r-bloggers.com/2009/09/comparison-between-15-different-cities/	September 6, 2009	the R user...		 0 Comments
RQuantLib 0.3.0 Windows build snag	https://www.r-bloggers.com/2009/09/rquantlib-0-3-0-windows-build-snag/	September 6, 2009	Thinking inside the box		 0 Comments
RQuantLib 0.3.0 released	https://www.r-bloggers.com/2009/09/rquantlib-0-3-0-released/	September 5, 2009	Thinking inside the box	"

This version, the first in a new ‘0.3.*’ release series, contains all the
work that Khanh Nguyen did during his Google Summer of Code participation
which I had mentioned a while
back and for which I acted as mentor.

 

And Khanh did great. We now have a lot more Fixed Income
functionality; I include the full ChangeLog entry below.  I also added some
simple calendaring support and the odd fix here or there while mentoring.
All of this had been available while technically ‘in progress’ thanks to
support provided by the R-Forge project
hosting where SVN checkouts as well as nightly binaries are provided.
A few more details are on the 
RQuantLib page
and I hope to add a few more examples.

 
As promised, the ChangeLog entry:

 "	 0 Comments
Polynomial regression techniques	https://www.r-bloggers.com/2009/09/polynomial-regression-techniques/	September 5, 2009	Todos Logos		 0 Comments
R Flashmob	https://www.r-bloggers.com/2009/09/r-flashmob/	September 4, 2009	Tal Galili	"Today I noticed a call for R users to gather around a single campfire for one hour and share their questions and answers. The campfire name is stackoverflow.com, a site dedicated for handling programming questions. The event details are bellow: 
 From: The R Flashmob Project
Subject: R Flashmob #2 You are invited to take part in R Flashmob, the project that makes the
world a better place by posting helpful questions and answers about the
R statistical language to the programmer’s Q & A site stackoverflow.com Please forward this to other people you know who might like to join. FAQ Q. Why would I want to join an inexplicable R mob? A. Tons of other people are doing it. Q. Why else? A. Stackoverflow was built specifically for handling programming questions.
It’s a better mousetrap. It offers search (and is well indexed by search engines),
tagging, voting, the ability to choose the “best” answer to a question, and the ability to
edit questions and answers as technology progresses. It has a karma system to
reward people who are happy to help and discourage MLJs (mailing list jerks). Q. Do the organizers of this MOB have any commercial interest in stackoverflow? A. None at all. We’re just convinced it is the best way to help and promote R. All
the content submitted to stackoverflow is protected by a Creative Commons
CC-Wiki License, meaning anyone is free to copy, distribute, transmit, and
remix the information on stackoverflow. All the content on stackoverflow is
regularly made available for download by the public. INSTRUCTIONS – R MOB #2
Location: stackoverflow.com
Start Date: Tuesday, September 8th, 2009
Start Time:
10:04 AM – US Pacific
11:04 AM – US Mountain
12:04 PM – US Central
1:04 PM – US Eastern
6:04 PM – UK
7:04 PM – Continental W. Europe
5:04 AM (Weds) – New Zealand (birthplace of R)
Duration: 50 minutes (1) At some point during the day on September 8th, synchronize your watch to
http://timeanddate.com/worldclock/personal.html?cities=137,75,64,179,136,37,22 (2) The mob should form at precisely 4 minutes past the hour and not beforehand. (3) At 4 minutes past the hour, you should arrive at stackoverflow.com, log in,
and post 3 R questions. Be sure to tag the questions “R”. See the posting
guidelines at http://stackoverflow.com/faq to understand what makes a good
question. (4) Follow R Flashmob updates at http://twitter.com/rstatsmob (5) Post twitter messages tagged #rstats and #rstatsmob during the mob,
providing links to your questions. (6) During the R MOB, you can chat with other participants on the #R channel
on IRC (freenode). To do this, install the Chatzilla extension on Firefox.
Click “freenode” on the main screen. Then type /join #R in the field at the
bottom of the screen. Then chat. (7) If you finish posting your three questions within the 50 minutes, stick
around to answer questions and give “up votes” to good questions and answers. (8) IMPORTANT: After posting, sign the R Flashmob guestbook at
http://bit.ly/6F8B2 (9) Return to what you would otherwise have been doing. Await
instructions for R MOB #3. This invitation already gained exposure from 3 blogs: I am waiting to see who else will join the fun. "	 0 Comments
ClipPath copies filename and path from windows for loading into R	https://www.r-bloggers.com/2009/09/clippath-copies-filename-and-path-from-windows-for-loading-into-r/	September 4, 2009	Stephen Turner		 0 Comments
R Flashmob #2	https://www.r-bloggers.com/2009/09/r-flashmob-2/	September 4, 2009	Paolo		 0 Comments
Compiling and Cross-compiling R packages for Windows (win32)	https://www.r-bloggers.com/2009/09/compiling-and-cross-compiling-r-packages-for-windows-win32/	September 3, 2009	Vinh Nguyen	"so recently i’m learning how to compile and cross-compile R packages from source for windows machines, which means i have to create windows binaries.   the first option is to build on a windows system, and the 2nd is to cross-build on an intel-based linux or mac system.  i will outline my experience. Send it off to a Build Farm
If you have the source package ready for install on a linux based system, you can send it off to a server and have it automatically build.  This is probably the easiest way. Building R packages and binaries on a Windows system.
References:
http://cran.r-project.org/doc/manuals/R-admin.html#Building-from-source
http://faculty.chicagobooth.edu/peter.rossi/research/bayes%20book/bayesm/Making%20R%20Packages%20Under%20Windows.pdf
http://www.biostat.wisc.edu/~kbroman/Rintro/Rwinpack.html
Requirements:
Rtools and R source file At the time of this writing, I am using R 2.9.2 and using Rtools29.
1.  First, install Rtools, which includes some basic unix utilities, compilers (MinGW), windows compatible perl, unicode, Tcl, and some bitmap stuff.
Install should be in C:Rtools and C:R.  Go ahead and install everything, including the Tcl stuff.  In the install, go ahead and click modify Path variable if you have admin access.  if not, add the subfolders of Rtools that has bin into the user ‘Path’ variable (either for User or for System) from the Control Panel > System > something > Advanced tab > Environment variable button.  The variable should be something like ‘H:Rtoolsbin;H:RtoolsMinGWbin;H:Rtoolsperlbin’.
2.  go to start > run > cmd to open a command window.  I placed my R source tar.gz file in H:R.  now, use tar zxvf to unarchive it.
3.   Move H:RTcl into the R-version folder that we just unarchived.  Move the folders ‘unicode’ and ‘bitmap’ from H:Rsrcgnuwin32 into R-versionsrcgnuwin32.  Main thing is we need the iconv.dll file.
4.  goto R-versionsrcgnuwin32.   type make (or make all) to compile.  this gave me a couple errors.  if u don’t have Tcl in the right location, edit the R-versionsrcgnu32MkRules and change the path for Tcl.  I also got an error where it couldn’t find the /tmp/ folder, so i just created H:tmp.  R successfully compiled: see R-versionbin.
5.  To make the windows binaries, I have a package folder prepared under mac/linux.  I copied it to the windows machine, and i did something like in H:R (where the package folder is in, say H:RMyPackage):
H:RR-versionbinRcmd.exe build –binary MyPackage
The source should be compiled into a dll and i get a zip file where i could install on other windows machines. Cross building package win32 (windows) binaries on an intel-based mac or linux
Since i don’t have a windows system (i did it on my brother’s computer), it could be convenient to build this on my mac or linux system.  The R core team used to support cross building, but stripped support as of v 2.9.  I first outline my experience with R 2.8 on my Mac OS X.  Look at the R-admin help file for cross-building in the 2.8.  I also got help from this post on R-dev, and a few inquiries with the poster through email. Requirements:
R source code file
MinGW for mac (I installed v.4.3.0 binaries and added the location to my PATH variable)
R (mac/linux, your system) binaries (version must match that of source.  i had R 2.8 currently installed at the time, so i cross-build this version).
R win32 binaries (same version as well): u can install this from source using the cross-build stuff, but i kept getting errors.  so, what i did, was used the windows installer and installed it using Wine on my Mac (~/.wine/drive_c/Program Files/R/R-version/ — the space in Program Files isn’t good, so i made a softlink via ln -s to ~/R) In R-version (extracted somewher), go to /src/gnuwin32, edit MkRules, changed:
BUILD=CROSS
no CHM, this version picks this up by default with CROSS
BINPREF=i386-mingw32-##i586-mingw32-
R_EXE=/usr/bin/R###’I failed to read the instructions to set R_EXE in MkRules’ Now, suppose the name of my package is MyPackage and it is locted in /Users/vinh/Downloads/Rwin/ (MyPackage folder is in here) Instructions here are found in README.package of gnuwin32: in my gnuwin32 folder, i did: Now I have MyPackage.zip that can be installed on windows machines!
Remember, ~/R is the softlinked version of where my windows version of R is. Next, i’m going to attempt cross-compiling on R 2.9.2, where support is no longer supported, but as the post i referenced above described, this should still be do-able. I will update this. "	 0 Comments
Let’s All Go Down to the Barplot!	https://www.r-bloggers.com/2009/09/let%e2%80%99s-all-go-down-to-the-barplot/	September 3, 2009	jebyrnes	"I’m back for another pean to ANOVA-loving experimental ecology.  Barplots (or point-range plots – but which is better is a discussion for a different time) are the thing!  And a friend of mine was recently asking me how to do a decent barplot with ggplot2 for data of moderate complexity (e.g., a 2-way ANOVA). So I tried it. And then I hit my head against a wall for a little while. And then I think I figured it out, so, I figured I’d post it so that there is less head-smashing in the world. To do a boxplot is simple.  And may statisticians would argue that they are more informative, so, really, we should abandon barplots.  Take the following example which looks at the highway milage of cars of various classes in two years. A few notes.  The x-axis her is class.  The fill attribute splits things by year (which is continuous, so we need to make it look like a factor).  And the final position=”dodge” really is the key.  It splits the bars for different years apart, and it shall come into play again in a moment. This code produces a perfectly nice boxplot:  Lovely!  Unless you want a barplot.  For this, two things must happen.  First, you need to get the average and standard error values that you desire.  I do this using ddply (natch).  Second, you’ll need to add in some error bars using geom_errorbar.  In your geom_errorbar, you’ll need to invoke the position statement again. This produces a perfectly serviceable and easily intelligible boxplot.  Only.  Only…  well, it’s time for a confession. I hate the whiskers on error bars.  Those weird horizontal things, they make my stomach uneasy.  And the default for geom_errorbar makes them huge and looming.  What purpose do they really serve?  OK, don’t answer that.  Still, they offend the little Edward Tufte in me (that must be very little, as I’m using barplots). So, I set about playing with width and other things to make whisker-less error bars.  Fortunately for me, there is geom_linerange, that does the same thing, but with a hitch.  It’s “dodge” needs to be told just how far to dodge.  I admit, here, I played about with values until I found ones that worked, so your milage may vary depending on how many treatment levels you have.  Either way, the resulting graph was rather nice. And voila!  So, enjoy yet another nifty capability of ggplot2!
 Great!  I will say this, though.  I have also played around with data with unequal representation of treatments (e.g., replace year with class or something in the previous example) – and, aside from making empty rows for missing treatment combinations, the plots are a little funny.  Bars expand to fill up space left by vacant treatments.  Try qplot(class, hwy, data=mpg, fill= manufacturer, geom=""boxplot"") to see what I mean.  If anyone knows how to change this, so all bar widths are the same, I’d love to hear it. "	 0 Comments
Truncated Normal Distribution	https://www.r-bloggers.com/2009/09/truncated-normal-distribution/	September 3, 2009	Forester		 0 Comments
update on the ant package	https://www.r-bloggers.com/2009/09/update-on-the-ant-package/	September 3, 2009	romain francois	"I have updated the ant package I described yesterday in this blog to add several things 
 The package now includes a demonstrative build.xml file in the examples directory Here is the result "	 0 Comments
Learning ggplot2: 2D plot with histograms for each dimension	https://www.r-bloggers.com/2009/09/learning-ggplot2-2d-plot-with-histograms-for-each-dimension/	September 3, 2009	Michael Kuhn		 0 Comments
Fitting lactation curves/functions in R	https://www.r-bloggers.com/2009/09/fitting-lactation-curvesfunctions-in-r/	September 3, 2009	Gregor Gorjanc		 0 Comments
R capable version of ant	https://www.r-bloggers.com/2009/09/r-capable-version-of-ant/	September 2, 2009	romain francois	ant is an amazing build tool. I have been using ant for some time to build the java code that lives inside the src directories of my R packages, see this post for example.  The drawbacks of this approach are :  The ant package for R is developed to solve these two issues. The package is source-controlled in r-forge as part of the orchestra project Once installed, you find an ant.R script in the exec directory of the package. This script is pretty similar to the usual shell script that starts ant, but it sets it so that it can use R with the following additional tasks  Here is an example build file that demonstrate how to use these tasks Here is what happens when we call the R special version of ant with this build script  	 0 Comments
Tip: get java home from R with rJava	https://www.r-bloggers.com/2009/09/tip-get-java-home-from-r-with-rjava/	September 2, 2009	romain francois	Assuming rJava is installed and works, it is possible to take advantage of its magic to get the path where java is installed: This is useful when you develop scripts that need to call a java program without assuming that java is on the path, or the JAVA_HOME environment variable is set, etc … 	 0 Comments
Analysis of Variance	https://www.r-bloggers.com/2009/09/analysis-of-variance/	September 1, 2009	rtutor.chiyau	"

In an experiment study, various treatments are applied to test subjects and the
response data is gathered for analysis. A critical tool for carrying out the analysis is
the Analysis of Variance (ANOVA). It enables a researcher to differentiate
treatment results based on easily computed statistical quantities from the treatment
outcome.
 read more "	 0 Comments
Adventures with Comcast: Part ohnoesnotanotherone in an ongoing series	https://www.r-bloggers.com/2009/10/adventures-with-comcast-part-ohnoesnotanotherone-in-an-ongoing-series/	October 31, 2009	Thinking inside the box	"

But I think this week may top everything.  I’ll just try to jot down some notes before I forget all the gory details:

 "	 0 Comments
Sorting a matrix/data.frame on a column	https://www.r-bloggers.com/2009/10/sorting-a-matrixdata-frame-on-a-column/	October 30, 2009	Timothée Poisot		 0 Comments
useR! 2006	https://www.r-bloggers.com/2009/10/user-2006-2/	October 30, 2009	Vincent Zoonekynd's Blog	"Last week, I attended the 2006 UseR! conference: here is a
(long) summary of some of the talks that took place in
Vienna — since there were up to six simultaneous talks, I
could not attend all of them…
 In this note: 
 For more information about the contents of the conference
and about the previous ones (the useR! conference was initially
linked to the Directions in Statistical Computing (DSC) ones):
 I attended two tutorials prior to the conference: one on bayesian statistics
(in marketing) and one on Rmetrics.
 There were 400 participants, 160 presentations.
 Among the people present, approximately 50% were using
Windows (perhaps less: it is very difficult to distinguish
between Windows and Linux), 30% MacOS, 20% Linux (mostly
Gnome, to my great surprise).
  See more … "	 0 Comments
R graphics	https://www.r-bloggers.com/2009/10/r-graphics-2/	October 30, 2009	Vincent Zoonekynd's Blog	"I just finished reading Paul Murrel’s book, “R graphics”. 
  There are two graphical systems in R: the old (“classical” —
in the “graphics” package) one, and the new (“trellis”, “lattice”,
“grid” — in the “lattice” and “grid” packages) one.
 The first part of the book, devoted to the old system, tries to be
as complete as a reference book, but fails. For instance, the
discussion on how to arrange several plots on a single page sticks to
tabular layouts and fails to mention the more flexible “fig” graphical
argument (to be honest, it is listed with terser explanations than in
the manual page and also appears, deeply hidden in an example, in an
appendix).
  On the other hand, the second and largest part, devoted to
grid graphics lives up to my expectations: it seems more
complete and does not duplicate information already
available on the web. You are probably already familiar with
some of the high-level lattice plots (xyplot(), histogram(),
bwplot()), but if you have already tried to understand how
they are implemented, or tried to write your own graphical
functions, you were probably confused by the differences
(and claimed lack thereof) between ""lattice"", ""panel"",
""grob"" and ""grid"" -- the book clarifies all that.
 The code of the examples in the book is available on the author's web site. 
 You will find, for instance, dendrograms (check the rpart and maptree
packages),
  table-like plots 
  or plots arranged in a tree (this can be seen as a generalization of
lattice plots, that present several facets of a dataset arranged on a
grid). 
  One whole chapter is devoted to the creation, from scratch, of an
oceanographic plot
  whose elements are then reused for a completely different plot.
   See more ... "	 0 Comments
The grammar of graphics (L. Wilkinson)	https://www.r-bloggers.com/2009/10/the-grammar-of-graphics-l-wilkinson-2/	October 30, 2009	Vincent Zoonekynd's Blog	" Though this book is supposed to be a description of the
graphics infrastructure a statistical system could
provide, you can and should also see it as a (huge,
colourful) book of statistical plot examples.
 The author suggests to describe a statistical plot in
several consecutive steps: data, transformation, scale,
coordinates, elements, guides, display.  The “data” part
performs the actual statistical computations — it has to be
part of the graphics pipeline if you want to be able to
interactively control those computations, say, with a slider
widget. The transformation, scale and coordinate steps,
which I personnally view as a single step, is where most of
the imagination of the plot designer operates: you can
naively plot the data in cartesian coordinates, but you can
also transform it in endless ways, some of which will shed
light on your data (more examples below). The elements are
what is actually plotted (points, lignes, but also
shapes). The guides are the axes, legends and other elements
that help read the plot — for instance, you may have more
than two axes, or plot a priori meaningful lines (say, the
first bissectrix), or complement the title with a picture (a
“thumbnail”). The last step, the display, actually produces
the picture, but should also provide interactivity
(brushing, drill down, zooming, linking, and changes in the
various parameters used in the previous steps).
 In the course of the book, the author introduces many
notions linked to actual statistical practice but too
often rejected as being IT problems, such as data mining,
KDD (Knowledge Discovery in Databases); OLAP, ROLAP,
MOLAP, data cube, drill-down, drill-up; data streams;
object-oriented design; design patterns (dynamic plots
are a straightforward example of the “observer pattern”);
eXtreme Programming (XP); Geographical Information
Systems (GIS); XML; perception (e.g., you will learn that
people do not judge quantities and relationships in the
same way after a glance and after lengthy considerations),
etc. — but they are only superficially touched upon,
just enough to wet your apetite.
 If you only remember a couple of the topics developped in
the book, these should be: the use of non-cartesian coordinates and,
more generally, data transformations;
scagnostics; data patterns, i.e., the meaningful reordering of
variables and/or observations.
  See more … "	 0 Comments
Statistics with R	https://www.r-bloggers.com/2009/10/statistics-with-r-2/	October 30, 2009	Vincent Zoonekynd's Blog	"I have just uploaded the new version of my “Statistics with R”:
 The previous version was one year and a half old, so in spite of
the fact I have not had much time to devote to it in the past two years, it
might have changed quite a bit — but it remains as incomplete as ever. "	 0 Comments
Use R 2009 Conference	https://www.r-bloggers.com/2009/10/use-r-2009-conference-2/	October 30, 2009	Vincent Zoonekynd's Blog	"I did not attend the conference this year,
but just read through the presentations.
There is some overlap with other R-related conferences,
such as R in Finance or the Rmetrics workshop.
  Here are some more detailed notes.
  See more … "	 0 Comments
Decimal log scale on a plot	https://www.r-bloggers.com/2009/10/decimal-log-scale-on-a-plot/	October 30, 2009	Timothée Poisot	R only does natural (neperian) log scales by default, and this is lame. Here is a simple code to do decimal log scale, pretty much a requirement for scientists… The force(x/y)lim options works for natural and log scales (for the later case, you need to specify the power of 10 that you want : c(-2,2) fixes the limit from 0.01 to 100) 	 0 Comments
Long R, Short Excel	https://www.r-bloggers.com/2009/10/long-r-short-excel/	October 29, 2009	Milk Trader		 0 Comments
Income inequality and partisan voting in the United States	https://www.r-bloggers.com/2009/10/income-inequality-and-partisan-voting-in-the-united-states/	October 29, 2009	Andrew Gelman		 0 Comments
Simple R figures	https://www.r-bloggers.com/2009/10/simple-r-figures/	October 29, 2009	Shige		 0 Comments
Kicking Ass with plyr	https://www.r-bloggers.com/2009/10/kicking-ass-with-plyr/	October 29, 2009	JD Long	Tonight (October 29, 2009) at 5:30 PM is the Chicago R meetup at Jaks tap. Here’s more info.  I’ll be making a presentation based on my earlier blog post about plyr. The presentation will only be 8 minutes long so I’ve had to pick and choose my info carefully. OK, who am I kidding? I had a couple of Schlitz (in a bottle!) for lunch over at Boni Vinos and slammed some slides together rather haphazardly. At any rate, here’s the presentation. I owe special thanks to all the folks in Twitter who reviewed these slides this week. A special shout out to @kenahoo who caught my one code typo! And also to @hadleywickham (author of plyr) who made some good suggestions, some of which I heeded. As a professor he should consider 15% application of his information to be a phenomenally high rate. Click the graphic to download the slides as a PDF:  If you’re wondering what my favorite beer is, I’ll give you a secret. My favorite beer is #3. That’s the one that makes me a persuasive and articulate public speaker. #4 makes me dance well. I hope to see you tonight. 	 0 Comments
Go long on close and sell on open	https://www.r-bloggers.com/2009/10/go-long-on-close-and-sell-on-open/	October 29, 2009	kafka	I found a description of supposed to be profitable strategy  on Bloomberg. The strategy is simple – buy S&P500 index on close and sell it on next day open. So, I tested this claim and got nice P/L curve:  Yes, since 1993 this strategy has generated the profit >300%. But, neither commissions or slippage are included:) Let’s consider 0.0007% commissions+slippage(7$ per 10000 trade). There we go:  Is it good? I will look for somethig better… 	 0 Comments
Bioconductor 2.5 is out	https://www.r-bloggers.com/2009/10/bioconductor-2-5-is-out/	October 29, 2009	Paolo		 0 Comments
Tips for Using StatET and Eclipse for Data Analysis in R	https://www.r-bloggers.com/2009/10/tips-for-using-statet-and-eclipse-for-data-analysis-in-r/	October 29, 2009	Jeromy Anglim		 0 Comments
Using the “foreign” package for data conversion	https://www.r-bloggers.com/2009/10/using-the-foreign-package-for-data-conversion/	October 27, 2009	Shige		 0 Comments
Literate programming, etc.	https://www.r-bloggers.com/2009/10/literate-programming-etc/	October 27, 2009	Shige		 0 Comments
MultBar : Advanced multiple barplot with SEM	https://www.r-bloggers.com/2009/10/multbar-advanced-multiple-barplot-with-sem/	October 26, 2009	Timothée Poisot	Producing this kind of graphs (below) in R can be a pain in the a*s.  Here is a simple code that requires that data are presented in lists (see the example below). 	 0 Comments
R 2.10.0 is Out!	https://www.r-bloggers.com/2009/10/r-2-10-0-is-out/	October 26, 2009	Paolo		 0 Comments
Free statistics e-books for download	https://www.r-bloggers.com/2009/10/free-statistics-e-books-for-download/	October 25, 2009	Tal Galili	This post will eventually grow to hold a wide list of books on statistics (e-books, pdf books and so on) that are available for free download.  But for now we’ll start off with just one  several books:  * * * Know of any more e-books freely available for download? Please write to us about them in the comments. 	 0 Comments
Example 7.16: assess robustness of permutation test to violations of exchangeability assumption	https://www.r-bloggers.com/2009/10/example-7-16-assess-robustness-of-permutation-test-to-violations-of-exchangeability-assumption/	October 24, 2009	Nick Horton		 0 Comments
A primer to Sweave with LyX	https://www.r-bloggers.com/2009/10/a-primer-to-sweave-with-lyx/	October 24, 2009	Jitao David Zhang		 0 Comments
Data Mining and R	https://www.r-bloggers.com/2009/10/data-mining-and-r/	October 24, 2009	Jeromy Anglim		 0 Comments
RSPerl : Using R from within Perl	https://www.r-bloggers.com/2009/10/rsperl-using-r-from-within-perl/	October 23, 2009	Stewart MacArthur		 0 Comments
Scivews-K got updated	https://www.r-bloggers.com/2009/10/scivews-k-got-updated/	October 23, 2009	Shige		 0 Comments
From ORD Sessions to R-Forge in 12 hours with RProtoBuf	https://www.r-bloggers.com/2009/10/from-ord-sessions-to-r-forge-in-12-hours-with-rprotobuf/	October 22, 2009	Thinking inside the box	"
Sounded good, and I needed an excuse to try to mix the awesome
Protocol Buffers 
with my favourite data tool, R.  What
are Protocol Buffers?  To quote from the Google overview page referenced above:
 
So three hours later, I had an implementation of the ‘addressbook
reader’ C++ example wrapped in a tiny yet complete R package that
passed R CMD check.  And one lingua franca for data has met
another.

 
So before going to bed, I quickly registered a new project at
R-Forge,
everybody’s favourite R hosting site, and thanks to the tireless Stefan Theussl (and some
favourable timezone differences) the project was approved and the stanza
available by the time I got up.  So I quickly filled the SVN repo and,
presto, we had the
RProtoBuf project at
R-Forge within 12 hours of the ORD Sessions hackfest.  I will try to
follow up on RProtoBuf in a couple of days, this may lead to some changes in
my Rcpp R / C++
interface package as well.

 "	 0 Comments
ISO week	https://www.r-bloggers.com/2009/10/iso-week/	October 22, 2009	Forester		 0 Comments
Leap years	https://www.r-bloggers.com/2009/10/leap-years/	October 22, 2009	Forester		 0 Comments
Aggreate electoral targeting with R	https://www.r-bloggers.com/2009/10/aggreate-electoral-targeting-with-r/	October 22, 2009	jjh	"Electoral targeting is the process of quantifying the partisan bias of a single voter or subset of voters in a geographic region. Bias can be calculated using an individual’s demographic and voting behavior or by aggregating results from an entire election precinct. Targeting is traditionally performed by national committees (e.g., National Committee for an Effective Congress, National Republican Congressional Committee), state political parties, interest groups (e.g., EMILY’s List, National Rifle Association), or campaign consultants. Targeting data is consumed by campaign managers and analysts, and it is used along with polling data to build strategy, direct resources, and project electoral outcomes.  While aggregate electoral targeting can build a sophisticated picture of a district, the mathematics behind targeting are very simple. Targeting can be performed by anyone with previous electoral data, and calculations can be done using 3×5 note cards, with simple spreadsheets, or high-end software packages like SPSS. The targeting methods discussed in this post are taken from academic publications on electioneering: Campaign Craft (Burton, Shea 2006) and The Campaign Manager (Shaw, 2004).  Although targeting data is usually usually inexpensive or free, a down-ballot campaign or a primary challenger might not have the connections or support of a PAC or party to obtain the data. In these cases, a campaign will probably purchase one of the books listed above to perform its own analysis. Even an established campaign may run its own analysis, possibly to test different turnout theories or to integrate additional data. This post is directed towards these groups. Together, we will assume the role of campaign consultant and perform an aggregate electoral analysis on the 13th House of Delegates seat (HOD#13) in the Commonwealth of Virginia. In HOD#13, the 18-year Republican incumbent Bob Marshall is being challenged by Democrat John Bell. This analysis will compute and visualize turnout, partisan bias, and a precinct ranking based on projected turnout and historical Democratic support.  The analysis of HOD#13 will be performed using R, an open-source computing platform. R is free, extensible, and interactive, making it an ideal platform for experimentation. The R package aggpol was created specifically for this tutorial, and it contains all the data and operations required to execute an aggregate electoral analysis. Readers can execute the provided R code to reproduce the analysis or simply follow along to learn how it was performed. Readers unfamiliar with R should read Introduction to R, which is available on the R project homepage. The electoral and registration data used were compiled from the Virginia State Board of Elections using several custom written parsers and two different PDF-to-text engines. Please contact me for source data or more information at: [email protected].  This section only applies to readers interested in recreating the analysis and graphics produced in this tutorial. To completely recreate this analysis you will need the following:  Now that the prerequisites are installed we can get started with our data analysis. Start up your R environment and load the required libraries by typing in the following commands: We need to attach the VAHOD data set that comes with aggpol. This data set contains precinct-level electoral returns for state and federal elections in the Commonwealth of Virginia from 2001 to 2008. Since we are focusing on HOD#13, we’ll need to select just the records that have to do with that seat. The data set contains precinct-level electoral results for the following races: U.S. President, U.S.Senate, U.S.House of Representatives, Virginia Governor, Senate of Virginia, and Virginia House of Delegates. This breadth of electoral returns allows us to build a very detailed profile of the partisan bias of a district.  We will first determine the historical partisanship in HOD#13. Since partisanship can fluctuate over the years and different seats have different turnout expectations, we’ll first need to see the major party support for every seat in each election for precincts in HOD#13. We can use the historical.election.summary function from the aggpol package to group the precinct results into district results, and then break them down by seat and year. esum now contains:  We now have major-party turnout for every election in our data set. To best visualize the results we’ll build a bar graph comparing major-party turnout in each seat over time. We first need to transpose the election summary object (esum) from a summary format to an observation format, one line per distinct year+district+party. The plyr package makes this task extremely simple. We will now use the powerful ggplot2 package to view the Republican and Democratic support for each election, in each seat, for our subset: Result:
 This graphic gives us a decent understanding of district-level electoral trends. For U.S. federal elections (figs.: PVP, USH, USS), we can see a distinct drop in Republican support moving towards 2008; the results for U.S. House (USH) and U.S. Senate (USS), in particular, show a strong increase in Democratic support. This growth correlates to statewide trends that resulted in the election of two Democratic Senators representing Virginia for the first time since 1970. General Democratic gains notwithstanding, the House of Delegates (fig.: HD) results aren’t as promising for a Democratic challenger. The incumbent Del. Marshall saw more than 60% support in three of the last four elections and saw no challenger at all in 2003. While the district may be trending more Democratic over time, the voters of HOD#13 are obviously big fans of Del. Marshall.  Now that we understand the historical partisanship of this district we need to understand historical turnout, allowing us to project of the number of votes required to win. We will utilize the historical.turnout.summary function from the aggpol package to produce a summary of turnout for this district. Looking at this table one can see some data collection problems in the 2001 HD elections. In recent years, precincts belonged to only one House of Delegates seat, but in 2001 and somewhat less so in 2003 some precincts are split and some have duplicate names and now information on how to allocate results from different races to precincts. The turnout numbers are slightly affected by these problems, but the aggpol attempts to correct this by substituting alternate years or even races if possible.  The take away from from the previous table is that turnout for the last four House of Delegates elections has hovered around 30%. This makes some political sense,  because Virginia holds state elections in odd-numbered years with no federal elections to drive up turnout. This leaves a lot of registered voters to be activated, but we need to delve down to the precinct level to find them. We use the district.analyze function of aggpol to aggregate all electoral results into a summary for each precinct. hd013s is a data frame with columns calculated for every precinct; several values for each major party and other values for the precinct as a whole. Those statistics are: These variables can be visualized with the following graphic, adapted –along with definitions above– from Campaign Craft (Burton, Shea).   The actual columns in the data frame returned from from district.analyze are: The most useful statistic above is the Average Party Performance (APP), which is an average of major-party turnout in the 3 closest recent elections. The APP describes supporter levels for a best-case scenario in a close election. We’ve already calculated the APP of each major party (app.dem, app.rep), but when a race doesn’t have a third party candidate what we’ll usually visualize is the share of the combined partisan performance that each party receives. We’ll add these variables to our summary data frame generated previously, one for each major party. Now that we have the APP and partisan vote share for each party, we can visualize the precinct-level terrain for the Democratic challenger Mr. Bell. This visualization should show us the democratic support for each precinct and give us an idea whinc precincts could be competitive. We’ll produce this visualization using a density plot + 1d histogram, adapted from the seatsVotes plot in the pscl package. We’ll also draw a cut-line down the 50% vote mark to to help find competitive precincts.  We can see a lot of precincts are between 48% and 53% Democratic, which means those precincts could potentially go for either candidate. We need to classify these results into something more solid. Let’s say precincts with less than 48% Democratic share are Safe Republican, 48-52% are Tossup, and greater than 52% are Safe Democrat. This is a simple representation but can be refined later. We’ll add a seat classification to our data frame using  the cut function: Now we need to visualize how many precincts fall into which classification, using a histogram this time instead of a density curve.  From the histogram we see that not only does a Republican candidate enjoy more “Safe” precincts, but even the majority of the tossup precincts have less than 50% Democratic share. While the precinct breakdown looks bad, a Democratic win in this district is theoretically possible if these tossup precincts are held. A Democratic candidate will face a tough challenge, so the next step will be identifying Democratic and Democrat-leaning precincts to target.  To make this target precinct list we’ll need a method to prioritize the precincts so that we can reach the most persuadable voters while spending the least resources. A popular method to  identify a precinct as high-value is to sort precincts by lowest projected turnout with highest Democratic vote share. Lower turnout means there are registered voters waiting to be convinced to show up, and high Democratic vote share means more of those voters will be Democrats.  Since we measured both of these values (turnout%, democratic vote share), it is very easy to order our data by turnout (ascending) and democratic average party performance (descending) using R. This sorted list is our critical intelligence to finding persuadable voters, but we need a better way to visualize the output. Since we have two scalar variables (turnout %, democratic vote share) we can use a scatter plot with the Democratic vote share on the Y axis and Turnout % on the X. We’ll also color each precinct with its seat classification we defined earlier (Safe Republican, Tossup, Safe Democrat):  This chart echoes what we’ve seen previously: the Democratic challenger faces an uphill battle, but there is room for a win. We see a single “Safe Democract” precinct with very low turnout, and five “Safe Republican” precincts that run the board in turnout. Given the high number of “Tossup” precincts, and the fact that they run the gamut as far as turnout is concerned, we’ll need to incorporate additional information into our prioritization. If we also rank precincts by current voter registration, we can focus on precincts where we stand to gain the most ground.  Before we continue, we need to make sure there is enough difference in precinct-to-precinct registration to have an impact. Let’s look at some statistics for the current registration in this district. There are on average 2,970 current registered voters in each precinct, but the standard deviation is 1,014 voters. A standard deviation that high tells us we need to take into account registration if we want to focus on the precincts with 4000 people and not 1000 people. A histogram of current registration will help us clarify this finding:  The standard deviation was correct: we see some very small precincts and some large precincts, but the majority are somewhere in the 2000-4000 range. The difference looks to be large enough to include current registration in our ranking. We need to look at the Democratic Vote Share vs Turnout % scatter plot again, but with the points scaled to the current precinct registration. Democratic vote share by Turnout % This plot is almost complete and ready to be analyzed. The last job is to label the points with ther precinct names. Our current precinct_name variable is actually a unique identifier with a FIPS county code, a precinct code, and a name, and it is too long for a point label. We’ll shrink it down to just the name and then we’ll recreate the scatter plot with the label:  From the chart we can see that a Democrat in the HD#013 will want to focus contact efforts on the precincts in the upper-left hand corner of the plot and will want to target larger precincts before smaller. Integrating the current registration into our previous sort command leaves us with the following sort order: Now that we have our ranking, we can figure out how much each precinct might offer. Let’s first see the number of votes required to win the seat, the number of votes we’re projected to receive given the calculated APP, previous turnout, and current registration. The district.summary function will provide us will all this information: We can see that the projected turnout (proj.turnout.count) is about 25,401, so the votes projected to win this district is only 12,702. Using the  Democratic APP, we can project Democratic turnout at 12,074, so we need to find 628 votes to win. How do we find these votes?  Lets go back to our sorted precinct list and take the top 30% and call them our target.precincts. We’ve got our target list, and we know we need 628 votes from them to bring our total to 50% + 1. Adding a small buffer to that number, we’ll take 640 target votes and allocate them across our target precincts, proportional to the number of registered voters in the precinct. Hopefully, this will set more realistic goals for larger and smaller precincts. The final column in the result is the target increase for that precinct (column: ‘inc’). With this information in hand the campaign field operations can devise a contact strategy to bring these voters to the polls on election day.  Playing the role of campaign consultant, we have analyzed previous electoral outcomes in the 13th seat of the House of Delegates in Virginia. We have shown how a Democratic candidate can leverage increasing Democratic support and low turnout to make this race competitive. We have also created a precinct targeting methodology that provides a high-level blueprint for resources planning. The analysis we performed performed is very standard, but using R makes our methodology unique. A down-ballot or primary-challenger campaign taking advantage of this methodology will spend less money and can experiment more on their targeting, potentially leading them to a win. Are you a Democrat running for the Virginia House of Delegates who would like to see the same data for your race? Or, are you a Democratic congressional candidate preparing for the 2010 cycle? Contact me at [email protected] for robust targeting data or other analysis. Follow Offensive Politics on twitter "	 0 Comments
ggplot2: Two Color XY-Area Combo Chart	https://www.r-bloggers.com/2009/10/ggplot2-two-color-xy-area-combo-chart/	October 21, 2009	learnr	"[email protected] blog shows how to fill in the area between two crossing lines in an Excel chart. This post was also published as a guest-post on PTS blog.  Let’s try to replicate this graph in ggplot2.  
 First, load ggplot2 and generate the data frame to be used in the example (I am using a slightly modified dataset, therefore the final result will differ somewhat from the original graph). Filling just the area between the two lines is accomplished easily in ggplot2, however as we would need the segments to be of different colour then some extra work is required. In order to change the fill colour at each point where two lines cross, the points of intersection need to be calculated. Now, just as an extra precaution and to make sure that calculations are correct, we check visually the location of the points of intersection: As I am planning to colour the plot above generated using geom_ribbon the points of intersection need also to be presented in the form expected by geom_ribbon (x, ymin, ymax) – a simple copy of y3 accomplishes this. Additional error-checking is also obviously needed, as is indicated by the position of the left and rightmost green dots on the above graph – any two lines can have a point of intersection which falls outside the limits of the particular plot. For ggplot2 to be able to vary the fill colour at each crossing of the lines it needs to know the start and end point of each coloured area. This means that the middle points of intersection need to be duplicated, as they would be part of two adjacent areas filled with different colours. Now the coordinates of two lines and the start/end points of coloured areas need to be combined into one dataframe in a long format. Each segment is filled with a different colour, but we want to limit the number of fill colours to two. "	 0 Comments
InferenceR looks interesting	https://www.r-bloggers.com/2009/10/inferencer-looks-interesting/	October 21, 2009	Shige		 0 Comments
[, [[, $: R accessors explained	https://www.r-bloggers.com/2009/10/r-accessors-explained/	October 21, 2009	Christopher Brown	"R Accessors For more than ten years, I have been teaching R both formally and informally.  One thing that I find often trips up students is the use of R’s accessors and mutators.  ( For those readers not from a formal computer science background, an accessor is a method for accessing data in an object usually an attribute of that object.)  A simple example is taking a subset of a vector: 

letters[1:3]

[1] ""a"" ""b"" ""c""

 As you can see, the result is a character vector containing the first three letters of letters vector. Good programming languages have a standard pattern for accessor and mutators.  For R, there are three: [, [[, and $.  This confuses beginners coming from other programming languages.  Java and Python have one: ‘.’.  Why does R need three? The reason derives from R’s data centric view of the world.  R natively provides vectors, lists, data frames, matrices, etc.  In truth, one can get by using only [ to extract information from these structures, but the others are handy in certain scenarios.  So much so that after a while, they feel indispensible.  I will explain each and hopefully by the end of this article you will understand why each exists, what to remember and, more importantly, when to each should be used. Subset with [ When you want a subset of an object use [. Remember that when you take a subset of an the object you get the same type of thing.  Thus, the subset of a vector will be a vector, the subset of a list will be a list and the subset of a data.frame will be a data.frame. There is one inconsistency, however.  The default in R is to reduce the results to the lowest dimension, so if your subset contains only one result, you will only get that one item which may be something of a different type.  Thus, taking a subset of the iris data frame with only one column 

class( iris[ , ""Petal.Length"" ] )

[1] numeric

 returns a numeric vector and not a data frame.  You can override this behavior with the little publicized drop parameter, which indicates not to reduce the result.  Taking the subset of iris with drop = FALSE 

iris[ , ""Petal.Length"", drop=FALSE ]

 is a proper data frame. Things to Remember: Extract one item with [[ The double square brackets are used to extract one element from potentially many.  For vectors yield vectors with a single value; data frames give a column vector; for list, one element: 

letters[[3]]

iris[[""Petal.Length""]]

 The mnemonic device, here is that the double square bracket look as if you are asking for something deep within a container.  You are not taking a slice but reaching to get at the one thing at the core. Three important things to remember: Interact with $ Interestingly enough, the accessor that provides the least unique utility is also probably used the most often used.  $ is a special case of [[ in which you access a single item by actual name.   The following are equivalent: 

iris$Petal.Length

iris[[""Petal.Length""]]

 The appeal of this accessor is nothing more than brevity.  One character, $, replaces six, [[“”]].  This accessor is handiest when doing interactive programming but should be discouraged for more production oriented code because of its limitations, namely the inability to interpolate the names or use integer indices. Things to Remember: That is really all there is to it.  [ – for subsets, [[ – for extracting items, and $ – for extracting by name. "	 0 Comments
Lots of maps using R	https://www.r-bloggers.com/2009/10/lots-of-maps-using-r/	October 21, 2009	Shige		 0 Comments
AlgoCompSynth Reading List	https://www.r-bloggers.com/2009/10/algocompsynth-reading-list/	October 21, 2009	Ed Borasky		 0 Comments
The geese are migrating, and so have I	https://www.r-bloggers.com/2009/10/the-geese-are-migrating-and-so-have-i/	October 20, 2009	Ed Borasky		 0 Comments
Editing while running R within Emacs	https://www.r-bloggers.com/2009/10/editing-while-running-r-within-emacs/	October 20, 2009	Shige		 0 Comments
Running JAGS via R2jags	https://www.r-bloggers.com/2009/10/running-jags-via-r2jags/	October 20, 2009	Shige		 0 Comments
Some Interesting Packages in R: swfDevice, RGoogleTrends, FlashMXML, SVGAnnotation	https://www.r-bloggers.com/2009/10/some-interesting-packages-in-r-swfdevice-rgoogletrends-flashmxml-svgannotation/	October 19, 2009	Yihui Xie	I love R because there are always exciting new packages which can be far beyond your imagination. Here I’d like to introduce a couple of packages that look really awesome: This package is still at a pre-alpha stage but you can see a sketch now in R-Forge: https://r-forge.r-project.org/projects/swfdevice/ Its author, Cameron, certainly knows well that I will be excited to see it, because I’ve been waiting for a long long time for the REAL Flash animation output in R. What I’ve done in my animation package is simply using SWF Tools to combine several “static” pictures (PNG or PDF, …) into a naive Flash animation — by “naive” I mean there is no interaction or real dynamic stuff in the Flash animation. Hopefully Cameron will provide a useful tool to create genuine Flash animations directly from R (with the help of the library libming). By the way, I have to mention that the tikzDevice package by Cameron and another author is also fantastic for generating high-quality graphics LaTeX. Ever heard of Google Trends? Duncan Temple Lang released an R package named “RGoogleTrends” that allows you to download Google Trends data directly from R. Basically this package uses RCurl to log in your Google account and send queries to get Google Trends data. Well, you may ask “why bother using a package since I can manually download the data by myself?”; just imagine R can automatically and dynamically do it for you, so you don’t have to open the web page every day. They are also written by Duncan Temple Lang. FlashMXML can record R graphics in MXML (a kind of XML language) and we can compile the XML file to Flash output. SVGAnnotation enables us to save R graphics in SVG format, which also supports animation. The function animate() will be of great help to my animation package, I think. You may check the Omegahat website for more interesting packages: http://www.omegahat.org 	 0 Comments
RPostgreSQL 0.1-6	https://www.r-bloggers.com/2009/10/rpostgresql-0-1-6/	October 19, 2009	Thinking inside the box	"

As always, all the details (including the full ChangeLog) are at the 
local RPostgreSQL page, 
the 
RPostgreSQL package page at CRAN
and of course the Google Code
repository for RPostgreSQL.

 "	 0 Comments
"Comments on the ""R Clinic"""	https://www.r-bloggers.com/2009/10/comments-on-the-r-clinic/	October 19, 2009	Jeromy Anglim		 0 Comments
Xasax closes shop	https://www.r-bloggers.com/2009/10/xasax-closes-shop/	October 18, 2009	Joshua Ulrich	"
 "	 0 Comments
GenEstim : A simple genetic algorithm for parameters estimation	https://www.r-bloggers.com/2009/10/genestim-a-simple-genetic-algorithm-for-parameters-estimation/	October 18, 2009	Timothée Poisot	The GenEstim function presented here uses a very simple genetic algorithm to estimate parameters. The function returns the best estimated set of parameters ($estim), the AIC ($information) at each generation, and the cost of the best model ($bestcost) at each generation. Results of running the program with a logistic function : 	 0 Comments
Dianne Reeves: Strings Attached at the CSO	https://www.r-bloggers.com/2009/10/dianne-reeves-strings-attached-at-the-cso/	October 16, 2009	Thinking inside the box	"

Given that Dianne Reeves (who we had seen in
just
a few month earlier in our neighbourgood) has plenty of stage presence,
the format made for a more intimate concert yet with plenty of groove at
times too. The three had been touring in Europe for 25 shows, and it was a
really nice performance and a great way to end the week.  Recommend if you
can catch them somewere. 



 "	 0 Comments
Software for Surviving Graduate School Part 1	https://www.r-bloggers.com/2009/10/software-for-surviving-graduate-school-part-1/	October 16, 2009	Jared Knowles		 0 Comments
The optimal way to do sweave	https://www.r-bloggers.com/2009/10/the-optimal-way-to-do-sweave/	October 16, 2009	Shige		 0 Comments
Filled contour with log-log scale	https://www.r-bloggers.com/2009/10/filled-contour-with-log-log-scale/	October 15, 2009	Timothée Poisot	"A quick workaround to have a filled.contour plot with natural log10-log10 scale (instead of the default natural log scale)


plotmat <- function(mat,main='',factor='M',MeasuredResponse='Coexistence')

{

X <- as.numeric(rownames(mat))

Y <- as.numeric(colnames(mat))

if(factor=='C')

{

Y <- Y/0.16

}

rownames(mat) <- as.numeric(X)

colnames(mat) <- as.numeric(Y)

colorFun <- colorRampPalette(c(""black"",""darkblue"",""blue"",""green"",

""orange"",'yellow',""red"",""darkred"",'white')) lX <- log(X, 10)

lY <- log(Y, 10) pretty.X.at		<-	pretty(range(lX),n=6)

pretty.X.lab	<-	round(10^pretty.X.at,0)

pretty.Y.at		<-	pretty(lY,n=4)

pretty.Y.lab	<-	round(10^pretty.Y.at,2) 	pretty.Y.lab[pretty.Y.lab>1] <- round(pretty.Y.lab[pretty.Y.lab>1],0)

pretty.Y.lab[(pretty.Y.lab>0.1)&(pretty.Y.lab<1)] <- round(pretty.Y.lab[(pretty.Y.lab>0.1)&(pretty.Y.lab<1)],1)
  filled.contour(lX,lY,mat,

axes=FALSE,

frame.plot=TRUE,

color=colorFun,

ylab= MeasuredResponse,

xlab='Time between perturbation events',

main=main,

key.title=title(main=""""),

key.axes=axis(4,at=pretty(vmat)),

plot.axes={ axis(1,at=pretty.X.at,labels=pretty.X.lab)

axis(2,at=pretty.Y.at,labels=pretty.Y.lab) })

}

 "	 0 Comments
Comprehensive Change Detection Suite: Free & Available	https://www.r-bloggers.com/2009/10/comprehensive-change-detection-suite-free-available/	October 15, 2009	jennarussell	Open Data Group has launched a changed detection project on Google Code, http://code.google.com/p/change-detection/. This is an introduction and demonstration of using open source software and the Data Mining Group’s Predictive Model Markup Language (PMML) standard to perform data analytics.  Specifically, we show how using multiple Baseline models over segments can be used to detect of anomalous behavior. Case studies, sample data sets, and access to open source analytic suite of software are available. 	 0 Comments
“I’m a Republican because…”, visualized with R	https://www.r-bloggers.com/2009/10/%e2%80%9ci%e2%80%99m-a-republican-because%e2%80%a6%e2%80%9d-visualized-with-r/	October 15, 2009	jjh	The GOP recently relaunched its main web site with a new design and numerous interactive and social features like Facebook integration, blogs, etc. Of particular interest is the GOP Faces section, which asks users to submit a photo and answer the question “Why are you a Republican?” Not being a Republican, I was curious to see if there were any common themes among the submissions that would lead to insights about being a Republican and GOP.com user. Not excited about actually reading all 180 reasons, I instead used R to download, transform, analyze and visualize the data for me.  I used several packages (XML and plyr) to fetch and extract reasons, and then tm to filter stop words and identify commonly used terms. Finally, I used ggplot2, the invaluable ggplot2 blook, and a helpful post from the R-help mailing list to perform the visualization.  R code And the output:   Click for a page-sized PDF, or the raw terms and frequency counts. The most common term is ‘freedom’, followed by ‘equal’, and ‘pro’. After those come ‘personal’, ‘government’, ‘people’, ‘school’, ‘family’, and ‘believe’. A more robust analysis could use term extraction (pro family, pro life, anti government) or stemming, and then feed the results into a better visualization. That would take more than the 10 minutes I spent so far, so I’m leaving that as an exercise to somebody else. As it is I have the most common answer as to why GOP.com visitors are Republicans: freedom. I think that’s probably why anybody belongs to any political party, but without a corpus from other parties I suppose we’ll never know.  	 0 Comments
R Tutorial Series: Introduction to The R Project for Statistical Computing (Part 2)	https://www.r-bloggers.com/2009/10/r-tutorial-series-introduction-to-the-r-project-for-statistical-computing-part-2/	October 15, 2009	John M. Quick	Welcome to part two of the Introduction to The R Project for Statistical Computing tutorial. If you missed part one, it can be found here. In this segment, we will explore the following topics. Before we start, you may want to download the sample data (.csv) used in this tutorial. Be sure to right-click and save the file to your R working directory. While values can be input directly into R, the most common method for obtaining data is to import it from preexisting sources. Most spreadsheets can be converted to CSV (comma-separated values) files, which are recommended for use with R. However, by way of the foreign package, a variety of alternative data files can be imported, such as ones generated in SPSS. Below are examples demonstrating how to import data using both methods. To import data from a csv file, use the read.csv(“FILENAME”) command, where FILENAME is the name of the file that you would like to import. When a file is read, the console displays its contents, as depicted in the screenshot below.  Similarly, the foreign package can be used to import files from other spreadsheet and statistical analysis programs. A hypothetical example of loading data from an SPSS file (.sav) follows. Note that there are a variety of read.FUNCTION commands available in R. Depending on your source file, you may be better off using a different version of the command than what has been presented here. Nonetheless, the process of importing data will remain the same. An important aspect of conducting statistical analyses in R concerns the use of variables. As with other programming languages, variables can be thought of as containers that store information and allow it to be manipulated. This contrasts with merely displaying information, as takes place in previous demonstrations of the read command. For example, when the command read.csv(“intro_pt2_data.csv”) was used, age and income data for 20 subjects was read into and displayed in the console. Now the numbers can be seen, but what if you want to conduct statistical analyses on the data? To do this, you would have to save the information into a variable using the  Thus, the line of code above creates a new variable named dataSet and sets it to equal the contents of the imported CSV file. Now that the contents of the spreadsheet have been stored in a variable, the individual data elements can be accessed. In the sample provided, age and income values were collected for 20 subjects and entered into a two-column spreadsheet. Since both age and income have their own column of values, each can be accessed individually using the format DATASET $COLUMN, where DATASET is the name of the variable that contains all of data (i.e. dataSet) and $COLUMN is the name of the column within the data (i.e. $Age or $Income). The following code demonstrates how individual variables within a dataset can be accessed and displayed. Data can also be saved as a frame. A data frame is very similar to a dataset in that it stores information and its variables can be accessed in the same way. However, data frames are displayed in a nice tabular format when printed in the R console. Additionally, operations can be conducted on data frames that cannot be done on regular dataset variables. Often, you will want to use both dataset and data frame variables when working in R. The differences between them will become more apparent in future tutorials. For now, know that you can create a data frame from a preexisting dataset via the data.frame(DATASET) command, where DATASET is the name of the variable containing the data. A convenient method for accessing variables comes thanks to the ability to attach datasets in R. This is accomplished through the attach(NAME) command, where NAME is the name of the dataset variable that you want to attach. This allows you to refer to variables within the dataset without the need to list the name of the dataset and the $ symbol. Hence, the example below accomplishes the same tasks as in the previous section, but with less code. Note that each time R is run, the dataset must be reattached. This method is most useful when you know that you will be working with a single dataset for an entire session. Furthermore, a data frame can be attached and used in the same manner as a dataset. Every time that you create a variable to store values in R, it is saved to the current Workspace. A Workspace is a repository for all of the objects managed during a session. For instance, when you assigned the variable “dataSet” to the contents of the sample CSV file, the dataSet object, complete with Age and Income data, was entered into the R Workspace. A Workspace can be saved at any time and loaded during a future session. Workspace files always end with the extention “.RData” and are a useful way to pick up your work where you left off at the end of a previous session. The essential functions related to Workspaces are demonstrated below. To save a Workspace file, use the save.image(“PATH/FILENAME.RData”) command, where PATH represents the directory path where you would like to save the new file (the working directory is used by default) and FILENAME is the name of the new file. Similarly, to load a Workspace file, use the load(“PATH/FILENAME.RData”) command, where PATH represents the directory path to the previously saved file (the working directory is used by default) and FILENAME is the name of the previously saved file. Furthermore, a list of all of the objects currently held in the Workspace can be displayed via the ls() function. Note that R also features a Workspace menu where each of the above tasks can be handled. The Workspace Browser (pictured) is especially useful for visualizing the contents of your current Workspace.  As discussed in part one of this tutorial, the R Console is where commands are issued and subsequent outputs are displayed. In contrast to the Workspace, where all of the objects in use are being stored, the Console is the complete history of the actions taken by those objects. Consider a meeting between people as an analogy to further explain the relationship between the Workspace and the Console. All of the individuals who attend the meeting are contained in a single room (i.e. the Workspace). Everything that the participants do and say is recorded in the meeting minutes (i.e. the Console). Thus, the Workspace contains objects (such as the people who attend a meeting) and the Console consists of a log of interactions between objects (such as what people say to each other during a meeting). The contents of the Console can be saved to a text file using File > Save As… from the menu. In fact, the same procedure can be executed from the Quartz window to produce a PDF of a particular graphic. Moreover, the contents in any of the R windows can be copied and pasted into another program, such as a word processor. Unlike a Workspace, which may be saved and reloaded from session to session to continue work, a Console is most useful for keeping track of what you have done in previous sessions. This history can be a reminder of where you left off during the last session, the results of prior analyses, how to execute certain functions, or an array of other items. A sample Console output is pictured below. Take notice of the contrast between this and the previous image of the Workspace Browser.  When getting started with R for the first time, or when exploring new facets of the program, it can be useful to get help from more experienced users. Fortunately, R has a large community with a strong online presence. Help documentation, FAQs, tutorials, and discussions can be found covering nearly every aspect of R that one would ever need or want to become familiar with. The following list represents just a few of the excellent R resources that have assisted me thus far. In spite of the abundance of R information available online, I have decided to create a series of my own tutorials for three main reasons. First, the R knowledge base is scattered across the internet, making it difficult for users to find what they need, when they need it. Second, information about R has been written by many people, in many places, at many times, causing inconsistencies in language and format to exist that challenge users’ ability to easily comprehend and apply the solutions that they find. Third, there is no cohesive set of R tutorials that appeals directly to my own (and others’) usage of the program, which leaves me searching for small bits of answers in many different places rather than finding holistic solutions. Thus, my goal in creating this series of tutorials is to provide fellow researchers with a coherent and unified set of essential statistical analyses that can be applied to diverse projects using the R system. 	 0 Comments
The Elements of Statistical Learning	https://www.r-bloggers.com/2009/10/the-elements-of-statistical-learning/	October 14, 2009	Paolo		 0 Comments
What’s Happening, Man?	https://www.r-bloggers.com/2009/10/whats-happening-man/	October 14, 2009	Ed Borasky		 0 Comments
RPostgreSQL 0.1-5	https://www.r-bloggers.com/2009/10/rpostgresql-0-1-5/	October 13, 2009	Thinking inside the box	"

This is a maintenance and bug fix release that addresses four of the seven
issue
at the Google Code page. Two more are really enhancement requests for
which we encourage patches as we are unlikely to have to write them, and the
last issue I have been unable to replicate.

 

This release has benefitted greatly from the generous help of
Neil Tiffin who now looks after all 
things OS X and keeps a good eye on the Google Code site (where I had, ahem.
overlooked the existence of these open issue tickets for a few months), 
Joe Conway who we can count on for
hard-core PostgreSQL issues as well as 
Jeff Horner
for general R, DBI and database smarts.

 

Some more details (including the full ChangeLog) are at the 
local RPostgreSQL page, 
the 
RPostgreSQL package page at CRAN
and of course the Google Code
repository for RPostgreSQL.

 "	 0 Comments
Investigation the relationship between two variables using a scatter plot	https://www.r-bloggers.com/2009/10/investigation-the-relationship-between-two-variables-using-a-scatter-plot/	October 13, 2009	Ralph	"The relationship between two variables can be visually represented using a scatter plot and will provide some insight into the correlation between the variables and possible models to describe the relationship. There are different ways to produce scatter plots in R making use of either the base graphics system, the lattice graphics library, ggplot2 or other packages. The R system has various data sets available for analysis, for example the Puromycin data which describes an experiment to study the relationship between reaction velocity and substrate concentration in an enzymatic reaction involving untreated cells or cells that were treated with Puromycin. The variable rate can be plotted against the variable conc to investigate the relationship. Using the lattice package we can use the xyplot function to create a graph with the following code: In this graph we do not distinguish between the untreated and treated cells and this code produces this graph:
 Reaction Rate plotted versus Concentration for Puromycin data Here the graph is now:
 Plot of the Reaction Rate against Concentration by Treatment "	 0 Comments
RExcel	https://www.r-bloggers.com/2009/10/rexcel/	October 13, 2009	Gregor Gorjanc		 0 Comments
Example 7.15: A more complex sales graphic	https://www.r-bloggers.com/2009/10/example-7-15-a-more-complex-sales-graphic/	October 13, 2009	Ken Kleinman		 0 Comments
Make Shift-Enter do a lot in ESS	https://www.r-bloggers.com/2009/10/make-shift-enter-do-a-lot-in-ess/	October 12, 2009	kjhealy	If you use Emacs and ESS to run R, then here’s a nice tweak I found on the Emacs Wiki. The following bit of elisp goes in your .emacs file (or equivalent). Starting with an R file in the buffer, hitting shift-enter vertically splits the window and starts R in the right-side buffer. If R is running and a region is highlighted, shift-enter sends the region over to R to be evaluated. If R is running and no region is highlighted, shift-enter sends the current line over to R. Repeatedly hitting shift-enter in an R file steps through each line (sending it to R), skipping commented lines. The cursor is also moved down to the bottom of the R buffer after each evaluation.  Although you can of course use various emacs and ESS keystrokes to do all this (C-x-3, C-c-C-r, etc, etc) it’s convenient to have them bound in a context-sensitive way to one command.  This is in my fork of the Emacs Starter Kit, by the way. 	 0 Comments
Zut alors!	https://www.r-bloggers.com/2009/10/zut-alors/	October 12, 2009	Thinking inside the box		 0 Comments
Public opinion on health care reform	https://www.r-bloggers.com/2009/10/public-opinion-on-health-care-reform/	October 12, 2009	Jim		 0 Comments
R Tutorial Series: Introduction to The R Project for Statistical Computing (Part 1)	https://www.r-bloggers.com/2009/10/r-tutorial-series-introduction-to-the-r-project-for-statistical-computing-part-1/	October 11, 2009	John M. Quick	R is a free, cross-platform, open-source statistical analysis language and program. It is also an alternative to expensive commercial statistics software such as SPSS. The environment for R differs from the typical point and click interface found in most professional office applications. Although it does take some effort to become familiar with, R ultimately proves to be an affordable, customizable, and expandable statistical analysis solution. This tutorial intends to quickly and easily bring new users up to speed with R. Only the most basic elements are covered. Detailed statistical analyses and advanced techniques will be covered in future articles. Below is a list of the topics to be covered in part one of this tutorial. Feel free to jump to a particular section of interest at any time. R is free, open-source software that runs on Mac OS, Windows, Linux, and Unix platforms. Download links for all versions can be found at the official R Project website (https://www.r-project.org). After downloading R, you should install the program as appropriate for your operating system. In Mac OS X 10.6, the installation process was simple and consistent with almost every other application that I have used. The R interface is composed of three main parts. The first is the Console window, which resembles a simple programming interface. This is the default view that loads when R is launched. The Console executes functions and commands, and displays outputs related to those operations. A second view, called the Quartz window, displays visual information such as graphs, histograms, and plots. It automatically appears when a related command is executed in the console window and can also be manually displayed through the Window menu. Lastly, the Editor window, which resembles a basic word processor, is called when a text file is opened in R. This is especially useful for looking back at past work done in the Console that can be applied to a new project. Together, the Console, Quartz, and Editor windows compose the R interface. While all are useful components, the vast majority of your time working with R will be spent in the Console window.    Commands are most commonly issued to R in the form of functions. These are called by entering the name of the function, followed by parentheses, and then pressing enter. Some functions have various arguments, or parameters, which can be specified inside the parentheses. Only one function can be called per line in the Console window. There is no terminal character (i.e. semicolon, comma) found at the end of a line, which differs from many other programming languages. A line is executed by pressing the return key. Afterwards, R will automatically display the output of the commands (where necessary) and jump down to a new, blank line in the Console. One example of an R function is q(). This is used to exit, or quit, the program and can be called as follows. One of the initial things that you want to do when you launch R for the first time is to set its working directory. This is the default location on your hard disk that R will look to read and write files. The working directory is comparable to what is called the “default folder” in many other applications. It is important to select a location that is easy to find and remember, so you can access your files when you need them. I dedicated an entire folder to R on my system, with subfolders for each project. To display the current working directory, use the function getwd(). To change the current working directory, use the function setwd(‘PATH’), replacing PATH with the directory path that you would like to use. Use getwd() again to verify that the change took place. Note that you have the option to set the working directory at any time. Do this when you want to access files in a new location, such as when you are working on multiple projects at the same time or at the start of a new project. The ability to install packages is a major benefit of R over its competitors. As an open project, anyone can contribute quality custom commands to the R community. Packages extend the functionality of R by enabling additional visual capabilities, statistical methods, and discipline-specific functions, just to name a few.  A number of CRAN repositories, or mirror sites that host R packages, are available. A complete listing can be found on the official website. When choosing a repository to download from, you may want to consider things such as its location, reputation, and relevance to your work. To set your mirror site, use the options(CRAN = “URL”) command, where URL is the url to the CRAN repository. In the example below, the user connects to the UCLA repository. To obtain a list of all packages available at a given mirror site, use the available.packages() command. To install a specific package, use the install.packages(“NAME”) command, where NAME is the name of the desired package. Note that you can install all packages on a mirror site using the install.packages(available.packages()) command. This is recommended, although at nearly five gigabytes as of this writing, an entire CRAN repository can take a significant amount of time to download and install. The other option is to wait until you know that you need specific packages and install them on an a la carte basis. To remove a specific package, use the remove.packages(“NAME”) command, where NAME is the name of the desired package. To update all installed packages, use the update.packages() command. For each out of date package that is found, you will be prompted to confirm the update, as demonstrated below. In this case, you should type “y” and press enter to continue. To use a package, it first needs to be loaded through the library(NAME) command, where NAME is the name of the package. Each time that R is run, you will have to reload any special packages that you need. This concludes part one of the introductory tutorial to using R. In part two, more of the basic features of R will be presented, including how to import data, create and use variables, and manage workspace and console files. 	 0 Comments
Chicago Marathon 2009	https://www.r-bloggers.com/2009/10/chicago-marathon-2009/	October 11, 2009	Thinking inside the box	"

First, of course, was the Chicago weather.  After two successive marathon in
excessive heat — the
2007 race I completed, poorly,
with its thousands of runners forced to abandon when the race was cancelled
due to excessive heat, and the 2008 version which I skipped as I
ran
Berlin that year just weeks before Chicago,
this year had forecasts of temperatures in the thirties and possibly snow the
night before.  Well, the weather cleared up — but with those clear skies we
still got a severe weather alert for the area due to frost!  So at the (now
earlier by 30 minutes) race start, it was very nippy and in the higher 30s,
improving steadily under sunny skies.  Overall, a little chilly and hence not
exactly ideal, but not too bad in the grand scheme of things.  A little windy
coming up Michigan Avenue.  But hey,
Wanjiru still finished with a course record
though still well off the world record.  So maybe not a fail after all. 

 

The second fail, though, was my GPS which I had just
blogged
about yesterday (albeit indirectly).  I had had my issues with the previous
(much simpler and older) Forerunner 205 which lost satellite tracking when
running downtown amidst the skyscrapers. I had high hopes that this newer model
would do better.  But no, not only did it loose track within the first few
miles, it even managed to outdo the older model by turning its GPS tracking off
in the later stages.  Now that’s a fail!

 

The third and final fail, unfortunately, concerns my run.  Training had gone
well enough to let me hope for another decent race. However, a latent cold
during the last two weeks had left me somewhat afraid I might not do well.
Things felt better yesterday, and I ended up running a decent first eighteen
miles to the waterstop the rest of my household was working.  And thereafter:
well let’s just say that the wheels came off.  I ended up with a 3:25:40 (or
around 7:51 min/mile and 4:53 min/km) which is not terrible but also not one
of my better races.

 

I updated the two race data geekery charts shown here before to illustrate the fail.
As I am running out of shades of blue for Chicago, I show the 2009 race in
purple.  A decent, flat chat indicating a reasonably steady pace throughout most
of the race … followed by one of the very worst finished.  And as discussed
above, I cannot really blame the weather either.

 

I also updated my ‘performance by race type through time’ chart which suggests
that I may be getting slower for marathons (if we allow for a non-linear
effect). Eek.


 
 

 "	 0 Comments
R for system administration and scripting	https://www.r-bloggers.com/2009/10/r-for-system-administration-and-scripting/	October 10, 2009	Thinking inside the box	"
One of such cases just happened a few minutes ago.
The aforementioned Garmin Forerunner 405
can cooperate quite nicely with Linux using the gant reader for the
ant wireless communication protocol between the usb hardware dongle and the
Garmin 405. (Sources for gant are both
this file and this
git archive.)  I had meant
to blog about this tool and the resulting files one of these days anyway, but
today I just want to mention that the default filenames created by the program
were somewhat horrid such as 20.09.2009 101112.TCX to denote the 20th of
September of this year at 10:11h and 12 seconds.  As we all know, filenames
with spaces are bad for the environment as well as plain annoying.  So I had made
the simple change in the C sources to switch to a saner format such as
20090920-101112.TCX (and I see that the git archive now contains a
similar fix).  But that still left me with some 80+ files with the dreaded
names.

 
There are of course many ways to skin this cat and to rename the files in
bulk.  However, I found the following four lines to be fairly succinct

 
Lastly, I do not mean to imply that Python or Perl or Ruby or (insert
favourite tool here) cannot do it equally well.  I simply meant to say that
programmatically creating new filenames is definitely easier in
R than it would have been in shell.
And as an added bonus, we even get fully parsed time objects that I could
have tested for.  But then tests and documentation never get written on a
Saturday. 

 "	 0 Comments
50000 Revisions Committed to R	https://www.r-bloggers.com/2009/10/50000-revisions-committed-to-r/	October 10, 2009	Yihui Xie	While these facts are revealing their great efforts in helping R users, we can see their work hours in committing revisions to R. For example, the answer to my question is clear in the graph below:  Prof Ripley Never Sleeps Here I only selected four authors who have largest number of commits during 1997~2009. We can see the changes of working hours along these years: Working hours of four R core members The patterns are clear: Kurt does not like burning night oil; Martin tends to work very early in the morning (esp during 2000~2004); Peter always work at mid-night (highly centered around 12pm); and for Prof Ripley, he works round the clock but most in the morning (probably that’s when he begins to “Ripley” users? after that time, less people dare to report bugs so his work decays exponentially?) 	 0 Comments
Visualizing sample relatedness in a GWAS using PLINK and R	https://www.r-bloggers.com/2009/10/visualizing-sample-relatedness-in-a-gwas-using-plink-and-r/	October 9, 2009	Stephen Turner		 0 Comments
celebrating R commit #50000	https://www.r-bloggers.com/2009/10/celebrating-r-commit-50000/	October 9, 2009	romain francois	Today, Brian Ripley commited the revision 50 000 into R svn repository.  so it is time to celebrate and have some fun with the svn log to analyze the 50 000 commits … with R of course. First we need to grab the full svn log, using command line svn, something like this:   … or you can download it from my website if you don’t have svn on your machine now we need to read the data into R : we might also be interested in release date, version number and size of the distribution of each R release that is archived on CRAN, which we can get like this : now we can do some graphics. I’m using lattice here because I am familiar with it, but I’m sure interesting plots could be done using ggplot2, in fact checkout this post from Yihui Xie using ggplot2 First I need to define some helper panel functions I’ll use in the plots below 	 0 Comments
"Comments on ""Ecological Statistics with R"""	https://www.r-bloggers.com/2009/10/comments-on-ecological-statistics-with-r/	October 8, 2009	Jeromy Anglim		 0 Comments
“Outlook cannot open this item.” and tasks missing	https://www.r-bloggers.com/2009/10/%e2%80%9coutlook-cannot-open-this-item-%e2%80%9d-and-tasks-missing/	October 8, 2009	heuristicandrew	Recently Microsoft Office Outlook 2007 started giving me the vague error message Outlook cannot open this item. The item may be damaged.  The message would appear randomly throughout the day.  Sometimes five error message boxes would be stacked up on top of each other. OK, but which item?  What kind of item?  Is it an email, appointment, or task? One clue was my task list was empty. In the past, I’ve had problems with reminders showing up hours or days late (though I haven’t noticed that lately). Someone in a forum suggested running Office Diagnostics.  Diagnostics found and fixed something, but it didn’t give details.  It didn’t help either.  Thanks, Microsoft. Then I found a solution for Outlook 2000 to run “outlook /cleanreminders”, but it does nothing in Outlook 2007. The solution for Office 2007 is to run click Start, click Run, type outlook.exe /resettodobar, and click OK.  Outlook will start, say “Loading” in the task pane for a few minutes, and then everything works again. According to the Outlook 2007 command line switches documentation, /resettodobar “Clears and regenerates the To-Do Bar task list for the current profile. The To-Do Bar search folder is deleted and re-created.” 	 0 Comments
Delete rows from R data frame	https://www.r-bloggers.com/2009/10/delete-rows-from-r-data-frame/	October 8, 2009	heuristicandrew		 0 Comments
What ‘The power of R’ is saying	https://www.r-bloggers.com/2009/10/what-the-power-of-r-is-saying/	October 7, 2009	the R user...		 0 Comments
Factor Analysis in R	https://www.r-bloggers.com/2009/10/factor-analysis-in-r/	October 7, 2009	Jeromy Anglim		 0 Comments
Efficient Variable Selection in R	https://www.r-bloggers.com/2009/10/efficient-variable-selection-in-r/	October 7, 2009	Jeromy Anglim		 0 Comments
R Commander: A Basic Statistics GUI for R	https://www.r-bloggers.com/2009/10/r-commander-a-basic-statistics-gui-for-r/	October 6, 2009	Stephen Turner		 0 Comments
Export Data Frames To Multi-worksheet Excel File	https://www.r-bloggers.com/2009/10/export-data-frames-to-multi-worksheet-excel-file-2/	October 6, 2009	Tal Galili	"A few weeks ago I needed to export a number of data frames to separate worksheets in an Excel file. Although one could output csv-files from R and then import them manually or with the help of VBA into Excel, I was after a more streamlined solution, as I would need to repeat this process quite regularly in the future. CRAN has several packages that offer the functionality of creating an Excel file, however several of them provide only the very basic functionality. The R-wiki page on exchanging data between R and Windows applications focuses mainly on the data import problem. My objective was to find an export method that would allow me to easily split a larger dataframe by values of a given variable so that each subset would be exported to its own worksheet in the same Excel file. I tried out the different ways of achieving this and documented my findings below.  
 The goal is to split the iris dataset by the unique values of iris$Species, and export it to a new Excel file with worksheets: setosa, versicolor and virginica. Two ways of storing the information were used for the purposes of this exercise: First create the data frames, and then the character vector with the object names – now we will have three separate data frames called setosa, versicolor and virginica. Storing data in a list of dataframes is obviously much more convenient, however, as you see later many (or most) functions don’t accept lists as input. Note dataframes2xls saves dataframes to an xls file. Its main function write.xls, is a wrapper around a utility called xls2csv. xls2csv makes use of the Python module pyExcelerator and the afm submodule of the Python module matplotlib, both of which are included in dataframes2xls. Appending to an existing file not possible, this negates the use of list of dataframes as input. One major shortcoming is that one needs to specify the names of exported dataframes manually. Also, I was not able to pass sheet names to the function if there was more than one sheet. There is a way to work around the manual specification of data frames, but as you can see it is not very intuitive: Note WriteXLS is a “Perl based R function to create Excel (XLS) files from one or more data frames. Each data frame will be written to a separate named worksheet in the Excel spreadsheet. The worksheet name will be the name of the data frame it contains or can be specified by the user”. Like dataframes2xls WriteXLS does not take lists as input, and therefore each of the data frames needs to be generated beforehand before calling the function(s). Also appending to a file is not possible as the Excel file, if it exists, is overwritten. Note xlsReadWrite saves a data frame, matrix or vector as an Excel file in Excel 97-2003 file format. The dataframe can be written to one sheet only. It is not possible to split the data between separate sheets, as append data to existing files feature is available in the Pro-version. The Pro-version has a 30-day trial, so I tried it out. Task accomplished. The Pro-version has several other nice features, such as the ability to save images to an Excel file, or to write Excel formulas. For reference, the licence costs are as follows. Note RODBC sqlSave function saves the data frame in the specified worksheet via ODBC after initiating the connection using a convenience wrapper odbcConnectExcel. This worked well. Another good thing to note is that you can append to already existing files. Note RDCOMClient allows to access and control applications such as Excel, Word, PowerPoint, Web browsers from within R session. As an alternative, rcom package provides similar functionality. Developer’s website provides some useful functions for exporting/importing dataframes to/from Excel. Now I have an Excel file open with an empty “Sheet1″. I delete it and, after specifying the save-directory (it defaults to My Documents) & filename , save the file. I have Office 2007 installed, so this file format is used by default. Another example of RDCOMClient in action can be seen in this R-help post. The Windows-only solutions were (expectedly) the most flexible with RDCOMClient-approach providing the greatest control over output. However, if formatting was not that important the simplest way to export data to multiple Excel worksheets would be via RODBC (if on Windows machine). "	 0 Comments
Making Emacs Buffer Names Unique Using the Uniquify Package	https://www.r-bloggers.com/2009/10/making-emacs-buffer-names-unique-using-the-uniquify-package/	October 5, 2009	erikr	"
If you ever work in Emacs with different files that all have the same name, this tip may be very useful for you.  In my experience, this most often happens when working with Makefiles from multiple projects or directories.  Assuming you have a buffer visiting a file called Makefile, by default Emacs will call the second buffer Makefile<2>. This is not very useful for figuring out which one is which.
 
Enter the uniquify package. It offers several options for making buffer names unique.  In my .emacs, I have the following two lines of code.
 
You can set the value to any of the following options, nil being the default. These examples are taken from the help file within Emacs. They use the information from the directories the files are located in to make the names unique.  The first buffer called “name” is visiting a file in the directory bar/mumble/, and the second buffer is visiting a file called “name” in directory quux/mumble.
 "	 0 Comments
Expressions in expresssion passed to functions that return functions	https://www.r-bloggers.com/2009/10/expressions-in-expresssion-passed-to-functions-that-return-functions/	October 5, 2009	R Tips	OK in the last two tips I explained how functions can return functions and how to leverage expressions.  Now lets get complicated.  This is really how my testing code works; expressions embeded in expressions that are passed to functions that return functions that do the actual testing. read more 	 0 Comments
Expressions!	https://www.r-bloggers.com/2009/10/expressions/	October 5, 2009	R Tips	I never really saw a use for these until I started testing code en mass.  I found myself doing lots and lots of copy and paste.  I heard it from Steve McConnell in his Code Complete book where he said if you are copying and pasting you are programming it wrong.  Well in R, expressions help you avoid copying and pasting code. read more 	 0 Comments
Functions from functions	https://www.r-bloggers.com/2009/10/functions-from-functions/	October 5, 2009	R Tips	I’ve been really busy lately working on my dissertation and applying for jobs. But I’ve found a few really cool things that I’d like to share.  These are pretty complex things so lets break it up into a few tips. First Tip: did you know that you can return a function  from a function? read more 	 0 Comments
Oil vs gas	https://www.r-bloggers.com/2009/10/oil-vs-gas/	October 5, 2009	kafka	"Recently, friend of my, got investment advise from his broker – long gas, because the price of gas is very low compared to oil. The broker didn’t indicate neither profit target or stop loss…
I got hooked on the idea. First of all, I ran linear regression on monthly prices of oil and gas from 01-12-1993 to 01-09-2009. I was skeptical about ‘oil vs gas’ idea, but after seeing this graph it became interesting. R-Square is high 0.7385. Check this out: (the prices are adjusted with log())  The spread between oil and gas indicates, that it was very high back in September.  Finally, I took two funds: USO (oil’s ETF) and UNG (gas’s ETF) to see how does it look on real time (ok ok daily) data. As we can see, in late September the spread reached a new high. The problem is, that data set is very limited and we can see only ~3 years.
 And the last one – green stands for oil’s log(price) and blue one for gas’s log(price).  "	 0 Comments
Including R Code in a Blog Post	https://www.r-bloggers.com/2009/10/including-r-code-in-a-blog-post/	October 5, 2009	Jeromy Anglim		 0 Comments
Using R to Analyze Baseball Games in “Real Time”	https://www.r-bloggers.com/2009/10/using-r-to-analyze-baseball-games-in-real-time/	October 4, 2009	erik	"
This post originally appeared on my WordPress blog on October 4, 2009. I present it here in its original form.
 
In order to honor the last day of the 2009 MLB regular season (excepting the Twins/Tigers tiebreaker Tuesday night), I was reading a book that combines a few of my favorite thing: statistics, R, and baseball.  The book, by Joseph Adler, is called Baseball Hacks, and I highly recommend it if you are interested in analyzing baseball data.  Joseph uses Excel for some tips, R for others, and shows you how to download historical and current baseball data for further analysis. One tip that the book offered was a way to download “real time” baseball data from MLB’s site in XML format.  I decided to try to write some R functions to retrieve, summarize, and analyze what was available. 
 
Joseph shows how, at least at the time of the writing of his book and this post, you can go here to download a wealth of XML data from past and current seasons.  If you drill down far enough into the directories, you can find a file called miniscoreboard.xml, which is the one I use for this analysis. 
 
Here are the R functions I wrote.  You can copy and paste them into your R session so that they are available to you.  The next section will describe how to use them. Writing these was fairly straightforward, and simply a matter of XML manipulation.  I admit that there may be far better ways to do this manipulation using the XML package, but this worked for now. 
 
Here is how to run some simple analyses on baseball games happening right now. This is the real value add for the idea of downloading data through R.  Obviously you could just go to your favorite sports site to find scores if you wanted to know how your team was doing, but pulling the data into R lets you further analyze the data, and even combine it with other data sources (e.g., weather). 
 
These functions are far from robust, and I think they only work for the current year (i.e., 2009, dates from 2008 were not working right). The format looks like it has changed over time, which is not surprising.  I only use a very small subset of the available data, even the miniscoreboard.xml file contains far more information than I summarize here. This is really the first time I have dealt with XML data, so I am sure there is a lot more that can be done, but for a one-day project, I think the results are pretty interesting. I will definitely provide the updates I make to these functions, and may even start a baseball R package if they grow extensive enough. I suppose this is a project I can work on in the off season!  
 "	 0 Comments
Using R to Analyze Baseball Games in “Real Time”	https://www.r-bloggers.com/2009/10/using-r-to-analyze-baseball-games-in-%e2%80%9creal-time%e2%80%9d/	October 4, 2009	erikr	"
In order to honor the last day of the 2009 MLB regular season (excepting the Twins/Tigers tiebreaker Tuesday night), I was reading a book that combines a few of my favorite thing: statistics, R, and baseball.  The book, by Joseph Adler, is called Baseball Hacks, and I highly recommend it if you are interested in analyzing baseball data.  Joseph uses Excel for some tips, R for others, and shows you how to download historical and current baseball data for further analysis. One tip that the book offered was a way to download “real time” baseball data from MLB’s site in XML format.  I decided to try to write some R functions to retrieve, summarize, and analyze what was available.
 
Joseph shows how, at least at the time of the writing of his book and this post, you can go here to download a wealth of XML data from past and current seasons.  If you drill down far enough into the directories, you can find a file called miniscoreboard.xml, which is the one I use for this analysis.
 
Here are the R functions I wrote.  You can copy and paste them into your R session so that they are available to you.  The next section will describe how to use them. Writing these was fairly straightforward, and simply a matter of XML manipulation.  I admit that there may be far better ways to do this manipulation using the XML package, but this worked for now.
 
Here is how to run some simple analyses on baseball games happening right now. This is the real value add for the idea of downloading data through R.  Obviously you could just go to your favorite sports site to find scores if you wanted to know how your team was doing, but pulling the data into R lets you further analyze the data, and even combine it with other data sources (e.g., weather).
 
These functions are far from robust, and I think they only work for the current year (i.e., 2009, dates from 2008 were not working right). The format looks like it has changed over time, which is not surprising.  I only use a very small subset of the available data, even the miniscoreboard.xml file contains far more information than I summarize here. This is really the first time I have dealt with XML data, so I am sure there is a lot more that can be done, but for a one-day project, I think the results are pretty interesting. I will definitely provide the updates I make to these functions, and may even start a baseball R package if they grow extensive enough. I suppose this is a project I can work on in the off season!
 "	 0 Comments
Calculating Scale Scores for Psychological Tests	https://www.r-bloggers.com/2009/10/calculating-scale-scores-for-psychological-tests/	October 4, 2009	Jeromy Anglim		 0 Comments
Using doc-view with auto-revert to view LaTeX PDF output in Emacs	https://www.r-bloggers.com/2009/10/using-doc-view-with-auto-revert-to-view-latex-pdf-output-in-emacs/	October 3, 2009	erikr	"
When I am authoring a  document in Emacs, such as a report or my CV, it is useful for me to compile the  source file periodically to see what the resulting file PDF looks like.  I used to run a separate PDF viewer to look at the output, but I now have a complete Emacs solution.
 
When writing a  document, I usually want the output to be a PDF file.  Accordingly, I put the following in my .emacs file.
 
I then split my Emacs frame into two buffers vertically, using C-x 3 (see screencast below).  After compiling my  file with C-c C-c, I visit the resulting PDF file in the other Emacs window.  The Emacs doc-view package will display the PDF file.
 
The final piece to the puzzle is to set files visited in doc-view-mode to auto-revert when changed on disk.  That way, then I update my  file and recompile with C-c C-c, the PDF in the other window will automatically update.
 
This is achieved by placing the following line in my .emacs.
 
Here is a screencast of this process in action.
  
This is a simple setup that I use to author reports, edit them, and see immediate updates to my output file without leaving Emacs.
 "	 0 Comments
Examples using R – Randomized Block Design	https://www.r-bloggers.com/2009/10/examples-using-r-randomized-block-design/	October 3, 2009	R – StudyTrails	 Let us look at the interaction plotand the box plotLet us now run the analysis of variance on the data, we will include the blocking variable in the analysis.the formula to be used in R is hardness~typeOfTip+testCoupon. > anova(aov(hardness~factor(typeOfTip)+factor(testCoupon)))Analysis of Variance Table Signif. codes: 0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1 A plot of the residuals should not show any pattern since the residuals are assumed to be independently distributed for analysis of variance. Here’s the plot of the residuals. A Quantil-Quantile plot can be used to check the distribution as well. The plot also shows the presence of outliers, if any. The plot is shown below.  Tukeys test can be used for pairwise comparison. Here’s the result of the Tukey’s testfit=aov(hardness~factor(typeOfTip)+factor(testCoupon))TukeyHSD(fit,which=’factor(typeOfTip)’,ordered=”TRUE”)A plot of the residuals should not have a pattern. Here’s a plot of the residuals.  We can check for variance. Here’s a method to check to equality of variance.> summary(lm(abs(fit$res)~typeOfTip)) Call:lm(formula = abs(fit$res) ~ typeOfTip) Residuals:Min 1Q Median 3Q Max-0.050000 -0.032812 -0.003125 0.026562 0.093750 Coefficients:Estimate Std. Error t value Pr(>t)(Intercept) 0.075000 0.024719 3.034 0.00893 **typeOfTip -0.006250 0.009026 -0.692 0.50000—Signif. codes: 0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1 Residual standard error: 0.04037 on 14 degrees of freedomMultiple R-squared: 0.03311, Adjusted R-squared: -0.03595F-statistic: 0.4795 on 1 and 14 DF, p-value: 0.5 Since the p-value is large, difference in variance cannot be stated. The Latin Square Design:If there are two blocking variables then the latin square design can be used.problem : Suppose that an experimenter is studying the effects of five different formulations of a rocket propellant used in aircrew escape systems on the observed burning rate. Each formulation is mixed from a batch of raw material that is only large enough for five formulations to be tested. Furthermore, the formulations are prepared by several operators, and there may be substantial differences in the skills and experience of the operators.Here’s the dataThe latin square structure is> matrix(treatments,5,5)[,1] [,2] [,3] [,4] [,5][1,] “A” “B” “C” “D” “E”[2,] “B” “C” “D” “E” “A”[3,] “C” “D” “E” “A” “B”[4,] “D” “E” “A” “B” “C”[5,] “E” “A” “B” “C” “D” An analysis of variance gives> anova(lm(formulations~factor(rawBatches)+factor(treatments)+factor(operators))) The data shows that the different formulations do produce different burning rate. Also there is a difference between the operators. However, no difference between the raw batches. 	 0 Comments
Data Mining and Statistics Video Course	https://www.r-bloggers.com/2009/10/data-mining-and-statistics-video-course/	October 3, 2009	Jeromy Anglim		 0 Comments
smoothScatter in base R	https://www.r-bloggers.com/2009/10/smoothscatter-in-base-r/	October 2, 2009	Gregor Gorjanc		 0 Comments
Make Safari behave as Firefox	https://www.r-bloggers.com/2009/10/make-safari-behave-as-firefox/	October 2, 2009	[email protected]		 0 Comments
R Object Tooltips in ESS	https://www.r-bloggers.com/2009/10/r-object-tooltips-in-ess-2/	October 1, 2009	erik	"
This post originally appeared on my WordPress blog on October 1, 2009. I present it here in its original form.
 
Whether at work or for personal projects, I use ESS a lot to perform interactive data analyses. The ability to write, edit, and submit R commands to an interactive R process is simply something I cannot imagine analyzing data without.  
 
One thing that I end up having to do a lot is inspect an object that I have just assigned to a variable in R.  To fix ideas, let us create a data.frame called df for this example. 
 
I just created the data.frame df, and I want to know if I did it correctly.  For instance, does it look like I expect it to? Does it have 100 observations like I want?  Do the variables have the right names?  Is the sex variable a factor with two levels?  In short, I want to call the str function using the object df as an argument. 
 
Here is the output I am interested in seeing: 
 
So, how can I quickly see the structure as shown above?  One idea is to switch over to my interactive R buffer in Emacs, type the command at the prompt, and then switch back to my code buffer to edit the data.frame command or continue programming. I dislike having to switch back and forth between the buffers for a one-off command though.
 
Alternatively, I could type str(df) in my code buffer, evaluate it, and decide to keep it or delete the line.  Since this is more of a quick check, without permanent results, I usually will not want to keep lines like this around, since they clutter up my program.  Typically, I am writing the program to be later run in BATCH mode, so I also do not want functions like that in my code since some can be excessively time-consuming depending on the size of the data.frame. 
 
Another option is to use the ESS function ess-execute-in-tb, by default this is bound to C-c C-t, which will prompt me for an expression to evaluate.  This is nice because I do not have to clutter my buffer with extraneous function calls.  However, after using this method for a while, I noticed that I had many patterns with my objects.  For data.frames, I would almost always use summary or str on them after assignment.  For factors, I would want to table the values after I created them, to be sure they looked right.  For numeric vectors, I would want to summarize them.  I also wanted to summarize model fits (e.g., lm).  I wanted to take advantage of my usage patterns so that I did not have to type so much after assigning an object to a variable. 
 
I therefore wrote an Emacs Lisp function that, when called via a key chord in Emacs, inspects the object at point, determines the class of that object, and based on the class, calls an R function on that object, showing the results in a tooltip.  For the df example above, I would just put point on ""df"", anywhere in the source code, and type C-c C-g (my default binding). A tooltip is then shown with the output of str(df).  
 
An example similar to this, along with several others are shown in this screencast. I think this is the best way to show how my Lisp function interacts with R to show object information in tooltips. 
 
Pretty nice! One thing to note is that the tooltips are displaying in a proportional font, not a monospace one.  I know at some point I had found a customizable variable to specify which font tooltips display in, but I apparently did not save it. If I find that variable, I will update this post to reflect how to do that. 
 
Here is the code you will need for this behavior.  It depends on having tooltip-show-at-point defined, which is found only in ESS 5.4 (the current version as of this post) or later.  I contributed tooltip-show-at-point to the ESS project a few months ago.  It is used to show argument tooltips when you type an opening parenthesis.  Perhaps my object tooltip function will find its way into a future version of ESS.  Here is the code. 
 
Notice that you can add your own object classes and functions fairly easily at the top of the program. There is a special ""other"" class which will be called for classes not defined otherwise.  
 
If you can think if anymore examples for types of objects that this would be useful for, feel free to post them in the comments. I think this is a very useful feature when interactively examining datasets, fitting models, and analyzing data. In general, I think there are many more interesting ways to have meta-data on objects available quickly within the ESS and R system.  I will be sure to share them as I explore ways to more efficiently do statistical analysis within the R environment.  
 "	 0 Comments
OpenMX, again	https://www.r-bloggers.com/2009/11/openmx-again/	November 30, 2009	Shige		 0 Comments
An OpenSSH software for Windows 7 / Vista	https://www.r-bloggers.com/2009/11/an-openssh-software-for-windows-7-vista/	November 30, 2009	Yu-Sung Su		 0 Comments
AMCMC	https://www.r-bloggers.com/2009/11/amcmc/	November 30, 2009	Shige		 0 Comments
A brief survey of R web interfaces	https://www.r-bloggers.com/2009/11/a-brief-survey-of-r-web-interfaces/	November 29, 2009	nsaunders	"I’m looking at ways to provide access to R via a web application.  First rule: see what’s available first, before you reinvent the wheel.  It’s not pretty. From the R Web Interfaces FAQ: 
So, aside from RApache and some very old-fashioned and/or broken CGI scripts, I conclude that there is little interest in writing beautiful, modern statistical web applications (notable exception).  Not so much a case of “reinventing” as “inventing”. "	 0 Comments
Back from Tokyo	https://www.r-bloggers.com/2009/11/back-from-tokyo/	November 29, 2009	Thinking inside the box	"
Lisa and I turned this into a brief one-week trip to Kyoto and Tokyo, and we
had a truly wonderful time on what was our first visit to Japan.  I should blog some
more about it, but now I will give in to the jet lag and catch up on some
sleep… 

 "	 0 Comments
Design of Experiments – Optimal Designs	https://www.r-bloggers.com/2009/11/design-of-experiments-%e2%80%93-optimal-designs/	November 29, 2009	Ralph	When designing an experiment it is not always possible to generate a regular, balanced design such as a full or fractional factorial design plan. There are usually restrictions of the total number of experiments that can be undertaken or constraints on the factor settings both individually or in combination with each other. In these scenarios computer generated designs, the optimal designs of a given size, can be identified from a list of candidate factor combinations. The library AlgDesign in R has facilities for optimal design searches based on the Federov exchange algorithm. An optimality criterion has to be selected by the investigator, currently D, A or I, and this criterion is minimise by searching for an optimal subset of a given size from the candidate design list. Given the total number of treatment runs for an experiment and a specified model, the computer algorithm chooses the optimal set of design runs from a candidate set of possible design treatment runs. This candidate set of treatment runs usually consists of all possible combinations of various factor levels that one wishes to use in the experiment. First stage, as always, is to make the package available for use: For illustrative purposes consider a four factor experiment, where the factors have 4, 3, 2, and 2 levels each respectively. Using the expand.grid function we can create a data frame of all possible combinations of the factor settings: The random number seed is set so that the algorithm can run: The function optFederov calculates an exact or approximate algorithmic design for one of three criteria, using Federov’s exchange algorithm. The first argument to the function is a formula for the intended model for the data and the data argument specifies the list of candidate points: In this example all of the factors in the candidate list appear in the model with a linear term. Quadratic or cubic terms can be included in this formula. The argument nTrials specifies the number of design points to select from the candidate list. The output from this function is: This provides details of the values of the optimality criteria for the design points selected from the candidate list, the row numbers and the levels for the factors for the chosen design points. 	 0 Comments
Probability of hypercubes…	https://www.r-bloggers.com/2009/11/probability-of-hypercubes%e2%80%a6/	November 28, 2009	Manos Parzakonis	…in R of course! There is a handy function to do those calculations. Normally (ahh!) you might resolve to a symbolic calculation package (Maple,Mathematica etc.)  but that is not the situation any more. The calculations are done with the mnormt package. Relevant functions exist in other packages as well (R : Distributions) 	 0 Comments
PBSadmb for R	https://www.r-bloggers.com/2009/11/pbsadmb-for-r/	November 27, 2009	Shige		 0 Comments
R Tutorial Series: Simple Linear Regression	https://www.r-bloggers.com/2009/11/r-tutorial-series-simple-linear-regression/	November 26, 2009	John M. Quick	Simple linear regression uses a solitary independent variable to predict the outcome of a dependent variable. By understanding this, the most basic form of regression, numerous complex modeling techniques can be learned. This tutorial will explore how R can be used to perform simple linear regression. Before we begin, you may want to download the sample data (.csv) used in this tutorial. Be sure to right-click and save the file to your R working directory. This dataset contains information used to estimate undergraduate enrollment at the University of New Mexico (Office of Institutional Research, 1990). Note that all code samples in this tutorial assume that this data has already been read into an R variable and has been attached. In R, the lm(), or “linear model,” function can be used to create a simple regression model. The lm() function accepts a number of arguments (“Fitting Linear Models,” n.d.). The following list explains the two most commonly used parameters. Note that the formula argument follows a specific format. For simple linear regression, this is “YVAR ~ XVAR” where YVAR is the dependent, or predicted, variable and XVAR is the independent, or predictor, variable. It is recommended that you save a newly created linear model into a variable. By doing so, the model can be used in subsequent calculations and analyses without having to retype the entire lm() function each time. The sample code below demonstrates how to create a linear model and save it into a variable. In this particular case, we are using the unemployment rate (UNEM) to predict the fall enrollment (ROLL). The output of the preceding function is pictured below.  From this output, we have determined that the intercept is 3957 and the coefficient for the unemployment rate is 1134. Therefore, the complete regression equation is Fall Enrollment = 3957 + 1134 * Unemployment Rate. This equation tells us that the predicted fall enrollment for the University of New Mexico will increase by 1134 students for every one percent increase in the unemployment rate. Suppose that our research question asks what the expected fall enrollment is, given this year’s unemployment rate of 9%. As follows, we can use the regression equation to calculate the answer to this question. Naturally, simple linear regression can be used to do much more than just calculate expected values. Here, the summary(OBJECT) function is a useful tool. It is capable of generating most of the statistical information that one would need to derive from a linear model. The example below demonstrates the use of the summary function on a linear model variable. The output of the preceding function is pictured below.  The summary(OBJECT) function has provided us with a wealth of information, including t-test, F-test, R-squared, residual, and significance values. All of this data can be used to answer important research questions related to our linear model. Yet again, the summary(OBJECT) function proves to be a valuable resource. It is worth remembering and using when conducting a variety of analyses in R. Although lm() was used in this tutorial, note that there are alternative modeling functions available in R, such as glm() and rlm(). Depending on your unique circumstances, it may be beneficial or necessary to investigate alternatives to lm() before choosing how to conduct your regression analysis. To see a complete example of how simple linear regression can be conducted in R, please download the simple linear regression example (.txt) file. Fitting Linear Models. (n.d.). Retrieved November 22, 2009 from http://sekhon.berkeley.edu/library/stats/html/lm.html Office of Institutional Research (1990). Enrollment Forecast [Data File]. Retrieved November 22, 2009 from http://lib.stat.cmu.edu/DASL/Datafiles/enrolldat.html 	 0 Comments
R and Java – JRI using eclipse.	https://www.r-bloggers.com/2009/11/r-and-java-jri-using-eclipse/	November 25, 2009	R – StudyTrails	This post explores how R can be called from within Java using JRI. We will use the example provided in the rjava package. The example class is called rtest2.java. Here are the steps to run R from java using eclipse. 1) Create a new project and copy the rtest.java and rtest2.java files from the rjava/jri package. The rjava project folder can be found at the place where R stores the package downloaded during the install step. It should be in a folder called ‘win-library’.  2) the rjava/jri folder in ‘win-library’ should also have the JRI.jar library and jri.dll file. copy R.dll from the bin directory of R into the rjava/jri folder.Here’s the folder hierarchy for JRI  3) add the JRI.jar in the project classpath in eclipse 4) Add the following entries into the run configuration of the product.THe path variable contains C:/Users/user/Documents/R/win-library/2.9/rJava/jri/;C:Program FilesRR-2.9.1bin That’s it. This should work. Run the rtest2 file and you should see R working. Some of the steps above may not be required. However, i have found this to be working after some research and would not like to experiment further. Please add your comments if it works and also if it does not. 	 0 Comments
ESS on Mac OS X	https://www.r-bloggers.com/2009/11/ess-on-mac-os-x/	November 25, 2009	Matti Pastell	One of the search terms that bring people frequently to my site is “install ESS on Mac OS X” or something like that. As it turns out installing ESS on OS X is really easy, but Google search does not really bring up good instructions. There are at least two easy options: That’s it! 	 0 Comments
Export Trade Clusters	https://www.r-bloggers.com/2009/11/export-trade-clusters/	November 24, 2009	Ben Mazzotta	This post, as with the prior ones on trade clusters, aims to help visualize patterns of trade in the OECD from 50 years of partner trade statistics. The data is rich, meaning we should be able to develop rich intuition by exploring it visually. These slides follow the method laid out in Jong-Eun Lee, “Two Maps for the World’s Trade Integration,” Applied Economics Letters, 11:4 (2004). All computations were performed in R. 	 0 Comments
Loading Big (ish) Data into R	https://www.r-bloggers.com/2009/11/loading-big-ish-data-into-r/	November 24, 2009	JD Long	"So for the rest of this conversation big data == 2 Gigs. Done. Don’t give me any of this ‘that’s not big, THIS is big’ shit. There now, on with the cool stuff: This week on twitter Vince Buffalo asked about loading a 2 gig comma separated file (csv) into R (OK, he asked about tab delimited data, but I ignored that because I use mostly comma data and I wanted to test CSV. Sue me.)  I thought this was a dang good question. What I have always done in the past was load my data into SQL Server or Oracle using an ETL tool and then suck it from the database to R using either native database connections or the RODBC package. Matti Pastell (@mpastell) recommended using the sqldf (SQL to data frame) package to do the import. I’ve used sqldf before, but only to allow me to use SQL syntax to manipulate R data frames. I didn’t know it could import data, but that makes sense, given how sqldf works. How does it work? Well sqldf sets up an instance of the sqlite database server then shoves R data into the DB, does operations on the tables, and then spits out an R data frame of the results. What I didn’t realize is that we can call sqldf from within R and have it import a text file directly into sqlite and then return the data from sqlite directly into R using a pretty fast native connection. I did a little Googling and came up with this discussion on the R mailing list. So enough background, here’s my setup: I have a Ubuntu virtual machine running with 2 cores and 10 gigs of memory. Here’s the code I ran to test: bigdf 
write.csv(bigdf, ‘bigdf.csv’, quote = F) That code creates a data frame with 3 columns. I created a single letter text column, then two floating point columns. There are 40,000,000 records. When I run the write.csv step on my machine I get about 1.8GiB. That’s close enough to 2 gigs for me. I created the text file and then ran rm(list=ls()) to kill all objects. I then ran gc() and saw that I had hundreds of megs of something or other (I have not invested the brain cycles to understand the output that gc() gives). So I just killed and restarted R. I then ran the following: library(sqldf)
f 
system.time(bigdf  That code loads the CSV into an sqlite DB then executes a select * query and returns the results to the R data frame bigdf. Pretty straightforward, ey? Well except for the dbname = tempfile() bit. In sqldf you can choose where it makes the sqlite db. If you don’t specify at all it makes it in memory which is what I first tried. I ran out of mem even on my 10GB box. So I read a little more and added the dbname = tempfile() which creates a temporary sqlite file on the disk. If I wanted to use an existing sqlite file I could have specified that instead. So how long did it take to run? Just under 5 minutes. So how long would the read.csv method take? Funny you should ask. I ran the following code to compare: system.time(big.df  And I would love to tell you how long that took to run, but it’s been running for half an hour all night and I just don’t have that kind of patience. -JD "	 0 Comments
NYT: SAS threatened by R	https://www.r-bloggers.com/2009/11/nyt-sas-threatened-by-r/	November 23, 2009	Stephen Turner		 0 Comments
RQuantlib	https://www.r-bloggers.com/2009/11/rquantlib/	November 23, 2009	Quantitative Finance Collector		 0 Comments
Memory Management in R: A Few Tips and Tricks	https://www.r-bloggers.com/2009/11/memory-management-in-r-a-few-tips-and-tricks/	November 23, 2009	Jeromy Anglim		 0 Comments
Type II Error	https://www.r-bloggers.com/2009/11/type-ii-error/	November 22, 2009	rtutor.chiyau	"

In hypothesis testing, a type II error is due to a failure of rejecting
an invalid null hypothesis. The probability of avoiding a type II error is
called the power of the hypothesis test, and is denoted by the quantity
1 – β .
 read more "	 0 Comments
Some sort of update to ggplot2	https://www.r-bloggers.com/2009/11/some-sort-of-update-to-ggplot2/	November 22, 2009	Andrew Gelman		 0 Comments
new R package : highlight	https://www.r-bloggers.com/2009/11/new-r-package-highlight/	November 22, 2009	romain francois	I finally pushed highlight to CRAN, which should be available in a few days. The package uses the information gathered by the parser package to perform syntax highlighting of R code The main function of the package is highlight, which takes a number of argument including : The package ships three functions that create such renderers  And additionally, the xterm256 package defines a renderer that allows syntax highlighting directly in the console (if the console knows xterm 256 colors) Let’s assume we have this code file (/tmp/code.R)  Then we can syntax highlight it like this : which makes these files : code.R.html and code.R.latex The package also ships a sweave driver that can highlight code chunks in a sweave document, but I'll talk about this in another post 	 0 Comments
R examine objects tutorial	https://www.r-bloggers.com/2009/11/r-examine-objects-tutorial/	November 21, 2009	John Mount	"This article is quick concrete example of how to use the techniques from  Survive R to lower the steepness of The R Project for Statistical Computing‘s learning curve (so an apology to all readers who are not interested in R).  What follows is for people who already use R and want to achieve more control of the software.
I am a fan of the R.  The R software does a number of incredible things and is the result of a number of good design choices.  However, you can’t fully benefit from R if you are not already familiar the internal workings of R.  You can quickly become familiar with the internal workings of R if you learn how to inspect the objects of R (as an addition to using the built in help system).  Here I give a concrete example of how to use the R system itself to find answers, with or without the help system.  R documentation has the difficult dual responsibility of attempting to explain both how to use the R software and explain the nature of the underlying statistics; so the documentation is not always the quickest thing to browse. First let’s give R the commands to build a fake data set that has a variable y that turns out to be 3 times x (another variable) plus some noise: This data set (by design) has a nearly a linear relation between x and y.  We can plot
the data as follows:  With data like this the most obvious statistical analysis is a linear regression.  R can very quickly perform the linear regression and report the results. We can read the report and see that the estimated fit formula is: y =  2.99150*x – 0.02609 (which is very close to the true formula y = 3*x) .  At this point the analysis is done (if the goal of the analysis is to just print the results).  However, if we want to use the results in a calculation we need to get at the numbers shown in above printout.  This printout contains a lot of information (such as the estimate fit coefficients, the standard errors, the t-values and the significances) that a statistician would want to see and want to use in further calculations.  But it is unclear how to get at these numbers.  For example: how do you get the “standard errors” (the numbers in the “Std. Error” column) from the returned model?  Are we forced to cut and paste them from the printed report?   What can you do? The documentation nearly tells us what we need to know.  help(lm) yields: 
The functions summary and anova are used to obtain and print a summary and analysis of variance table of the results. The generic accessor functions coefficients, effects, fitted.values and residuals extract various useful features of the value returned by lm.
 To a newer R user this may not be clear (as there are technical issues from both R and statistics quickly being run through).   However, the experienced R user would immediately recognize from this help that what is returned form summary(model) is an object (not just a blob of text) and that looking at the class of the returned object (which turns out to be summary.lm) might tell them what they would need to know. Typing: Yields: 
coefficients: a p x 4 matrix with columns for the estimated coefficient, its standard error, t-statistic and corresponding (two-sided) p-value. Aliased coefficients are omitted.
 But if you are not very familiar with R you might miss that the summary function returns a useful object (instead of blob of text).  Also you might only know to look at help(summary) which  does not describe the location of the desired standard errors (but does have a reference to summary.lm, so if you are patient you might find it).  We describe how to find the information you need by using R’s object inspection facilities.  This is a “doing it the hard way” technique for when you do not understand the help system or you are using a package with less complete help documentation. First  (using the techniques described in the slides:  Survive R) examine the model to see if the standard errors are there: We found the coefficients, but did not find the standard errors.  Now we know the standard errors are reported by summary(model), so they must be somewhere.  Instead of performing a wild goose chase to find the standard errors let’s instead trace how the summary method works to find where it gets them.  If we type print(summary) we don’t get any really useful information.  This is because summary is a generic method and we need to know what type-qualified name the summary of a linear model is called. So we see our model is of type lm so the summary(model) call would use a summary method called summary.lm (which as we saw is also the returned class of the summary(model) object).  As we mentioned the solution is in help(summary.lm), but if the solution had not been there we could still make progress:  we could dump the source of the summary.lm method: We actually deleted the bulk of the print(summary.lm) result because the important thing to notice is that the method is huge and that it returns an object instead of a blob of text.  The fact that the method summary.lm was huge means that it is likely calculating the things it reports (confirming that the standard errors are not part of the model object).  The fact that an object is returned means that what we are looking for may sitting somewhere in the summary waiting for us.  To find what we are looking for we convert the summary into a list (using the unclass() method) and look for something with the name or value we are looking for: And we have found it.  The named slot summary(model)$coefficients is in fact a table that has what we are looking for in the second column.  We can create a new list that will let us look up the standard errors by name (for the variable x and for the intercept): Now that we have the stdErrors in list form we can look up the numbers we wanted by name. And we finally have the standard errors.  But why did we want the standard errors?  In this case I wanted the standard errors so I could plot the fit model and show the uncertainty of the model.  As, is often the case, R already has a function that does all of this.  Also (as is often the case) the R function that does this asks the right statistical question (instead of the obvious question) and can draw error bars that display the uncertainty of future predictions.  The uncertainty in future prediction is in fact different than the uncertainty of the estimate (what was most obvious to calculate from the standard errors) and (after some reflection) is what I really wanted.  Having these sort of distinctions already thought out is why we are using a statistics package like R instead of just coding everything up.  These calculations are all trivial to implement- but remembering to perform the calculations that answer the right statistical questions can be difficult.  The built in R solution of plotting the the fit model (black line) and the region of expected prediction uncertainty (blue lines) is as follows:  And we are done. Related posts: "	 0 Comments
My implementation of Berry and Berry’s hierarchical Bayes algorithm for adverse events	https://www.r-bloggers.com/2009/11/my-implementation-of-berry-and-berrys-hierarchical-bayes-algorithm-for-adverse-events/	November 20, 2009	John Johnson		 0 Comments
Mapping Biomes	https://www.r-bloggers.com/2009/11/mapping-biomes/	November 20, 2009	joe	"Recently (2008) the European Space Agency produced GlobCover (ESA GlobCover Project, led by MEDIAS-France), the highest resolution (300m) global land cover map to date. GlobCover uses 21 primary land cover classes and many more sub-classes. Land cover classification (LCC) schemes divide the earth into biomes. Biomes are the simplest way to classify vegetation which can be applied globally. LCC makes sense because the boundaries between different ecosystems (ecotones) are sharp. However, definitions vary and there is no agreed standard set of biomes.[1] 
 Puntarenas is a province on the pacific coast of Costa Rica.  The province has a typical mix of tropical land cover. This includes some spectacular examples of Pacific Rainforest, notably on the Osa Peninsula. Puntarenas has an area of ≈ 11,000 sq. km or about 120,000 GlobCover pixels. 
 13 land cover types are present in the GlobCover map below. The barplot on the right shows the total amounts present in each class.  
  The GlobCover legend (above) has mixed land cover classes, where more than one biome occurs  inside a map pixel. This is especially true in the man-made biomes (agriculture) . For example, there are three cropland land cover types depending on the relative amounts of other vegetation present. R is a programming language, not a specialised geographic information system (GIS) such as GRASS or commercial packages. However applications of R to spatial problems is a growth industry.[2] 
 A GlobCover map similar to the above can be produced for any area of interest. The Geospatial Data Abstraction Library (GDAL) should be installed on your system. FWTools is the place to go. You also need R packages sp and rgdal installed. The regional GLOBCOVER map for Central America can be downloaded from ESA here. GlobCover is in GeoTiff format i.e. a Tiff image file which contains georeferencing information. The following GDAL command (from command line, or run from R using shell) creates a 4° x 4° submap centred on Costa Rica.


gdalwarp GLOBCOVER_200412_200606_V2.2_CentralAmerica_Reg.tif -te -86 8 -82 12 costaRica.tif

 costaRica.tif is read into R using the rgdal package: library(rgdal)

costa <- readGDAL(""costaRica.tif"") costa has class SpatialGridDataFrame, which is a class defined in the package sp (loaded when rgdal is loaded). Administative boundaries for Costa Rica were obtained from Global Administrative Areas www.gadm.org (see Revolution R blog post) con <- url(""http://gadm.org/data/rda/CRI_adm1.RData"")

load(con)

close(con) Costa Rican provinces are now contained in the object gadm of class SpatialPolygonsDataFrame. The boundaries of Puntarenas province (excluding Cocos Island) are extracted as follows: Polygons(list(Polygon([email protected][[6]]@Polygons[[27]]@coords),Polygon([email protected][[6]]@Polygons[[25]]@coords)),""puntarenas"")

temp <- SpatialPolygons(list(temp),proj4string=CRS(proj4string(gadm)))

punt.sp <- SpatialPolygonsDataFrame(temp, data.frame(cbind(2,2), row.names=c(""puntarenas"")))  # puntarenas

 The overlay() method  is used to extract the land cover map puntarenas from costa:  puntarenas <- costa

puntarenas.overlay <- overlay(costa,punt.sp)  # 1 in interior of puntarenas polygons, 0 outside

puntarenas$band1 <- costa$band1*puntarenas.overlay 
 
 Unfortunately overlay() is rather slow, because it applies point.in.polygon() to the entire raster. Eventually puntarenas appears as a SpatialGridDataFrame which can be plotted using standard R tools such as image(). The code needed to generate the above plot is here. 
 
 [1] For example, the International Biosphere-Geosphere Programme (IGBP) land cover legend used 17 biomes. The University of Maryland map used 14 biomes. At much lower resolution, numerical weather forecasting models such as US National Center for Climate Prediction Global Forecasting System (NCEP-GFS) also use alternative schemes. 
  
 [2] Roger S. Bivand, Edzer J. Pebesma, and Virgilio Gómez-Rubio.  Applied Spatial Data Analysis with R.  Springer, New York, 2008 
 
 
 

 "	 0 Comments
Working on a drug safety project	https://www.r-bloggers.com/2009/11/working-on-a-drug-safety-project/	November 20, 2009	John Johnson		 0 Comments
Tactical asset allocation using blotter	https://www.r-bloggers.com/2009/11/tactical-asset-allocation-using-blotter/	November 18, 2009	Joshua Ulrich	"
 "	 0 Comments
Design of Experiments – Power Calculations	https://www.r-bloggers.com/2009/11/design-of-experiments-%e2%80%93-power-calculations/	November 18, 2009	Ralph	Prior to conducting an experiment researchers will often undertake power calculations to determine the sample size required in their work to detect a meaningful scientific effect with sufficient power. In R there are functions to calculate either a minimum sample size for a specific power for a test or the power of a test for a fixed sample size. When undertaking sample size or power calculations for a prospective trial or experiment we need to consider various factors. There are two main probabilities of interest that are tied up with calculating a minimum sample size or the power of a specific test, and these are: A decision needs to be made about what difference between the two groups being compared should be considered as corresponding to a meaningful difference. This difference is usually denoted by delta. The base package has functions for calculating power or sample sizes, which includes the functions power.t.test, power.prop.test and power.anova.test for various common scenarios. Consider a scenario where we might be buying batteries for a GPS device and the average battery life that we want to have is 400 minutes. If we decided that the performance is not acceptable if the average is more than 10 minutes (delta) lower than this (390 minutes) then we can calculate the number of batteries to test: For this example we have assumed a standard deviation of 6 minutes for batteries (would either be assumed or estimated from previous data) and that we want a power of 95% in the test. Power is defined as 1 – beta, the Type II error probability. The default option for this function is for 5% probability of alpha, a Type I error. The test will involve only one group so we are considering a one-sample t test and only a one sided alternative is relevant as we do not mind if the batteries perform better than required. The output from this function call is as follows: So we would need to test at least 6 batteries to obtain the required power in the test based on the other parameters that have been used. 	 0 Comments
Confidence we seek…	https://www.r-bloggers.com/2009/11/confidence-we-seek%e2%80%a6/	November 18, 2009	Manos Parzakonis	Estimating a proportion at first looks elementary. Hail to aymptotics, right? Well, initially it might seem efficient to iuse the fact that . In other words the classical confidence interval relies on the inversion of Wald’s test. A function to ease the computation is the following (not really needed!). An exact confidence interval is the so-called Blake’s confidence interval A comparison is easily made along the following lines of code You can see the difference at extremes! The blacerci function is adapted form A. Agresti’s site [link]. Look it up… Lawrence D. Brown, T. Tony Cai and Anirban DasGupta, “Interval Estimation for a Binomial Proportion”, Statistical Science 2001, Vol. 16, No. 2, pp. 101–133 [pdf] Laura A. Thompson, An R (and S-PLUS) Manual to Accompany Agresti’s Categorical Data Analysis, 2009 [pdf | R code] 	 0 Comments
Quantitative link strength for APE cophyloplot	https://www.r-bloggers.com/2009/11/quantitative-link-strength-for-ape-cophyloplot/	November 17, 2009	Timothée Poisot	Just add a third column with link strength to the association matrix 	 0 Comments
swfDevice is nearing completion	https://www.r-bloggers.com/2009/11/swfdevice-is-nearing-completion/	November 17, 2009	cameron	My new R package, swfDevice, is getting close to its first release. This package enables native R graphics output as swf (flash) files. It also as the ability to create animations with player controls.  The main project page is here and the results of the test suite are here. Here are some samples: http://swfdevice.r-forge.r-project.org/swfDevice_test29.swf http://swfdevice.r-forge.r-project.org/swfDevice_test28.swf There are still a few things left to be done such as clipping and a working windows build. 	 0 Comments
R tip: Extracting median from survfit object	https://www.r-bloggers.com/2009/11/r-tip-extracting-median-from-survfit-object/	November 17, 2009	Stewart MacArthur		 0 Comments
R functions for Dienes (2008) Understanding Psychology as a Science	https://www.r-bloggers.com/2009/11/r-functions-for-dienes-2008-understanding-psychology-as-a-science/	November 17, 2009	Thom Baguley	Dienes, Z. (2008). Understanding Psychology as a Science: An Introduction to Scientific and Statistical Inference. Basingstoke: Palgrave Macmillan. 	 0 Comments
Seminar: Reproducible Research with R, LaTeX, & Sweave	https://www.r-bloggers.com/2009/11/seminar-reproducible-research-with-r-latex-sweave/	November 16, 2009	Stephen Turner		 0 Comments
Infomaps using R – Visualizing German unemployment rates by district on a map	https://www.r-bloggers.com/2009/11/infomaps-using-r-visualizing-german-unemployment-rates-by-district-on-a-map/	November 16, 2009	markheckmann	"Lately, David Smith from REvolution Computing set out to challenge the R community with the reprocuction of a beautiful choropleth map (= multiple regions map/thematic map) on US unemployment rates he had seen on the Flowing Data blog. Here you can find the impressing results. Being a fan of beautiful visualizations I tried to produce a similar map for Germany. 1. Getting the spatial country data The first step resulted in getting data to draw a map of the German administrative districts. Unfortunately, the maps for Germany do not come along in the map package, which would mean I could easily adopt the code results from the challenge. Getting data: The GADM database of Global Administrative Areas has the aim to provide data of administrative districts for the whole world on different levels (country, state and county level). The data can be downloaded as as a shapefile, an ESRI geodatabase file, a Google Earth .kmz file and very convenient for R users, as an Rdata file. 
 2. Getting socio-demographic data (e. g. unemployment rates by administrative district): A lot of data is available online at www.statistikportal.de. On this site you find links to several data bases. To get the unemployment stats by county I clicked my way through: Regionaldatenbank Deutschland -> Arbeitsmarkt -> Arbeitsmarktstatistik der Bundesagentur für Arbeit -> Arbeitslose nach ausgewählten Personengruppen sowie Arbeitslosenquoten – Jahresdurchschnitt – (ab 2008) regionale Tiefe: Kreise und krfr. Städte -> Werteabruf -> save as CSV format. This table contains all the information I need, although for some reson, for a few districts there is no data listed. I also looked for another source. On Regionalatlas a nice online visualization tool is offered. In the menu I selected unemployment rate 2008 as indicator. Besides the nice visualization you get, there is a menu button “tables” where you can retrieve a html table of the data. I simply copied and pasted it into a .txt file which gives me a tab seperated value format I can read in R. But still: some districts are not listed. Here is a pdf file containing the data. 3. Preparing the data Now I have two datafiles: One (gadm) containaing the spatial information, the other one (unempl) containing the unemployment rates. It turns out that the same districts are not always named alike. Sometimes the name comes along with a supplement or in other cases the deviations are more severe so that simple parsing will not do it.  I decided to take the quick-and-dirty route and do a fuzzy matching, which surely is prone to errors, very slow and not at all elegant… Well, never underestimate the rawness of raw data. 4. Plotting the data On Claudia Engel’s Anthrospace blog I found an R script already perfect to make use of the data provided. The Rdata files turn out to contain SpatialPolygonsDataFrame so we can print the data without any further preparation using the sp package.  This looks nice. To produce a color vector to visualize the unemployment rate the two data sets have to be merged.  It seems that the districts for which no data was available mainly belong to the states Sachsen-Anhalt and Sachsen. Also you can see that east of Germany has got a much higher unemplyoment rate than the west. The same holds true for a north-south comparison. Besides ths sp package there are many other ways to produce such a graphic. I will now take another approch using shapefile data which also is available on GDAM. The data is availabe as a .zip file which includes several dBase files for all levels (3=district, 1=state etc.).  What I like about all this, it that it is pretty simple to draw almost any country you like. Besides the very messy part of data preparation it is only a few lines of code and the results are nice. "	 0 Comments
R in Action – early thoughts	https://www.r-bloggers.com/2009/11/r-in-action-early-thoughts/	November 16, 2009	Paolo Sonego		 0 Comments
The Top Scores for Canabalt, Take 2	https://www.r-bloggers.com/2009/11/the-top-scores-for-canabalt-take-2/	November 15, 2009	John Myles White	As promised on Thursday, here’s my second pass at a statistical analysis of Canabalt scores. There are some useful results I’ll present right at the start, and then there are some results that are more or less worthless, except that working through my own mistakes helped me to think more clearly about statistical modeling in general and about Poisson models for discrete data in particular. First, let’s review the reason why I proposed analyzing the scores in the first place. My goal was to determine whether or not it was reasonable to assume that learned skill really influences your score on Canabalt. If you’ve played for more than ten games in a row, I think it’s intuitively clear that you can warm up and start to do better. Whether or not this holds up to a rigorous analysis is another matter, but I haven’t gotten around to collecting the right data yet. The mere fact that you can do better with a little bit of practice suggests that the distribution of scores cannot be entirely attributed to chance variation: there’s at least some level of skill that matters. What’s really interesting, though, is how differently different people can perform, regardless of how long they’ve had to warm up. Originally, I thought that it would be useful to see how well a probabilistic model could explain the distribution of scores. I’ll return to my results fitting probabilistic models in the second part of this post, but I now realize that the basic idea behind this approach was deeply flawed. Even if the distribution of scores looks random, it could easily be the case that it’s the distribution of skill levels that’s random, and that, for any given skill level, your score is more or less deterministic. Though that point doesn’t apply here (where we know that there’s variation in the scores that a single player gets), I think the point in general makes clear that the quality of fit of a probabilistic model to empirical data says nothing about the determinism implicit in the process that generates the data. (Which I should have known already, being a fan of Laplace’s demon.) Instead, if we want to understand the importance of skill, we should look directly at the difference in average scores between people of seemingly high skill and those of seemingly low skill — that is, we should simply look at the distributions of scores for people with different values of some operationalization of skill level to see how much they differ. How do we measure skill level? I tried two naïve approaches, and both gave pretty similar results. The first approach is to consider all of the players who have at least one score in the top 10% of scores. The other approach is to consider all of the players whose median score is in the top 10%. I chose the median rather than the mean, because the mean will be biased by a single very high score, which could make these two measures of skill almost identical. Once you have the two groups defined in your data set using these rules, you can simply perform a t-test to compare the performance of these two groups. Both times, I find that the scores are blatantly different across the two groups. The expert users’ mean score is always above 10,000 under both definitions, while the non-expert users’ mean score is around 3,500. In short, I’d say that there’s no way that you can realistically argue that skill doesn’t play a part in determining the score you get. NB: The R code for doing all of this is at the bottom of this post. During the original conversation with my fellow graduate students about Canabalt that inspired this post and the one before it, I claimed that the distribution of scores looked like a Poisson distribution based on my (now known to be poor) recollection of the shape of the Poisson distribution. So, when I started my analyses, my first idea was to fit a Poisson distribution to my data using a call to the glm function in R: To convince myself that this was the correct way to fit a constant Poisson model in R, I derived the maximum likelihood estimator for the mean parameter of a Poisson distribution given a set of sample observations by hand. The math deriving the right parameter estimate is outlined at the bottom of this post and can easily be skipped if that sort of thing is opaque or boring to you. Once you fit this model, you can simulate data from it and see how much it looks like the original data. To refresh your memory, the original score data looks like this: The results of a Poisson simulation look like this: As you can see, this simulation is laughably bad. The Poisson model, lacking a variance parameter, is an absolutely terrible model for the scores from Canabalt. Knowing that I had been basically defeated, I tried a discretized Normal distribution (with a useful independent variance parameter) that was bound to fail because of its symmetry. The results from the Normal distribution look like this: This is also pretty terrible: there are actually negative results in the data set,  no skew at all (which is obviously a given with any parameterization of the Normal distribution) and no outliers. The class of other models I could think of trying beyond these two didn’t seem much more promising, so I didn’t continue on. I hope that’s a reflection of my weak knowledge of probability theory more than anything else. I’d still be interested in seeing whether there is a canonical probability distribution out there that fits the data qualitatively, but I haven’t had any luck searching through lists of distributions online. The desired distribution should: The third point above makes the Poisson distribution, with its small variance, useless, while the second point makes a discretized Normal, with its problematic symmetry, useless. If you have any solutions, please let me know. (I should note that I’ve considered, but decided against, using an over-dispersed Poisson model because I’m not familiar enough with its analytic properties.) You can download my data set here. 	 0 Comments
OpenMX	https://www.r-bloggers.com/2009/11/openmx/	November 15, 2009	Shige		 0 Comments
R Tutorial Series: Scatterplots	https://www.r-bloggers.com/2009/11/r-tutorial-series-scatterplots/	November 12, 2009	John M. Quick	A scatterplot is a useful way to visualize the relationship between two variables. Similar to correlations, scatterplots are often used to make initial diagnoses before any statistical analyses are conducted. This tutorial will explore the ways in which R can be used to create scatterplots. Before we start, you may want to download the sample data (.csv) used in this tutorial. Be sure to right-click and save the file to your R working directory. This dataset contains pre and post test scores for 66 subjects on a series of reading comprehension tests (Moore & McCabe, 1989). Note that all code samples in this tutorial assume that this data has already been read into an R variable and has been attached. The simplest way to create a scatterplot is to directly graph two variables using the default settings. In R, this can be accomplished with the plot(XVAR, YVAR) function, where XVAR is the variable to plot along the x-axis and YVAR is the variable to plot along the y-axis. Suppose that we want to get a picture of the relationship between pretest 1 (PRE1) and posttest 1 (POST1). The following example demonstrates how to use the plot(XVAR, YVAR) function to visualize this relationship. The output of the preceding function is pictured below.  When beginning to analyze a dataset, researchers often want to get a complete picture of all relationships, rather than just a single one. Conveniently, the plot() function can also be run on an entire set of data. The format for this operation is plot(DATAVAR), where DATAVAR is the name of the R variable containing the data. Suppose now that our interest is in visualizing all of the scatterplots at once, in order to diagnose the various relationships present in our data. The following example demonstrates how to use the plot(DATAVAR) function. The output of the preceding function is pictured below.  Note that the image above has been resized to fit on this page. In the R Quartz Window, the scatterplots could be made much larger for easier viewing. Up to this point, we have been using the default values for all of our scatterplots’ elements. However, R also allows for the customization of scatterplots. In addition to x and y axis variables, the plot() function also accepts the following arguments (“The Default Scatterplot Function”, n.d.). Now let’s recreate the original plot depicting the relationship between pretest 1 and posttest 1 with more detailed and meaningful parameters. The output of the preceding function is pictured below.  There are numerous graphical arguments available to functions in R. In this tutorial, just a few of the common aesthetic options will be addressed below (“Set or Query Graphical Parameters”, n.d.). Even more arguments are accepted by the plot() function. Take a look at the referenced page if you wish to explore further options. Now let’s recreate the plot of posttest 1 on pretest 1 yet again, but this time with the inclusion of customized aesthetic parameters. The output of the preceding function is pictured below.  Note that the c() function is used for a number of the parameters in the plot function above. This allows one to define multiple values as a “vector” that can be fed into a single argument. For example, if one wanted to use only a single line color, then col = “red” would be acceptable. However, to use multiple colors, all items must be placed into a vector such as col = c(“red”, “green”, “blue”). Without using a vector for multiple colors, as in col = “red”, “green”, “blue”, an error would occur because the colors would be treated as separate arguments rather than a single entity. To see a complete example of how scatterplots can be created in R, please download the plot examples (.txt) file. R has much more sophisticated graphic capabilities than have been demonstrated in this tutorial. In fact, opportunities exist to make very complex and unique visuals. To see examples of the kinds of charts that can be generated with R, I recommend that you visit the R Graph Gallery (François, 2006). François, R. (2006). R graph gallery: Enhance your data visualization with R. Retrieved November 11, 2009 from http://addictedtor.free.fr/graphiques Moore, D., and McCabe, G. (1989). Introduction to the practice of statistics [Data File]. Retrieved October 27, 2009 from http://lib.stat.cmu.edu/DASL/Datafiles/ReadingTestScores.html Set or Query Graphical Parameters. (n.d.). Retrieved November 11, 2009 from http://sekhon.berkeley.edu/graphics/html/par.html The Default Scatterplot Function. (n.d.). Retrieved November 11, 2009 from http://sekhon.berkeley.edu/graphics/html/plotdefault.html 	 0 Comments
Canabalt	https://www.r-bloggers.com/2009/11/canabalt/	November 12, 2009	John Myles White	At the office today, I got into a discussion with two of my fellow graduate students about the distribution of scores you can get while playing Canabalt. Because (1) the layout of the levels in the game is fully randomized and (2) the difficulty of certain actions (specifically jumping through windows) is exceptionally high, we were intrigued by the possibility that a fully random model of scores, which completely ignores player-specific skill levels, could account for the distribution of scores you see in the real world. While thinking about this on my way home from Rockville tonight, I decided that I should write a simple web spider to parse the notices on Twitter that Canabalt automatically generates. Thankfully, other people had already done just this, so I discovered quickly that you only need to search for www.canabalt.com on Twitter to get the relevant information. After spidering the results of this search query, I constructed the following histogram of scores posted recently to Twitter using Hadley Wickham’s ggplot2 package for R. Here’s the results: While generating this plot, a question I had asked several other R users  about a few months ago came up again: is there no way to get the Y axis label to be anything other than “count” when you generate a histogram? The qplot simply ignores any ylab argument you pass in, so I suspect that the answer is “yes, you simply cannot change this default without hacking the ggplot2 source code.” Sometime this weekend I plan to follow this short piece up with a longer post containing more substantial statistical analyses of this data. If you’ve got interesting ideas for analyses, let me know. 	 0 Comments
Graph Examples from Visualizing Data by William Cleveland	https://www.r-bloggers.com/2009/11/graph-examples-from-visualizing-data-by-william-cleveland/	November 12, 2009	Ralph	The trellis graphics approach was pioneered by various statistical researchers and the ideas are used extensively in the book “Visualizing Data” by William Cleveland. There are various resources on the website for trellis graphics including S code for creating the majority of the graphs that appear in the book. Inspired by efforts on the Learning R blog to recreate the examples from Deepayan Sarkar’s book on lattice using ggplot2 I have decide to undertake a similar exercise based on the scripts that have been made available for creating the graphs from the book. The script has code for six figures from the first chapter and my efforts to recreate these graphs produced the following results. Figure 1.1 Figure 1.2 The code for this example was: This was straightforward to recreate and the main difference is that the Trellis display has an aspect to make the panels square. Histogram of height by singing part Figure 1.3 Figure 1.4 The code for this example was: This graph was easy to recreate. Scatterplot of NOx against Ethanol Figure 1.5 The code for this example was: This graph was easy to recreate. Scatterplot of NOx against Carbon Figure 1.6 The code for this example was: As with the previous example with conditioning the panels are stretched to fit the plot region which differs from the Trellis graphic that is created. Dot plot of barley yields conditioning by year and location 	 0 Comments
Sweave-Lyx from terminal on Mac	https://www.r-bloggers.com/2009/11/sweave-lyx-from-terminal-on-mac/	November 12, 2009	Gregor Gorjanc		 0 Comments
Example 7.17: The Smith College diploma problem	https://www.r-bloggers.com/2009/11/example-7-17-the-smith-college-diploma-problem/	November 12, 2009	Nick Horton		 0 Comments
Create Animations in PDF Documents Using R	https://www.r-bloggers.com/2009/11/create-animations-in-pdf-documents-using-r/	November 11, 2009	Yihui Xie	"In fact, the key point is the LaTeX package named animate, which can be used to insert image frames into a PDF document to generate an animation. The interface of animations created by this package is quite similar to the HTML animation page by the R package animation, moreover, it also uses JavaScript (in PDF) to animate the image frames. Here is an example: Download the demo: Brownian Motion in PDF (205K)
 The PDF document will be automatically opened if there is nothing wrong with LaTeX and your PDF viewer; if nothing happened, you can find the PDF document brownian.motion.pdf in the directory ani.options(""outdir""). The animation will work in Acrobat Reader 9.0, and I do not know if other PDF viewers can deal with JavaScript correctly (AFAIK, the default PDF viewer in Mac OS will not). "	 0 Comments
Introduction à Monte Carlo en R	https://www.r-bloggers.com/2009/11/introduction-a-monte-carlo-en-r/	November 11, 2009	xi'an	Following a proposal by Springer-Verlag Paris, I have decided to translate Introducing Monte Carlo Methods with R with George Casella into French, since a new collection of R books (in French) is planed for the Spring of 2010. The translation will a priori be done by Joachim Robert and Robin Ryder, under my supervision and with the support of Springer-Verlag Paris. I have already translated the first chapter as I needed to cut most of the R coverage, since this collection assumes a prior knowledge of R and aims at a smaller number of pages (around 200) to keep the price as low as possible. 	 0 Comments
Rcpp 0.6.8	https://www.r-bloggers.com/2009/11/rcpp-0-6-8/	November 10, 2009	Thinking inside the box	"
So 0.6.8 went onto CRAN and into
Debian earlier in the day.  Beside the
aforementioned fix, I also split off a more class headers and implementations
into their own files but changed no actual functionality.

 
As always, more details are at the Rcpp page.

 "	 0 Comments
R / Finance 2010 Call for Papers	https://www.r-bloggers.com/2009/11/r-finance-2010-call-for-papers/	November 9, 2009	Thinking inside the box	"

So without further ado, and given the success of our initial
R / Finance 2009 conference about
R in Finance, here is the call for papers for next spring:

 
 
R/Finance 2010: Applied Finance with R
April 16 and 17, 2010
Chicago, IL, USA
 
The second annual R/Finance conference for applied finance using R
will be held this spring in Chicago, IL, USA on April 16 and 17, 2010.
 The two-day conference will cover topics including portfolio
management, time series analysis, advanced risk tools,
high-performance computing, market microstructure and econometrics.
All will be discussed within the context of using R as a primary tool
for financial risk management and trading.
 
One-page abstracts or complete papers (in txt or pdf format) are
invited for consideration. Academic and practitioner research
proposals related to R are encouraged. We will accept submissions for
full talks, abbreviated “lightning talks”, and a limited number of
pre-conference tutorial sessions.  Please indicate with your
submission if you would be willing to produce a formal paper (10-15
pages) for a peer-reviewed conference proceedings publication.
 
Presenters are strongly encouraged to provide working R code to
accompany the presentation/paper.  Data sets should also be made
public for the purposes of reproducibility (though we realize this may
be limited due to contracts with data vendors). Preference may be
given to presenters who have released R packages.
 
Please send submissions to: committee at RinFinance.com
 
The submission deadline is December 31st, 2009.
 
Submissions will be evaluated and submitters notified via email on a
rolling basis. Determination of whether a presentation will be a long
presentation or a lightning talk will be made once the full list of
presenters is known.
 
R/Finance 2009 included keynote presentations by Patrick Burns, Robert
Grossman, David Kane, Roger Koenker, David Ruppert, Diethelm Wuertz,
and Eric Zivot.  Attendees included practitioners, academics, and
government officials. We anticipate another exciting line-up for 2010
and will announce details at the conference website
http://www.RinFinance.com as they become available.
 
For the program committee:
 
See you in Chicago in April! 

 "	 0 Comments
Using Faceting in ggplot2 to create Trellis-like Plots	https://www.r-bloggers.com/2009/11/using-faceting-in-ggplot2-to-create-trellis-like-plots/	November 9, 2009	Ralph	One of the main strengths of the Trellis graphics paradigm is the use of panelling to divide data into subsets to investigate whether patterns are consistent as the conditioning variables change. In the ggplot2 package the terminology for specifying these separate panels is faceting and can be used to create similar displays. In our previous post we consider creating a scatter plot for the data in R relating to the growth of Orange trees. If we wanted to create separate panels (graphs) for each of the trees we use the facets argument with the code below: This will produce a graph like this: Scatterplot Facet Example for Orange Tree data The downside with this graph is that the individual panels are stretched vertically which may distort the interpretation of the graph. We can add the facet aspect to the graph object created by the qplot function using code such as: The facet_wrap function takes a single variable for conditioning and places the panels on multiple lines, which in this example will lead to a more appealing display: Scatter plot with Panels on Multiple lines This only scratches the surface of the power of faceting in the ggplot2 library. There are many options that can be controlled to change the appearance to suit a specific set of data. 	 0 Comments
QQ plots of p-values in R using ggplot2	https://www.r-bloggers.com/2009/11/qq-plots-of-p-values-in-r-using-ggplot2/	November 9, 2009	Stephen Turner		 0 Comments
MATLAB style stem plot with R	https://www.r-bloggers.com/2009/11/matlab-style-stem-plot-with-r/	November 9, 2009	Matti Pastell	"Recently I wanted to plot an impulse response function with R and missed the MATLAB style stem plot for doing it. I couldn’t find an R function for it with a quick Google search so I made my own. So here is the function and a small example :
 
 "	 0 Comments
R package for sequence analysis	https://www.r-bloggers.com/2009/11/r-package-for-sequence-analysis/	November 9, 2009	Shige		 0 Comments
LondonR slides	https://www.r-bloggers.com/2009/11/londonr-slides/	November 9, 2009	romain francois	I was in london last week to present RemoteREngine at the LondonR user group sponsored by mango solutions.  Apart from minor technical details and upsetting someone because I did not mention that he once presented a much simpler solution to a quite different problem, it went pretty good and people were interested in what the package can do Essentially, RemoteREngine is an implementation of REngine using java rmi (remote method invocation) for the data transport.  This allows a (or several) client java application to embed an R engine that lives in a different java virtual machine, perhaps on a different physical machine. In a way it is quite similar to the Rserve implementation of REngine, but rmi gives better control over the data transport and we get things Rserve does not currently do such as support for environments or references.  The slides are available here and will probably also make their way to the conference site at some point 	 0 Comments
Generalized Estimating Equations | General, R, and SPSS Resources	https://www.r-bloggers.com/2009/11/generalized-estimating-equations-general-r-and-spss-resources/	November 9, 2009	Jeromy Anglim		 0 Comments
Rcpp 0.6.7	https://www.r-bloggers.com/2009/11/rcpp-0-6-7/	November 8, 2009	Thinking inside the box	"

One change is that a new type RcppList was added which allows us
to build R ‘list’ types in C++. In particular, this makes it possible to have
data structures of different types and dimensions. We had not previously
accomodated this as our data structures where more regularised (i.e. think
matrices or vectors of various types). The need for this came out of the work with the
recently started RProtoBuf package which interfaces the Google
ProtoBuf library.  The incredible Romain François has joined the
project and added a number of nice tricks; this will need another blogpost
sooner rather than later. In the meantime, check out the RProtoBuf pages at R-Forge.

 
Another small change was the addition of a self-contained example of a
function callback from C++ into R which wasn’t all that well documented
before; this was in response to a user request.

 
Finally, I started RcppList as a first step in reorganising the
code a little better and split the class header and body off into separate
files. This required changes to the infrastrucure files
Makevars, cleanup, … and I promptly forgot the
Makevars.win Windoze variant which will require a 0.6.8 release
real soon.

 "	 0 Comments
New R-Forge Site for Quantitative Pedology	https://www.r-bloggers.com/2009/11/new-r-forge-site-for-quantitative-pedology/	November 8, 2009	dylan	Just back from the annual meetings, and it looks like there is a significant interest in collaborative R coding of soils-related algorithms and visualization. A new R-forge site has been created to host Algorithms for Quantitative Pedology. Public release of the ‘soil’ package should be ready in a couple weeks. Soil Profile Dendrogram read more 	 0 Comments
Network of People who Twitter about R	https://www.r-bloggers.com/2009/11/network-of-people-who-twitter-about-r/	November 7, 2009	Drew Conway	On Tuesday I will be speaking to the Bay Area R group about doing social network analysis in R with igraph.  During the talk I will (hopefully) be doing some live SNA on the audience using R and generating data via Twitter.  As a  preview, or small taste for those not able to attend, I have generated network data from the last 100 tweets—as of yesterday afternoon—that included the hash-tag #rstats. The figure below is the 2-core of the full network.  As you might imagine, the full network is quite large, but the vast majority of nodes are pendants; therefore, visualizing the 2-core is much more useful.  Those that twittered with #rstats are in red and labeled, while all of the intervening nodes are in blue.  I am a bit of a wallflower in this crowd, but I am a relative newcomer to both Twitter and R so this makes sense.  You can see that powerhouse gurus like @CMastication, @revodavid, @dataspora, @hamiltonulmer, etc. are very central.  Also, this visualization nicely depicts the size and connectedness of the R community on Twitter. For example, the network contains only 181 nodes  and has a diameter of 7.  On the other hand, there are several disparate cliques around the central actors (e.g., @gappy3000 and myself form a small clique). I look forward to digging further into the Twitter data on Tuesday, and if you are attending and have any suggestion or specific questions please let me know and I will try to work them in. 	 0 Comments
PMML and Open Source Data Mining	https://www.r-bloggers.com/2009/11/pmml-and-open-source-data-mining/	November 6, 2009	Alex Guazzelli		 0 Comments
R Tutorial Series: Zero-Order Correlations	https://www.r-bloggers.com/2009/11/r-tutorial-series-zero-order-correlations/	November 6, 2009	John M. Quick	One of the most common and basic techniques for analyzing the relationships between variables is zero-order correlation. This tutorial will explore the ways in which R can be used to employ this method. Before we start, you may want to download the sample data (.csv) used in this tutorial. Be sure to right-click and save the file to your R working directory. This dataset contains pre and post test scores for 66 subjects on a series of reading comprehension tests (Moore & McCabe, 1989). Note that all code samples in this tutorial assume that this data has already been read into an R variable and has been attached. The most fundamental way to calculate correlations is to directly operate on two variables. In R, this can be done using the cor() function. The cor() function accepts the following arguments (“Correlation, Variance…”, n.d.). In most cases, x and y are the only arguments that you will use when running the cor() function. The basic format for calculating a correlation is cor(VAR1, VAR2), where VAR1 and VAR2 are the variables that you would like to correlate. Suppose that our research question is: “How does a subject’s pretest 1 score relate to his or her posttest 1 score?” The following example demonstrates how to use the cor() function to calculate the correlation between pretest 1 (PRE1) and posttest 1 (POST1). When beginning to analyze a dataset, researchers often want to get a complete picture of all correlations, rather than just a single one. Conveniently, the cor() function can also be run on an entire set of data. The format for this operation is cor(DATAVAR), where DATAVAR is the name of the R variable containing the data. Suppose now that our research question is: “How do all of the test scores in the dataset relate to each other?” The following example demonstrates how to use the cor() function to calculate all of the correlations in a dataset. The output of the preceding function is pictured below.  To see a complete example of how correlational analysis can be conducted in R, please download the correlational analysis example (.txt) file.  Correlation, Variance and Covariance (Matrices). (n.d.). Retrieved October, 27, 2009 from http://sekhon.berkeley.edu/stats/html/cor.html Moore, D., and McCabe, G. (1989). Introduction to the practice of statistics [Data File]. Retrieved October, 27, 2009 from http://lib.stat.cmu.edu/DASL/Datafiles/ReadingTestScores.html 	 0 Comments
Creating scatter plots using ggplot2	https://www.r-bloggers.com/2009/11/creating-scatter-plots-using-ggplot2/	November 6, 2009	Ralph	The ggplot2 package can be used as an alternative to lattice for producing high quality graphics in R. The package provides a framework and hopefully simple interface to producing graphs and is inspired by the grammar of graphics. The main function for producing graphs in this package is qplot, which stands for quick plot. The first two arguments to the function are the name of objects that contain the x and y variables for the plot that is being created. Like many functions in R there is a data argument that can be used to specify a data frame to look in for the variables. As a first example we create a scatterplot of age and circumference for the data set in R that has measurements of the growth of Orange trees. The code to produce this graph is very simple and is shown below: This produces the following graph: Scatterplot Example 1 The main thing with this graph is that we are ignoring the different trees and looking at the overall trend. If we want to distinguish between the growth for the trees separately we can use different colours for the plotting symbols and add a legend to indicate which colour corresponds to a given tree. The colour argument is used to specify a variable and qplot will automatically created a legend based on the levels of this categorical variable. We adjust our code to be: and the graph now looks like: Scatterplot Example 2 That is a nice improvement on the initial graph as we can visually compare the growth trends for the five trees. We can build additional elements into our graph, such as adding a smoother to show a trend, by making use of the geom argument which is used to specify what type of display is being created. The package has a nice feat that allows us to specify a vector with multiple elements to build up additional elements to the graph. We can add a smoother to the original plot with the code below: This produces the following graph: Scatterplot Example 3 An alternative would be to change from plotting with symbols to joining the points with lines. This change again makes use of the geom argument as follows: The graph now looks like: Scatterplot Example 4 with a separate coloured line for each tree. 	 0 Comments
Top Five Open Source Projects of 2009	https://www.r-bloggers.com/2009/11/top-five-open-source-projects-of-2009/	November 5, 2009	Ed Borasky		 0 Comments
opentick alternatives	https://www.r-bloggers.com/2009/11/opentick-alternatives/	November 5, 2009	Joshua Ulrich	"
 "	 0 Comments
R has a JSON package	https://www.r-bloggers.com/2009/11/r-has-a-json-package/	November 5, 2009	nsaunders	Named rjson, appropriately.  It’s quite basic just now, but contains methods for interconversion between R objects and JSON.  Something like this: Use cases?  I wonder if RApache could be used to build an API that serves R data in JSON format? 	 0 Comments
Show me the mean(ing)…	https://www.r-bloggers.com/2009/11/show-me-the-meaning%e2%80%a6/	November 5, 2009	Manos Parzakonis	Well testing a bunch of samples for the largest population mean isn’t that common yet a simple test is at hand. Under the obvious title “The rank sum maximum test for the largest K population means” the test relies on the calculation of the sum of ranks under the combined sample of size , where  is the common size of the k’s samples. For illustration purposes the following data are used. They consist of 6 samples of 5 observations. Next we construct a convenient matrix and we compute the sample ranks So we would test whether the 4th sample has the largest population mean. First we need critical values. So, we cannot accept the hypothesis of the largest mean for the 4th sample. Look it up… Gopal K. Kanji, 100 Statistical Tests , Sage Publications [google] 	 0 Comments
Scivews-K got updated again	https://www.r-bloggers.com/2009/11/scivews-k-got-updated-again/	November 4, 2009	Shige		 0 Comments
R’s xtabs for total weighted read coverage	https://www.r-bloggers.com/2009/11/rs-xtabs-for-total-weighted-read-coverage/	November 4, 2009	Jeremy Leipzig		 0 Comments
Split, apply, and combine in R using PLYR	https://www.r-bloggers.com/2009/11/split-apply-and-combine-in-r-using-plyr/	November 4, 2009	Stephen Turner		 0 Comments
LondonR tomorrow night	https://www.r-bloggers.com/2009/11/londonr-tomorrow-night/	November 2, 2009	mikeksmith's posterous	"
 LondonR Date:  Tuesday 3rd November Time: 6pm  – 9.30pm Venue: Shooting Star Public house,  129    City Rd London, EC1, United Kingdom +44 20 7929 6818 Introduction: Richard Pugh – mangosolutions To   register, for more information or to speak at the next LondonR meeting please email us The presentations from the last events are available here. Click here to register on the London R mailing list I’m heading to the LondonR meeting tomorrow night at the Shooting Star pub (E1) hosted by Mango Solutions. It’ll be interesting to hear how folks from industries other than the Pharma world are using R. BTW – the address for the pub is a little misleading. We think it’s this one: Shooting Star, Liverpool St E1 7JF Permalink 

	| Leave a comment  »
 "	 0 Comments
Welsh test by permutations	https://www.r-bloggers.com/2009/11/welsh-test-by-permutations/	November 1, 2009	Timothée Poisot		 0 Comments
R Tutorial Series: Summary and Descriptive Statistics	https://www.r-bloggers.com/2009/11/r-tutorial-series-summary-and-descriptive-statistics/	November 1, 2009	John M. Quick	"Summary (or descriptive) statistics are the first figures used to represent nearly every dataset. They also form the foundation for much more complicated computations and analyses. Thus, in spite of being composed of simple methods, they are essential to the analysis process. This tutorial will explore the ways in which R can be used to calculate summary statistics, including the mean, standard deviation, range, and percentiles. Also introduced is the summary function, which is one of the most useful tools in the R set of commands. Before we start, you may want to download the sample data (.csv) used in this tutorial. Be sure to right-click and save the file to your R working directory. This dataset contains hypothetical age and income data for 20 subjects. Note that all code samples in this tutorial assume that this data has already been read into an R variable and has been attached. In R, a mean can be calculated on an isolated variable via the mean(VAR) command, where VAR is the name of the variable whose mean you wish to compute. Alternatively, a mean can be calculated for each of the variables in a dataset by using the mean(DATAVAR) command, where DATAVAR is the name of the variable containing the data. The code sample below demonstrates both uses of the mean function. Within R, standard deviations are calculated in the same way as means. The standard deviation of a single variable can be computed with the sd(VAR) command, where VAR is the name of the variable whose standard deviation you wish to retrieve. Similarly, a standard deviation can be calculated for each of the variables in a dataset by using the sd(DATAVAR) command, where DATAVAR is the name of the variable containing the data. The code sample below demonstrates both uses of the standard deviation function. Keeping with the pattern, a minimum can be computed on a single variable using the min(VAR) command. The maximum, via max(VAR), operates identically. However, in contrast to the mean and standard deviation functions, min(DATAVAR) or max(DATAVAR) will retrieve the minimum or maximum value from the entire dataset, not from each individual variable. Therefore, it is recommended that minimums and maximums be calculated on individual variables, rather than entire datasets, in order to produce more useful information. The sample code below demonstrates the use of the min and max functions. The range of a particular variable, that is, its maximum and minimum, can be retrieved using the range(VAR) command. As with the min and max functions, using range(DATAVAR) is not very useful, since it considers the entire dataset, rather than each individual variable. Consequently, it is recommended that ranges also be computed on individual variables. This operation is demonstrated in the following code sample. Given a dataset and a desired percentile, a corresponding value can be found using the quantile(VAR, c(PROB1, PROB2,…)) command. Here, VAR refers to the variable name and PROB1, PROB2, etc., relate to probability values. The probabilities must be between 0 and 1, therefore making them equivalent to decimal versions of the desired percentiles (i.e. 50% = 0.5). The following example shows how this function can be used to find the data value that corresponds to a desired percentile.  Note that quantile(VAR) command can also be used. When probabilities are not specified, the function will default to computing the 0, 25, 50, 75, and 100 percentile values, as shown in the following example. In the opposite situation, where a percentile rank corresponding to a given value is needed, one has to devise a custom method. To begin, consider the steps involved in calculating a percentile rank. From the preceding steps, the formula for calculating a percentile rank can be derived: percentile rank = length(VAR[VAR <= VAL]) / length(VAR) * 100, where VAR is the name of the variable and VAL is the given value. This formula makes use of the length function in two variations. The first, length(VAR[VAR <= VAL]), counts the number of data points in a variable that are below the given value. Note that the ""<="" operator can be replaced with other combinations of the , and = operators, supposing that the function were to be applied to different scenarios. The second, length(VAR), counts the total number of data points in the variable. Together, they accomplish steps one and two of the percentile rank computation process. The final step is to multiply the result of the division by 100 to transform the decimal value into a percentage. A sample percentile rank calculation is demonstrated below. A very useful multipurpose function in R is summary(X), where X can be one of any number of objects, including datasets, variables, and linear models, just to name a few. When used, the command provides summary data related to the individual object that was fed into it. Thus, the summary function has different outputs depending on what kind of object it takes as an argument. Besides being widely applicable, this method is valuable because it often provides exactly what is needed in terms of summary statistics. A couple examples of how summary(X) can be used are displayed in the following code sample. I encourage you to use the summary command often when exploring ways to analyze your data in R. This function will be revisited throughout the R Tutorial Series. The output of the preceding summary is pictured below.  The output of the preceding summary is pictured below.  To see a complete example of how summary statistics can be used to analyze data in R, please download the summary statistics analysis example (.txt) file. Thank you for participating in the Summary and Descriptive Statistics tutorial. I hope that it has been useful to your work with R and statistics. Please let me know of any feedback, questions, or requests that you have in the comments section of this article. Our next guide will be on the topic of Zero-Order Correlations. "	 0 Comments
Happy New Year with R	https://www.r-bloggers.com/2009/12/happy-new-year-with-r/	December 31, 2009	Yihui Xie	I have to admit that the previous post on Christmas is actually not much fun. Today I received another pResent from Yixuan which is more interesting:  Basically the code deals with letter polygons (i.e. glyphs) and plot them with proper projections from 3D to 2D space: Thanks, Yixuan! By the way, there are even more interesting demos in the fun package, just install the package by install.packages('fun', repos = 'http://r-forge.r-project.org') to see demo(package = 'fun')$results[, 'Item']. 	 0 Comments
Updates about R-bloggers, a Happy new year, and one request	https://www.r-bloggers.com/2009/12/updates-about-r-bloggers-a-happy-new-year-and-one-request/	December 31, 2009	Tal Galili		 0 Comments
Because it’s Friday: The inner life of a cell	https://www.r-bloggers.com/2009/12/because-its-friday-the-inner-life-of-a-cell/	December 31, 2009	David Smith	Happy New Year, everyone. In celebration of the New Year, enjoy this celebration of the workings of life from Harvard University.   More at the link below: 3 Quarks Daily: The inner life of the cell 	 0 Comments
R/Finance 2010, April 16-17 in Chicago	https://www.r-bloggers.com/2009/12/rfinance-2010-april-16-17-in-chicago/	December 31, 2009	David Smith	Today is the last day to submit abstracts for the R/Finance 2010 conference to be held in Chicago on April 16-17. If you’re not planning on speaking, but are interested in applications of R in Finance, be sure to add this to your calendar — last year’s conference was an outstanding event. Here’s some more information about the conference from the website: The two-day conference will cover topics including portfolio management, time series analysis, advanced risk tools, high-performance computing, and econometrics. All will be discussed within the context of using R as a primary tool for financial risk management and trading. We strongly encourage R users, as well as package authors to submit an abstract for presentation at this years conference. The 2010 conference will build upon the success of last year’s event. Including traditional keynotes from leading names in the R and finance community, presentation of contributed papers, short “lightning-style” presentations as well as the chance to meet and discuss colloboratively the future of the R in Finance community. R/Finance 2010: Applied Finance with R 	 0 Comments
R in the NYT	https://www.r-bloggers.com/2009/12/r-in-the-nyt/	December 30, 2009	Ethan Fosse		 0 Comments
Brief Analysis of Abdulmutallab (Christmas Day bomber) Web Posting Data	https://www.r-bloggers.com/2009/12/brief-analysis-of-abdulmutallab-christmas-day-bomber-web-posting-data/	December 30, 2009	Drew Conway	Thanks to Evan Kohlman at the NEFA Foundation for compiling, and Danger Room for publicizing, the data set of all of Farouk Abdulmutallab’s posts to the Islamic Forum on Gawaher.com.  Since Evan took the initiative to download and save the  raw HTML data, I thought it would be useful to go one step further and parse it into a more useable (analyzable?) format.  With a little work in Python and html5lib I was able to convert the HTML into a long comma-delimited file with observation data for post date, time, title, contents, number of views, and number of replies. With this new data set I did some arm-chair analysis to see what—if any—interesting could be found.  Using R along with ggplot2 I generated several visualizations that contain some notable observations.  As I sure you are eager to get your hands on the data, I will say before moving on that the CSV file can be downloaded at the ZIA Code Repository along with an R file used to generate the visualizations and analysis after the jump. UPDATE: The intrepid Michael Bommarito of Computational Legal Studies took it one step further, and downloaded and parsed all of Abdulmutallab’s postings and correspondences on the web forum.  More data for the hungry masses, thanks Mike!  First, I am not one for content analysis, but friend and R expert Josh Reich pulled the data and created a nice Wordle of Abdulmutallab’s posts.  Some interesting things here, notably the prominence of words like “think,” “help,” and “want.”  Rather than look at the posts themselves, I was interested in the activity; and as such I begin with a histogram of his postings over the time perdiod. Other than having a very approximate seasonal tendency with two peaks, it is difficult to claim that Abdulmutallab’s posting density followed any strict pattern.  Instead, at this binning (every 30 days from the first post, ignore the date label–they are wrong) it appears his activity came in ebbs and flows.  Given this bustiness may be useful to also examine the popularity of Abdulmutallab’s posts.  Although we have no benchmark for mean post popularity on Gawaher.com, we can examine within the sample.  Below is a scatter plot of his post’s views versus replies, with points sized and colored by the ratio of replies to views. This data is quite skewed, so logs were taken, and from this we can see the relationship is rather linear.  There are; however, several notable posts that receive nearly one reply for every two views (large red points).  For the final analysis I combine both time and activity to see if there were any periods where Abdulmutallab received notably high attention from forum users.  The plot below shows his post’s view and reply counts in chronological order.  Two interesting observation from this visualzation.  First, while the previous plot indicates that several posts receive a high ratio of replies to views, this plot shows that these high ratio posts are not the most popular.  In fact, the most popular posts (by view count) have relatively few replies, and all happen within a short time span.  This latter observations is most interesting, as clearly Abdulmutallab was writing on something very interesting to the Gawaher.com audience during this span.  The next step would be to go back into the data, examine that time period and analyze the posts’ content. Another idea would be to create a model to examine if lagged number of replies predicted length or content of a post.  There may also be something interesting to say about the content of a post’s title and the how that attracts users.  Unfortunately, without the content of those replies and only the count it is hard to determine what the relationship is. What are your ideas? Photo: Danger Room 	 0 Comments
Use plyr instead of _apply() in R	https://www.r-bloggers.com/2009/12/use-plyr-instead-of-_apply-in-r/	December 30, 2009	Stephen Turner		 0 Comments
What’s up with Darwin’s weather?	https://www.r-bloggers.com/2009/12/whats-up-with-darwins-weather/	December 30, 2009	David Smith	Darwin is a the capital city of Australia’s Northern Territory. Lying on the coast of far Northern Australia, it’s situated well in the tropics and as a result has hot, steamy, monsoonal weather. Darwin’s weather has already had impact on urban culture, and now it seems it’s had a political impact too: it’s been at in the middle of the recent “Climategate” “scandal”. A climate-change skeptic observed a discontinuity in the raw temperature records at Darwin airport in 1942 (which, as it turned out, was caused by a change in the equipment), and claimed that the standardization of the data was an overt attempt to present a cooling trend as a warming trend. This claim was roundly denounced in The Economist (prompting yet another reply and rebuttal). Matthew Markus wasn’t satisfied with drawing conclusions from a back-and-forth in the blogosphere, though. He decided to get hold of the raw data from the Global Historical Climate Network and attempt to reproduce this “smoking gun” graphic for himself in R.   Markus provides all the details and R code for accessing and plotting the temperature data in a blog post. There’s lots of practical advice there, including how use the various command-line tools to uncompress and inspect the data (although you’ll need a Unix-ish machine to follow along) as well as all the R commands for reading in the data. (It makes for a nice demonstration of reading fixed-format data files into R, actually.) You’ll also see how to standardize and plot the data, with a result fairly close to the original above:  Matthew Markus: Raw Darwin Airport Temperature Data with R    	 0 Comments
Top Ten Must-Have R Packages for Social Scientists	https://www.r-bloggers.com/2009/12/top-ten-must-have-r-packages-for-social-scientists/	December 29, 2009	Ethan Fosse		 0 Comments
Start your engines; it’s a Linux era!	https://www.r-bloggers.com/2009/12/start-your-engines-it%e2%80%99s-a-linux-era/	December 29, 2009	Manos Parzakonis	Well, I’m writing this from my new system. After years on hiatus I migrated to Linux, once again. Setting up a full system on Linux for a Greek user had been one of the greatest challenges. First,of all setting up writing, reading & printing in Greek was the biggest obstacle, I still recall memories of 2000/2001 when installing the 9th (?) edition of Mandrake Linux, supposed to have full Greek language integration. As if? Second, comes networking with Linux. Win-modems ruled Greek market and made it really painful to read all these articles to even start hoping that you might be near configuring your Internet connection. Yet, I used Linux till 2003 or 2004 with no Internet connection and used my Windows to surf the net… Courageous, right? Last night I was surfing for open source ideas and stumbled upon Wuby. Wubi is a windows-ish application for Ubuntu, the hottest for users of my clan! I installed on my disk, chose my graphical environment (Gnome, no regrets) and one thing left to check…the Internet highway. Turns out that, routers (those using Ethernet ports!) are plug & go. Neat! Unfortunatelly, I’m stuck with my USB model my provider gave a couple of  years ago ;( Sad thoughts paid a short visit I must admit. But, even this  was no real problem, UbuDSL has everything under control :thumps up: Of, course I installed RKWard, there’s a Windows version, too. I never got to make it work, though…  	 0 Comments
C++ exceptions at the R level	https://www.r-bloggers.com/2009/12/c-exceptions-at-the-r-level/	December 29, 2009	romain francois	"I’ve recently offered an extra set of hands to Dirk to work on the Rcpp package, this serves a good excuse to learn more about C++ Exception management was quite high on my list. C++ has nice exception handling (well not as nice as java, but nicer than C).  With previous versions of Rcpp, the idiom was to wrap up everything in a try/catch block and within the catch block, call the Rf_error function to send an R error, equivalent of calling stop. Now things have changed and, believe it or not, you can now catch a C++ exception at the R level, using the standard tryCatch mechanism This, combined with the new inline support for Rcpp, allows to run this code, (also available in the inst/examples/RcppInline directory of Rcpp) Here, we create the funx ""function"" that compiles itself into a C++ function and gets dynamically linked into R (thanks to the inline package). The relevant thing (at least for this post) is the throw statement. We throw a C++ exception of class ""std::range_error"" with the message ""boom"", and what follows shows how to catch it at the R level: ... et voila Under the carpet, the abi unmangling namespace is at work, and the function that grabs the uncaught exceptions is much inspired from the verbose terminate handler that comes with the GCC Part of this was inspired from the new java exception handling that came with the version 0.8-0 of rJava, but cooked with C++ ingredients "	 0 Comments
RGedit	https://www.r-bloggers.com/2009/12/rgedit/	December 29, 2009	Shige		 0 Comments
Speed-reading files, revisited	https://www.r-bloggers.com/2009/12/speed-reading-files-revisited/	December 29, 2009	David Smith	In a post earlier this month, it seemed as though compressing a data file before reading it into R could save you some time. With some feedback from readers and further experimentation, we might need to revisit that conclusion To recap, in our previous experiment it took 170 seconds to read a 182Mb text file into R. But if we compressed the file first, it only took 65 seconds. Apparently, the benefits of reducing the amount of disk access (by dealing with a smaller file) far outweighed the CPU time required to decompress the file for reading. In that experiment, though, each file was only read once. If you simply repeat the read statement on the uncompressed file, you see a sudden decrease in the time required to read it:  > system.time(read.table(“bigdata.txt”, sep=“,”))    user  system elapsed  165.042   1.316 165.807  > system.time(read.table(“bigdata.txt”, sep=“,”))    user  system elapsed   94.248   0.934  94.673  (This was on MacOS, using the R GUI. I also tried using R from the terminal on MacOS, and also from the R GUI in Windows, using both regular R and REvolution R. There were some slight variations in the timings, but in general I got similar results.) So what’s going on here (other than my embarrassing failure as a statistician to replicate my measurements the first time round)? One possibility is that we’re seeing the effects of disk cache: when you access data on a hard drive, most modern drives will temporarily store some of the data in high-speed memory. This makes it faster to access the file in subsequent attempts, for as long as the file data remains in the cache. But that doesn’t explain why we don’t see a similar speedup in repeated readings of the compressed file:  > system.time(read.table(“bigdata-compressed.txt.gz”, sep=“,”))    user  system elapsed   89.464   0.868  90.436  > system.time(read.table(“bigdata-compressed.txt.gz”, sep=“,”))    user  system elapsed   97.651   1.035  98.887   I’d expect the second reading to be faster if disk cache had an effect, so I don’t think disk cache is the culprit here. More revealing is the fact that the first use of read.table in any R session takes longer than subsequent ones. Reading from the gzipped file is slower than reading from the uncompressed file if it’s the first read of the session:   > system.time(read.table(“bigdata-compressed.txt.gz”, sep=“,”))    user  system elapsed  150.429   1.304 152.447    > system.time(read.table(“bigdata.txt”, sep=“,”))    user  system elapsed   78.717   0.986  79.773  So what’s going on here? (This was using R from the terminal under MacOS; I got similar results using the R GUI on MacOS.) I don’t have a good explanation, frankly. Maybe the additional time is required by R to load libraries or to page in the R executable (but why would it scale with the file size, then?). Note that we got the speed benefits from reading the uncompressed file second, which rules out disk cache having any significant benefits. If any one has any good explanations, I’d love to hear them.So what file type is the fastest for reading into R? Reader Peter M. Li took a much more systematic approach to answering that question than I did, running fifty trials for compressed and uncompressed files using both read.table and scan. (We can safely assume that this level of replication nullifies any first-read or caching effects.) He also tested Stata files (an open, binary data file format that R can both read and write). Peter also tested different file sizes for each file type, with files containing one thousand, 10 thousand, 100 thousand, one million and 10 million observations.  His results are summarized in the graph below, with log(file size) on the X axis and log(time to read) on the Y axis:   So, what can we conclude from all of this? Let’s see:          	 0 Comments
tooltips in R graphics; nytR package	https://www.r-bloggers.com/2009/12/tooltips-in-r-graphics-nytr-package/	December 28, 2009	jackman	At Doug Rivers’ suggestion, I started investigating tooltips as a way to label points in R graphs.  An example appears at the top of my blog, where I plot the ideal points (revealed preferences) of the (current) 111th U.S. House of Representatives against Obama vote share in their district in 2008 (SVG). I’m using the RSVGTipsDevice package in R for this, which seems to be the only way to get tooltips into R graphics output.  SVG is a reasonable enough vehicle when pushing output to the web, but PDF would be preferable.   As far as I can tell, (a) the “raw” PDF one needs to generate a tooltip isn’t difficult, and there are lots of examples out there to copy from (e.g., the various tooltip packages for pdftex), but (b) there is no way to push arbitrary PDF to a PDF device from R.  Hacking/re-writing the pdf device in R might be one way to go, but I’m not going to do that.  Post-processing the PDF with a program with decent PDF support might be another way to attack this (maybe python). In other R-for-polisci news, Shane Conway told me about a R package he has but together for getting NYTimes data into R.  The NYTimes has APIs letting you get into their in-house data, such as roll call data from the U.S. Congress.  Shane’s package is essentially a R interface to to the NYTimes API for the Congressional data.  Nice, although you’ll need a NYTimes API key for this.    See also the R code snippet for NYTimes scraping from Stanford’s own Claudia Engel. I use my own readKH function in pscl to hit Jeff Lewis’ site; Jeff scrapes the Congressional web sites in close to real time, but the NYTimes has some nice contextual data as well.    	 0 Comments
inline 0.3.4 released	https://www.r-bloggers.com/2009/12/inline-0-3-4-released/	December 28, 2009	Thinking inside the box	"
A new release of 
Rcpp
will probably follow in due course as we can now depend on this new 
inline
version; Romain has also put nice new code in around exceptions and we may
add some more on external pointers. Good times …

 "	 0 Comments
Introduction to the Grammar of Graphics with ggplot2 in R	https://www.r-bloggers.com/2009/12/introduction-to-the-grammar-of-graphics-with-ggplot2-in-r/	December 28, 2009	VCASMO - drewconway		 0 Comments
Example 7.19: find the closest pair of observations	https://www.r-bloggers.com/2009/12/example-7-19-find-the-closest-pair-of-observations/	December 28, 2009	Ken Kleinman		 0 Comments
Estimated Net Worth of SoilWeb- Our Online Soil Survey	https://www.r-bloggers.com/2009/12/estimated-net-worth-of-soilweb-our-online-soil-survey/	December 28, 2009	dylan	"According to the excellent source code evaluation tool, SLOCCount, our online soil survey (SoilWeb) code is worth about $268,543 and would require about 2 years of development time to re-create from scratch with a single developer working full-time. This is a fairly close estimate, as I have been working (part-time) on this code-base for 3 years now with most of the time spent in the first 2 years. For comparison, the current development version of GRASS 6 is based on about 230 person-years of development time and is worth about 31 million dollars. Neat!  
SLOCCount Output read more "	 0 Comments
Capture system commands as R objects with system(…, intern=T)	https://www.r-bloggers.com/2009/12/capture-system-commands-as-r-objects-with-system-internt/	December 28, 2009	Stephen Turner		 0 Comments
Video: Visualizing data in R using ggplot2	https://www.r-bloggers.com/2009/12/video-visualizing-data-in-r-using-ggplot2/	December 28, 2009	David Smith	At the most recent New York R User Group meetup, the topic was creating graphics in R with the ggplot2 package. Drew Conway’s talk, “Making pretty pictures with ggplot2” gave several practical examples of visualizing data with ggplot2 and is well worth checking out: You can follow along with Drew’s slides, which are downloadable from his blog. (Update: you can now also watch the video and slides in sync — thanks, Drew!.) You’ll also find there video and slides of another talk, “Introduction to the Grammar of Graphics with ggplot2 in R” from Harlan Harris. Zero Intelligence Agents: Visualizing Data with R and ggplot2 (Video) 	 0 Comments
Visualizing Data in R with ggplot2	https://www.r-bloggers.com/2009/12/visualizing-data-in-r-with-ggplot2/	December 28, 2009	VCASMO - drewconway		 0 Comments
SQL group by in R	https://www.r-bloggers.com/2009/12/sql-group-by-in-r/	December 27, 2009	Chris	The R statistical computing environment is awesome, but weird. How to do database operations in R is a common source of questions. The other day I was looking for an equivalent to SQL group by for R data frames. You need this to compute summary statistics on subsets of a data set divided up by some categorical variable. It turns out there are several ways to get the same effect, some more limited than others. The best answer seems to be plyr. It automates the The split-apply-combine strategy for data analysis you’d use otherwise. The ddply splits a data frame into subset data frames, performs some function on the subsets, and returns the results as a recombined data frame. Here’s a few links: A Fast Intro to Plyr for R, Block-processing a data frame with plyr and  Split, apply, and combine in R using PLYR. This paper is worth reading. It introduces the library and also gives you a nice framework (split-apply-combine) for thinking about a whole class of data-munging problems. A coworker (thanks, Gustavo) pointed out that this is a lot like Google’s MapReduce. Some commands that get you part of the way there are: split, by, tapply (nicely explained here), aggregate. The R wiki has an entry on Performing calculations within sub-sets of a data-frame that uses the reshape library. You could always use sqldf or RSQLite. Several options are discussed here. You can cobble up a fully general process using split, some form of sapply, and unsplit. But, that’s what plyr does automatically. Side notes: While fooling around with this, I noticed that, for some crazy reason, split.data.frame splits matrices into nice subunits, but split has the ugly side-effect of reducing matrices to vectors. Also, Google has a style guide for R. More R mini-tutorials: 	 0 Comments
Merry Christmas using R	https://www.r-bloggers.com/2009/12/merry-christmas-using-r-2/	December 25, 2009	David Smith	Yihui creates an Christmas greeting using R:  See the R code to create an animated message in Flash here: 	 0 Comments
Because it’s Christmas: Microbe Mario	https://www.r-bloggers.com/2009/12/because-its-christmas-microbe-mario/	December 25, 2009	David Smith	He’s no Father Christmas, but he is dressed in festive red and green, and he’s made of bacteria. This image, Mario, was submitted to the 2009 international Genetically Engineered Machine (iGEM) competition by Team Osaka from the nanobiology laboratories at the University of Osaka, Japan. They genetically engineered bacteria to express fluorescent proteins and carotenoid pigments to create works of art.    See more works of microbial art at this New Scientist gallery. Have a great Christmas, everyone. We’ll be back next week. New Scientist: Masterworks in Petri dishes  	 0 Comments
Another Visualization of Unemployment	https://www.r-bloggers.com/2009/12/another-visualization-of-unemployment/	December 24, 2009	Diego		 0 Comments
Error : .onLoad failed in ‘loadNamespace’ for ‘RWeka’	https://www.r-bloggers.com/2009/12/error-onload-failed-in-%e2%80%98loadnamespace%e2%80%99-for-%e2%80%98rweka%e2%80%99/	December 24, 2009	heuristicandrew	"After installing Weka/RWeka in R, you may get this error if you try to load RWeka in the same session: require(RWeka)

Cannot create Java virtual machine (-4)

Error : .onLoad failed in 'loadNamespace' for 'RWeka'

 Solution: Just close R and re-open it. Cause: Apparently the installation requires some initialization. Tested on R 2.10.1 on Windows XP. "	 0 Comments
A web-based graphics application based on R	https://www.r-bloggers.com/2009/12/a-web-based-graphics-application-based-on-r/	December 24, 2009	David Smith	FlowingData recently took a look at Jeroen Ooms’ latest web-based statistical tool based on R. We’ve looked at his tools for random-effects models and finance visualizations before, but this one is a more general tool for creating graphs from data sets using the ggplot2 package. It’s pretty slick. All you need to do is upload a data set (in comma-separated .csv or tab-delimited .txt format) and then you can use the grammar of graphics philosophy underlying ggplot2 to layer various graph types using your data, and also to facet it (like panels in Trellis or lattice) by categorical variables. Here’s an example I created in just a few minutes using the cereals dataset from the Data and Story Library:  If you want to try it out yourself, a trick is that most of the interaction is through right-clicking on the middle pane where the graphics appear: use the menus to set up your X and Y variables, and then choose the type of graphic with the “New Geom Layer (2D)” menu. There are many other features, which you can see in action in the video below. But the best feature of all, which I would have missed except for the video, is that it shows you the code to create the graph in R using ggplot2. There’s a tiny tiny arrow at the very bottom of the application: click it to show the code pane. The code it displays is exactly the code you would write as an R programmer to create the chart. This makes Jereon’s application a really useful tool for learning the capabilities and syntax of ggplot2.     Jeroen Oons: ggplot2 (via FlowingData)   	 0 Comments
Beancounter minor bug fix release 0.8.9	https://www.r-bloggers.com/2009/12/beancounter-minor-bug-fix-release-0-8-9/	December 23, 2009	Thinking inside the box		 0 Comments
Compare performance of machine learning classifiers in R	https://www.r-bloggers.com/2009/12/compare-performance-of-machine-learning-classifiers-in-r/	December 23, 2009	heuristicandrew		 0 Comments
R in India: The Hindu	https://www.r-bloggers.com/2009/12/r-in-india-the-hindu/	December 23, 2009	David Smith	The Hindu, a leading English-language newspaper in India, published an article on December 21 about doing research with open-source tools and R got a prominent mention:   Though commercial statistical packages are popular among researchers, their licensing costs drive people away from them. In this context, R https://www.r-project.org, the open source/free statistical package, which is fast becoming the darling of researchers/analysts, assumes significance. The great advantage of R is that it can be downloaded and installed on your machine without any licensing worries. Yet another advantage of R is that one can run it on multiple platforms such as Linux, Mac and Windows. Naturally, this adds to the autonomy of a researcher. Unlike in the past, students of this generation prefer different operating systems to be run on their laptops. In a classroom setting of this kind, teaching statistics with commercial packages becomes quite unwieldy — all cannot afford to purchase software for different platforms. In this regard, commercial packages pale in comparison with R, which has no such restrictions. Besides being free, the advantage of this statistical software is its “extensibility” feature. R allows its users to enhance its functionality by creating new functions (this is similar to the extensions we find in Firefox). Other open-source tools mentioned in the article include Firefox, AbiWord, Open Office, and the Zotero Firefox extension and Jabref (bibliography managers). The Hindu: Doing research with open source tools  	 0 Comments
malapportionment in the U.S. Senate	https://www.r-bloggers.com/2009/12/malapportionment-in-the-u-s-senate/	December 23, 2009	jackman	The 40 Republican senators currently in the U.S. senate represent 36% of the U.S population.  See the graph below (click on the thumbnail for PDF).  This is something I’ve been meaning to compute for a while now, mapping the cumulative distribution of senators’ ideal points onto the cumulative distribution of state population (each state counts twice, once for each senator, and of course we exclude DC and PR from the calculation of the total population). With the relatively high degree of partisan polarization in roll call voting we place all 40 Republicans to the right of the Democratic senators.  And — as recent roll calls on the health care issue have made clear — in the current 111th Senate the Democrats plus Sanders and Lieberman constitute 60 votes, with the most conservative Democrat (Nelson, NE) the “veto pivot”.   Between them, the 60 Dems represent 64% of the population.   My prior expectation was for more “small state” bias pushing that 64% higher, but (a) Dems represent some small states (e.g., VT, ND, DE, MT, RI, HI have 2 Dem senators); (b) Republicans have the 2 TX and GA senators, and the OH/FL/NC Senate delegations are split. The “bias” is a little more pronounced at the chamber median threshold: i.e., the 50 “most liberal” (Democratic) senators represent 57% of the population.   	 0 Comments
The Life Scientists at FriendFeed: 2009 summary	https://www.r-bloggers.com/2009/12/the-life-scientists-at-friendfeed-2009-summary/	December 23, 2009	nsaunders	" The Life Scientists 2009 (Note: this post is a work in progress)

The contributors
First of all, take a look at yourselves.  There are, allegedly, 1250 subscribers to the group, but I can only retrieve profiles for 1053 of them. 248 of you are rather shy, opting for the default avatar and one or two of you look rather like porn stars.  If nothing else, this illustrates the difficulty of compiling reliable user statistics. Here’s how this image was assembled. User pictures were fetched using this script: ImageMagick was used to make the montage: Fetching the data
In a recent post, I presented Ruby code to fetch FriendFeed entries using the API.  To recap: entries cannot be fetched by date, so we have to employ a loop and the “?start=N” URL parameter to go back in time.  Naming the script tls2csv.rb, I went back to late 2008 like so: That generates a CSV file with 5 fields:  entry id, date, time, likes count and comments count.  If you wish, you can get it from Dropbox. Analysis using R
OK – let’s get to work.  First, load the data into R: Posts, likes and comments per day
We’ve retrieved 2363 unique posts.  First question: how much activity is there on any given day?  We’d like to sum the posts, likes and comments for each day, then visualise the activity.  Here’s one way to do that, again using the calendar heat map.  We use table() to sum posts by date and the plyr function ddply() to create a data frame for both likes and comments, summed by date:  Life Scientists 2009: daily posts, likes and comments How many posts, likes and comments can we expect per day, or likes/comments per post? Quite a range.  A median of 6 posts, 20 likes and 18 comments per day.  Or, 2 likes and 1 comment per post.
To me, this indicates a reasonable level of activity in the room:  there is new material and discussion most days and a post stands a good chance of receiving a like or a comment.  Another interesting observation is that although the “barrier to like” is lower than the “barrier to comment”, posts tend to be highly-commented rather than highly-liked.  Does this suggest that if a topic is interesting, important or controversial, it provokes comment and comments provoke more comments, whereas likes don’t generate more likes? Looking at those numbers is a little dull, though.  Let’s do a simple qplot from ggplot2 to look at how many likes and comments each post receives and if they are related:  Life Scientists 2009: likes/comments distributions and correlations Posts that generated the most discussion
This is probably what you came for.  Here are the top 10 posts, sorted first by likes and then by comments.  First, here’s the R code to sort a data frame by one of its factors (comments, then likes): Since we have the entry ID, we can retrieve the entries as JSON, e.g. using curl: Without further ado, the most popular posts and their contributor. Life Scientists: most commented entries Life Scientists: most liked entries As to why they were the most discussed entries – you’ll just have to go and read them.  The discussions don’t always revolve around the initial subject of the post 🙂 "	 0 Comments
RInside release 0.2.0	https://www.r-bloggers.com/2009/12/rinside-release-0-2-0/	December 22, 2009	Thinking inside the box	"

The biggest news is that we now support builds on Windows — if and only
if you use the R toolchain — more on that below. Since I had first released
RInside people
had emailed about support on Windows. However, Richard Holbrey deserves
special mention as he actually sat down and tried to build it. Over a few
email exchanges we made decent progress, but stalled.  More recently, I
sat down and had another go and lo and behold, it works.  This required a few
#ifdef statements here and there, as well as a function
setenv() which Windows does not have — Richard kindly helped
here and borrowed most of the code from R itself.

 
Now, to make this plain:  you need gcc (as this is how R
itself is built). This gcc compiler is the native compiler on Linux and OS X.
But on Windows this requires installing the
Rtools compiled by Duncan Murdoch
as detailed in the
The Windows Toolset
appendix to the R Installation manual.  Do not, I repeat, do not report an
error if you fail to build this with another compiler as this is not supported.

 
That said, if you have the proper tools, things are peachy. Install the
package, change into the examples directory installed with it
and say make (or on Windows: make -f Makefile.win).
That’s it — now run the examples (which on Windows will most likely require
setting the environment variable R_HOME).  Now you can run
simple examples like the one from the
previous
blog post or any of the other ones.

 
Happy embedding!



 "	 0 Comments
Multilevel and Longitudinal Modeling in Stata	https://www.r-bloggers.com/2009/12/multilevel-and-longitudinal-modeling-in-stata/	December 22, 2009	Ethan Fosse		 0 Comments
Visualizing Unemployment in Mexico	https://www.r-bloggers.com/2009/12/visualizing-unemployment-in-mexico/	December 22, 2009	Diego		 0 Comments
CPP package: exposing C++ objects	https://www.r-bloggers.com/2009/12/cpp-package-exposing-c-objects/	December 22, 2009	romain francois	"I’ve just started working on the new package CPP, as usual the project is maintained in r-forge. The package aims at exposing C++ classes at the R level, starting from classes from the c++ standard template library. key to the package is the CPP function (much inspired from the J function of rJava). The CPP function builds an S4 object of class “C++Class”. The “C++Class” currently is a placeholder wrapping the C++ class name, and defines the new method (again this trick or making new S4 generic comes from rJava). For example to create an R object that wraps up a std::vector, one would go like this:  This is no magic and don’t expect to be able to send anything to CPP (C++ does not have reflection capabilities), currently only these classes are defined : std::vector, vector, vector and set Because C++ does not offer reflection capabilities, we have to do something else to be able to invoke methods on the wrapped objects. Currently the approach that the package follows is a naming convention. The $ method create the name of the C routine it wants to call based on the C++ class the object is wrapping, the name of the method, and the types of the input parameters. So for example calling the size method for a vector&lt:int> object yields this routine name: “vector_int____size”, calling the push_back method of the vector<double>
class, passing an integer vector as the first parameter yields this signature : “vector_double____push_back___integer” …. (the CPP:::getRoutineSignature implements the convention) Here is a full example using the set<int> class. Sets are a good example of a data structure that is not available in R. Basically it keeps its objects sorted Currently the package is my excuse to learn about the standard template library, and it is quite possible that the functionality will be merged into the Rcpp it currently depends on. Because of this volatility, I’ll use the Rcpp-devel mailing list instead of creating a new one. "	 0 Comments
Forecasting the weather with R	https://www.r-bloggers.com/2009/12/forecasting-the-weather-with-r/	December 22, 2009	David Smith	The US National Centers for Environment Prediction (NCEP) produces weather forecasts for the entire world from a model that’s updated every 6 hours. The data is made freely available, and with a couple of free tools to convert the data and R you can easily produce am unpdated global weather forecast like this (click to enlarge):  (Check out the heat-wave in Australia!) Physicist and climate scientist Joe Wheatley provides all the details, including R code. All you need is R, the ncdf package to process the data file, and a free command-line tool to convert the file from the format provided by NCEP. Biospherica: NCEP Global Forecast System 	 0 Comments
Singapore, February 19-20: Computational Topics in Finance	https://www.r-bloggers.com/2009/12/singapore-february-19-20-computational-topics-in-finance/	December 21, 2009	David Smith	With all of the winter snows in the US this weekend, a trip to equatorial climes sounds pretty good right about now. That makes this email announcement from Rmetrics leader Diethelm Wuertz all the more tempting: Conference on ‘Computational Topics in Finance’February 19/20, 2010, National University of Singapore Dear R/Rmetrics Community, We would like to announce the first ‘Computational Topics in Finance‘ conference, taking place on February 19/20, 2010, at the National University of Singapore. The conference will bring together developers, practitioners, and users from academia, finance and insurance, providing a platform for common discussions and exchange of ideas. The topics will include using R/Rmetrics in finance, but the conference is by no means confined to R. Preceding the conference, February 19/20, 2010, we will be giving a two-day ‘Basic R for Finance‘ course. You can find out more about both events on our website, http://www.rmetrics.org/. We would like to invite you to take part in the conference, and we are now accepting submissions; please send your one-page abstracts to [email protected] The submission deadline is February 10, 2010. We look forward to seeing you in Singapore. Wishing you merry Christmas and a happy new year, Diethelm WuertzJuri HinzMahendra MehtaDavid Scott There’s also a poster you can download and print with more information. Rmetrics.org: Singapore Conference 2010 – Computational Topics in Finance  	 0 Comments
Animation video of rgl in action	https://www.r-bloggers.com/2009/12/animation-video-of-rgl-in-action/	December 21, 2009	Tal Galili	"Duncan Murdoch just posted a youtube video presenting an animation clip of a 3d rgl object.  Duncan even went further and wrote an explanation on how he made the video: here are the steps I used:
1.  Design a shape to be displayed, and then play with the animation functions to make it change over time.  Use play3d to do it live in R, movie3d to write the individual frames of the movie to .png files.
2.  Use the ffmpeg package (not an R package, a separate project at http://ffmpeg.org) to convert the .png files to an .mp4 file.  The individual frames totalled about 1 GB; the compressed movie is about 45 MB.
3.  Upload to Youtube.  I’m not a musician, so I had to use one of their licensed background tracks, I couldn’t write my own.  I spent a lot of time picking one and then adjusting the timing of the video to compensate.  Each render/upload cycle at full resolution took about an hour and a half.  It’s a lot faster to render in a smaller window with fewer frames per second, but it’s still tedious.   It’s easier to synchronize if you actually have a copy of the music locally, but Youtube doesn’t let you download their music.  So the timing isn’t perfect, but it’s good enough for me! Wonderful work Duncan, thanks for sharing! "	 0 Comments
MCMCglmm	https://www.r-bloggers.com/2009/12/mcmcglmm/	December 21, 2009	Shige		 0 Comments
Sweave in TeXShop	https://www.r-bloggers.com/2009/12/sweave-in-texshop/	December 21, 2009	Stewart MacArthur		 0 Comments
Rcpp and inline example	https://www.r-bloggers.com/2009/12/rcpp-and-inline-example/	December 20, 2009	Thinking inside the box	"
The following R code defines a character
variable gslrng. This variable contains a short C++ code
segment, which is then transformed by the function cfunction
into a function of two arguments assigned to funx:

 
The signature argument to cfunction defines two
variables s and n — which the C++ function then
reads in from R and converts to two integers seed and
len.
seed is used to initialize the random-number generator, and
len draws are then taken and stored in the STL vector
v which returned at the end.

 
As the R level, we now have a function of two arguments returning a vector of
RNG draws of the given lenth and using the given seed.

 
Also note how we tell cfunction to add the GSL include line,
specify that we want to compile and link against Rcpp and provide
-I and -L arguments to compile and link with the
GSL. (The include statement is not needed as the compiler would have found them
in /usr/include anyway, but it shows how to set this if needed.)
 
Finally, we simply call our freshly compiled, linked and loaded C++
function with arguments zero for the seed and five for the length, and print
the results vector returned to R from C++.

 "	 0 Comments
got R? get social science for R!	https://www.r-bloggers.com/2009/12/got-r-get-social-science-for-r/	December 19, 2009	Tal Yarkoni	Drew Conway has a great list of 10 must-have R packages for social scientists. If you’re a social scientist (or really, any kind of scientist) who doesn’t use R, now is a great time to dive in and learn; there are tons of tutorials and guides out there (my favorite is Quick-R, which is incredibly useful incredibly often), and packages are available for just about any application you can think of. Best of all, R is completely free, and is available for just about every platform. Admittedly, there’s a fairly steep learning curve if you’re used to GUI-based packages like SPSS (R’s syntax can be pretty idiosyncratic), but it’s totally worth the time investment, and once you’re comfortable with R you’ll never look back. Anyway, Drew’s list contains a number of packages I’ve found invaluable in my work, as well as several packages I haven’t used before and am pretty eager to try. I don’t have much to add to his excellent summaries, but I’ll gladly second the inclusion of ggplot2 (the easiest way in the world to make beautiful graphs?) and plyr and sqldf (great for sanitizing, organizing, and manipulating large data sets, which are often a source of frustration in R). Most of the other packages I haven’t had any reason to use personally, though a few seem really cool, and worth finding an excuse to play around with (e.g., Statnet and igraph). Since Drew’s list focuses on packages useful to social scientists in general, I thought I’d mention a couple of others that I’ve found particularly useful for psychological applications. The most obvious one is William Revelle‘s awesome psych package, which contains tons of useful functions for descriptive statistics, data reduction, simulation, and psychometrics. It’s saved me me tons of time validating and scoring personality measures, though it probably isn’t quite as useful if you don’t deal with individual difference measures regularly. Other packages I’ve found useful are sem for structural equation modeling (which interfaces nicely with GraphViz to easily produce clean-looking path diagrams), genalg for genetic algorithms, MASS (mostly for sampling from multivariate distributions), reshape (similar functionality to plyr), and car, which contains a bunch of useful regression-related functions (e.g., for my dissertation, I needed to run SPSS-like repeated measures ANOVAs in R, which turns out to be a more difficult proposition than you’d imagine, but was handled by the Anova function in car). I’m sure there are others I’m forgetting, but those are the ones that I’ve relied on most heavily in recent work. No doubt there are tons of other packages out there that are handly for common psychology applications, so if there are any you use regularly, I’d love to hear about them in the comments! 	 0 Comments
Rcpp 0.7.0	https://www.r-bloggers.com/2009/12/rcpp-0-7-0/	December 19, 2009	Thinking inside the box	"
This release has a couple new features :

 "	 0 Comments
Because it’s Friday: The decline of empires	https://www.r-bloggers.com/2009/12/because-its-friday-the-decline-of-empires/	December 18, 2009	David Smith	Here’s a neat visualization of the decline of the British, Spanish, Portugese and French empires from 1800 to present day. It’s definitely more art than stats — judging by the relative size of India and Australia I think the circles are scaled to area, not population — but it definitely does capture the drama and the ebb and flow of colonial history. One particularly smart aesthetic choice was to let the independent colonies fade out of the visualization and to focus on the dwindling empires themselves. Fast-forward to around 1915 to see what I mean.    via Andrew Sullivan: The decline of empires  	 0 Comments
Using Complex Numbers in R	https://www.r-bloggers.com/2009/12/using-complex-numbers-in-r/	December 18, 2009	John Myles White	"This post is a continuation of my series dealing with matrix operations for image processing. My next goal is to demonstrate the construction of simple low-pass and high-pass spatial frequency filters in R. It’s easy enough to construct simple versions of these filters in R using the Fast Fourier Transform (also known as the FFT), but, because the FFT is a slightly complicated tool, I’m going to build up to using it progressively over a few posts. For starters, I want to review the use of complex numbers in R. As always, if you’re interested in reviewing the mathematics of complex numbers, I’d start by browsing references online. The tutorial here seemed good to me at first glance, though I can’t claim to have read it through. Because complex numbers are implemented in the “base” package, it’s very easy to start working with them. To construct the complex number x + iy, you use complex: It’s conventional in mathematics to use z to refer to a complex number, so I’ll continue on with that tradition. As always occurs with mathematical data types in R, you can convert other objects to class “complex” using as.complex: And you can test that an object is complex using is.complex: Beyond those standard operations, there are five essential mathematical operations you’d want to use on complex numbers. First off, you want to be able to extract the real and imaginary components of a complex number. You can do this using Re and Im respectively: The (x, y) representation of numbers is easier to understand at first, but a polar coordinates representation is often more practical. You can get the relevant components of this representation by finding the modulus and complex argument of a complex number. In R, you would use Mod and Arg: This corresponds to the intuition that i should be at a distance 1 from the origin and an angle of pi / 2. Finally, you’ll want to be able to take the complex conjugate of a complex number; to do that in R, you can use Conj: As you can see, the modulus of z equals z times the conjugate of z, which is exactly what you expect. Now, historically, complex numbers were invented so that you could find the square root of negative numbers. By default, sqrt does not return a complex number when you ask for the square root of a negative number. Instead, it produces a NaN error, as you can see below: To get the complex square root, you need to cast your negative number as a complex number using as.complex before applying sqrt: In practice, complex numbers can be used to solve a huge range of problems, not least of which is the efficient representation of the FFT of an input vector. For fun, I thought I’d show a simple application: building a fractal using the Mandelbrot set. To quote Wikipedia, 
Mathematically the Mandelbrot set can be defined as the set of complex values of c for which the orbit of 0 under iteration of the complex quadratic polynomial zn+1 = zn2 + c remains bounded. That is, a complex number, c, is in the Mandelbrot set if, when starting with z0 = 0 and applying the iteration repeatedly, the absolute value of zn never exceeds a certain number (that number depends on c) however large n gets.
 We can translate this definition into R pretty easily by making certain assumptions about the exactness we want in our results. For my purposes, I’ll say that a number is in the Mandelbrot set if, after 100 iterations of the Mandelbrot algorithm, it’s never once had a modulus greater than 1,000,000. As you’ll see from the image I produced, this assumption works out pretty well, though it takes some real number crunching to get results out of it for reasonable sized data sets. Once we have this algorithm, we can easily generate an image of the Mandelbrot set by producing a matrix of complex numbers and depicting their inclusion in the set using image: And here’s the resulting image: UPDATE: Thanks to Giuseppe for pointing out the rounding error in the original code, producing an image with lines in it. "	 0 Comments
The unmarried parenthood rate in Mexico	https://www.r-bloggers.com/2009/12/the-unmarried-parenthood-rate-in-mexico/	December 18, 2009	Diego		 0 Comments
Plot ROC curve and lift chart in R	https://www.r-bloggers.com/2009/12/plot-roc-curve-and-lift-chart-in-r/	December 18, 2009	heuristicandrew		 0 Comments
SAS and R included on R bloggers	https://www.r-bloggers.com/2009/12/sas-and-r-included-on-r-bloggers/	December 18, 2009	Ken Kleinman		 0 Comments
Visualizing Data with R and ggplot2 (video w/ slides)	https://www.r-bloggers.com/2009/12/visualizing-data-with-r-and-ggplot2-video-w-slides/	December 18, 2009	Drew Conway	On December 3, 2009 I presented a brief talk at the NYC R meetup on how to create data visualizations with R using the immensely powerful ggplot2 package.  The talk is very light on motivation but heavy on examples, so it may be more useful to those with some R and/or ggplot2 experience.  The speaker that preceded me, Harlan Harris, provided a much more robust introduction to ggplot2; his slides are available here (requires OpenOffice) and I highly recommend them to anyone interested in learning the grammar of graphics and ggplot2. UPDATE: Harlan’s talk is now available online as well, and is embedded below my talk after the jump–enjoy! UPDATE II: I have invested in VCASMO to be able sync the slides with the video.  I have updated the presentations below, and look forward to using this technology again for the January NYC R meetup. I would be great if we could get one of those nifty video and slide syncs for the meetup talks—but until then—tThe slides and video are available after the jump, and I highly recommend viewing them in full-screen mode to get the most out of the slides—enjoy!  Download slides from my talk  Download slides from Harlan’s talk 	 0 Comments
Joining data frames in R	https://www.r-bloggers.com/2009/12/joining-data-frames-in-r/	December 17, 2009	Chris	Want to join two R data frames on a common key? Here’s one way do a SQL database style join operation in R. We start with a data frame describing probes on a microarray. The key is the probe_id and the rest of the information describes the location on the genome targeted by that probe. We also have a bunch of measurements in a numeric vector. For each probe (well, a few probes missing due to bad data) we have a value. Let’s join up these tables, er data frame and vector. We’ll use the match function. Match returns a vector of positions of the (first) matches of its first argument in its second (or NA if there is no match). So, we’re matching our values into our probes. Merge is probably more similar to a database join. If we have two data frames, we can use merge. Let’s convert our vector tp to a data frame and merge, getting the same result (in a different sort order). There’s a good discussion of merge on Stack Overflow, which includes right, left, inner and outer joins. Also the R wiki covers both match and merge. See also, the prior entry on select operations on R data frames. 	 0 Comments
Image Compression with the SVD in R	https://www.r-bloggers.com/2009/12/image-compression-with-the-svd-in-r/	December 17, 2009	John Myles White	Over the next few posts, I’m going to be reviewing the use of R to implement the most commonly used matrix techniques for image manipulation. The code will be surprisingly simple to understand, because the real magic behind these techniques lies in the mathematics that R provides an abstract interface to. To start, I’m going to review the singular value decomposition of a matrix, also known as the SVD. If you’d like to understand the magic behind the scenes here, you can read about the mathematical definition of the SVD on Wikipedia or you can watch Gilbert Strang’s lectures. If your command of linear algebra isn’t great, I suspect that the mathematical details of the SVD will be totally opaque to you. Fortunately, the details of the underlying mathematics aren’t at all necessary for appreciating the SVD’s usefulness for manipulating images: it is arguably the best way to compress matrices into smaller representations that contain the essential information from the original matrices. The easiest way to understand this is to see it in action, so I’m going to show how the SVD allows for any degree of compression of an image represented as a real-valued matrix. Obviously, the first thing we have to do is to represent our example image as a matrix with real-valued entries. To do this, we’ll need to use a simple technique for loading an image into R that I found on the Quantitative Ecology blog. The first thing to do is use ImageMagick to transform a TIFF file into a PPM file: Then we can use the “pixmap” package from CRAN to read the PPM image into R as an image object containing three matrices: one for the red components, one for the green components, and one for the blue components. The exact code I used was this: Before we go any further, we should look at the image we’ll be playing with. Rather than work with the composite of the three color matrices, I found it easier to work with only a single color. For this specific image, I found that the green matrix looked best, so I’ll use only that. At the risk of seeming self-indulgent, I’m using my own Gravatar icon as an example image because faces are particularly easy to recognize under the strange color scheme I’ll be using, which I chose because it really simplifies the visualization code. To visualize the matrix as an image, I discovered two very helpful graphical functions, image and heat.colors. image depicts the value of a matrix using a color scheme you specify; heat.colors produces a set of colors that could be used in a heat map with a granularity you specify as the first argument. Using these functions, we can see our raw input matrix: The images generated by image are at an unfortunate angle for viewing, so I edited them post hoc with a single 90º clockwise rotation in Preview to give this sort of image: If you want to play with the original PPM image file, you can get it here. Clearly, we’ve got a recognizable face here. Feeling good about that, we can start to play with the SVD of this matrix. The R function to generate the SVD of a matrix is simply svd: svd returns a list with three entries: d, u and v. d is a vector of the singular values ordered by decreasing size; u and v are matrices that are combined with a diagonal matrix form of d to give the original input matrix. To simplify things, I’ll set up variables to contain the value of these elements of the SVD output list: We can check that the SVD works by multiplying the relevant matrices: If this is the SVD by itself, it’s not clear that it’s useful computationally: we’ve just rewritten one matrix as the product of three matrices and accumulated a small bit of error along the way. The real utility of the SVD lies in the singular values: they represent, in decreasing order, the most important information about the original matrix. To see this, you can shrink the input matrices and produce a compressed form of the matrix. For instance, my original matrix is a 212 by 201 image. Using the SVD, we can represent it as the product of a 212 by 2 matrix, a 2 by 2 matrix, and a 2 by 201 matrix. This massively reduces the amount of information we’re storing, but you can already see the outline of our input image in the results: To get a feel for how this works, we can iterate over the number of singular values we include in our compressed form: This produces the following images: As you can see, the images get better quite quickly. Indeed, this last image looks great, despite the large reduction in the number of entries we’re keeping from the original matrix. I’d say that’s a pretty compelling case for the value of the SVD. It would be easy to combine the compressed matrices for red, green and blue to get a compressed version of our input image using its original color scheme. I’ll leave that as an exercise for anyone interested in playing with this technique on their own. Of course, the SVD has tons of other uses, but this simple hack for image compression struck me as pretty interesting, as well as being remarkably simple to implement in R. 	 0 Comments
R Blogs	https://www.r-bloggers.com/2009/12/r-blogs/	December 17, 2009	Ralph	There are many blogs on Statistics, R and other related topics scattered around the internet. The R bloggers website provides a central hub where feeds from participating blogs are collated so that they can be viewed from a single website. This resources certainly appears to be a good idea so that people can more easily identify blogs with information about the use of the R statistical software system. 	 0 Comments
Why use plyr?	https://www.r-bloggers.com/2009/12/why-use-plyr/	December 17, 2009	David Smith	The “apply” family of functions in R (apply, sapply, lapply) is a very powerful suite of tools for iterating through structures of data and returning the combined results of each iteration. But with great power comes great responsibility (or something like that): these functions can sometimes be frustratingly difficult to get working exactly as you intended, especially for newcomers to R. That’s where the functions from Hadley Wickham’s plyr package come in: they create a unified interface for iterating over data in R with a consistent syntax for specifying what the inputs are, and what the outputs should be.  JD Long captures both the frustration that come with mis-applying apply and the joy of overcoming it with plyr in this 5-minute video. Be sure to check the associated blog post for the background and comments.   	 0 Comments
Data Profiling in R	https://www.r-bloggers.com/2009/12/data-profiling-in-r/	December 17, 2009	learnr	In 2006 UserR conference Jim Porzak gave a presentation on data profiling with R. He showed how to draw summary panels of the data using a combination of grid and base graphics.  Unfortunately the code has not (yet) been released as a package, so when I recently needed to quickly review several datasets at the beginning of an analysis project I started to look for alternatives. A quick search revealed two options that offer similar functionality: r2lUniv package and describe() function in Hmisc package.  r2lUniv package performs quick analysis either on a single variable or on a dataframe by computing several statistics (frequency, centrality, dispersion, graph) for each variable and outputs the results in a LaTeX format. The output varies depending on the variable type. One can specify the text to be inserted in front of each section. The function rtluMainFile generates a LaTeX main document design and allows to further customise the report. The resulting tex-file can then be converted into pdf. A sample output for the mpg-variable:  The final pdf-output can be seen here: r2lUniv_report.pdf. The describe function in Hmisc package determines whether the variable is character, factor, category, binary, discrete numeric, and continuous numeric, and prints a concise statistical summary according to each. The latex report also includes a spike histogram displaying the frequency counts. The easiest and fastest way is to print the results to the console. Alternatively, one can convert the describe object into a LaTeX file. cat is used to generate the tex-report. A sample output for the mpg-variable:  The final pdf-report can be seen here: Hmisc_describe_report.pdf. Both of the functions provide similar snapshots of the data, however I prefer the describe function for its more concise output, and also for the option to print the analysis to the console. Whilst I like the summary plots generated by r2lUniv I find them hard to read in the pdf-report because of the small font-size of the labels. 	 0 Comments
Matrix Algebra in R: Resources, Videos, Textbooks	https://www.r-bloggers.com/2009/12/matrix-algebra-in-r-resources-videos-textbooks/	December 17, 2009	Jeromy Anglim		 0 Comments
Quick Review of Matrix Algebra in R	https://www.r-bloggers.com/2009/12/quick-review-of-matrix-algebra-in-r/	December 16, 2009	John Myles White	Lately, I’ve been running a series of fMRI experiments on visual perception. In the interests of understanding the underlying properties of the images I’m using as stimuli, I’ve been trying to learn more about the matrix transformations commonly used for image compression and image manipulation. Thankfully, R provides simple-to-use implementations for all of the matrix operations I wanted to play around with, so it’s been quite easy to get started. For the next few posts, I thought that I’d review the standard matrix techniques for image compression and editing, giving full examples of their implementation in R and demonstrations of their real-world value. Before I start, I should make sure that you’re familiar with basic matrix operations in R. I imagine almost every R user knows a little bit about matrix algebra and probably knows the basics of using R to perform matrix algebra, but here’s a quick review to make sure I don’t leave anyone in the dark: You can build a matrix in R using the matrix function: You can test whether an item you’ve been given is a matrix using is.matrix and you can convert appropriate objects to matrices using as.matrix: The matrix I’ve been building in the examples above is a diagonal matrix, so you could also construct it using diag: In general, you can generate the n by n identity matrix as: I’ll be exploiting a more interesting use of diag in my next post, so let’s see how you can build matrices other than the identity with diag by specifying a vector of entries along the diagonal: This form of diag turns out to be extremely useful, as you’ll see once I cover the SVD’s syntax in R. The three core operations that can be performed on matrices are addition, scalar multiplication and matrix multiplication. These are easy to work with in R: Be careful with the * operator: it does not perform matrix multiplication, but rather an entry-wise multiplication: The next few matrix operations are a little more complex: transposition and inversion. Transposition is the easier of the two. To get the transpose of a matrix, you simply call the t function: In contrast, inversion is a little more complex, partly because the function you’d want to use has a non-obvious name: solve. The reason that solve is called solve is that it’s a general purpose function you can use to solve matrix equations without wasting time computing the full inverse, which is often inefficient. If you want to know more about the computational efficiency issues, you should look into the ideas behind the even faster variant, qr.solve. Now, you probably know this already, but the definition of a matrix’s inverse is that the product of the matrix and its inverse is the identity matrix, if the inverse exists. I always find this a good way to make sure that I’m correctly computing the inverse of a matrix: The car package defines an inv function, which is simply a new name for solve: I think this is pretty clever, though I’m loathe to import a package just for this one snippet. More interestingly, the MASS package defines a ginv function, which gives the matrix pseudoinverse, a generalization of matrix inversion that works for all matrices: In practice, you can usually get around using the pseudoinverse, but it’s nice to know that it’s at hand all the time.  Eigenvectors are surely the bane of every starting student of linear algebra, though their considerable power to simplify problems makes them the darling of every applied mathematician. Thankfully, R makes it easy to get these for every matrix: Last, but not least, you can get metadata about the shape of a matrix using dim, nrow and ncol: Hopefully those operations were already familiar to any readers out there, as I doubt that they’ll be clear from this short explanation without prior knowledge. I just felt compelled to review them before using any of them in my next set of posts. Tomorrow, I’ll start going through more interesting matrix algorithms in R, beginning with the SVD. 	 0 Comments
According to Microsoft, the fourth paradigm of science is data	https://www.r-bloggers.com/2009/12/according-to-microsoft-the-fourth-paradigm-of-science-is-data/	December 16, 2009	David Smith	In scientific discovery, the first three paradigms were experimental, theoretical and (more recently) computational science. A new book of essays published by Microsoft (and available for free download — kudos, MS!) argues that a fourth paradigm of scientific discovery is at hand: the analysis of massive data sets. The book is dedicated to the late Microsoft researcher Dr Jim Gray, who pioneered the idea with the catchphrase: “It’s the data, stupid”. The basic idea is that our capacity for collecting scientific data has far outstripped our present capacity to analyze it, and so our focus should be on developing technologies that will make sense of this “Deluge of Data” (as this New York Times review of the book — well worth a read — calls it).  Dr Gray’s call-to-arms was not to develop isolated super-powerful super-computers but “to have a world in which all of the science literature is online, all of the science data is online, and they interoperate with each other.” This dream is already close to a reality in some scientific domains like astronomy, where advanced instruments routinely generate petabytes of data available for public analysis. And with further developments in distributed and high-performance computing, with freely-available high-scale data management tools like Hadoop, and with advanced open-source data-analysis tools like R rapidly adapting to the scales of these data sets, the fourth paradigm is certain to become a mainstream reality in other scientific domains as well.  Microsoft Research: The Fourth Paradigm: Data-Intensive Scientific Discovery   	 0 Comments
NCEP Global Forecast System	https://www.r-bloggers.com/2009/12/ncep-global-forecast-system/	December 16, 2009	joe	"Just about everyone is familiar with weather maps. There are many situations where it is useful to combine the underlying numerical weather data with other types of information. Accessing  the weather data is a necessary first step. The output from the U.S. National Centers for Environmental Prediction (NCEP) Global  Forecast System (GFS) is freely available. The surface resolution of the model is ≈ 0.3º× 0.3°. The model runs every 6 hours, producing forecasts at 3-hourly intervals extending out to 16 days. As an example of output from GFS, the map (below) shows the predicted average  temperature at 2 metres over the entire globe for the next 24 hr (date of this post). The map shows predicted cold conditions in Europe, and the continuing heatwave in Australia.  GFS forecasts are in a format called GRIB2. According to Wikipedia, “GRIB (GRIdded Binary) is a mathematically concise data format commonly used in meteorology to store historical and forecast weather data.” GRIB files contain physical fields such as temperature, humidity etc defined on a spatial grid, as well as boundary conditions such as vegetation type and elevation. The data might be assimilated from observations, or output from a forecast model. The first step is to translate the GRIB into a raster format such as netcdf which can be read in R. For example, the GRIB2 file gfs.2009121700/gfs.t00z.sfluxgrbf03.grib2 contains the 3-hr forecast surface data on 17 Dec 2009  produced at 00 UTC (midnight universal time). An inventory of the data contained in this file can be seen here. Download this forecast as temp.grb loc=file.path(""ftp://ftp.ncep.noaa.gov/pub/data/nccf/com/gfs/prod/gfs.2009121700/gfs.t00z.sfluxgrbf03.grib2"")

download.file(loc,""temp.grb"",mode=""wb"") To read temp.grb a utility called wgrib2 needs to be installed on your system. Then data such as land fraction can extracted into a netcdf file LAND.nc using the R shell command shell(""wgrib2 -s temp03.grb | grep :LAND: | wgrib2 -i temp00.grb -netcdf LAND.nc"",intern=T) The ncdf package can now be used to read the contents of LAND.nc. library(ncdf)

landFrac <-open.ncdf(""LAND.nc"")

land <- get.var.ncdf(landFrac,""LAND_surface"")

x <- get.var.ncdf(landFrac,""longitude"")

y <- get.var.ncdf(landFrac,""latitude"") The 1152×576 matrix land takes values 1 for land and 0 for water (sea-ice is 1). x and y are the longitude and latitude of the non-uniform GFS grid. 2m temperature data can be read in the same way. The average of the first 8  forecasts was called t2m.mean and plotted using image.plot() from the fields package: library(fields)

rgb.palette <- colorRampPalette(c(""snow1"",""snow2"",""snow3"",""seagreen"",""orange"",""firebrick""), space = ""rgb"")#colors

image.plot(x,y,t2m.mean,col=rgb.palette(200),axes=F,main=as.expression(paste(""GFS 24hr Average 2M Temperature"",day,""00 UTC"",sep="""")),axes=F,legend.lab=""o C"")

contour(x,y,land,add=TRUE,lwd=1,levels=0.99,drawlabels=FALSE,col=""grey30"") #add land outline  "	 0 Comments
email yourself when a script or a job is done	https://www.r-bloggers.com/2009/12/email-yourself-when-a-script-or-a-job-is-done/	December 15, 2009	Vinh Nguyen	i usually run long simulations in R on a remote server.  i’ve read an article on linux journal about emailing yourself after something happens…i forgot.  since my jobs are long, and i usually have to run multiple jobs sequentially, it’d be nice to look at my results immediately when they’re gone.  good thing *nix based systems have the mail (or mailx) command: found the stuff here. in R, i execute: of course, these are empty emails.  u can use “cool! 	 0 Comments
R Site Search with the ‘sos’ Package	https://www.r-bloggers.com/2009/12/r-site-search-with-the-sos-package/	December 15, 2009	Matt Bascom	"> help.search(‘petal.length’)No help files found with alias or concept or title matching‘petal.length’ using regular expression matching. > library(sos)> (PL  
                          Now suppose you conduct more than one search for variations on a phrase.  Since the search result is an exact match to the search phrase, it may be useful to conduct more than one search and combine the outcomes.  You can manipulate the search results like you can manipulate any other data frame.  Suppose you are working on a project that requires analyzing financial derivatives.  You can concatenate the results with a pipe | operator to create the union of the two data frames: > fd > fo > fdo  The sos package is available from your favorite CRAN mirror now.  The code above is described in depth in a vignette: vignette(“sos”) Spencer Graves will discuss search capabilities in R, as well as good practices in package development and remote collaboration, on September 15, 2010 for the San Francisco Bay Chapter of the Association for Computing Machinery.  Be sure to pencil in the date.  Time and Location are to be determined.  "	 0 Comments
Browse R Graphics with the R Graph Gallery and the R Graphical Manual	https://www.r-bloggers.com/2009/12/browse-r-graphics-with-the-r-graph-gallery-and-the-r-graphical-manual/	December 15, 2009	Stephen Turner		 0 Comments
R Tutorial Series: Graphic Analysis of Regression Assumptions	https://www.r-bloggers.com/2009/12/r-tutorial-series-graphic-analysis-of-regression-assumptions/	December 15, 2009	John M. Quick	An important aspect of regression involves assessing the tenability of the assumptions upon which its analyses are based. This tutorial will explore how R can help one scrutinize the regression assumptions of a model via its residuals plot, normality histogram, and PP plot. Before we begin, you may want to download the sample data (.csv) used in this tutorial. Be sure to right-click and save the file to your R working directory. This dataset contains information used to estimate undergraduate enrollment at the University of New Mexico (Office of Institutional Research, 1990). Note that all code samples in this tutorial assume that this data has already been read into an R variable and has been attached. Before testing the tenability of regression assumptions, we need to have a model. In the segment on simple linear regression, we created a single predictor model to estimate the fall undergraduate enrollment at the University of New Mexico. The complete code used to derive this model is provided in its respective tutorial. This article assumes that you are familiar with this models and how it was created. Therefore, a shorthand method for generating the model is displayed below. A residuals plot can be used to assess the assumption that the variables have a linear relationship. The plot is formed by graphing the standardized residuals on the y-axis and the standardized predicted values on the x-axis. An optional horizontal line can be added to aid in interpreting the output. The unstandardized predicted values can be generated using the predict(MODEL) function and the unstandardized residuals can be obtained via the resid(MODEL) function. In both cases, MODEL refers to the variable containing the regression model. Respectively, these values can be standardized by subtracting the mean and dividing by the standard deviation. The standardized data can be plotted using the plot() function (see Scatterplots). Lastly, abline(0,0) can be used to add a horizontal line to the plot. The code necessary to create a standardized residuals plot is presented below. Note that abline(0,0) must be executed after the plot is generated and while the Quartz window is open. The plot resulting from the preceding code is pictured below.  In general, values that are close to the horizontal line are predicted well. The points above the line are underpredicted and the ones below the line are overpredicted. The linearity assumption is supported to the extent that the amount of points scattered above and below the line is equal.  The residuals plot can also be used to test the homogeneity of variance (homoscedasticity ) assumption. Look at the vertical scatter at a given point along the x-axis. Now look at the vertical scatter across all points along the x-axis. The homogeneity of variance assumption is supported to the extent that the vertical scatter is the same across all x values.  A histogram can be used to assess the assumption that the residuals are normally distributed. In R, the hist(VAR, FREQ) function will produce the necessary graph, where VAR is the variable to be charted and FREQ is a boolean value indicating how frequencies are to be represented (true for counts, false for probabilities). Then, in similar fashion to abline(), a normal curve can be added to the histogram via the curve(EXPR, ADD) function, where EXPR is the type of curve to plot (here, “dnorm”) and ADD is a boolean value indicating whether or not to add the curve to the existing window.  The following code demonstrates how to create a residuals histogram for our model. Note that curve() must be executed after the plot is generated and while the Quartz window is open. The plot resulting from the preceding code is pictured below.  To the extent that the histogram matches the normal distribution, the residuals are normally distributed. This gives us an indication of how well our sample can predict a normal distribution in the population.  A PP Plot can also be used to assess the assumption that the residuals are normally distributed. To create a PP Plot in R, we must first get the probability distribution using the pnorm(VAR) function, where VAR is the variable containing the residuals. Then we can use the plot(VAR, SORT) function to create the graph, where VAR is the variable containing the residuals and SORT makes use of our calculated probability distribution. Note that the ppoints() and length() functions are incorporated into the VAR parameter in this case. Lastly, the abline(0,1) function is used to draw a diagonal line across the plot for comparison purposes. Recall that abline(0,1) must be executed after the plot is generated and while the Quartz window is open. The plot resulting from the preceding code is pictured below.  Here, the distribution is considered to be normal to the extent that the plotted points match the diagonal line.  To see a complete example of how the regression assumptions of linearity, homoscedasticity, and normality can be analyzed visually in R, please download the regression assumptions example (.txt) file. Office of Institutional Research (1990). Enrollment Forecast [Data File]. Retrieved November 22, 2009 from http://lib.stat.cmu.edu/DASL/Datafiles/enrolldat.html Svetina, D., & Levy, R. (2009). Regression Assumptions [Text File]. Retrieved December 7, 2009 from EDP 552: Multiple Regression and Correlation Methods [Protected Website]. 	 0 Comments
Object-Oriented Programming in R: The Setter Methods	https://www.r-bloggers.com/2009/12/object-oriented-programming-in-r-the-setter-methods/	December 14, 2009	John Myles White	With a little guidance from the indefatigable Hadley Wickham, I figured out today how to implement the setter methods that were missing from my example user class. To review, let’s rebuild the getter methods for my user object: With these working, my goal was to write setter methods that would work like the ideal code below: Defining this sort of setter method turned out to require a little effort. I wasted quite a lot of time walking down dead ends, but, thankfully, Hadley gave me the exact piece of information I was missing early this morning. The dead ends were interesting in themselves, though, so I’ll review them in another post tomorrow. For now I’ll just explain the correct implementation for my desired setter methods, which you can see below: There are three important ideas here: With this in mind, a final user class looks like this: 	 0 Comments
NYT on breast cancer screening and probability	https://www.r-bloggers.com/2009/12/nyt-on-breast-cancer-screening-and-probability/	December 14, 2009	David Smith	The New York Times last weekend looked at the controversy around the recent changes to the mammogram guidelines from a mathematical perspective. Compared to the analysis based on Bayes’ Theorem from the Harvard Social Science Statistics blog (which apparently caused some controversy itself: that post was deleted and later replaced after some errors apparently crept into the calculations), this article argues from a simple scenario with made-up (but plausible) numbers: Assume there is a screening test for a certain cancer that is 95 percent accurate; that is, if someone has the cancer, the test will be positive 95 percent of the time. Let’s also assume that if someone doesn’t have the cancer, the test will be positive just 1 percent of the time. Assume further that 0.5 percent — one out of 200 people — actually have this type of cancer. Now imagine that you’ve taken the test and that your doctor somberly intones that you’ve tested positive. Does this mean you’re likely to have the cancer? Surprisingly, the answer is no. To see why, let’s suppose 100,000 screenings for this cancer are conducted. Of these, how many are positive? On average, 500 of these 100,000 people (0.5 percent of 100,000) will have cancer, and so, since 95 percent of these 500 people will test positive, we will have, on average, 475 positive tests (.95 x 500). Of the 99,500 people without cancer, 1 percent will test positive for a total of 995 false-positive tests (.01 x 99,500 = 995). Thus of the total of 1,470 positive tests (995 + 475 = 1,470), most of them (995) will be false positives, and so the probability of having this cancer given that you tested positive for it is only 475/1,470, or about 32 percent! This is to be contrasted with the probability that you will test positive given that you have the cancer, which by assumption is 95 percent.  It’s a nice example of how our intuition about probabilities can often be out of step with reality. New York Times: Mammogram Math 	 0 Comments
The Grammar of Graphics: ggplot2 package	https://www.r-bloggers.com/2009/12/the-grammar-of-graphics-ggplot2-package/	December 14, 2009	Ralph	The grammar of graphics approach to constructing graphs has been implemented in the ggplot2 package in R. The author of the package, Hadley Wickham, has provided a website with many details of using the system to create nice looking graphics. The package removes many of the awkward parts of setting up graphical display that characterise other approaches in R. 	 0 Comments
R 2.10.1 released	https://www.r-bloggers.com/2009/12/r-2-10-1-released/	December 14, 2009	David Smith	The latest update to R, R 2.10.1, is now available for download in source form from your local CRAN mirror. Binary versions (for Mac, Windows, and Linux) will become available over the next few days. As a maintenance release, this update focuses on minor changes and bug fixes. The complete list of changes is available in the NEWS file, but some of the more significant changes include:  R-announce mailing list: R 2.10.1 is released 	 0 Comments
Example 7.18: Displaying missing value categories in a table	https://www.r-bloggers.com/2009/12/example-7-18-displaying-missing-value-categories-in-a-table/	December 14, 2009	Ken Kleinman		 0 Comments
RQuantLib 0.3.1 released	https://www.r-bloggers.com/2009/12/rquantlib-0-3-1-released/	December 13, 2009	Thinking inside the box	"
Full changelog details, examples and more details about this package are at
my RQuantLib page.

 "	 0 Comments
Galton’s quincunx in R	https://www.r-bloggers.com/2009/12/galtons-quincunx-in-r/	December 13, 2009	Gregor Gorjanc		 0 Comments
CRU graph yet again (with R)	https://www.r-bloggers.com/2009/12/cru-graph-yet-again-with-r/	December 13, 2009	John Mount	"IowaHawk has a excellent article attempting to reproduce the infamous CRU climate graph using OpenOffice: Fables of the Reconstruction.   We thought we would show how to produced similarly bad results using R.
 If the re-constructed technique is close to what was originally done then so many bad moves were taken that you can’t learn much of anything from the original “result.”   This points out some of the pratfalls of not performing hold-out tests, not examining the modeling diagnostics and not remembering that linear regression models fail to low-variance models (i.e. when they fail they do a good job predicting the mean and vastly under-estimate variance). Our article not an article on global warming, but an article on analysis technique.  Human driven global warming is either happening or not happening independent of any bad analysis.  Finding the physical truth is a bigger harder job than eliminating some bad reports (the opposite of a bad report is not necessarily the truth). Bad analyses can have many different sources (mistakes, trying to jump ahead of your colleagues on something you believe is true, trying to fake something you believe is false or be figments of overly harsh critics) and we have not heard enough to make any accusations. First: load the data (I re-formatted it at bit so R can read it: jonesmannrogfig2c.txt,  data1400.dat_.txt   ) , perform the principle components reduction and fit a first
model. We used only 5 principle components as modeling variables, because as is typical of principle component analysis- beyond the first few components the components become vanishingly small and unsuitable to use in modeling (see graph pcomp below).  However, this gave a model with far smaller R-squared than people are reporting, so lets add in a lot of components like everybody else does (bad!). This is a degenerate model that essentially didn’t fit (thought the significance on PC10 component fools the fitter, but PC10 can’t be usable- it is essentially noise).  Graphically we can see the fit is not very useful (despite having  a little bit of R-squared) by looking at the graph of the fit plotted in the region of fitting.  Notice how the fit variance is much smaller than the true data variance even in the region of training data, this is typical of bad regression fits.  Now the statement they wanted to make is that the present looks nothing like the past.  The past is only available through the fit model so what you would hope is that the model looks like the present and then the model itself separates the past and present.  Instead as you see in the graphs above and below this fails two ways: the model looks nothing like the present and the model’s past looks a lot like the model’s present.  What we could do to falsely drive the conclusion (which itself may or may not be true, it just is not supported by this technique, model or data) is create the infamous graph where we switch from modeled data in the past to actual data in the present and then act surprised that the two did not line up (which they did at no step during the fitting).  I don’t have the heart to unify the colors or remove the legend, but here is the graph below:  The reason the blue points look different than the others is they came from the average temperature data instead of the model (where everything else came from).  Switching the series is essentially assuming the conclusion that recent past looks very different than the far past. Essentially this methodology was so poor it could not have illustrated or contradicted recent global warming.  There are plenty of warning signs that the model fitting are problematic and the conclusion illustrated in the last graph can not actually be proved or disproved from this data (the proxy variables are too weak to be useful, that is not to say there are not other better proxy variables or modeling techniques).  The problems of the presentation are, of course, not essential problems in detecting global warming (which likely is occurring and likely will be a drain on future quality of life) but problems found in a single bad analysis. Related posts: "	 0 Comments
The Most Basic Elements of Object-Oriented Programming in R	https://www.r-bloggers.com/2009/12/the-most-basic-elements-of-object-oriented-programming-in-r/	December 13, 2009	John Myles White	Until recently, I’ve never had any reason to learn how to define my own classes in R. Having learned this week, I was surprised to find out how easy it is to start implementing classes in R. If you know nothing about creating classes and class methods in R, here’s a very quick overview of the three core ideas behind R’s object system. If you already know the basics of defining classes in R, I’d suggest skipping this post, as it’s not likely to be of much value to you. The first thing you should know is that the S3 object-oriented system feels like a hack that was put on top of the original S language. If you’re familiar with Perl, you’ll feel right at home with the basic ideas behind this approach; you’ll probably also appreciate the conceptual simplicity of the results. With that in mind, you should be aware that any normal piece of R code is filled with objects, because the base packages and most well-designed user packages use lots of custom classes. You can see this for yourself by starting to use class to perform introspection on some of the items in your programs: More interesting than looking at the classes of existing objects is defining new classes of your own. Thankfully, class, like names and many other functions in R, can be assigned to directly, like so: If you’re familiar with Perl, you can think of this assignment to class as analogous to using bless. If you’re not a Perl user, hopefully this approach to defining classes still makes sense to you, because it’s so simple: the only thing that decides the class of an object is the value you’ve set for the class attribute. To convince yourself that the class of an object is just an attribute, you can use attr: This simple metadata driven approach to building objects is the core of the S3 object system, as far as I can tell. Of course, setting class by itself is pretty useless: you want to be able to define class methods. That’s where the other two main ideas come in. The first trick is to use generic methods to get polymorphism out of the language; the second trick is to define methods on your classes using a simple naming convention. To see how this works, let’s use an example that should be familiar to anyone who’s ever built a database-backed website. Instead of my would-be octonion class, we’ll consider a user object. To define an object of class ‘user’, we can do the following: The first thing that comes to mind after building this object is that we should define getter and setter methods for accessing and modifying the contents of this user object. For example, we want a way to get the id attribute for our object. We’d also like our approach to generalize to other objects with an id attribute. Ideally, we should be able to write something like this: Now, it is obviously possible to define an id function that operates only on user objects, but that’s not the right approach if we also want to have another sort of object that would have an id method as well. This polymorphism concern is the problem that generic methods and specialized naming conventions solve. First, we are going to define a specialized id method that only operates on user objects. Because it will only work on ‘user’ objects, we’ll call it id.user: Given this, we can then define a generic method that will operate on objects of many classes and reroute our general function calls to the correct class-level method: Here UseMethod just searches for an id.user function, finds it and then calls it with user as its argument. If we had a ‘profile’ object, id would search for an id.profile function and call that. You can see this by trying id on a variable whose class we set to be profile using class. With these ideas in mind, it’s easy to do something similar for the rest of our attributes as well: Defining setter methods is a little more tricky. We could use a trick like the eval hacks I used to implement push and pop recently, but Hadley Wickham wisely chastened me for doing that. I’m still trying to decide what’s the best approach. As I see it, there are least three possible method call styles you might define: I would really like to start implementing this last sort, but I haven’t figured out how to yet. The others can be implemented using the generic methods approach I just outlined above, along with some ugly calls to eval for the second call style. To implement the first, simply do something like this: If you have suggestions on how to implement the third approach, please let me know. Also, I am not at all happy with the use of NULL in my current setter implementation, because that makes it impossible to set the value of an attribute to NULL. If people have suggestions, I’d very much appreciate them. Does R implement arity-specific function definitions? 	 0 Comments
Missing data imputation	https://www.r-bloggers.com/2009/12/missing-data-imputation/	December 12, 2009	Quantitative Finance Collector		 0 Comments
A central hub for R bloggers	https://www.r-bloggers.com/2009/12/a-central-hub-for-r-bloggers/	December 12, 2009	Paolo Sonego		 0 Comments
Summarising data using bar charts	https://www.r-bloggers.com/2009/12/summarising-data-using-bar-charts/	December 12, 2009	Ralph	A bar graph is a frequently used type of display that compares counts, frequencies, totals or other summary measures for a series of categories, e.g. sales in different market sectors or in quarters in a financial year. The bar graph can be laid out with the categories either on the vertical or horizontal axis of the display – depending on whether we consider making a vertical or horizontal comparison is easier for interpreting the graph. In R there are multiple ways for creating graphs, including the base graphics, lattice graphics and the ggplot2 grammar of graphics approach. To illustrate how we can create a bar chart using these packages we will make use of some data taken from the FAO statistics website for the UK in 2007. The data is for production (in metric tonnes) of the top five, in terms of production, food and agricultural commodities. The first step before creating the graphs is to prepare the data in a format that can be used by the graphing functions. As this dataset is small we can manually create the data object. To make the labels on the graph less cluttered the production is recorded as 1,000s of metric tonnes. The R code to create the data object is shown here: The levels argument is explicity defined to make sure that the ordering is as required from largest to smallest production rather than being alphabetical which would be how the categories are ordered otherwise. Base Graphics The base graphics in R provide a function barplot that we can use to create a bar chart. The first argument to the function is the name of the object with the data. The names argument is used to provide the labels for the categories in the graph. We also specify the text for the labels for the x-axis, y-axis and title of the graph with the xlab, ylab and main arguments respectively. The function call is: to produce the following graph: Base Graphics Bar Chart This graph is visually appealing with sensible space between the bars for the five commodity categories. Lattice Graphics In the lattice graphics package the barchart function is used to create bar charts. The x and y variables are specified using a formula, which is the standard way when using Trellis graphics. The variable on the vertical axis is specified on the left hand side of the formula and the variable for the horizontal axis is on the right hand side, where they are separated by the tilda character. This code produces the following graph: Lattice Graphics Bar Chart The main visual difference compared to the base graphics example is the default colours for the bars which is much brighter than the base graphics example. There is also a large gap between the bars in the display. ggplot2 The create the bar chart in the ggplot2 package we use the ggplot function to specify the data to appear in the graph then gradually add in the other components of the graph.  We specify the data frame where the data is stored and then use the aes argument to identify the x and y variables. The geom_bar function is used to create a bar chart display with the specified data and the last three options in the example are for creating the various labels to be added to the graph. The graph itself is constructed piece by piece to add the various layers and components on top of the base layer: This code produces the following graph: gg plot2 Bar Chart The layout of this graph differs mainly with the grid background layout, which by default is a gray with white lines. 	 0 Comments
Already a competitor?!	https://www.r-bloggers.com/2009/12/already-a-competitor/	December 11, 2009	xi'an	When looking around on Amazon, I found that “Introducing Monte Carlo Methods with R” was associated with another very recently published (same day as ours!) book, “Understanding Computational Bayesian Statistics“, by William Bolstad, that seems to mostly cover the same ground as us (with some connections with Bayesian Core for prior modelling in regression and logistic models). Although R seems to be less proeminently advocated than in our Use R! volume, I am quite curious to see what exactly is in this book and how much of a competitor it is! (Given that it is the same length as ours (about 315 pages), I am however a bit surprised at the high $110’s asked for this book.) 	 0 Comments
new R package : bibtex	https://www.r-bloggers.com/2009/12/new-r-package-bibtex/	December 11, 2009	romain francois	I’ve pushed to CRAN the package bibtex package The package defines the read.bib function that reads a file in the bibtex format. The code is based on bibparse The read.bib function generates an object of class citationList, just like utils::citation 	 0 Comments
Struggling with apply() in R	https://www.r-bloggers.com/2009/12/struggling-with-apply-in-r/	December 11, 2009	JD Long	It’s common knowledge that I struggle wrapping my head around the apply functions in R. That is illustrated very clearly in the following discussion on Stack Overflow:  Dirk’s comment is actually spot on. I’ve asked the same damn question at least 4-5 times. Only I didn’t really understand it was the same question. That’s one of the problems of not really being good at something; it’s hard to think abstractly about it. I’m not really good at R, so sometimes I don’t realize that multiple concepts are related. As I talk with other new users of R it’s clear that unless they come from a programming language with an apply-esque construct they likely are struggling with R. I think most of the confusion comes from a) not understanding what data format apply() is going to return and b) not understanding anonymous functions. With this in mind I did a little screencast illustrating how this struggle plays out for a new users. I also show why I use the plyr package for much of the stuff other folks use apply() for. Any feedback you have is appreciated. This is my first stab at a screencast, so I am still trying to figure out the best approach/method as well as how many drinks puts me on the Ballmer Peak.  EDIT: it’s been pointed out that I misuse some terminology a number of times. I should have named my year vector “yearVector.” By calling it “yearList” I then refer to the vector as a list. I was using “list” in the vernacular, but since list is a specific R data structure it is confusing that I named a vector a name with “list” in it. 	 0 Comments
Must-Have R Packages for Social Scientists	https://www.r-bloggers.com/2009/12/must-have-r-packages-for-social-scientists/	December 11, 2009	Drew Conway	"After recently having to think critically about the value of various R packages for social science research, I realized that others might find value in a post on “must-have” R packages for social scientists.  After the immensely popular post on this topic for Python packages a follow-up seemed appropraite.  If you conduct social science research but are desperately clinging onto your SAS, SPSS or Matlab licenses; waiting for someone to convince you of R’s value, please allow me to be the first to try. R is a functional programming language that allows for seamless data exploration, manipulation, analysis and visualization.  The community using and supporting the language has exploded over the last several years, which has lead to the development of several immensely useful packages, many of which have direct  application in the social sciences.  Below are the R packages I use on a weekly/daily/monthly basis (in no particular order) and highly recommend to any R users; new or old. 
 Put simply, Zelig is a one-stop statistical shop for nearly all regression model specifications. Using a uniform syntax across model types, and several extremely useful plotting functions, the package’s autor Gary King (Political Science and Statistics at Harvard University) calls Zelig “everyone’s statistical software,” which is a very accurate description.  if there is one R package that every social scientist should have it is Zelig! Download Zelig One of the advantages of R as a functional language is it contains a set of convenient base functions for plotting data.  While useful when exploring a dataset, they are–for lack of a better word–ugly, and this is where ggplot2 comes in.  Using the Grammar of Graphics manifesto as a guide, creator Hadley Wickham designed ggplot2 to “take the good parts of base and lattice graphics and none of the bad parts,” and he succeed.  This is the premier R package for conveying your analysis visually. Download ggplot2 I have combined the two competing network analysis packages in R into a single bullet because each has its strengths and weaknesses, and as such there is value in leaning and using both.  The igraph package approaches network analysis from the mathematics/physics/graph theoretic perspective, including several advanced metrics and random graph models.  In contrast, Statnet was primarily designed for social science, and its primary advantage is the inclusion of a series of functions for estimating and testing ERGM/p* graph models. Download igraph
Download Statnet Also brought to you by R guru Hadley Wickham, the plyr package assist reachers in the least glamorous aspect of their work—data manipulation and cleaning.  One of R’s great advantages is its ability t handle very large datasets, and plyr is there to help you break these large data problems into smaller and more manageable pieces. Download plyr Also developed by Gary Kind, Amelia II contains a sets of algorithms for multiple imputation of missing data across a wide range of data types, such as survey, time series and cross sectional.  As missing data problems are ubiquitous in social science research the functions contained in this package provide a powerful solution to these issues. Download Amelia II This package is used to fit and compare Gaussian linear and nonlinear mixed-effects models.  For those examining complex time series data with various correlation structures the nlme provides a number of options for fits, tests and plotting. Download nlme Unlike newer version of Python, the current build of R does not contain native functionality for distributing jobs across high-performance computing clusters.  The SNOW and Rmpi packages provide this functionality, and are highly recommended to any researcher with access to an HPC environment running R. Download SNOW
Download Rmpi Both of these packages convert R summary results into LaTeX/HTML table format.  The xtable package is a general solution, while the apsrtable package, developed by fellow political science grad student Michael Malecki, will output tables in the APSR format&mdas;for those of you fortunate enough to need to use this format. Download xtable
Download apsrtable
 Got panel data? If so, you need plm, which contains all of the necessary model specifications and tests for fitting a panel data model; including specifications for instrumental variable models. Download plm As I stated, R is great for dealing with large datasets; however, occasionally you will encounter a dataset so large that it can grind R’s base I/O functions to a halt.  As the name suggests, the sqldf packages overcomes this by allowing uses to perfrom SQL statements directly on R data frames, greatly increasing efficiency. Download sqldf I hope that you will explore and use the packages above that you do not already have familiarity with.  To those who have never used R and/or have an irrational phobia of the language, let this list provide the appropriate motivation.  Also, to those R experts out there, I welcome any suggestions for more useful R packages for the social science inclined! "	 0 Comments
R> if (done=TRUE) tweet me!	https://www.r-bloggers.com/2009/12/r-if-donetrue-tweet-me/	December 11, 2009	Manos Parzakonis	Let’s say that you’re fitting a cumbersome model so time is not to waste over a PC staring at the screen half anxious-half bored… Then, you can always leave and go on with meetings and all your daily routine and have R notify you the results! How? We will illustrate the situation above using some Bayesian Model Averaging code adapted by Martin Feldkircher & Stefan Zeugner. You should download the code and source everything in R except for the example in the end (after the definition of the functions!). This is gonna take s o m e time (really!), so you could let R working and go out for a cup of coffee (typical of Greek people!). Add the following at the end of the above code. Would you really care enough to check whether the fit is done when outside? 	 0 Comments
Because it’s Friday: Detecting Cylons	https://www.r-bloggers.com/2009/12/because-its-friday-detecting-cylons/	December 11, 2009	David Smith	Battlestar Galactica (Ronald D Moore’s reimagined version of the rather cheesy 70’s sci-fi series) has been my favourite TV series (of any genre) of recent years, so I’m especially excited that Chris Bilder has given me the chance to blog about it. Chris, an Associate Professor in the Department of Statistics at the University of Nebraska-Lincoln, recently published an article in Chance Magazine looking at how a statistical method may have resolved a critical plot point early in the series. Let’s set the scene. Dr Gaius Baltar, a brilliant scientist among a band of 50,000 human refugees from system of planets devastated by the evil robotic Cylon race, has a problem. Some Cylons look just like humans, and may have infiltrated the refugee population. Baltar has invented a blood test that can distinguish humans from Cylons, but it takes 11 hours to test a blood sample, and there’s only one testing machine. On screen, Baltar discusses the problem with his imaginary Cylon companion (look, it’s complicated, ok?) named Six:  Six: Well, that’ll take a while. Gaius Baltar: 21,956 days. Six: 60.1534 years.  Gaius Baltar: Well, now, let’s figure in a few hours for sleep here and there. We’ll call that an even 61, shall we? Chris explains in the article (you can find a PDF preprint here, if you don’t subscribe to Chance) that the problem might have been resolved by employing group testing. Rather than testing each blood sample sequentially, Baltar could have mixed a number of blood samples together and tested the combined sample for Cylon characteristics. Assuming the number of Cylon agents in the fleet was small (turns out there were fewer than 10)  this method could rule out whole swaths of subjects with a single test, or conversely identify one or more suspect pools for individual testing.  Group Testing isn’t science-fiction, either: it’s used in the real world for testing diseases where the incidence of the disease is rare, but the cost (in time or resources) of testing is high. Group testing is used to screen blood donations for HIV and other viruses, for example. Of course, the size of the group pooled together for the group test matters: too small, and you’re not getting much efficiency out of the group method; too large and you risk invalidating the test by making the active agent (Cylon cells in our case, I guess) too difficult to detect in a highly-diluted sample. In addition to choosing a group size, there are also various procedures you could choose to allocate subjects to groups and how to retest a “positive” group. Chris used R to plot the amount of time (in years) it would have taken Baltar to detect all the cylons in the fleet using various methods and group sizes:  Using the “Halving” procedure and a group size of 500 (and assuming 7 Cylons in the fleet), Baltar could have completed his testing in 101 days. In the series, Baltar abandons his testing assuming it could never have been completed. But Baltar was a nasty piece of work anyway, so who knows what he was really thinking? Chris Bilder: Supplementary Material for Human or Cylon?  	 0 Comments
A Lot of Deaths are Partly Self-Induced	https://www.r-bloggers.com/2009/12/a-lot-of-deaths-are-partly-self-induced/	December 11, 2009	John Myles White	"I’m a little surprised by Andrew Gelman’s post today, doubting the wisdom of a passage from Gary Becker’s work that reads: 
According to the economic approach, therefore, most (if not all!) deaths are to some extent “suicides” in the sense that they could have been postponed if more resources had been invested in prolonging life.
 I disagree with Gelman in finding this passage unimpressive or even slightly ridiculous. I think there is an important insight that Becker is trying to push on: we humans have far more control over our lives than we care to admit. I also think that not admitting the extent of our power to control our lives exculpates us for many of our failures, which is why it is one of the more popular approaches to dealing with the problems in our lives. As La Rochefoucauld always pointed out, nothing trumps self-love. And I don’t think that emphasizing the unexpectedly large amount of control we have in our lives is just an idea that the economic approach has to offer us. I think you’d get the same idea out of reading Sartre’s discussion of a young man choosing whether or not to fight in the French Resistance in Existentialism is a Humanism. But, to focus on the thrust of Gelman’s argument, my problems are really with the following passage: 
My impression, though, is that people are dying all the time without wanting to do so, and Becker’s argument seems pretty silly to me. To spell it out in a little more detail: Suppose a person is standing on the sidewalk and is run over by an out-of-control car. I don’t see how you can call it suicidal of the pedestrian that the driver was not paying attention. Nor do I see it as suicidal if someone develops kidney cancer and dies, nor do I see it as suicidal if a kid is playing and falls out of a high window, or if someone in the Middle East is hit by a bomb while sitting in a school, etc. Beyond this, just as there used to be millions of people who smoked and died of cancer before people knew that smoking kills, I’m sure there are millions of people now doing something that they don’t realize is dangerous.
 The first section is worrisome, because it seems to be catering to the availability bias: we humans are almost certain to overestimate the number of deaths caused by car accidents, so, by referring to it without explicit numbers, we’re likely to anchor ourselves on inaccurate quantities. In general, once any emotional topic comes up, I feel the need to find something more immutable than intuitions to trust in. Once a kid falls out of a window, rationality tends to follow them. Now, I freely concede that all of the examples Gelman lists are not at all suicide-like, but what I worry about is the relative importance of those causes of death amid the frequent causes of death, at least in the U.S. I especially worry about this given the last two sentences of the above-cited passage: why focus on the “millions of people now doing something that they don’t realize is dangerous?” Why not focus instead on the millions of people killing themselves by smoking, fully cognizant of their self-destruction all the while? What possible reason would you have for focusing on one or the other? I’d say that, in general, you can focus on those who smoke if you want to prove Becker right, and that you can focus on those who don’t if you want to prove him wrong. Either way, I am reasonably certain that you’ll implicitly distort your internal estimates of the relative numbers. With that in mind, I think the right approach is to get some plausible estimate from real data. Here’s a simplistic start: (1) Go to the CDC website and find a reasonably current list of the leading causes of death in the U.S. here. (2) Mark each item as a possible point for Becker’s case. This is  clearly debatable, but I’ll take as granted that “heart disease” (over-eating and under-exercising),  “chronic lower respiratory diseases” (smoking), “diabetes” (over-eating and under-exercising), and “influenza and pneumonia” (refusing vaccination) are sometimes self-induced. My own bias is to assume that they’re usually self-induced, but that’s pure intuition, so please don’t trust that idea. In fact, what I’d really like is for you, dear reader, to prove me wrong with more elaborate statistics. With these marked data points, I get a data set that looks like this: Copying and pasting the resulting table and running two lines of R code lets me avoid the relevant mental arithmetic: And I get 48% of all deaths as being self-induced. That’s definitely not “almost all”, but plainly a close call for “most.” Obviously this number is dubious at best: my point is only that so many cognitive biases operate in thinking about this sort of issue that you have to use numbers as a form of mental hygiene. Obviously pushing this issue into the realm of statistics only endangers my claims, since Gelman is so much more capable than I am as a statistician, but I think my general point is accurate: whether a given person thinks the numbers support Becker or disprove him says more about that person’s general outlook on life than about the actual question at hand. "	 0 Comments
Times Series Methods versus Recurrence Relations	https://www.r-bloggers.com/2009/12/times-series-methods-versus-recurrence-relations/	December 10, 2009	John Myles White	This term, I’ve been sitting in on Rene Carmona’s course on Modern Regression and Time Series Analysis. Much of the material on regression covered in the course was familiar to me already, but I’ve never felt that I had a real command of times series analysis methods. When Carmona defined the AR(p) model in class a few weeks ago, it struck me that, though I’d seen the defining equation several times before, I’d never realized earlier that the AR(p) model subsumes all possible linear recurrence relations. Also, the AR(p) model has the nice property that, if you already know the correct value of p, fitting the AR(p) model can be done with an ordinary least squares regression. With these observations in mind, I decided to see how well I could derive the formulas for simple recurrence relations from a small data set. The results I got on my 2.4 GHz Intel Core 2 Duo MacBook are a useful case study in the dangers of naively using the default methods for fitting AR(p) models, as well as a particularly clear example of the inevitable inaccuracies in floating point arithmetic. I hope John D. Cook will forgive me for using the Fibonacci sequence as my example. While I totally agree with John that the Fibonacci sequence is not the ideal object to study if you’re interested in day-to-day programming tasks, its simplicity makes it perfect for understanding how recurrence relations work. The workhouse for fitting an AR(p) model in R is, predictably, the ar function. To see how well it would work for my purposes, I stored the first 15 Fibonacci terms in a vector and ran ar using all of its defaults settings. Here’s the results: These results are pretty terrible: the order for the model is chosen to be 1, which is clearly wrong. Given the wrong order, it’s no surprise that the estimated coefficient is off, though it’s strange that the result is so far off from the ideal coefficient for an order 1 model, which is (1 + sqrt(5)) / 2, or 1.618034. Thankfully, you can force ar to use the order you want by overriding some of the defaults: You choose your preferred order using order.max, but this will only be an upper bound if you allow the function to use AIC scores to determine the order of the AR(p) model. To figure out what was going wrong, I decided to use lm instead of ar. To do that, I needed subsets of my input data: Given these subset inputs, the call to lm is simple: As you can see, lm gets the right results, more or less. The non-zero intercept value is unfortunate, but suggests how easily floating point errors slip into these calculations. Having gotten good results with lm, I decided to review ar a bit more: this led me to the conclusion that I should try using the method = 'ols' setting instead of method = 'yule-walker'. This clearly works, though I find the output line about the intercept term confusing. I have to say that I’m a little surprised that the Yule-Walker method gives such bad results in this example: I’m not sure yet whether this is caused by the small sample size, a data set that can be fit without any error, intrinsic problems with the method, or something else I can’t even conceive of. Knowing that ar could work if the OLS method was enforced, I decided to try letting the AIC have its way again after reintroducing this method level constraint: As you can see, this also works, though there’s an error that I assume is a result of having input data that can be perfectly fit. In short, I think the take away lesson is that you can easily find the formula for recurrence relations using ar as long as you make sure you use ordinary least squares for fitting the various possible models. Just to confirm that the OLS method would also find the ideal coefficient if forced to use an order 1 model, I ran ar one last time: That result is satisfying and further confirms that the problems I had at the start are entirely attributable to using the Yule-Walker method with this data set. [EDIT 12.11.2009: Replaced unfortunate term 'sparse' with non-overloaded word 'small'.] 	 0 Comments
APIs: I wish the life sciences would learn from social networks	https://www.r-bloggers.com/2009/12/apis-i-wish-the-life-sciences-would-learn-from-social-networks/	December 10, 2009	nsaunders	"I was prompted by a thread on the apparent decline of FriendFeed to look for evidence of declining participation in my networks.

First, a quick and dirty Ruby script, tls.rb to grab the Life Scientists feed and count the likes and comments: By default, the API call returns the last 30 items, starting at zero.  You can move back in time by running this script with, for example, “tls.rb 30″.  Really, there should be a check to see if ARGV[0] is an integer but in fact the argument can be absent (or nothing at all) and it will be ignored.  I did say quick and dirty. The script returns CSV with entry ID, date, time, likes count and comments count, looking like this: One big drawback of the FriendFeed API is that you cannot retrieve entries by date, or a range of dates.  By experimenting with values of “?start=N” in the URL, it seemed that N=3600 retrieved entries from late 2008 onwards.  And so: Be aware that this will not retrieve all posts for 2009 and there will also be duplicate entries – which we can filter out by entry ID.  To remove duplicates and 2008 entries: We’re not quite there yet.  We have unique records but they can have the same date.  We need to sum the counts and likes for each date.  Should have done that in the Ruby script really…but we can use awk, to sum the likes, as follows: Just substitute $5 to sum the comments. Last step:  read the file into R, download Paul Bleicher’s calendarHeat.R code and generate plots: That was quick, relatively easy and most of all, fun.
In contrast, I’ve been trying to mine microarray data from the NCBI GEO database for the best part of 8 months now.
 There’s an API of sorts but getting the results that I want is not quick, easy and most certainly not fun. Is it any wonder that all the cool kids want to be web developers, not data scientists? "	 0 Comments
Demography package	https://www.r-bloggers.com/2009/12/demography-package/	December 10, 2009	Shige		 0 Comments
The R Graphical Manual	https://www.r-bloggers.com/2009/12/the-r-graphical-manual/	December 10, 2009	David Smith	R has a lot of functions. Add in all the packages on CRAN, and then there’s a lot of functions. You can search through the contents of the function documentation at places like Rseek.org, but the R Graphical Manual takes a different tack on exploring the functions in R: rather than browsing by the name or description of the functions, you browse by the graphics generated by the functions. One of the very clever design decisions by the R Core group was to enforce that every function should include documentation, and that the documentation should include a working example. Those examples actually act as unit tests — part of the process of testing R and R packages is validating that the example code actually runs without error. Because graphics are such an integral part of the R language, many such examples include code that generates graphs. As far as I can tell (although it’s not altogether clear), the site works by running the example code included with each function and saving any graphs they generate. You can then browse through all the graphs. Click on one that interests you to see the function and the example code that generated it. You can also browse through graphics from particular domains (Pharmacokinetics or Finance, for example), as defined by the various CRAN Task Views. R Graphical Manual: Index 	 0 Comments
“Introducing Monte Carlo Methods with R” is out!	https://www.r-bloggers.com/2009/12/%e2%80%9cintroducing-monte-carlo-methods-with-r%e2%80%9d-is-out/	December 9, 2009	xi'an	That’s it!, “Introducing Monte Carlo Methods with R” is out, truly out, I have received a copy from Springer by express mail today! (If you need any further proof, it is also advertised as In stock by Amazon.) Given that the printer exactly reproduces the pdf file sent to Springer, there is no element of surprise as in my earliest book (where I found a particularly horrendous typo made by the French publisher on the back cover!) but it is nonetheless a very pleasant feeling to take (finally!) hold of one’s new book! Since there must be remaining typos and even more obscure points, feel free to contact George Casella or myself for corrections and precisions. 	 0 Comments
OpenMx with mixture distribution support	https://www.r-bloggers.com/2009/12/openmx-with-mixture-distribution-support/	December 9, 2009	Shige		 0 Comments
Announcing R-bloggers.com: a new R news site (for bloggers by bloggers)	https://www.r-bloggers.com/2009/12/announcing-r-bloggers-com-a-new-r-news-site-for-bloggers-by-bloggers/	December 9, 2009	Tal Galili	"I already wrote about R-bloggers on the R mailing list, so it only seems fitting to write about it more here. I will explain what R-bloggers is and then move to explain what I hope it will accomplish. 
 R-Bloggers.com is a central hub of content collected from bloggers who write about R (in English) and if you are an R blogger you can join it by filling in this form. I built the site with the aspiration to  help R bloggers and users to connect and follow the “R blogosphere”. When I am writing these words, R-bloggers already has 17 blogs in it, and I hope for many (many) more. How does R-Bloggers operate? This site aggregates feeds (only with permission!) from participating R blogs. The beginnings of each participating blog’s posts will automatically be displayed on the main page with links to the original posts; inside every post there is a link to the  original blog and links to other related articles.  While all participating blogs have links in the “Contributors” section of our sidebar What does R-Bloggers offer it’s visitors?  Who started R-Bloggers (and way)? R Bloggers was started by Tal Galili (well, me).  After searching for numerous R blogs I decided that there must be more R blogs our there then he knows about, and maybe the best way for finding them is to make them find him. After writing about it in the R mailing list, I got some good feedbacks but also questions about why use only R blogs and not all the R feeds that exist. Who is the website actually for (when there are services like Google reader for us to read our feeds with), and what am I hoping it will do. So here is what I answered: For me there are two audiences:
 One is that of the web 2.0 power users. That is, people who know what RSS is and use it, maybe evern write their own blogs. These people have only one problem (as I see it) that R-bloggers tries to solve, and that is to know who else lives in their ecosystem. Who else they should follow.
For that, google reader recommendation system is great, but not enough. A much better system is if there was a one place where all R bloggers would go, write down their website, and all of us would know they exist. That is what R-bloggers offers for the power users. I think this is also why over 20 of them subscribed to the site RSS feed.
BTW, The origin of this idea came to me when I was trying to find all the dance bloggers for my wife (who is a dance researcher and blogger herself). After a while we started http://www.dancebloggers.com/ while knowing of only 10 bloggers. They list now has over 80 bloggers, most of which we would have not known about without this hub.
The same thing I am trying to do for the R community, that is way I hope more R bloggers would write about the service – so their network of readers which includes other R bloggers would add themselves and we will all know about them.
If that was my only purpose, a simple directory would have been enough. But I also have a second one and that is to help the second audience. The second audience I am thinking of are people of our community who are not so much early adopters (and actually quite late adapters) of the new facilities that the new web (a.k.a: web 2.0) provides.
To them the all RSS thing is too much to look at, and they are used to e-mails. And because of that they are (until now) disconected from many of the R bloggers out there, simply because it is in-efficient for them to go through all these blogs each day (or even week). So for them, to see all the content in one place (and even get an e-mail about it) would be (I hope) a service. I believe that’s why 5 of them (so far) has subscribed via e-mail.
I also hope teachers will direct their students to this as a resource for getting a sense of what people who are using R are doing.
Another thing that hints me about the R community is seeing how the “facebook fan box” is still empty. Which tells me that (sadly) very few R users are actively using facebook as a means for connecting with the outer networks of people out there. All I wrote also explains why R-bloggers will only take feeds of bloggers and only (as much as can be said) their posts that are centered around R  (hence the website name   ).
It both follows what Gabor talked about – having a site who’s content is only about R. But also what I wish, which is to have “content” in the sense of articles to read (mostly). And not so much things like news feeds of wikipedia or new packages published. I hope this post will both notify people about this new resource, encourage more R bloggers to join, and will help for future people to better understand what this R-Bloggers thing is all about   "	 0 Comments
R Function Usage Frequencies, Addendum	https://www.r-bloggers.com/2009/12/r-function-usage-frequencies-addendum/	December 9, 2009	John Myles White	Since people have asked, here is a GitHub page with all of the code used to generate my R function usage analyses: cran-function-usage-analysis. 	 0 Comments
Abstract Data Type Operations in R	https://www.r-bloggers.com/2009/12/abstract-data-type-operations-in-r/	December 9, 2009	John Myles White	This morning, I got a chance to read enough of the R Language Definition to finish my implementations of push and pop. While I was at it, I also wrote implementations of unshift, shift, queue and dequeue. Here they are: In general, the secret to writing these pseudo-macros is to use substitute. For the three functions that need to return a value as well as edit the passed parameter, you also need to use new.env, assign and get to edit the relevant symbol tables during function execution. To check that these functions work, try the following examples after defining the functions above: 	 0 Comments
Happy 1st Birthday, Revolutions!	https://www.r-bloggers.com/2009/12/happy-1st-birthday-revolutions/	December 9, 2009	David Smith	Wow, doesn’t time fly! It’s hard to believe that this blog is one year old already: our first post was on December 9, 2008. I remember when we first had the idea of doing a blog exclusively devoted to R that some wondered if there would be enough news about R to warrant daily updates, but with the community growing so explosively over the past year there’s actually been more than I can keep up with! So thanks to everyone who’s been using and writing about R for giving me such great stuff to link to.  Cheers to all! — David. 	 0 Comments
The top 5 R functions	https://www.r-bloggers.com/2009/12/the-top-5-r-functions/	December 9, 2009	David Smith	John Myles White (who did the Canabalt scores analysis from last month) was trying to decide which R functions to spend time learning, and asked the obvious followup question: Which functions in R are used the most? With no readily-available answer, John answered the question himself, by counting the number of times each function is called in all the packages available on CRAN. He then ranked the functions in two ways: first, by the total number of times each function is called in all the source code for all the packages, and then by counting the number of packages that use each function at least once. The top five functions are therefore:  The order varies by the method used, and technically “if” and “function” are keywords, not functions. But John has helpfully provided the data in CSV format for both the package method and total uses method, so you can do your own analysis. Also interesting to note is that the occurrences table is another example of the power-law distribution in action.   John Myles White: R Function Usage Frequencies (Take 2)   	 0 Comments
Colour matching feature in R	https://www.r-bloggers.com/2009/12/colour-matching-feature-in-r/	December 8, 2009	Samuel Brown		 0 Comments
Un-Wrapping a Sphere with R	https://www.r-bloggers.com/2009/12/un-wrapping-a-sphere-with-r/	December 8, 2009	dylan	" 
Premise
I was recently asked to print out a fabric pattern that could be used to cover a sphere, about the size of a ping pong ball, for the purposes of re-creating a favorite cat toy (quite important). Thinking this over, I realized that this was basically a map projection problem– and could probably be solved by scaling an interrupted sinusoidal projection to match the geometry of a ping pong ball. Below are some R functions, and examples of how this endeavor evolved. Thanks to Greg Snow for this helpful post on the R-mailing list, describing how to preserve linear measurement when composing a figure in R. So far the pattern doesn’t quite fit.  
Update
It looks like it was not the printer’s fault– I had used the wrong radius for a ping pong ball: 16mm instead of 19mm or 20mm (there are 38mm and 40mm  diameter ping pong balls). Updated files are attached. Sinusoidal Projection read more "	 0 Comments
Package Update Roundup: Nov 2009	https://www.r-bloggers.com/2009/12/package-update-roundup-nov-2009/	December 8, 2009	David Smith	This is a list of new or updated packages that were released for R in November, as announced on the r-packages mailing list. To include other updates on this list, please email David Smith. For a complete list of all updates on CRAN, see the CRANberries archive for November 2009. Follow package name links for ratings and other information on crantastic.org. debug, a debugger for R functions, with code display, graceful error recovery, line-numbered conditional breakpoints, access to exit code, flow control, and full keyboard input, has been updated with bug fixes. dse, a package for multivariate ARMA and state space time series modelling and forecasting, has been updated.  The distr-family of packages (distr, distrEx, distrSim, distrTEst, distrTeach, distrMod, distrEllipse, distrDoc, startupmsg and SweaveListingUtils) have been extensively updated. EvalEst is a new Dynamic Systems Estimation package for evaluation of estimation methods. (These functions were formerly part of the dse bundle.) frailtypack, a package for fitting frailty models using maximum penalized likelihood estimation, has been updated. ff, a package for memory-efficient storage of large data on disk and fast access functions, has been updated to support large data.frames, csv import/export and more. mvbutils, a package of tools for organization of workspaces, function/documentation editing with backups, package construction and updating and more, has been updated with some new experimental features. NMF, a new package that implements a number of standard algorithms to perform Nonnegative Matrix Factorization and provides a flexible framework to easily test and develop new methods, was released. TraMineR, a package for mining, describing and visualizing sequences of states or events and more generally discrete sequential data, has been updated. 	 0 Comments
Sciviews	https://www.r-bloggers.com/2009/12/sciviews/	December 8, 2009	Shige		 0 Comments
R Tutorial Series: Multiple Linear Regression	https://www.r-bloggers.com/2009/12/r-tutorial-series-multiple-linear-regression/	December 8, 2009	John M. Quick	In R, multiple linear regression is only a small step away from simple linear regression. In fact, the same lm() function can be used for this technique, but with the addition of a one or more predictors. This tutorial will explore how R can be used to perform multiple linear regression. Before we begin, you may want to download the sample data (.csv) used in this tutorial. Be sure to right-click and save the file to your R working directory. This dataset contains information used to estimate undergraduate enrollment at the University of New Mexico (Office of Institutional Research, 1990). Note that all code samples in this tutorial assume that this data has already been read into an R variable and has been attached. In R, the lm(), or “linear model,” function can be used to create a multiple regression model. The lm() function accepts a number of arguments (“Fitting Linear Models,” n.d.). The following list explains the two most commonly used parameters. Note that the formula argument follows a specific format. For multiple linear regression, this is “YVAR ~ XVAR1 + XVAR2 + … + XVARi” where YVAR is the dependent, or predicted, variable and XVAR1, XVAR2, etc. are the independent, or predictor, variables. It is recommended that you save a newly created linear model into a variable. By doing so, the model can be used in subsequent calculations and analyses without having to retype the entire lm() function each time. The sample code below demonstrates how to create a linear model with two predictors and save it into a variable. In this particular case, we are using the unemployment rate (UNEM) and number of spring high school graduates (HGRAD) to predict the fall enrollment (ROLL). The output of the preceding function is pictured below.  From this output, we can determine that the intercept is -8255.8, the coefficient for the unemployment rate is 698.2, and the coefficient for number of spring high school graduates is 0.9. Therefore, the complete regression equation is Fall Enrollment = -8255.8 + 698.2 * Unemployment Rate + 0.9 * Number of Spring High School Graduates. This equation tells us that the predicted fall enrollment for the University of New Mexico will increase by 698.2 students for every one percent increase in the unemployment rate and 0.9 students for every one high school graduate. Suppose that our research question asks what the expected fall enrollment is, given this year’s unemployment rate of 9% and spring high school graduating class of 100,000 students. As follows, we can use the regression equation to calculate the answer to this question. When creating a model with more than two predictors, the lm() function can again be used. Simply, one can just continue to add variables to the FORMULA argument until all of them are accounted for. A three predictor model is demonstrated below. It seeks to predict the fall enrollment (ROLL) via the unemployment rate (UNEM), number of spring high school graduates (HGRAD), and per capita income (INC). The output of the preceding function is pictured below.  From this output, we can determine that the intercept is -9153.3, the coefficient for the unemployment rate is 450.1, the coefficient for number of spring high school graduates is 0.4, and the coefficient for per capita income is 4.3. Therefore, the complete regression equation is Fall Enrollment = -9153.3 + 450.1 * Unemployment Rate + 0.4 * Number of Spring High School Graduates + 4.3 * Per Capita Income. This equation tells us that the predicted fall enrollment for the University of New Mexico will increase by 450.1 students for every one percent increase in the unemployment rate, 0.4 students for every one high school graduate, and 4.3 students for every one dollar of per capita income. Let’s revisit our research question, this time including a per capita income of $30,000. A multiple linear regression model can be used to do much more than just calculate expected values. Here, the summary(OBJECT) function is a useful tool. It is capable of generating a wealth of important information about a linear model. The example below demonstrates the use of the summary function on the two models created during this tutorial. The output of the preceding functions is pictured below.   The summary(OBJECT) function has provided us with t-test, F-test, R-squared, residual, and significance values. All of this data can be used to answer important questions related to our models. Although lm() was used in this tutorial, note that there are alternative modeling functions available in R, such as glm() and rlm(). Depending on your unique circumstances, it may be beneficial or necessary to investigate alternatives to lm() before choosing how to conduct your regression analysis. To see a complete example of how multiple linear regression can be conducted in R, please download the multiple linear regression example (.txt) file. Fitting Linear Models. (n.d.). Retrieved November 22, 2009 from http://sekhon.berkeley.edu/library/stats/html/lm.html Office of Institutional Research (1990). Enrollment Forecast [Data File]. Retrieved November 22, 2009 from http://lib.stat.cmu.edu/DASL/Datafiles/enrolldat.html 	 0 Comments
R Function Usage Frequencies, Take 2	https://www.r-bloggers.com/2009/12/r-function-usage-frequencies-take-2/	December 8, 2009	John Myles White	"Yesterday, Hadley Wickham commented on my post on the frequency of calling various R functions that it would be helpful to have the number of packages that call a function in addition to the number of times that the function is called. I compiled the relevant data last night: you can grab it here This data set includes a row for every function I found, indexed by each of the packages and files in which it was used. At this higher level of resolution, I record the number of times each function was called. To get a sense of the correspondence between these measures, below I’ve plotted the number of packages using each function in my data set against the log number of times each function is called: And here’s a new top 25 most called functions table:
  "	 0 Comments
Implementing Push and Pop in R	https://www.r-bloggers.com/2009/12/implementing-push-and-pop-in-r/	December 7, 2009	John Myles White	Having grown up with Perl, there are two functions that I desperately miss while programming in R: push and pop. Continually writing tries my patience, while writing makes me feel like I’m programming in C, rather than a higher-level programming language. That said, here’s a simplistic hack to provide something like an implementation of push and pop in R: Both of these functions are more than a little ugly, because you have to pass in a string that names the vector you want to change, rather than providing its name as a bareword. Even worse, this version of pop doesn’t let you get the value of the item you pop off of the stack, because I’m not sure how to introduce a temporary variable into the parent’s environment without occasionally clobbering the value of an existing variable. If I knew more about scoping and lazy evaluation in R, I think I could implement these two functions as pseudo-macros and solve both concerns. If you know how to do this, please do let me know. 	 0 Comments
R, REvolution named in top analytic trends for 2010	https://www.r-bloggers.com/2009/12/r-revolution-named-in-top-analytic-trends-for-2010/	December 7, 2009	David Smith	Author and enterprise software executive Nenshad Bardoliwalla lists his Top 10 Trends for 2010 in Analytics, Business Intelligence, and Performance Management at the website Enterprise Irregulars. If you’ve been following the Business Intelligence space (and who hasn’t, right?) you’ll recognize some familiar themes: predictive analytics, Web 2.0, Software-as-a-Service, risk, IBM. What’s interesting about this list is that it raises some new issues related to data, visualization and open source. Take a look at trends 6, 7 and 8: 6. The undeniable arrival of the era of big data will lead to further proliferation in data management alternatives. Actual statistical analysis (as opposed to mere OLAP-style aggregation) of large data sets is a priority for many organizations today, following the lead of the big Web players like Google and Facebook. Naturally, Hadoop gets a mention here. 7. Advanced Visualization will continue to increase in depth and relevance to broader audiences. Business is moving beyond bar charts and (gasp!) pie charts to adopt more interactive and revealing ways of looking at data. Interactive visualization software like Tableau and Spotfire get mentions here, but I was particularly interested to see integrating data with maps (via Google Maps) listed as a trend. 8. Open Source offerings will continue to make in-roads against on-premise offerings. According to Bardoliwalla, “Open Source offerings in the larger BI market are disrupting the incumbent, closed-source, on-premise vendors.” Pentaho and Jaspersoft are the obvious players in BI, but it was great to see that R and REvolution Computing were also listed as open-source solutions for statistical analysis. Bardoliwalla continues: “These offerings have absolutely reached a level of maturity where they are capable of being deployed in the enterprise right alongside any other commercial closed-source vendor offering.” With regard to REvolution R Enterprise, I think that’s true for advanced predictive analytics applications today (especially when integrated with a BI front-end like Pentaho); as its capabilities are developed further I expect we’ll see it deployed for more and more mainstream applications. Enterprise Irregulars: The Top 10 Trends for 2010 in Analytics, Business Intelligence, and Performance Management  	 0 Comments
Advanced SNA with R, via Harvard Political Nets Conference	https://www.r-bloggers.com/2009/12/advanced-sna-with-r-via-harvard-political-nets-conference/	December 7, 2009	Drew Conway	End of semester madness has begun, so posting will likely be light this week.  To keep you busy, David Lazer recently posted the five video series of the Goudreau-Hunter tutorial on social network analysis using R from the 2009 Harvard Political Networks Conference. Each video run about 60 minutes, so these will literally provide hours of edutainment.  The first couple videos are introductions that review the basics, so I jump to video three below.  Goudreau-Hunter Political Networks 2009 3 of 5 from David Lazer on Vimeo.  	 0 Comments
R Function Usage Frequencies	https://www.r-bloggers.com/2009/12/r-function-usage-frequencies/	December 7, 2009	John Myles White	"A few months ago I decided to apply word frequency analysis ideas to R code. My idea was simple: the functions that one should invest the most effort into learning are precisely those functions that are used most frequently in real world code. In fact, this simple idea can be applied to spoken languages as valuably as to programming languages: the words that will most improve your ability to understand day-to-day speech in a foreign language are precisely the words that occur most frequently in day-to-day speech. Our language classrooms tend to screw this up by emphasizing words like “spoon” over “dude”, even though people my age will use the word “dude” many more times a day than they’ll use “spoon”.1 In large part, this is because the usefulness of a word tends to be confounded with its respectability when you learn a language in an academic setting. This over-emphasis on respectability is why you’re never taught curse words, even though they’re as practically useful as the majority of other words. But I digress. Returning to R, the attached CSV data set contains the results of my word frequency analysis. To produce this data set, I spidered all of the source code for every R package on CRAN, split the resulting corpus text into lexical tokens, and counted the occurrences of each token. To make things simpler, I decided to treat every token followed by a parenthesized expression as a function, even though this gives me some syntactical units like if in my list of functions. I think the resulting raw data set is probably as useful as any summaries I can offer of it, but I’ll offer some obvious results for those interested. Function call frequencies in R, unsurprisingly, seem to follow Zipf’s law or some variety thereof. You can see this fairly clearly in the following logarithmic plot: And here are the top 25 most frequently used functions in R, including some syntactical units:
  Some fun extensions to this work would be to use this frequency data set as a standard for the abnormality of code style: you could compare an individual programmer’s code to the standard frequency data to see which functions such-and-such a programmer tends to overuse and underuse. I personally tend to underuse stop and I never use attr at all. Another interesting project would be to use this data set as input to a text classifier that would attempt to predict the author of a piece of R code based on token frequency information, in line with Mosteller and Wallace’s famous analysis of the Federalist Papers or anti-spam programs. "	 0 Comments
A web application for R’s ggplot2	https://www.r-bloggers.com/2009/12/a-web-application-for-r%e2%80%99s-ggplot2/	December 7, 2009	Tal Galili	"One of the exciting new frontiers for R programming is of creating website interfaces to R code. At the forefront of this domain is a young and (very) bright man called Jeroen Ooms, whom I had the pleasure of meeting at useR 2009 (press the link to see his presentation). Today Jeroen announced a new version (0.11) of his web interface to ggplot2. See it here:
http://www.yeroon.net/ggplot2/  As Jeroen wrote: 
New features include 1D geom’s (histogram, density, freqpoly), syntax mode (by clicking the tiny arrow at the bottom), and some additional facet options. And some minor improvements and fixes, most notably for Internet Explorer.
The data upload has not been improved yet, I am working on that. For now, it supports .csv, .sav (spss), and tab delimited data. Please make sure your filename has the appropriate extension and every column has a header in your data. If you export a dataframe from R, use:
write.csv(mydf, ”mydf.csv” , row.names=F). If you upload an spss
datafile, none of this should be a concern.
Supported browsers are IE6-8, FF, Safari, and Chrome, but a recent browser is highly recommended. As always, feedback is more than welcome.
 Here is a little demo video that shows how to use the new features:  The datafile from the demo is available at http://www.yeroon.net/ggplot2/myMovies.csv. I wish the best to Jeroen, and hope to see many more such uses in the future. "	 0 Comments
pgfSweave now on CRAN	https://www.r-bloggers.com/2009/12/pgfsweave-now-on-cran/	December 6, 2009	cameron	"At long last pgfSweave has finally made its way to CRAN. http://cran.r-project.org/web/packages/pgfSweave/index.html The pgfSweave R package is about speed and style of graphics. For speed,
the package provides capabilities for “caching” graphics generated
with Sweave on top of the caching funcitonality of cacheSweave. For
style the pgfSweave package facilitates the integration of R graphics
with LaTeX reports through the tikzDevice package and eps2pgf utility.
With these tools, figure labels are converted to LaTeX strings so they
match the style of the document and the full range of LaTeX math
symbols/equations are available. The backbone of pgfSweave is a a new driver for Sweave
(pgfSweaveDriver). The driver provides new chunk options tikz, pgf and
external on top of the cache option provided by cacheSweave. This package started as a fork (hack) of cacheSweave to suit our own
narrow purposes and has since become a valuable tool for much of our
own work.  We hope you will find it useful as well. -Cameron Bracken and Charlie Sharpsteen "	 0 Comments
Create factor variables in R	https://www.r-bloggers.com/2009/12/create-factor-variables-in-r/	December 6, 2009	Manos Parzakonis	Instead of the factor() function which usually applies after defining a vector there’s the gl() base function to do this in one step, eg 	 0 Comments
digest 0.4.2	https://www.r-bloggers.com/2009/12/digest-0-4-2/	December 6, 2009	Thinking inside the box		 0 Comments
Design of Experiments – Blocking and Full Factorial Experimental Design Plans	https://www.r-bloggers.com/2009/12/design-of-experiments-%e2%80%93-blocking-and-full-factorial-experimental-design-plans/	December 6, 2009	Ralph	When considering using a full factorial experimental design there may be constraints on the number of experiments that can be run during a particular session, or there may be other practical constraints that introduce systematic differences into an experiment that can be handled during the design and analysis of the data collected during the experiment. Blocking is a technique used in design of experiments methodology to deal with the systematic differences to ensure that all the factors of interest and interactions between the factors can be assessed in the design. When blocking occurs one or more of the interactions is likely to be confounded with the block effects but a good choice of blocking should hopefully ensure that it is a higher order interaction that would be challenging to interpret or not be expected to be important that is confounded. The conf.design package in R is described by its author as a small library contains a series of simple tools for constructing and manipulating confounded and fractional factorial designs. The function conf.design can be used to  construct symmetric confounded factorial designs. A very simple example would be a three factor experiment where each factor has low and high settings (levels). If we wanted to divide the experiment into two blocks of four experimental units then we could confounded the block effect with the three way interaction between the factors. The following code would create the required design plan: The first argument is a matrix, with a single row in this case as there are only two blocks, which specifies the levels of the factors for the effect to be confounded with the blocks. The output from this function call is: This shows two blocks, labelled 0 and 1, and the settings of the experiments to run in each block. In the first block the four factor combinations would be: The remaining four combinations are use in the second block of experiments. 	 0 Comments
The animation package	https://www.r-bloggers.com/2009/12/the-animation-package/	December 5, 2009	Shige		 0 Comments
Two more R related web sites	https://www.r-bloggers.com/2009/12/two-more-r-related-web-sites/	December 5, 2009	Shige		 0 Comments
R twitts…	https://www.r-bloggers.com/2009/12/r-twitts%e2%80%a6/	December 5, 2009	Manos Parzakonis	You may already know this… #rstats I think I might get an account when get to connect to the Internet in my mobile phone   	 0 Comments
Advances in Social Science Research Using R	https://www.r-bloggers.com/2009/12/advances-in-social-science-research-using-r/	December 5, 2009	Shige		 0 Comments
Because it’s Friday: The Earth, with rings	https://www.r-bloggers.com/2009/12/because-its-friday-the-earth-with-rings/	December 4, 2009	David Smith	To wind down your afternoon, relax and enjoy the view from various places around the globe of the Earth’s rings in the sky … if the Earth, like Saturn, had rings subject to the Roche Limit.  YouTube: What Earth Would Look Like With Rings Like Saturn  	 0 Comments
[S. Lynch] Introduction to Applied Bayesian Statistics and Estimation for Social Scientists	https://www.r-bloggers.com/2009/12/s-lynch-introduction-to-applied-bayesian-statistics-and-estimation-for-social-scientists/	December 4, 2009	Manos Parzakonis	"Well, that’s a good book that you shouldn’t miss “Introduction to Applied Bayesian Statistics and Estimation for Social Scientists”. Why you shouldn’t miss it? Coz, it’s practical and I mean p r a c t i c a l big time!!! I don’t own tons of (traditionally) printed books but that’s one of the few breaking the rule. I easily rank it above The Bayesian Core… First stop at all costs the book’s webpage to download the R & Winbugs code… Springer | Amazon | Google Books | S. Lynch | Book Page
 "	 0 Comments
How R is disrupting a billion-dollar market	https://www.r-bloggers.com/2009/12/how-r-is-disrupting-a-billion-dollar-market/	December 4, 2009	David Smith	Zack Urlocker, the EVP for Products at MySQL before it was acquired by Sun (and now an executive at Sun) has written an article at InfoWorld suggesting that the recent rash of articles looking at how IBM’s acquisition of SPSS is affecting the BI market dominated by SAS are downplaying the real agent of change in the space: R.  According to Urlocker, the BI battle isn’t between IBM and SAS, instead “the little known open source project R may be the disruptor in this billion-dollar market”. He goes on:  R continues to gather momentum, just as Linux, Apache, MySQL, and JBoss have in recent years. It’s disrupting the market from the bottom, attracting new users who cannot afford the expensive license fees from IBM or SAS. R claims dozens of books on Amazon about the topic, 2,000 open source packages and extensions, and an estimated million users worldwide. Recently, an open source company that provides an optimized version of R, Revolution Computing, received an injection of capital from North Bridge Ventures and Intel. Who would be equipped to lead a company competing against billion-dollar incumbents? None other than Norman Nie, founder and former CEO of SPSS. Game on.  InfoWorld: The BI battle isn’t between IBM and SAS 	 0 Comments
R information	https://www.r-bloggers.com/2009/12/r-information/	December 4, 2009	Shige		 0 Comments
swfDevice: Help Wanted (Windows Build) and Outlook	https://www.r-bloggers.com/2009/12/swfdevice-help-wanted-windows-build-and-outlook/	December 3, 2009	cameron	swfDevice is an R package I am developing that produces swf (flash) files natively from R. If anyone can step up and help get a working windows build up and running that would be great! I have little expertise with windows and I am just about stuck as what to do next. The task involved linking to some precompiled dlls and such since ming depends on libpng, zlib, libungig and freetype. If you would like to help, send me an email (cameron.bracken [at] gmail.com). Todo before initial CRAN release: Todo in the future (possibly before initial release but probably not): 	 0 Comments
Using Git with R-Forge OR Adding an local Git branch to track an Subversion Repo	https://www.r-bloggers.com/2009/12/using-git-with-r-forge-or-adding-an-local-git-branch-to-track-an-subversion-repo/	December 3, 2009	cameron	These instructions were originally from instructions from my friend Charlie. I really like to use Github but R-Forge has alot of nice features for package development, so why not get the best of both worlds. Say you have a git repo (with an R package or something else) but want to create a local brach which tracks an svn repo (from R-Forge) such that you can merge changes from your git repo in and then commit them to svn. First use the following command to grab the current svn:  This will create a new git repository that is a mirror of whatever svn repo you pointed at. This may take a while if you have a big existing svn repo. If you want to add a svn bound branch to an existing git repo: Edit .git/config and add the following:  Once you’ve done this, you have created an entry for a new git remote server- fetch the status and history of it’s branches using the following:  If this command is successful, git branch -a should show:  In the list of available branches. This branch is a remote branch- to make a local copy, do the following:  Now you have a local branch which is bound to the svn server- once again straight up adding/removing/moving files in this branch should work fine. It’s merging that gets you into trouble. The point is that you don’t want git infecting your subversion repository with any of its awesomeness. Exposure to the kind of concentrated badass possessed by git causes poor SVN’s head to pop. So, the merge must be done as follows:  The --squash option tells git to leave out any info about what happened during the merge and act like it just mashed a bunch of files into your svn branch using copy commands. Once you’ve added/copied/moved/squashmerged new files into your local svn branch- do the following to push the changes to the svn repo:  To pull changes from the svn repo run the following:  	 0 Comments
R2admb	https://www.r-bloggers.com/2009/12/r2admb/	December 3, 2009	Shige		 0 Comments
My Five Rules for Data Visualization	https://www.r-bloggers.com/2009/12/my-five-rules-for-data-visualization/	December 3, 2009	Drew Conway	"Tonight the NYC R Meetup will be discussing data visualization in R using ggplot2.  As part of tonight’s meeting I will be providing a very brief show and tell, which includes mostly code examples and external resources.  This exercise has had me thinking quite a bit about data visualization.  In addition, a few days ago the Security Crank (great new blog) pinged me on the apparent uselessness of network analysis visualizations in the defense and intelligence communities.  As I say in my comment at SC, I agree; however, only in that the method is abused by those that view it as only a means to generate “pretty pictures.”  All of this has touched off a very important point about data analysis; possibly the most important, which is how best to convey an analysis visually. Consumers of data analytics are very rarely analysts themselves, so those in the business of generating plots, figures, chats, graphs, etc. most not only be expert in the analytical process, but also in choosing the best format and medium for relaying that knowledge to an audience.  Admittedly, I am not Edward Tufte, Ben Fry, or David McCandless, but I have been around long enough to know what does and does not work, and as such here (in no particular order) are my five rules for data visualization.
 This, to me, is the most difficult aspect of creating high quality data visualizations.  As the creators we are often intimately familiar with the data,  and thus take its subtleties for granted.  Some people recommend asking yourself “would my Grandmother understand this,” but why insult Grandma’s intelligence?  Here’s the bottom line: you have to decide the most efficient means of plotting the data (we’ll get to this), then you have a chart title, legend, possibly some axis labels, and if you are bold a short (140 characters is a good limit) footnote to get your point across.  The best visualizations only require a subset of these to be effective, but once you have added the appropriate data accoutrements the chart better be self-explanatory.  Very simple and imperfect example: restaurant tipping trends between men and women.   Why is the chart on the right better? First, it has more explanatory value.  By splitting the data into two parts we are able to see the x-axis shift for men, i.e., in general they are tipping on higher bills.  Also, we are able to use color in a more valuable way; rather than using it to distinguish between sex we can use it to highlight outliers and note general trends.  Next, by reducing the amount of data in each plot the information is conveyed more efficiently.  Finally, it achieves our ultimate goal, which is always to provide more answers than questions.
 Most of us will not have the resources to use professional data visualizations suites, but even so these tools are often limited by the scope and vision of their creators.  Explore the open-source and general purpose data visualization options out there, learn the three best that fit your needs, and always be open to learning the new stuff—it will pay off.   The visualizations above examine the same data, and even use a similar  technique to visualize it, but clearly the example on the right is conveying a more informative story.  Admittedly, this visualization, which I generated, in many ways violates my first rule; however, it is still telling a story (e.g., there is a strong underlying structure among four notable communities of VC firms). The visualization on the left, taken from an initial attempt at analyzing this data, tells almost no story; save that the network is highly complex and there exist some disconnected firms.  In my defense, I was first excited that there was a built-in Scotch whiskey dataset in R, but I also wanted to show what could be done with a single line of code.  Clearly, however, the color scheme I used is taking away from the story.  The default color scheme in ggplot2 wants to use a gradient, which may be useful in some cases, but not here.  To improve the above example I should override this default and construct a more informative color scheme; such as setting a base color for each Scotch type (e.g., blue for blends and green for single malts). On the other hand, if a reduction and/or visualization method has be successful in the past then it will likely b e successful in the future, so do not be afraid to reuse and recycle.  Many of the most successful data visualizers have distinguished themselves by creating a method for visualization and sticking with it (think Gapminder).  Not only will it possibly make you famous, but putting in the effort to create a useful method for combining, reducing and visualizing data will mean your efforts are more streamlined in the long term. So that’s it.  Nothing too profound there, but I wanted to post this in order to start a conversation.  In that vein, what did I miss and where do you disagree?  As always, I welcome your comments. "	 0 Comments
ggplot2: Overplotting In a Faceted Scatterplot	https://www.r-bloggers.com/2009/12/ggplot2-overplotting-in-a-faceted-scatterplot/	December 3, 2009	learnr	Hadley Wickham recently shared a nice tip on how to get a faceted scatterplot plot with all points in the background of each plot. This technique makes a clever use of setting the faceting variable to NULL so that all points are plotted in light grey in all the facets. Update 17 May 2010 bch asked in the comments below, how to achieve the same when there are two facets. The method is the same, now one would need to exclude both of the facetting variables from the dataset used to draw the light grey points. 	 0 Comments
ohloh	https://www.r-bloggers.com/2009/12/ohloh/	December 2, 2009	romain francois	I’ve been invited me to create my account on ohloh 	 0 Comments
LaTeX and Sweave	https://www.r-bloggers.com/2009/12/latex-and-sweave/	December 1, 2009	Jason	"LaTeX is a typesetting language that is known for its beautiful mathematical formulas. It is similar to HTML in that you write your document in plain text and process (or compile) it to create a postscript or PDF file. You will need to download a LaTeX processor for your platform. On Windows, TeXnicCenter (http://www.texniccenter.org/) is very good and on Mac TeXShop (http://www.uoregon.edu/~koch/texshop/) is very good. Both these applications provide a text editor in addition to the LaTeX processors. Here are a list of excellent resources for learning LaTeX (note that many of these are in psarelated/LaTeX folder on Dropbox): Sweave allows for the embedding of R code directly in your LaTeX documents. The best resource is the Sweave User Manual (http://www.stat.uni-muenchen.de/~leisch/Sweave/Sweave-manual.pdf). In short, simply place the following in your LaTeX file: <>=
2 + 2
@ There is a Sweave function in R. Issuing the Sweave(file.choose()) command will first prompt for the file (by convention use Rnw file extension), execute all the R code and generate a tex file. This file can then be run through your typical LaTeX processor or editor. Once you go through the Sweave manual, I recommend printing page 13 which lists all the options that can be issued to Sweave. To create presentations, the Beamer class is what is generally used. The LaTeX beamer class homepage has several exmaples (http://latex-beamer.sourceforge.net/). The Beamer manual is included on Dropbox or when you download the classes (I do believe Beamer is built into both TeXnicCenter and TeXShop). Laslty, I have included a few of my own files on Dropbox for a recent paper and presentation I’ve done. I used Sweave for the paper (see Bryer.LocatingStudents.Paper.Rnw). I also included the generated LaTeX file (Bryer.LocatingStudents.Paper.tex) as well as the final PDF version. For the presenation, I used Beamer but since I was working from the paper, I simply cut and paste sections from the generated tex file to Bryer.LocationStudents.Presentation.tex. Note the first two lines of this file: %\documentclass[handout]{beamer}
\documentclass{beamer} The percent character is a comment line in LaTeX so in order to create the handout version I simply commented out the second line instead of the first. Both versions are included on Dropbox for your review. "	 0 Comments
Design of Experiments – Full Factorial Designs	https://www.r-bloggers.com/2009/12/design-of-experiments-%e2%80%93-full-factorial-designs/	December 1, 2009	Ralph	In designs where there are multiple factors, all with a discrete group of level settings, the full enumeration of all combinations of factor levels is referred to as a full factorial design. As the number of factors increases, potentially along with the settings for the factors, the total number of experimental units increases rapidly. In many cases each factor takes only two levels, often referred to as the low and high levels, the design is known as a 2^k experiment. Given a three factor setup where each factor takes two levels we can create the full factorial design using the expand.grid function: which creates the following design: We could also make use of the gen.factorial function from the AlgDesign package. In this function we use a vector to specify the number of levels for each of the variables, the number of variables and possibly the names of the variables. To create the full factorial design for an experiment with three factors with 3, 2, and 3 levels respectively the following code would be used: The center option makes the level settings symmetric which is a common way of representing the design. The full design is: 	 0 Comments
Get Started with Machine Learning in R	https://www.r-bloggers.com/2009/12/get-started-with-machine-learning-in-r/	December 1, 2009	Stephen Turner		 0 Comments
Updated slides for ‘Introduction to HPC with R’ (now with correct URLs)	https://www.r-bloggers.com/2009/12/updated-slides-for-introduction-to-hpc-with-r-now-with-correct-urls/	December 1, 2009	Thinking inside the box	"
As mentioned
yesterday,
I spent a few days last week in Japan as I had an opportunity to present the
Introduction to High-Performance Computing with R tutorial at the
Institute for Statistical Mathematics 
in Tachikawa near Tokyo thanks to an invitation by
Junji Nakano.

 
An updated version of the presentations slides (with a few typos corrected)
is now available as is a
2-up handout version.

Compared to previous versions, and reflecting the fact that this was the
‘all-day variant’ of almost five hours of lectures, the following changes were made:
 
Comments and suggestions are, as always, appreciated.


 "	 0 Comments

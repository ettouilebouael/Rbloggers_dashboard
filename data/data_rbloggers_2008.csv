title	link	date	author	text	comment_rbloggers
New material on the GGobi book web page	https://www.r-bloggers.com/2008/01/new-material-on-the-ggobi-book-web-page/	January 30, 2008	Di Cook		 0 Comments
The hammer, or the sledgehammer? A small study in simulation	https://www.r-bloggers.com/2008/01/the-hammer-or-the-sledgehammer-a-small-study-in-simulation/	January 29, 2008	John Johnson		 0 Comments
Shading overlapping area of curves in R	https://www.r-bloggers.com/2008/01/shading-overlapping-area-of-curves-in-r/	January 28, 2008	Yu-Sung Su		 0 Comments
Heuristics for statistics	https://www.r-bloggers.com/2008/01/heuristics-for-statistics/	January 28, 2008	dan	SIMPLE WAYS TO DETECT AND COMMUNICATE STATISTICAL EFFECTS  Decision Science News is fond of heuristics and the Simonian view that for many problems organisms face, optimization is a fiction and satisficing makes us smart.  Statistics is an area in which it is easy to see precision that isn’t there and find “optima” in problems that lack them.  It can be refreshing to look at a problem in a simplified form to get a feeling for what is going on before obsessing over insignificant digits. Andrew Gelman is previewing a few working papers on rules of thumb that make it easy to detect and communicate statistical effects. “Recommended reading,” says Decision Science News, quoting itself. Mini Talk on Simple Statistical Methods Splitting a predictor at the upper quarter or third and the lower quarter or third Scaling regression inputs by dividing by two standard deviations Photo credit: http://www.flickr.com/photo_zoom.gne?id=327299636 	 0 Comments
Hack-at-it 2007	https://www.r-bloggers.com/2008/01/hack-at-it-2007/	January 28, 2008	Di Cook		 0 Comments
ERGMs in R	https://www.r-bloggers.com/2008/01/ergms-in-r/	January 23, 2008	Michal	Developers of statnet, a collection of packages for R for fitting Exponential Random Graph Models (ERGM), issued a major update. First change is that the main package is now called ergm. Secondly, a set of additional packages has been made available. Apart from package network, that provides a class system for relational data on which statnet relies, there are couple of new ones, for example rSonia and dynamicnetwork facilitating work with the SONIA visualizer of network dynamics, but also many more. And, last but not least, thirdly, (almost) all of them are now available on CRAN websites. Another interesting news is the forthcoming special issue of Journal of Statistical Software which is going to be devoted to these new developments. Preliminary versions of the articles are already available on statnet website. I especially welcome the paper by Carter Butts that throughly explains the functionality of the network package that has been available already for some time, but scarce on-line documentation made the conscious use very difficult. 	 0 Comments
Resources for S4 classes and methods	https://www.r-bloggers.com/2008/01/resources-for-s4-classes-and-methods/	January 23, 2008	Michal	"Learning the programming in the new S4 system of classes and methods in R can be quite cumbersome, even though the methods package is very well documented. That is why I collected some of the info and materials that I am aware of on a separate page here. I warmly welcome any suggestions for extending this, for now short, collection! Where to look for information on programming with S4 system of classes and methods for R? Here is a open-ended collection of links to resources on learning programming in S4 system. Suggestions more than welcome!
 Books Articles and notes Presentation slides Links Packages Sometimes it is worthwhile to look at the source code of the available packages and learn from that. Here is are the packages that can be instructive: stats4, lme4. "	 0 Comments
GillespieSSA 0.5-1 is released	https://www.r-bloggers.com/2008/01/gillespiessa-0-5-1-is-released/	January 15, 2008	Mario Pineda-Krch	I just uploaded GillespieSSA 0.5-1 to CRAN. Now it’s just a matter of days before it has propagated itself across all CRAN mirrors. This version consists primarily of revisions I made in response to the reviewer comments on the paper where the package is introduced (submitted to the Journal of Statistical Software). There are some minor changes in the functionality of the ssa.plot() function but otherwise the changes consists entirely of buglet fixes and improvements to the documentation. One of the more interesting comments I got addressed the use of a character vector to pass the propensity functions to the wrapper function ssa(). For example, normally one would define the propensity functions for a logistic growth model as  This is the way it would be defined if the simulation is invoked using the higher level wrapper function ssa(). One can, however, also pass the propensity vector as a function by directly invoking the lower-level method function (ssa.d, ssa.btl, ssa.etl, ssa.otl, …). For the logistic growth model this could be done like so, The obvious advantage of this approach is that the propensity vector is simpler to define and maintain throughout the simulation. It is also likely that the simulation would run faster (or at least being simpler to optimize) without the extra over head imposed by the higher-level wrapper function. The disadvantage is that setting up the simulation is a tad more involved since one has to “manually” update the state vector, time variable and collect the output data (which, by the way, the above routine does not do).  Perhaps the most interesting consequence of directly invoking the lower-level method functions is that one now can vary the parameters of the model during the simulation allowing for temporal environmental heterogeneity, e.g. varying vital rates and carrying capacity over time. 	 0 Comments
Moving average/median	https://www.r-bloggers.com/2008/01/moving-averagemedian/	January 7, 2008	vikasrawal	?rollmean (package=zoo)?rollmedian (package=zoo)?runmed (package=stats) 	 0 Comments
Quantile regression in R	https://www.r-bloggers.com/2008/01/quantile-regression-in-r/	January 5, 2008	dataninja	Roger Koenker, a quantile regression crusader, has an R package that implements the procedure. It is called quantreg, and it is documented here. This package has apparently been around for quite some time, but I was only recently turned on to quantile regression, so it was under my radar. 	 0 Comments
Find type of variables in a data frame	https://www.r-bloggers.com/2008/02/find-type-of-variables-in-a-data-frame/	February 20, 2008	vikasrawal	sapply(a,class) gives type of field (character, numeric, or factor) for each variable in the data frame a. 	 0 Comments
A wraper function to convert coda files into a BUGS object	https://www.r-bloggers.com/2008/02/a-wraper-function-to-convert-coda-files-into-a-bugs-object/	February 11, 2008	Yu-Sung Su		 0 Comments
R graph with two y-axes	https://www.r-bloggers.com/2008/02/r-graph-with-two-y-axes/	February 4, 2008	Rob J Hyndman	I’ve been asked how to do this several times, so I thought it might help to put an example online.  	 0 Comments
Drop unused factor levels	https://www.r-bloggers.com/2008/02/drop-unused-factor-levels/	February 4, 2008	Forester		 0 Comments
R package: codetools	https://www.r-bloggers.com/2008/02/r-package-codetools/	February 4, 2008	Yu-Sung Su		 0 Comments
Paper on the Gillespie Stochastic Simulation Algorithm in press	https://www.r-bloggers.com/2008/02/paper-on-the-gillespie-stochastic-simulation-algorithm-in-press/	February 1, 2008	Mario Pineda-Krch	Just got news that my revisions to the reviewer’s comments on my paper GillespieSSA: Implementing the Gillespie Stochastic Simulation Algorithm in R were accepted. Hence, this paper is not officially in press in the Journal of Statistical Software.   Here’s the abstract: The deterministic dynamics of populations in continuous time are traditionally described using coupled, first-order ordinary differential equations. While this approach is accurate for large systems, it is often inadequate for small systems where key species may be present in small numbers or where key reactions occur at a low rate. The Gillespie stochastic simulation algorithm (SSA) is a procedure for generating time-evolution trajectories of finite populations in continuous time and has become the standard algorithm for these types of stochastic models. This article presents a simple-to-use and flexible framework for implementing the SSA using the high-level statistical computing language R and the package GillespieSSA. Using three ecological models as examples (logistic growth, Rosenzweig-MacArthur predator-prey model, and Kermack-McKendrick SIRS metapopulation model), this paper shows how a deterministic model can be formulated as a finite-population stochastic model within the framework of SSA theory and how it can be implemented in R. Simulations of the stochastic models are performed using four different SSA Monte Carlo methods: one exact method (Gillespie’s Direct method); and three approximate methods (Explicit, Binomial, and Optimized tau-leap methods). Comparison of simulation results confirms that while the time-evolution trajectories obtained from the different SSA methods are indistinguishable, the approximate methods are up to four orders of magnitude faster than the exact methods. A preprint is available on my Publication page. 	 0 Comments
Results of the St. Pat’s 10 Miler and 5K	https://www.r-bloggers.com/2008/03/results-of-the-st-pats-10-miler-and-5k/	March 23, 2008	Chris	Recently I ran the St. Pat’s 10 Miler in Atlantic City, Nj. It was my first official running event ever and I enjoyed it lot. Shortly after the race the official results have been posted on the Internet. The data did not only include the number and times of the participants but also gender and age. Looking at the finisher time distribution it shows that most runners finished at around 90 minutes:  How does age affect the finishing time?  The code to generate the images: 	 0 Comments
Plotting contours	https://www.r-bloggers.com/2008/03/plotting-contours/	March 18, 2008	Forester		 0 Comments
An Example for Just Another Gibbs Sampler (JAGS)	https://www.r-bloggers.com/2008/03/an-example-for-just-another-gibbs-sampler-jags/	March 4, 2008	Yu-Sung Su		 0 Comments
Writing Rd files in Vim	https://www.r-bloggers.com/2008/03/writing-rd-files-in-vim/	March 3, 2008	Michal	"I have made a small “translation” of a portion of “Writing R Extensions” manual about the Rd files to a vimhelp format. You can find it on vim.org, under this link http://www.vim.org/scripts/script.php?script_id=2177. If that does not work
just search for a script “rd” there. The script itself is called “rdhelp.txt”. "	 0 Comments
Ordered logistic model with varying intercepts (random effects)	https://www.r-bloggers.com/2008/03/ordered-logistic-model-with-varying-intercepts-random-effects/	March 2, 2008	Yu-Sung Su		 0 Comments
speed issue in R computing: apply() vs a loop	https://www.r-bloggers.com/2008/04/speed-issue-in-r-computing-apply-vs-a-loop/	April 23, 2008	Yu-Sung Su		 0 Comments
An R Wiki	https://www.r-bloggers.com/2008/04/an-r-wiki/	April 21, 2008	nsaunders	It’s been ages since I visited the R website, so I don’t know how long they’ve had a wiki.  It’s built using DokuWiki, one of my personal favourites. This is a great leap forward for R documentation, which is somewhat notorious for being (a) difficult to find and (b) difficult to understand when you find it.  If you’re a power R user and have a spare moment, please contribute. 	 0 Comments
The CDK/Metabolomics/Chemometrics Unconference results	https://www.r-bloggers.com/2008/04/the-cdkmetabolomicschemometrics-unconference-results/	April 7, 2008	Egon Willighagen		 0 Comments
Heteroscedasticity	https://www.r-bloggers.com/2008/04/heteroscedasticity/	April 6, 2008	vikasrawal	"If a model is estimated using the following code:
lm(y~x1+x2)->p 1. bptest(p) does the Breuch Pagan test to formally check presence of heteroscedasticity. To use bptest, you will have to call lmtest library. 2. If the test is positive (low p value), you should see if any transformation of the dependent variable helps you eliminate heteroscedasticity. Also check if the right hand side of the model is okay. 3. If 2 does not work, you can use the white’s heteroscedasticity-corrected covariance matrices to make inference. Package car has a function hccm that gives you the heteroscedasticity-corrected covariance matrix (there is a similar function in package sandwich also). coeftest(p,vcov=hccm(p)) will give you the results of the tests using this matrix. Use these results instead of summary(p). library(lmtest)
library(car)
bptest(p)
coeftest(p,vcov=hccm(p))
 Tags: GNU-R  "	 0 Comments
The disappearing mouse pointer…	https://www.r-bloggers.com/2008/05/the-disappearing-mouse-pointer/	May 31, 2008	[email protected]		 0 Comments
S3 vs S4, efficiency issues	https://www.r-bloggers.com/2008/05/s3-vs-s4-efficiency-issues/	May 23, 2008	Michal	While developing some new simulation code with S4 system I stumbled upon some big difficulties in terms of computational efficiency. That lead me to diging into archives of Rhelp and Rdevel looking for clues. I found some interesting threads that address almost exactly the same problems that I do. Read for yourself here and here, including the follow-ups by John Chambers and others. It is almost two years since I started to use S4 extensively for almost anything I develop in R. The transparency of the code and the ease of maintenance is so much greater in S4 than in S3. Not mentioning multiple inheritance, validity checks etc. Things seem to have improved since 2003 as this example, based on one of the posts mentioned above gave back then: So at least 3 times slower in S4 case. Now, on my P4 3.2Ghz, with R 2.7.0 gave  which is comparable. Nevertheless, I tried code profiling on my simulation and the output revealed that the majority of the CPU time was spent on method dispatch etc. so the difference might be still substantial. Right now my code works. Perhaps at some point I’ll port some portion to S3 and compare the results… 	 0 Comments
R2jags: A Package for Running jags from R	https://www.r-bloggers.com/2008/05/r2jags-a-package-for-running-jags-from-r/	May 21, 2008	Yu-Sung Su		 0 Comments
JAGS 1.0.2 is released	https://www.r-bloggers.com/2008/05/jags-1-0-2-is-released/	May 9, 2008	Martyn	JAGS 1.0.2 is now out. This is a patched release for the stable version, so there are no major changes in library functionality.   The major changes are: 	 0 Comments
Multiple Assignments or the Idiosyncrasies of R and SAS	https://www.r-bloggers.com/2008/05/multiple-assignments-or-the-idiosyncrasies-of-r-and-sas/	May 8, 2008	fernandohrosa	"Working on top of someone else’s code at work a couple of weeks ago I stumbled upon a piece of SAS code that went like this: Being raised in the land of open-source and having programmed in R since I was in college, that struck me at first as an implicit cast error, as check was previously declared integer. Or a misguided attempt to create an indicator variable. What surprised me even more was that not only there was no cast error as well as the code ran and gave a sensible result in the context of the program I was editing. If you still did not spot the weirdness of the statement above, try running the exact same piece of code in R (parenthesis added on purpose to return the result of the assignment): Now the explanation: due to the way assignments and comparison operators are defined across both languages, the same code in R will result in a Multiple Assignment, erasing the previous value of check and writing a character string over it, the same with Name, which will also be set to character. In SAS, where the = operator does assignments as well as comparisons (though one could argue that you could use the ‘eq’ mnemonic instead) the same expression evaluates if the CHAR variable Name is equal to the constant ‘Netter’, and the result of this logical expression (0 or 1) is then attributed to the check (NUMERIC) variable. Venables, W.N, Ripley, B. D. (2000) S Programming.
Slaughter, S.J., Delwiche, L. D. (2004) Little Sas Book.   "	 0 Comments
Error capture	https://www.r-bloggers.com/2008/05/error-capture/	May 6, 2008	Forester		 0 Comments
How to make SVN work on R-Forge and your local driver?	https://www.r-bloggers.com/2008/05/how-to-make-svn-work-on-r-forge-and-your-local-driver/	May 2, 2008	Yu-Sung Su		 0 Comments
Paper on GillespieSSA now published	https://www.r-bloggers.com/2008/05/paper-on-gillespiessa-now-published/	May 2, 2008	Mario Pineda-Krch	My paper on the GillespieSSA package has now been published in the latest volume of  Journal of Statistical Software. Check it out. 	 0 Comments
GGobi in Wikipedia	https://www.r-bloggers.com/2008/06/ggobi-in-wikipedia/	June 27, 2008	Di Cook	" GGobi now has an entry in  Wikipedia. Feel free to edit it!

 "	 0 Comments
Hack-at-it 2008	https://www.r-bloggers.com/2008/06/hack-at-it-2008/	June 27, 2008	Di Cook	The 2008 Hack-at-it was held June 22-23, just before Data Vis VI, in Bremen Germany. The main topics for the meeting were:  	 0 Comments
maps of Poland in R	https://www.r-bloggers.com/2008/06/maps-of-poland-in-r/	June 23, 2008	Michal	Bogdan Taranta (website in Polish) is developing a set of functions for drawing administrative maps of Poland. I wrote similar functionality for myself already some time ago, but only for voivodships, i.e. top-level administrative units, and also in a rather low resolution. Bogdan’s contribution seems to contain all levels, at high resolution, and including the regions’ capital cities and towns! I look forward to testing that! 	 0 Comments
Smart programming saves computing times	https://www.r-bloggers.com/2008/06/smart-programming-saves-computing-times/	June 21, 2008	Yu-Sung Su		 0 Comments
One R tip a day	https://www.r-bloggers.com/2008/06/one-r-tip-a-day/	June 21, 2008	Michal	I just encountered this website http://onertipaday.blogspot.com/: One R tip per day. Very nice collection of R tips. 	 0 Comments
Speed up R! Make R Run Faster!	https://www.r-bloggers.com/2008/06/speed-up-r-make-r-run-faster/	June 16, 2008	Yu-Sung Su		 0 Comments
Paper describing the weaver package published in Computational Statistics	https://www.r-bloggers.com/2008/06/paper-describing-the-weaver-package-published-in-computational-statistics/	June 14, 2008	seth	"It seems like a lifetime ago that I developed the weaver package for caching code chunks in Sweave documents.  The paper that I presented at the DSC 2007 has finally been published in Computational Statistics.  The title is Caching Code Chunks in Dynamic Documents: The weaver package.  Here’s the abstract: Authoring dynamic documents can become tedious for authors when a document contains one or more time consuming code chunks and each edit requires reprocessing all of the document. We introduce the weaver package that allows computationally expensive code chunks to be cached in order to speed up the edit/process/review cycle for dynamic documents authored using the Sweave framework.
 
And here a link to an unofficial pdf and the weaver package.
 Technorati Tags: Bioconductor, R  "	 0 Comments
Unmessing my pdfs.	https://www.r-bloggers.com/2008/06/unmessing-my-pdfs/	June 1, 2008	[email protected]		 0 Comments
R books	https://www.r-bloggers.com/2008/07/r-books/	July 30, 2008	Rob J Hyndman	 Amazon.com Widgets 	 0 Comments
How to Recover the Missing X(1) for the USL Scalability Model	https://www.r-bloggers.com/2008/07/how-to-recover-the-missing-x1-for-the-usl-scalability-model/	July 29, 2008	Neil Gunther		 0 Comments
download option price data from Yahoo	https://www.r-bloggers.com/2008/07/download-option-price-data-from-yahoo/	July 29, 2008	Quantitative Finance Collector		 0 Comments
Quantile Regression	https://www.r-bloggers.com/2008/07/quantile-regression/	July 29, 2008	Quantitative Finance Collector		 0 Comments
What is faster, Winbugs or Openbugs?	https://www.r-bloggers.com/2008/07/what-is-faster-winbugs-or-openbugs/	July 14, 2008	Andrew Gelman		 0 Comments
Guerrilla Data Analysis Class – Seats Still Available	https://www.r-bloggers.com/2008/07/guerrilla-data-analysis-class-seats-still-available/	July 14, 2008	Neil Gunther		 0 Comments
Get your R on	https://www.r-bloggers.com/2008/07/get-your-r-on/	July 14, 2008	dan	"useR! CONFERENCE, AUGUST 12-14 2008, DORTMUND GERMANY  
 Impressive statistical computing types like Andrew Gelman, Gary King, and others will be presenting at this year’s useR! conference. Decision Science News might just have to hop over and check it out. The program looks great. Those interested in learning R might be interested in our Decision Science News R tutorials one and two. About the Conference useR! 2008, the R user conference, takes place at the Fakultät Statistik, Technische Universität Dortmund, Germany from 2008-08-12 to 2008-08-14. Pre-conference tutorials will take place on August 11.
The conference is organized by the Fakultät Statistik, Technische Universität Dortmund and the Austrian Association for Statistical Computing (AASC). It is funded by the R Foundation for Statistical Computing. Following the successful useR! 2004, useR! 2006, and useR! 2007 conferences, the conference is focused on 1. R as the “lingua franca” of data analysis and statistical computing,
2. providing a platform for R users to discuss and exchange ideas how R can be used to do statistical computations, data analysis, visualization and exciting applications in various fields,
3. giving an overview of the new features of the rapidly evolving R project. As for the predecessor conference, the program consists of two parts: 1. invited lectures discussing new R developments and exciting applications of R,
2. user-contributed presentations reflecting the wide range of fields in which R is used to analyze data. A major goal of the useR! conference is to bring users from various fields together and provide a platform for discussion and exchange of ideas: both in the formal framework of presentations as well as in the informal part of the conference in Dortmund’s famous beer pubs and restaurants. Prior to the conference, on 2008-08-11, there are tutorials offered at the conference site. Each tutorial has a length of 3 hours and takes place either in the morning or afternoon. "	 0 Comments
UseR!2008 aftermath	https://www.r-bloggers.com/2008/08/user2008-aftermath/	August 30, 2008	Michal	Earlier in August I was on the R user conference which this year took place in Dortmund. It was a quite an exciting event gathering around 500 people from around the globe and featuring 170 presentations and talks. Topics varied from new developments in the R system, newly implemented statistical methods as well as plethora of very interesting, and often visually nice, applications. You can find the list of abstracts and also slides for majority of the talks on the conference’s website.  I left the conference, I’m sure similarly to others, with a wealthy bag of new R “tricks”. The next UseR is planned to take place next summer in Rennes (France). The website is already there. Among the available slides are the slides from my talk which was derived from my recent paper on coordination in dynamic social networks in heterogeneous groups. This talk focused on a computer simulation I performed for the paper. You can find the slides here. Additionally here (QuickTime, 37Mb) is a movie based on the simulation I describe there. More on the paper itself soon. 	 0 Comments
R Design Flaws #1 and #2:  A Solution to Both?	https://www.r-bloggers.com/2008/08/r-design-flaws-1-and-2-a-solution-to-both/	August 25, 2008	Radford Neal	I’ve previously posted about two design flaws in R. The first post was about how R produces reversed sequences from a:b when a>b, with bad consequences in “for” statements (and elsewhere).  The second post was about how R by default drops dimensions in expressions like M[i:j,] when i:j is a sequence only one long (ie, when i equals j). In both posts, I suggested ways of extending R to try to solve these problems.   I now think there is a better way, however, which solves both problems with one simple extension to R.  This extension would also make R programs run faster and use less memory. Recall the problems, and the solutions I proposed previously… To stop sequences from reversing, we need a new operator to use rather than 1:n, which suffers from the reversal problem when n is zero (which can’t be changed for compatibility reasons).  I suggested 1:>:n.  To stop dimenions from being dropped, I suggested using semicolons rather than commas to separate subscripts. My new suggestion is that in the most common case where the indexing vector is a sequence, we could use the same new operator introduced to solve the reversing problem — ie, we write M[1:>:n,] to get the first n rows, and define it so that the result isn’t converted to a vector when n is one. Now, this may seem like it won’t work.  If 1:>:n returns a vector, then when n is one, it’s a vector of length one, which has to (by default) lead to the dimension being dropped if ordinary subscripting by scalars is to work (since scalars in R are really vectors of length one). The solution is for 1:>:n to not return a vector, but instead return a new data type that just records its two operands.  This new data type — perhaps it could be called an indexing pair — would have to be recognized by “for” statements and by the subscripting operator.  A dimension indexed by such an indexing pair would never be dropped.  One could add an operator :: for the ascending indexing pair operator (but it unfortunately has no obviously mnemonic descending counterpart). One disadvantage of this solution is that it doesn’t address the dropped dimension problem in the general case where the vector index may not be a sequence.  But indexing by an ascending sequence is by far the most common case, so having to write drop=FALSE for the others may be OK.  (However, perhaps knowledge of the drop=FALSE option would be less common if it is needed less often.) Similary, this solution doesn’t address the problem of reversing sequences outside the context of for loops and subscripting, but those are the most common uses. One extra advantage of introducing indexing pairs is that they will take up a trivial amount of storage.  In contrast, if you use 1:1000000 to iterate a million times in a for loop, R will allocate 4 Megabyes of storage to hold this sequence.  Producing this sequence also takes time, of course.  It would be possible for R to avoid this cost when using 1:1000000 in a for loop, by treating this combination specially, but it doesn’t (in version 2.4.1 at least): Notice how the memory usage goes up by 3.9 Megabytes during the for loop. Another extra advantage of introducing :>: (or ..) is that the precedence of this operator could be made lower than that of any other operator (with no loss, since an indexing pair wouldn’t be a valid operand for any other operator).   Expressions like 1:>:n-1 would then do what the programmer meant (unlike 1:n-1). So, does anyone see any flaws in this solution? 	 0 Comments
More stats sites	https://www.r-bloggers.com/2008/08/more-stats-sites/	August 20, 2008	John Johnson		 0 Comments
Visualize Copulas	https://www.r-bloggers.com/2008/08/visualize-copulas/	August 20, 2008	Quantitative Finance Collector		 0 Comments
Design Flaws in R #2 — Dropped Dimensions	https://www.r-bloggers.com/2008/08/design-flaws-in-r-2-%e2%80%94-dropped%c2%a0dimensions/	August 19, 2008	Radford Neal	"In a comment on my first post on design flaws in the R language, Longhai remarked that he has encountered problems as a result of R’s default behaviour of dropping a dimension of a matrix when you select only one row/column from that dimension.   This was indeed the design flaw that I was going to get to next!  I think it also points to what is perhaps a deeper design flaw. I’ll give a toy example of the problem, which shows up regularly in real (but more complicated) contexts.  Consider the following function, whose arguments are a matrix X and a vector s of column indexes, and which returns a vector containing the  Euclidean norm of each row of X, looking only at columns in s: Here’s an example of its use: Perhaps it would be interesting to see how the norms get smaller as we drop leading dimensions: Oops…  When the loops gets to where k has the value 4, the subset k:4 contains just one dimension.  When R comes to evaluating X[,s], it doesn’t return a matrix with one column, but rather a vector.  The apply function doesn’t work on vectors, so we get an error message rather than the answer. There’s a fix.  The “["" operator takes a ""drop""argument, which defaults to TRUE, giving the behaviour above, but which can be explicitly set to FALSE to disable conversion from a matrix to a vector in these circumstances.  If we modify the function as follows: We get the right answers: There are two problems with this fix.  One is that in some programs, one needs to put drop=FALSE in numerous places, making the code rather hard to read.  The more serious problem, and what makes this a real design flaw, is that writing code without drop=FALSE is so much easier that it's often left out even when it is needed.  Indeed, since the errors typically occur only for extreme cases, it's easy for the programmer to not realize there's a problem. (As an aside, this is another context where the reversing sequences problem arises — if n is 0, subset.norms2(M,1:n) should produce all zeros, but doesn't, not because of a bug in subset.norms2 but because 1:0 doesn't produce the empty sequence.) Can we solve the dimenion dropping problem by just changing the default for drop from TRUE to FALSE?  Of course not, since this would break too many existing programs.  But even if backward compatibility weren't a problem, this wouldn't be a good solution, since the times when we want drop to be TRUE are even more numerous than when we want it to be FALSE. Look at the following, for instance: If drop defaults to FALSE, ordinary subscripting with single indexes for both dimensions will give a 1x1 matrix!  Now, R will treat matrices as vectors (and scalars) as needed in many contexts, but propagating a dim attribute (what marks something as a matrix) all over the place will sooner or later cause problems. What can be done at this point?  I'm not sure.  The best idea I can come up with is to let subscripts be separated by semicolons as well as commas, with use of a semicolon signaling that drop will be FALSE.  In other words, M[i;j] would be equivalent to M[i,j,drop=FALSE], but much more concise.  People could start using the semicolon whenever they're not trying to extract single elements, without breaking old programs. The root cause of the dropped dimension problem seems to be that R does not have scalars.  Instead, what looks like a scalar is actually a vector of length one.  To R, there is no difference between M[2,3] and M[2:2,3:3].  So there's no way for the first of these to drop dimensions and the second to retain them. It's probably hopeless to try to change now, but I think it would have been better for scalars to be treated as vectors or converted to vectors as necessary, without pretending that they're always vectors.  Not having real scalars may seem like a unification or simplification, but it just creates problems.  These start with the first thing a new user sees on trying R out, which is likely to be something like this: Good.  R knows how to add 2 and 2.  But what's the ""[1]"" doing there? "	 0 Comments
High-performance computing in R at useR! 2008	https://www.r-bloggers.com/2008/08/high-performance-computing-in-r-at-user-2008/	August 12, 2008	Mario Pineda-Krch	The useR! 2008 meeting is about to commence. Although I am not able to go this year I will be keeping a close eye on the talks and slides that (I assume) will be posted.  Last years useR! meeting (which I attended) was a great experience and considering the list of participants for this years meeting, it is bound to be a very interesting meeting. Dirk Eddelbuettel has already posted the slides to his talk entitled “Introduction to High-Performance R” on his web site. Clearly, anyone using R on, say, computer clusters would find this highly relevant.  On a side note, it would be interesting to have some idea of how commonly R is used on computer clusters. I have been using R on our in-house cluster for a few years now, and although it may sound a bit counterintuitive to run a high-level interpreted language such as R in a high-performance computing environment such as a cluster (most people would use a low-level compiled language like C for example), using R has the big advantage of allowing very fast prototyping. In other words, although the R code runs slower on the cluster than native C code would do it takes much less time to code it. Of course, for certain classes of models there is no other option that using C (or a similar low-level language), but if you can get away with R, a cluster allows you to run your serial code (or parallelized R code) in parallel with potentially huge time savings.  This is from the “Mario’s Entangled Bank” blog ( http://pineda-krch.com ) of Mario Pineda-Krch, a theoretical biologist at the University of California, Davis. 	 0 Comments
Updated version of GGobi: 2.1.8	https://www.r-bloggers.com/2008/08/updated-version-of-ggobi-2-1-8/	August 12, 2008	Hadley Wickham		 0 Comments
Process Simulation in R	https://www.r-bloggers.com/2008/08/process-simulation-in-r/	August 12, 2008	Quantitative Finance Collector		 0 Comments
R-code for Vasicek estimation	https://www.r-bloggers.com/2008/08/r-code-for-vasicek-estimation/	August 8, 2008	Quantitative Finance Collector		 0 Comments
Link C with R	https://www.r-bloggers.com/2008/08/link-c-with-r/	August 7, 2008	Yu-Sung Su		 0 Comments
Design Flaws in R #1 — Reversing Sequences	https://www.r-bloggers.com/2008/08/design-flaws-in-r-1-%e2%80%94-reversing%c2%a0sequences/	August 6, 2008	Radford Neal	"The R language for statistical computing has become the standard for academic statistical research, for the very good reason that it’s better than the alternatives.  It’s far from perfect however.  I could come up with a long “wish list” of desired  features it lacks, but that’s not what I’ll do in this series of posts.  Instead, I’ll concentrate on real design flaws — aspects of the language that make it all too easy to write unreliable programs.  Many of these are inherited from R’s predecessors, S and S-Plus, and may even originate in the very earliest version of S, which was seen more as a calculator than as a programming language. By far the most commonly-encountered design flaw in R is in the “:” operator for producing sequences, whose most common use is in “for” statements. Here’s an example: This approximates a definite integral by summing up the areas of n trapezoids defined using a grid of n+1 points from a to b. The area of one trapezoid is the width of its base, (b-a)/n, times the average of the heights of its two side, (f(left)+f(right))/2, where left and right are adjacent grid points. The function f is used twice at every grid point except for the very first and very last. To avoid wasting time, the program evaluates f only once at these n-1 middle points. This is handled by the “for” loop. I’ve avoided a minor design flaw in R here, by using 1 : (n-1) to define the range of values for j in the loop, which should be 1, 2, …, n-1.  Beginning R programmers usually write 1:n-1, and only find out after hours of debugging that this is interpreted as (1:n)-1, and so produces the sequence 0, 1, …, n-1.  But at least they usually realize they have a bug. The main problem is far more insidious.  The program above works fine, as long as n is greater than 1.  And it usually is, since you usually want to approximate the integral using more than one trapezoid, if you’re interested in getting an accurate result.  But n=1 is an entirely valid value, and sooner or later will come up, perhaps when a user cares more about speed than accuracy, or perhaps when another program automatically chooses this value of n for some reason.  Here’s an illustration of what happens: The exact answer is 63.  Even using n=2, one gets pretty close.  But 202.5 when using n=1?  The correct value for the approximation using one trapezoid is 3 x (32+62)/2 = 67.5. What’s happened?  Well, when n=1, the sequence 1 : (n-1) defining the range in the “for” statement is 1:0. Rather than interpreting this as the empty sequence, R decides that you must have wanted a sequence of decreasing values, namely 1 and 0.  So the body of the “for” loop is done twice, rather than not at all, with the unfortunate result seen above. Automatically reversing the direction of the sequence p:q when p>q must have seemed like a convenience to the original designers, who probably had in mind the situation where this expression is typed in directly by a user, who knows whether p>q or p. It’s a terrible idea in a programming language, however. Almost always, the logic of the program will require either an increasing sequence or a decreasing sequence, not one direction some times and the other direction other times.  And in many programs, the empty sequence will be needed as a valid limiting case, and of course is never generated with automatic reversal. Now, it clearly is possible to write working programs despite this design flaw.  For a sequence starting at 1, seq(length=n) will work.  You might think that seq(1,n,by=1) would also work, but instead it produces an error message when n=0.  You can also use an “if” statement to bypass the “for” loop when it shouldn’t be done, as below: Unfortunately, all these solutions are more work to write, and harder to read.  It’s much easier to write programs that don’t guard against problems of reversing sequences (which crop up in places other than “for” loops too).  The result is programs that mostly work, most of the time. Can this flaw in R be fixed?  At this point, changing the specification of the “:” operator is impossible — it would invalidate too many existing programs.  New operators could be introduced, however. For example, “:>:” could generate an increasing sequence, with a:>:b producing the empty sequence if a>b.  Similarly, “:” and “
         


Related
ShareTweet




To leave a comment for the author, please follow the link and comment on their blog:  Radford Neal's blog » R Programming.

R-bloggers.com offers daily e-mail updates about R news and tutorials about learning R and many other topics. Click here if you're looking to post or find an R/data-science job.

Want to share your content on R-bloggers? click here if you have a blog, or  here if you don't.
 "	 0 Comments
Welcome to FOSS Trading	https://www.r-bloggers.com/2008/09/welcome-to-foss-trading/	September 28, 2008	Joshua Ulrich	"
 "	 0 Comments
Group-level variances and correlations	https://www.r-bloggers.com/2008/09/group-level-variances-and-correlations/	September 23, 2008	Yu-Sung Su		 0 Comments
Design Flaws in R #3 — Zero Subscripts	https://www.r-bloggers.com/2008/09/design-flaws-in-r-3-zero-subscripts/	September 21, 2008	Radford Neal	Unlike the two design flaws I posted about before (here, here, and also here), where one could at least see a reason for the design decision, even if it was unwise, this design flaw is just  incomprehensible.  For no reason at all that I can see, R allows one to use zero as a subscript without triggering an error.  (Remember that in R, indexes for vectors and matrices start at one, not zero.) This is of course a terrible decision, because it makes debugging harder, and makes it more likely that bugs will exist that have never been noticed. So what does R do with a zero subscript, seeing as it’s meaningless?  It just ignores it, which is possible because it views all numeric subscripts as vectors, that extract or replace a set of elements, not necessarily just one.   So R simply removes all zeros from a vector used as a subscript, producing a shorter vector. Here’s what happens (with the current version of R, 2.7.2): Contrast this with what happens when you use a subscript that is too large: Extending vectors automatically when an assignment is made beyond the end can obviously be useful (though it might be wiser not to).  Returning NA when extracting an element beyond the end is also a sensible action (though signalling an error immediately might be more useful for debugging). And negative subscripts are usefully defined as referring to their complement. But what possible use is there for ignoring zero subscripts rather than signalling an error? It’s perhaps belabouring the obvious, but let me explain that signalling an error when a zero subscript is used is desirable because this is a very common sort of program bug.  It can easily arise when a program is scanning backwards through the vector elements, and goes one step too far.  It can also easily arise when data is initialized to zeros, with the intent to replace the zeros with something sensible later, but actually some zeros are never replaced. The way R behaves when zero is used as a subscript when replacing elements is particularly bad, since doing nothing at all can easily lead to an apparently working program that produces wrong answers.  (The behaviour of returning an empty vector when zero is used as a subscript when extracting an element is more likely to produce an error later on, so that at least the problem will be evident.) So what should be done?  That’s easy — change R so that use of zero as a subscript produces an immediate error.  That’s trivial to do (mixing positive and negative subscripts produces an immediate error now, so the apparatus for it must be there).  Might that break some existing programs?  Yes, it will.  But 99.9% of those programs are already broken.  The users just don’t know it, thinking that the answers they get are correct when they’re not.  The remaining 0.1% of these broken programs were written by really stupid programmers who thought that exploiting an obscure and unwise feature in order to produce a really hard-to-understand program was a good idea.  It wasn’t. Along with this, R should be changed so that using NA as a subscript when replacing elements in a vector also produces an error.  What to do with NA subscripts used to extract elements is a little bit harder to decide, but it seems to me that something about the following is a bit funny: 	 0 Comments
pmin and pmax	https://www.r-bloggers.com/2008/09/pmin-and-pmax/	September 16, 2008	R Tips	Did you know that there are multiple versions of the min and max function.  make sure that you are using the right one.  pmin and pmax are the ‘parallel’ versions of the min and max function, meaning that they can take vector arguments and return vectors back.  Much better than setting up your own apply function.  So make sure that you are using the right version. read more 	 0 Comments
Highlights from useR!	https://www.r-bloggers.com/2008/09/highlights-from-user/	September 11, 2008	Leslie Hawthorn		 0 Comments
rgraph6 on R-Forge	https://www.r-bloggers.com/2008/09/rgraph6-on-r-forge/	September 4, 2008	Michal	I have moved my rgraph6 R package to R-Forge. R-Forge is a website that facilitates development of R packages by providing services for version control (through Subversion), automatic checking and building of the packages including binaries for Windows and MacOS, as well as for collaboration with other R users/developers. The rgraph6 package has been already available through my private mini-repository. It provides an interface to a pretty compact format for storing undirected graphs as sequences of printable ASCII characters which is quite useful for handling large libraries of undirected graphs. The format itself is due to Brendan McKay. The detailed description of it is available here and is also included within the rgraph6 package. Two crucial functions of the package are written in C. As my knowledge of C is rather low it might be far from perfect. If you know C well you are more than welcome to have a look at the sources and suggest some improvements. I believe that one of the crucial things is checking for the size of the character sequences that are converted to binary numbers and then to decimal. I’m not sure whether it will work for arbitrary network sizes. I plan to put a public advertisement on R-Forge to look for people who would be willing to do that. Actually that was one of the reasons I opened the package development to others through R-Forge. So don’t be shy and go ahead!   From now on I will not release any future version of rgraph6 through this website. All will be distributed through rgraph6‘s website on R-Forge. The older versions will still be available though. You can install the current version from R-Forge directly from R with: 	 0 Comments
evolutionary algorithm optimization	https://www.r-bloggers.com/2008/09/evolutionary-algorithm-optimization/	September 4, 2008	Quantitative Finance Collector		 0 Comments
How do you measure a major league slugger?	https://www.r-bloggers.com/2008/09/how-do-you-measure-a-major-league-slugger/	September 1, 2008	mike	" I gave a talk last month at SAP Labs in Palo Alto, along with Jim Porzak of ResponSys, introducing the R Statistical Language to a Business Intelligence interest group.  The goal was to highlight how open source tools, like R, can be used to build predictive models.  The example I gave centered around baseball and a simple question:  how do you measure a baseball slugger? Michael Lewis, in  Moneyball , described how the baseball analyst Bill James was frustrated by the fact that major league hitters were consistently rated by their batting averages.  James wrote:   “a hitter should be measured by his success in that which he is trying to do, … create runs. It is startling, when you think about it, how much confusion there is about this.” – Bill James, 1979 Baseball Abstract However, since teams create runs, not batters, the only way to connect batting statistics with runs is to use team averages.  The idea is that if we know which statistics predict runs at the team level, these statistics could be used to measure individual hitters. I decided to test the value of three batting statistics myself — batting average, slugging percentage, and OPS (on-base plus slugging) — and see how well they predicted team runs, using MLB team data for the years 2000-2005 (available from baseball-databank.org). The results are shown in the three scatter plots below, and no surprise, Bill James is right:  a team’s overall batting average (top-most chart) is a comparatively poor predictor of how many runs it will score in an average game.  Slugging percentage (middle plot) is a slightly better predictor, and OPS (bottom plot) is the best of the three statistics I looked at:  it has a 0.95 correlation with runs scored (the r shown in the upper right corner of the plots is the Pearson correlation coefficient, the red lines represent least-squares fits to the points).

I highlighted a couple of interesting outliers in the top batting average plot: teams that achieved a high level of scoring with a comparatively low team batting average.  Who were these teams?  None other than Billy Beane’s 2000 and 2001 Oakland Athletics.  This suggests that the As management may have found excess value in fielding players who — despite having slightly lower batting averages — were capable of generating runs. These results show what Bill James and others already know:  that a baseball slugger should not be measured by his batting average, but by OPS or other hybrid statistics that best correlate with his success at generating runs.  There is nothing novel about the results of my analysis.  But what I hope is novel is showing how it can be done using open source tools (R and MySQL), free data (baseball-databank.org), and a few lines of code ( sabermetrics using R page).  "	 0 Comments
Plotting Math in R	https://www.r-bloggers.com/2008/10/plotting-math-in-r/	October 29, 2008	Yu-Sung Su		 0 Comments
The most popular programming languages in 2008	https://www.r-bloggers.com/2008/10/the-most-popular-programming-languages-in-2008/	October 26, 2008	Yu-Sung Su		 0 Comments
Working with directories	https://www.r-bloggers.com/2008/10/working-with-directories/	October 24, 2008	R Tips	"Something quite annoying to me is when I get an R script and I have to change all of the file references in the script. I get something like this this:


source(""C:\\Documents and Settings\\UserName\\Data\\...\\File1.R"")

data<-read.table(""C:\\Documents and Settings\\UserName\\Data\\...\\SomeData.dat"")


Besides these being unsightly, file references are also a pain to change.  Usually I just delete all but the filename.  How can I get away with that? because R remembers what directory you are dealing with and makes all file references relative. so the above becomes:
 read more "	 0 Comments
Rmetrics – Basics of Option Valuation	https://www.r-bloggers.com/2008/10/rmetrics-basics-of-option-valuation/	October 22, 2008	Quantitative Finance Collector		 0 Comments
What I’ll be presenting at O’Reilly Money Tech 2009	https://www.r-bloggers.com/2008/10/what-ill-be-presenting-at-oreilly-money-tech-2009/	October 21, 2008	mike	" (April 2009 Update:  Unfortunately, The Money Tech Conference was indefinitely postponed, but fortunately I will be presenting a version of this talk in July at OSCON 2009). I’ve been invited to speak at O’Reilly’s Money Tech conference this coming February 4-6th in New York City and thought I’d share the abstract for my talk here.  I’ll likely be in New York for several days, if you’d like to get together to chat about data drop me a line! My talk is entitled “Open Source Analytics: Visualization and Predictive Modeling of Big Data with the R Programming Language”
 ABSTRACT Just as the explosion of online data catalyzed the development of
storage technologies such as Hadoop, new challenges in data analytics
– turning terabytes into actionable insights — demand new tools.  R,
an open-source language for statistical computing and graphics, is an
extensible, embeddable, and industry-strength solution for analytics.
In this session, I showcase R’s power by building predictive models
for Brazilian soybean harvests and baseball slugger salaries. DESCRIPTION The economics of data aggregation and analysis are being disrupted by
falling costs for storage and CPU power, the continuing shift of
business processes online, and the deluge of data that is being
generated as a consequence. Satellite images, SEC filings, supply chain data (RFID data streams),
online prices, and newsgroup content represent just a few of the data
sources that hold potential for predictive modeling of markets. Much of this data does not fit within existing paradigms for business
analysis: either its size overwhelms traditional desktop tools such as
Excel, or else its unique dimensions (such as geocodes) prevent its
being pipelined into more powerful, but narrowly designed, analysis
tools.  Finally, closed-source tools cannot keep pace with the leading
edge of innovation in statistical and machine-learning algorithms. Enter the open source programming language R.  R has been dubbed the
lingua franca for statistical computing and graphical analysis, with a
pedigree tracing back several decades at Bell Labs.  Though its
million-plus users are concentrated within academia, R is gaining
currency within several high-profile quantitative analysis groups,
including Google’s Customer Insights team and Barclays Global
Investors.  In addition, R’s extensibility via user-contributed
packages has spawned an active developer community. In this session, I will focus on applying R’s powerful visualization
tools to guide the construction of predictive models, using the kind
of large, multidimensional data sets that increasingly confront
quantitative analysts.  Along the way, I will highlight R’s packages
for inferential statistics, its compact modeling syntax, and its ease
of connectivity with persistent data stores. The two specific examples I will discuss are: – an analysis of NASA’s Landsat imagery of Brazil’s center-west
agricultural regions to detect correlates for soybean harvest yields,
and a derived predictor of the Brazilian soybean market based in part
on these correlates. – a validation of Bill James’ sabermetrics approach to batting
performance using 30 years of Major League Baseball statistics, and a
derived predictor for batters’ salaries. For all of its strengths, R has an admittedly steep learning curve.
While source code for the examples will be provided online, this talk
will emphasize techniques and working examples over technical details.
The goal of this session is to give quantitative analysts the courage
to invest in learning the R language, by showcasing R’s power,
highlighting its features, and providing examples of its use for
innovative applications. "	 0 Comments
Another solution to the R to Word table problem	https://www.r-bloggers.com/2008/10/another-solution-to-the-r-to-word-table-problem/	October 17, 2008	John Johnson	You'll need the package svViews (part of the SciViews series of packages) for this one. Basically, what the above does is create a temporary file to hold the RTF document, then write the RTF code to recreate the table, then add the RTF file to Word. The table can then be manipulated as desired. Unfortunately, as the SciViews is Windows only, the automation part of this process is Windows only, but the file creation is not. Since myfile is a string variable, an R function can be written to execute a series of Applescript commands on the Macintosh. 	 0 Comments
Using R in consulting: playing nice with Microsoft Word	https://www.r-bloggers.com/2008/10/using-r-in-consulting-playing-nice-with-microsoft-word/	October 17, 2008	John Johnson		 0 Comments
Time series packages on R	https://www.r-bloggers.com/2008/10/time-series-packages-on-r/	October 16, 2008	Rob J Hyndman	There is now an official CRAN Task View for Time Series. This will replace my earlier list of time series packages for R, and provide a more visible and useful entry point for people wanting to use R for time series analysis. If I have missed anything on the list, please let me know. 	 0 Comments
Econometric tools for performance and risk analysis	https://www.r-bloggers.com/2008/10/econometric-tools-for-performance-and-risk-analysis/	October 13, 2008	Quantitative Finance Collector		 0 Comments
R’s working directory	https://www.r-bloggers.com/2008/10/r%e2%80%99s-working-directory/	October 3, 2008	Michal	"
Do you usually start R with a desktop icon or some other shortcut? Are you tired of using setwd and getwd each time after you start R to get the working directory correctly? If so, then your days of suffering might be just coming to an end.
 
Having the working directory set correctly is very convenient. You can both read and write files to the proper place without typing (on Windows, usually very long) path names. There are couple of solutions:
 
One way to achieve this is to have a setwd function call at the top of your scripts. You then run it every time you do the computations in that script. For example to have at the top of a file a following line:
 
It is a nice approach, but things get complicated if you move files to different computers, say from home to your office, and have different directory structures, disk names etc. Of course you can change it every time. Or perhaps keep couple of versions and have all of them but one commented, for example:
 
Which is also OK, but for me is too much micromanagement. Also, it becomes a problem if the script is not intended for interactive use.
 
An alternative might be work with Windows shortcuts for starting R. In shortcut’s properties there is a “Start in” field in which you can put the path to the desired folder. If you start R with the modified icon then R’s working directory will be correctly set. With that approach you can have, say, couple of R icons on your desktop, each to different project folders.
 
This is convenient unless you work on 10 projects. Each time you may have to create yet another shortcut.
 
Another approach is to set the environment PATH variable. If you add the path to R’s executable to it then you will be able to start R from whatever directory in the system you want.
 
To modify the PATH variable you need to right-click on “My Computer” and select “Properties” then go to “Advanced” tab and “Environment variables” button. The way to modify the PATH variable depends on where you installed R. Usually it is something like c:program filesRR 2.7.0bin.
 
I use this approach myself with Total Commander and its command line. Wherever I am on the disk I can start R in that directory just bu typing rgui and pressing Enter. You can also use Windows Console (cmd) for that.
 
Yet another way is to add a command to your context menu (the one appearing when you right-click on things). By right-clicking on a folder and choosing “R” option you can start R with that folder set as the working directory.
 
To set up such a command you have to modify Windows Registry and will require, I believe, administrative privileges. Look here for details how to do this.
 
Any other ideas or suggestions? "	 0 Comments
First and easy steps with R and Sweave	https://www.r-bloggers.com/2008/11/first-and-easy-steps-with-r-and-sweave/	November 28, 2008	:)-k		 0 Comments
Sweave.sh plays with cacheSweave	https://www.r-bloggers.com/2008/11/sweave-sh-plays-with-cachesweave/	November 26, 2008	Gregor Gorjanc		 0 Comments
Calculating an N50 from Velvet output	https://www.r-bloggers.com/2008/11/calculating-an-n50-from-velvet-output/	November 25, 2008	Jeremy Leipzig		 0 Comments
Use anchoring to lose years off your age	https://www.r-bloggers.com/2008/11/use-anchoring-to-lose-years-off-your-age/	November 21, 2008	Diego		 0 Comments
Setting up Textmate to use R	https://www.r-bloggers.com/2008/11/setting-up-textmate-to-use-r/	November 21, 2008	:)-k		 0 Comments
Plot symbols in R	https://www.r-bloggers.com/2008/11/plot-symbols-in-r/	November 18, 2008	Yu-Sung Su		 0 Comments
Call C from R and R from C	https://www.r-bloggers.com/2008/11/call-c-from-r-and-r-from-c/	November 17, 2008	Forester		 0 Comments
Multivariate dependence with copulas	https://www.r-bloggers.com/2008/11/multivariate-dependence-with-copulas/	November 17, 2008	Quantitative Finance Collector		 0 Comments
Sweave Engine for TeXShop	https://www.r-bloggers.com/2008/11/sweave-engine-for-texshop/	November 17, 2008	cameron	Sweave is an awesome utility for including the output of R code in a LaTeX document which I have started using regularly. TeXShop is my favorite editor/viewer for LaTeX under Mac OS X for many reasons. One of which is the speed of the edit -> compile -> view process which it enables.  Therefore, I have been reluctant to use Sweave in the past because of the extra LaTeX compilation step that is not supported by default in TeXShop. Recently, my friend Charlie Sharpsteen first showed me this way to get around this with a custom “engine”. The engine file looke like this: Sweave.engine  Place this file in ~/Library/TeXShop/Engines/, make sure PATH includes the location of you TeX distribution. Make sure executable permissions (like 744) are set on both files or TeXShop will give an error message.  Now open (or re-open) TeXShop and select “Sweave” from the list at the top left.  Now you can use command-t to compile Sweave and view your document in one step! Thanks Chuck!  	 0 Comments
Using the booktabs package with Sweave and xtable	https://www.r-bloggers.com/2008/11/using-the-booktabs-package-with-sweave-and-xtable/	November 16, 2008	cameron	The xtable package in R can output R data as latex tables. Used in conjunction with Sweave it is possible to automatically generate tables in a report.  Needless to say this provides a really appealing possibility of never typing data into a table again.  The only problem is that I think the default latex tables don’t look that great. Enter booktabs… The booktabs package in latex makes really nice tables, especially if there is math in your table that might run up against the regular \hline in the tabular environment. Buried away in the xtable documentation are some options that make this possible.  See the example below:  To compile, Sweave needs to be run on it.  Running Sweave is an pre-processing step in the latex compilation process  and then check out your awesome new table that you didn’t type!  	 0 Comments
R function to reverse and complement a DNA sequence	https://www.r-bloggers.com/2008/11/r-function-to-reverse-and-complement-a-dna-sequence/	November 13, 2008	fabiomarroni	"Warning!!
This post is intended for documentation only. I would like to remind everyone (me in first place!) that the comp() function of the (seqinr) package can complement a DNA sequence, and rev() function of Rbase can reverse a character vector. Using a combination of the two you can reverse, complement, and reverse complement sequences as well.  Complements (and eventually reverse) a DNA sequence, which has to be inserted as a character vector, no matter if lower or uppercase.
Limitations:
1) Cannot work with RNA, only DNA
2) Cannot reverse without complementing. You can complement and reverse complement, but not just reverse.Author Fabio Marroni (http://www.fabiomarroni.altervista.org/)
Arguments:
x:character vector, the DNA sequence.
rev: logical. If TRUE, the function will return the reverse complemente, if FALSE, it will return the complementary sequence. The default value is TRUE. Value:
The complemented (and eventually reverse) sequence, as a character vector. There are several web sites which can easily complement and reverse a DNA sequence (and RNA as well).
The advantage of using this piece of code is that it is possible to automatically reverse complement a series of sequences: I had several primers to reverse/complement and I didn’t want to copy and paste them every time. Only now I found a web site in which you can copy and paste the primers on different lines and get the reverse complement of each primer on a different lines. You may want to try it: http://arep.med.harvard.edu/cgi-bin/adnan/revcomp.pl.
However, the versatility of R allows you to automatically retrieve the reverse complement and (for example) save each of the primer in a different text file.
Also, there is a nice library in R (seqinr) which can reverse complement and perform several other tasks (http://cran.r-project.org/web/packages/seqinr/index.html). Since my R programming skills are “limited”, comments and suggestions are welcome!
  Thanks to rhi for providing code for complementing without reversing. I paste it below.  "	 0 Comments
Modeling Financial Time Series with S-PLUS	https://www.r-bloggers.com/2008/11/modeling-financial-time-series-with-s-plus/	November 12, 2008	Quantitative Finance Collector		 0 Comments
Saving a Workspace	https://www.r-bloggers.com/2008/11/saving-a-workspace/	November 11, 2008	R Tips	R will can save the users workspace at the end of a session so that he can take it up again where he left off. I personally don’t like doing this but there are times when one would want to save their work, especially after complex and time consuming computations. read more 	 0 Comments
Quantitative Risk Management R package	https://www.r-bloggers.com/2008/11/quantitative-risk-management-r-package/	November 5, 2008	Quantitative Finance Collector		 0 Comments
R/Finance 2009: Applied Finance with R	https://www.r-bloggers.com/2008/12/rfinance-2009-applied-finance-with-r/	December 30, 2008	Joshua Ulrich	"
 "	 0 Comments
New stuff in the gdata R package	https://www.r-bloggers.com/2008/12/new-stuff-in-the-gdata-r-package/	December 30, 2008	Gregor Gorjanc		 0 Comments
RQuantLib 0.2.10	https://www.r-bloggers.com/2008/12/rquantlib-0-2-10/	December 29, 2008	Thinking inside the box		 0 Comments
Rcpp now in Debian	https://www.r-bloggers.com/2008/12/rcpp-now-in-debian/	December 25, 2008	Thinking inside the box		 0 Comments
High-Performance Computing with R	https://www.r-bloggers.com/2008/12/high-performance-computing-with-r/	December 25, 2008	Gregor Gorjanc		 0 Comments
Very flattering	https://www.r-bloggers.com/2008/12/very-flattering/	December 24, 2008	Thinking inside the box		 0 Comments
Statistics for Neuroscience (Neuroscience 9506b)	https://www.r-bloggers.com/2008/12/statistics-for-neuroscience-neuroscience-9506b/	December 23, 2008	Paul Gribble		 0 Comments
gdata gains trimSum function	https://www.r-bloggers.com/2008/12/gdata-gains-trimsum-function/	December 20, 2008	Gregor Gorjanc		 0 Comments
Rtools and Cygwin on MS Windows	https://www.r-bloggers.com/2008/12/rtools-and-cygwin-on-ms-windows/	December 20, 2008	Gregor Gorjanc		 0 Comments
Extra moments measure	https://www.r-bloggers.com/2008/12/extra-moments-measure/	December 16, 2008	Quantitative Finance Collector		 0 Comments
R matrices in C functions	https://www.r-bloggers.com/2008/12/r-matrices-in-c-functions/	December 13, 2008	Forester		 0 Comments
Memory limit management in R	https://www.r-bloggers.com/2008/12/memory-limit-management-in-r/	December 13, 2008	Gregor Gorjanc		 0 Comments
Functions for portfolio analysis	https://www.r-bloggers.com/2008/12/functions-for-portfolio-analysis/	December 11, 2008	Quantitative Finance Collector		 0 Comments
Convert Splus to R	https://www.r-bloggers.com/2008/12/convert-splus-to-r/	December 10, 2008	Quantitative Finance Collector		 0 Comments
Computational Finance with R	https://www.r-bloggers.com/2008/12/computational-finance-with-r/	December 5, 2008	Joshua Ulrich	"
 "	 0 Comments
Some of my other R-resources	https://www.r-bloggers.com/2008/12/some-of-my-other-r-resources/	December 5, 2008	:)-k		 0 Comments
